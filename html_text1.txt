



















Long-term traffic flow forecasting using a hybrid CNN-BiLSTM model - ScienceDirect

















      JavaScript is disabled on your browser.
      Please enable JavaScript to use all the features on this page.
      

Skip to main contentSkip to article
ScienceDirectJournals & BooksHelpSearchMy accountView PDFDownload full issueSearch ScienceDirectOutlineAbstractKeywords1. Introduction2. Related work3. Problem description, data collection and processing4. CNN-BiLSTM architecture5. Baseline models6. Experiments7. Conclusions and future workCRediT authorship contribution statementDeclaration of Competing InterestAcknowledgementsData availabilityReferencesShow full outlineCited by (66)Figures (11)Show 5 more figuresTables (6)Table 1Table 2Table 3Table 4Table 5Table 6Engineering Applications of Artificial IntelligenceVolume 121, May 2023, 106041Long-term traffic flow forecasting using a hybrid CNN-BiLSTM model☆Author links open overlay panelManuel Méndez, Mercedes G. Merayo, Manuel NúñezShow moreOutlineAdd to MendeleyShareCitehttps://doi.org/10.1016/j.engappai.2023.106041Get rights and contentUnder a Creative Commons licenseopen accessAbstractThe increase of road traffic in large cities during the last years has produced that long and short-term traffic flow forecasting is a critical need for the authorities. The availability of good traffic flow prediction methods is a must to make informed decisions concerning (punctual) traffic congestions. Previous work has shown that the accuracy of these methods decreases if we consider urban traffic and long-term predictions. In this paper we present a hybrid model, combining a Convolutional Neural Network and a Bidirectional Long–Short-Term Memory network, and apply it to long-term traffic flow prediction in urban routes. This model combines the capability of CNN to extract hidden valuable features from the input model and the capability of BiLSTM to understand the temporal context. In order to assess the usefulness of our model, we considered four streets of the city of Madrid with different characteristics and compared the results of our proposed model with the ones obtained by eight widely used baseline models. The results show that our hybrid model outperforms the baseline models with respect to three metrics commonly used in regression: mean absolute error, root mean squared error and accuracy.Previous article in issueNext article in issueKeywordsDeep learningHybrid modelsTraffic flow forecasting1. IntroductionUrban sprawl implies a high growth of the traffic flow in big cities. Because of this, traffic congestion is one of the main problems of most big cities. Traffic flow forecasting is an essential tool for traffic management, specially, in the most congested and frequented roadways. The high presence of signalisation in cities makes urban traffic flow more unpredictable and random than traffic flow in freeways. In most cases, temporal trends (monthly, daily or hourly) are insufficient to optimally model traffic flow. Other factors such as meteorological conditions, traffic flow in nearby roadways and type of day (working day, public holiday or weekend) are also essential to model traffic flow.Traffic flow forecasting can be classified in short-term and long-term. On the one hand, short-term models are able to predict traffic flow from a few minutes to one hour in advance, that is, in a very close future. On the other hand, long-term models are able to predict traffic flow from two hours to several days in advance. It seems reasonable that in short-term models, temporal trends have a very high influence in the traffic flow. However, in long-term models, the influence of temporal trends decreases and we have to look for other factors, such as the ones mentioned above, whose influence will be more significant.By reviewing the existing literature on traffic flow prediction, we noticed that the majority of the approaches consider predictions in the short-term. In these approaches, high accuracy is usually achieved only when predicting the values a few minutes in advance. Unfortunately, this is not a very useful information to manage the traffic flow because it will be too late to hint drivers about bad conditions. In order to provide advanced trip plannings to users, we think that it is more important to develop models that can make accurate predictions in longer terms.By focusing on the architectures used to forecast traffic flow in recent work, we can mention the frequent usage of Long–Short-Term Memory (LSTM) networks. These models can easily learn and understand the sequence information. However, they are adapted to deal with temporal correlations and they can only use the features given by the dataset. In order to improve its performance, some authors have suggested the hybridisation between LSTMs and Convolutional Neural Networks (CNNs). CNNs have the capability to filter out noise and extract new and valuables features from the input matrix. Thus, an LSTM will use the new information given by a CNN to identify temporal patterns and make the corresponding forecasts. Therefore, a hybrid CNN-LSTM model usually will outperform the accuracy of a single LSTM model.Bidirectional LSTM (BiLSTM) networks are an extension of LSTM networks in which two LSTMs are applied: one is applied to the input data (forward) and the other one is applied to the reverse of the input data (backward). Experimental evidence (Siami-Namini et al., 2019) shows that in tasks related to time series, BiLSTM models offer better predictions than LSTMs, although LSTMs reach the equilibrium faster than BiLSTMs. As the granularity increases, the temporal dependencies are fewer. Because of this, it is needed to find new relationships between variables that do not necessarily have to be temporal. In this line, the application of convolutional operations to the input matrix before the application of the BiLSTM model is a promising approach that we will explore in this paper.In this work we propose a novel hybrid CNN-BiLSTM architecture to forecast long-term traffic flow. The goal is to leverage the potential of a CNN block to extract complex characteristics from the input matrix and the capability of a BiLSTM block to find temporal dependencies between variables by understanding doubly (forward and backward) the context in each situation. In order to evaluate the performance of our CNN-BiLSTM model, we compare it with eight baseline models. In particular, we compare it with CNN-LSTM, in order to determine whether it is worth to use BiLSTM rather than LSTM, and with a BiLSTM, in order to determine whether it is worth to use a hybrid model or not.As a case study, we will apply our model and we will compare it with the baselines in four main roadways of Madrid. We take data from the official website of Madrid City Council, being data originally extracted from sensors that are part of a network of permanent traffic stations.11.1. ContributionsThe main contributions of the paper are summarised as follows:1.We developed a novel architecture that can be used in time series forecasting. This architecture is based on the hybridisation of a CNN and a BiLSTM neural network.2.We applied the new model to forecast the traffic flow in four stations of Madrid. The model showed a very good performance with respect to three metrics.3.A side result of our experiments was the creation of a historical database on traffic flow measured in four stations of Madrid and a procedure to built such a database in other stations. The database currently contains flow traffic information (of the considered station), temporal information, nearest stations information and context information.4.With the goal of optimising the model, we performed a careful fine-tuning of the considered hyperparameters.5.In order to show the usefulness of our model, we compared it with several powerful baseline models typically used in time series forecasting.6.The code of our proposed method and the datasets used in this paper are freely available at https://github.com/MMH1997/CNN-BiLSTM-network.1.2. OutlineThe rest of the paper is organised as follows. In Section 2, we briefly review related work. In Section 3 we present the main characteristics of the problem that we will confront and will describe the data used in it. In addition, we will explain the data collecting and data pre-processing phases. In Section 4, we describe the proposed hybrid model. In Section 5, we briefly present the main characteristics of each baseline model. In Section 6, we present our experiments and discuss the quality/accuracy of the proposed model, in particular, in comparison with baseline models. Finally, in Section 7, we give our conclusions and outline some directions for future work.2. Related workShort-term traffic flow forecasting is a recurrent case study in order to show the quality of forecasting models. In fact, many machine learning models have been developed to tackle this issue. We can classify them in machine learning models and deep learning models. Machine learning models such as auto-regressive models (ARIMA or similar) (Lin and Huang, 2021, Chen et al., 2011), K-nearest neighbours (KNN) (Hong et al., 2015), linear regression (Li, 2020) or Support Vector Regression (SVR) (Soon et al., 2019) are usually applied. In recent years, deep learning models such as, long–short term memory networks (LSTM) (Chu et al., 2021, Kang and Zhang, 2020, Poonia and Jain, 2020), multi layer perceptron networks (Shubhangi and Pratibha, 2021), bidirectional LSTM (BiLSTM) networks (Abduljabbar et al., 2021) and radial basis function networks (RBF) (Zhu et al., 2014, Abdi et al., 2010) have also been used.Work in the scope of long-term traffic flow forecasting is less common. First, statistical models are usually not used in recent work. We can mention the use of a two context-aware Random Forest model (Zarei et al., 2013) and the hybridisation of an ARIMA model and a support vector machine model (Wang et al., 2017). In contrast, artificial neural networks are commonly used. For example, a recurrent neural network (RNN) with a GPU-based implementation has been used to forecast long-term traffic flow in the cities of Odense and Beijing, obtaining a high accuracy (Belhadi et al., 2020). Some authors have used graph neural networks (GNN) to forecast traffic flow. Very recent work presents a spatial–temporal graph attention network that learns the dynamic graph structure and spatial–temporal dependency of the data (Kong et al., 2022). This model is evaluated in two public datasets collected in California. Also very recently, an overview of recent spatio-temporal GNN models for traffic forecasting has been presented (Bui Khac Hoai et al., 2022). However, recent approaches tend to use hybrid neural networks, combining at least two different models, and are able to obtain better accuracy. For example, the combination of an LSTM model and an encoder–decoder architecture using a hard attention mechanism (Wang et al., 2021) was developed and applied to data from Los Angeles. Following a similar line, a hybrid model which combines wavelet networks, LSTM and CNN has been used to forecast next day traffic flow in England Li et al. (2021) while a hybrid model, which combines a graph convolutional network and an LSTM, was proposed to forecast long-term traffic flow in New York (Peng et al., 2021). A CNN auto-encoder and a BiLSTM auto-encoder have been used to detect anomalies in traffic flow (Méndez et al., 2022a). Finally, it is worth to mention that multi-task learning has also been used in traffic flow forecasting (Zhang et al., 2021). The idea is to combine the data of different cities (in this case, Xi’an and Chengdu) to obtain a single model.Now, we will focus on models similar to our proposal. First, it is worth pointing out that BiLSTM models are not commonly used to work with time series, although recent studies show that BiLSTM reaches higher accuracy than LSTM due to their capability to use input data twice for training (Siami-Namini et al., 2019). However, in other topics, mostly related to natural language processing, BiLSTMs have been successfully applied. For example, in mention extraction tasks (Lin et al., 2019) or to write support systems (Makarenkov et al., 2019). Moreover, a single CNN very rarely has been employed in time series problems because its nature is not adequate for it. However, specially in recent years, it has been often combined with classical time series models to extract hidden information and, thus, increase the performance of the model. For example, a hybrid model composed by an LSTM, a CNN and an auto-encoder has been applied to forecast electricity consumption in Brazil (Rick and Berton, 2022) while the combination of a DNN and an LSTM has been used to detect phishing URLs (Ozcan et al., 2023). Similarly, models which combine CNNs and BiLSTMs are not frequent in the literature and most of them are tailored to solve problems related to natural language processing such as automatic speech recognition (Passricha and Aggarwal, 2020), text classification (Li et al., 2018), sentiment analysis (Abu Kwaik et al., 2019, Rhanoui et al., 2019) and fake news detection (Balwant, 2019). Other environments where these hybrid models have been recently used are video summarising (Hussain et al., 2020), DNA-protein binding predicting (Zhang et al., 2020) and remaining useful life estimation (Song et al., 2021).An interesting line of work, where hybrid models are also present, takes genetic programming as base model. These approaches have been use to forecast the stock market (Ari and Alagoz, 2023) and electricity power generation (Peiris et al., 2022). In addition, hybrid genetic programming and neural network models have been used to predict wind power (Zameer et al., 2017), forecast pollutants concentration (Ari and Alagoz, 2022) and suspended sediment concentration in rivers (Li et al., 2022).To the best of our knowledge, a CNN-BiLSTM hybrid model has not been used to deal neither with time series nor with traffic flow forecasting.3. Problem description, data collection and processingIn this section we briefly describe the problem that we confront and the data that we will use. In particular, we will sketch the process to transform data into the format used by our model.3.1. Problem descriptionOur problem is to forecast long-term traffic flow in major roadways of big cities. Specifically, we will forecast the flow 12, 24, 48 and 72 h in advance. We will use data of the four last available timesteps. Let h be a given timestep and h+k be the timestep in which we desire to make the forecasting: we will use nine predictor variables, corresponding to timesteps ranging from h−3 to h, to forecast the target variable in the h+k timestep. In order to provide a solution to this problem, our final goal is to find a model which explains, with an error as small as possible, the behaviour of the target variable over time. Let x1,…,x9 be the nine predictor variables (we will describe them in the next section), y be the target variable (we will also describe it in the next section) and ϵ∈R+ be an error. Our model can be expressed by the following equation: (1)y(h+k)=fx1(h−3),…,x1(h),x2(h−3),…,x2(h),…x9(h−3),…,x9(h)+ϵ3.2. Data description In this section we will identify the predictor variables and the target variable. We will also indicate the sources of the data that we used for our experiments.Table 1. Target stations and their auxiliary stations.Target station: location (direction)Aux. station 1: location (direction)Aux. station 2: location (direction)P/Castellana (N-S)C/José Abascal (W-E)A/ General Perón (E-W)C/Arturo Soria (N-S)C/Alcalá (W-E)C/Costa Rica (W-E)P/Sta María (S-N)P/del Prado (N-S)P/Infanta Isabel (E-W)C/Gran Vía (W-E)C/Hortaleza (S-N)C/Alcalá (E-W)Download: Download high-res image (715KB)Download: Download full-size imageFig. 1. Location of the target stations in a map of Madrid.The target variable is the hourly number of vehicles passing through a traffic station in a certain direction. In order to perform experiments that could validate the usefulness of our model, we selected four traffic stations in the city of Madrid, targeting streets with big traffic flows but having different characteristics. Three of them are located in core city areas and one of them is a main entry/exit roadway of the city. The stations are:•Paseo de la Castellana (P/Castellana) taking data in the North–South direction. This is a major street that crosses Madrid from downtown to North.•Calle Arturo Soria (C/Arturo Soria) taking data in the North–South direction. This is a long street that crosses Madrid from North to South in the East part of the city.•Paseo de Santa María de la Cabeza (P/Sta María) taking data in the South–North direction. This is an important communication channel from downtown to the South of the city.•Calle Gran Vía (C/Gran Vía) taking data in the West–East direction. This is a shopping and touristic centrally located street.Fig. 1 shows the distribution of the four target stations in a map of Madrid.The first group of predictor variables are: type of day (public holiday, working day or weekend), daily average temperature, daily minimum temperature, daily maximum temperature and daily rain. In addition, we also consider as predictor variables the hourly amount of vehicles in the target station and in two auxiliary stations (two different for each target station). We selected the auxiliary stations by taking into account the traffic intensity (we prefer stations in roadways with high traffic intensity) and the proximity to the target station. Table 1 shows the corresponding auxiliary stations for each target station. Finally, we also use the hour in the timestep as a predictor variable. Therefore, we have nine predictor variables. For each prediction, we collect input data from four different hours: for each prediction we use 36 predictor variables. Fig. 2 shows an extract of the initial data-set (before pre-processing) where we can see, for each hour value, the values corresponding to the predictor and target variables.For our experiments, data have been extracted from the websites of different institutions. Meteorological data have been obtained from AEMET2 and correspond to measures in El Retiro meteorological station, located in the centre of Madrid, while type of day and traffic data have been extracted from the Madrid City Hall website.3Download: Download high-res image (158KB)Download: Download full-size imageFig. 2. Extract of initial data (Arturo Soria station).3.3. Data analysisNext, we will make a brief descriptive analysis of the hourly traffic flow in the target stations that we will consider in our experiments. In Fig. 3 we observe similar ranges in three of the stations. The range of the fourth one, P/Castellana, is wider. Moreover, we appreciate a very small number of atypical values which will be considered actual values and not caused by a human error, as we will discuss in the next section. In Fig. 4 we can observe that the hourly evolution of the traffic flow follows a similar pattern in all the stations (each of them in its corresponding range). Summarising, this evolution shows an increase on the number of vehicles from 5 a.m. until approximately 9/10 a.m. Then, there is a valley with small oscillations until approximately 6/7 p.m. At this time, a big decrease in the traffic flow begins until 5 a.m. of the next day. We note that the variation between day and night in C/Gran Vía is smaller than the one happening in the rest of stations (in relative terms). This implies that traffic flow in C/Gran Vía is significantly less dependent on temporal variables compared to the rest of stations. Download: Download high-res image (97KB)Download: Download full-size imageFig. 3. Target data in each station (box-plot).Download: Download high-res image (184KB)Download: Download full-size imageFig. 4. Mean of vehicles by hour in each target station.3.4. Data pre-processingData pre-processing has not been an easy task. In this section, we will describe step by step the whole process performed so that users making use of our work can try and mimic these steps to forecast traffic flow in other cities.•In the first step, we downloaded the different data-sets from the corresponding websites. Once all data-sets were downloaded, we unified the format of the date column (the junction column) in all data-sets. Then, we transformed daily data into hourly data and joined all data-sets.•In the second step, we added non available (NA) data by using the mean between previous and next values (all NA data are numerical). We also categorised data which required it and scaled all data between 0 and 1.•Next, we normalised all the variables (predictor variables and target variable) between 0 and 1 by using the well-known MinMaxScaler method. This step was applied in order to transform all the numerical variables to the same scale.•Subsequently, we transformed data from a sequence to pairs of input (predictor variables) and output (target variable), that is, we transformed a time series problem into a supervised machine learning problem.•Finally, we reshaped data from a flatten vector to a matrix with nine columns (one by predictor variable) and four rows (one by timestep). The reshape is needed because the proposed model requires data in matrix form as input.It should be mentioned that we did not remove outliers. We manually analysed outlier data and concluded that, in all cases, they corresponded to a notorious event happening in Madrid and they were not due to a human error.4. CNN-BiLSTM architectureIn this section, we will describe the proposed model: a hybrid CNN-BiLSTM. First, this architecture is built to leverage CNN advantages for filtering and removing noise data, obtaining substantial internal valuable information of the time series. The removal of the noise data is performed by applying dimension reduction. Therefore, non-relevant information, that is, the noise, is not included in the reduced matrix. The hidden internal valuable information is obtained by applying the convolutional operation: the filters or kernel matrix pass through the input matrix, highlighting different characteristics depending on the kernel and, thus, obtaining hidden information. Second, this architecture is built to leverage the capability of a BiLSTM network to model and forecast both in short-term and long-term cases of temporal data sequences. The power of BiLSTM models lies in the training process. In this model, input data is trained twice, from left to right and from right to left, in parallel. Both context interpretations are combined in the output, providing more comprehensive information of the data context in this model than, for example, in a unidirectional LSTM model. We might say that the CNN acts as an encoder layer mechanism that learns new features from the input data sequences, while the BiLSTM acts as a decoder that learns the temporal relations in data stream and outputs the traffic flow prediction.The proposed model has two differentiated blocks. The CNN block has a CNN layer, a pooling layer and a flatten operator. The CNN block output will be used as input by the BiLSTM block. This second block includes a BiLSTM layer, a dropout layer and a dense layer. Since convolutional and pooling layers were originally developed to extract features from an image, they only work with matrix data. Therefore, we will need to format input data to be in matrix form as we explained in Section 3.4. Next, we will describe in detail the CNN and BiLSTM blocks.4.1. CNN blockAs usual, convolutional and pooling layers aim to filter incoming data to extract important information from the matrix. Convolutional operations are performed by the convolutional layer between input data and smaller matrices called kernels or filters. Usually, several filters are considered and they can be seen as windows with a given height and width that slide a certain number of cells, usually called stride, across the input matrix by applying the convolution operation on each sub-region of the matrix that it intersects. This process obtains a convoluted matrix, which represents a singular feature of the original input matrix. Obviously, the application of several filters will carry out that more different convoluted features will be obtained. A model that uses several convoluted features will more accurately represent the input matrix than a model that only uses the original input matrix. Therefore, the performance of a given model will be improved by using more filters. The convolutional operation can be formally expressed as follows. Let H be the input matrix, I be the kernel matrix and m and n be the indexes of rows and columns of the result matrix (R). We have (2)R[m,n]=(H⋅I)[m,n]=∑j∑kI[j,k]⋅H[m−j,n−k]The convolutional layer uses the ReLU activation function. This is the most widely used activation function applied in CNN models. Its main advantage is that it does not activate every neuron at the same time because it transforms all negative values into 0. Due to this, ReLU has a big computational efficiency. Actually, ReLU is six times faster to train than other activation functions such as tanh (Chou et al., 2017). ReLU is defined as follows: (3)f(x)=x+=max(0,x)The convolutional layer is followed by a max pooling layer. A pooling layer is a sub-sampling method that aims to reduce the dimension of the convolutional matrix by removing values but keeping the most important features detected by each filter. There are several types of pooling layers, but the most widely used, being the one used in our model, is the max pooling layer. Similarly to the previous description of filter, we use a window to traverse the matrix. We take the values of each patch of the matrix as input and return the maximum of each patch. This layer produces new matrices that can be considered as summarised versions of the convolutional matrices. The pooling operation also induces an increase of the model robustness. That is, small modifications in the input will not affect the output. Finally, the pooling layer is followed by a flatten operator, which unrolls the values at the last dimension in order to reshape the next layer input.4.2. BiLSTM blockThe second component of the model is a BiLSTM block composed by a BiLSTM network, a dropout layer and a dense layer.In order to understand the performance and architecture of a BiLSTM network, we will first describe an (unidirectional) LSTM network. An LSTM network is a type of recurrent neural network (RNN). Traditional RNNs do not have a previously defined structure of layers. They use cyclic connections in their hidden layers that provide short-term memory to the model and, therefore, the capability of working with sequence data (as time series). However, traditional RNNs have the issue known as long-term dependencies problem: a gradual forgetfulness of past information as the number of neurons increases. LSTMs solve this problem by saving relevant information through all LSTM units in a kind of conveyor belt known as memory cell.Each LSTM unit consists of a memory cell and three gates that regulate the information flow by deciding which information will be forgotten and which one will remain in the model. This is the key to learn long-term dependencies by the LSTM and solve the previously mentioned issue. These three gates are called forget gate, input gate and output gate. Next, we briefly describe their features.In the forget gate, a sigmoid function is applied to the information included in the current input (Xt) and the previous hidden state (h−1). This function, denoted by ft, returns a value between 0 and 1 that indicates the percentage of the information that will be kept.The input gate takes the information from the current input and from the previous hidden state and passes them through a second sigmoid function, transforming this information into values between 0 and 1. The same information also passes through a tanh function, which helps to regulate the network, returning a value between −1 and 1. Then, the sigmoid output (it) is multiplied by the tanh output (C̃t) to decide which information is important to keep.Now, we have enough information to compute the cell state. First, the previous cell state (Ct−1) is multiplied by the forget output. Then, the output of the input gate is added, updating the cell state with new values considered relevant by the network. The result of these two operations is the new cell state (Ct).Finally, we have the output gate. It decides the value of the next hidden state. First, the information from the current input and from the previous hidden state passes through a third sigmoid function. Then, the new cell state passes through a tanh function. Both of these outputs are multiplied point-by-point. The output of the output gate (ot) will be the new hidden state (ht). Therefore, the new cell state and the new hidden state will be carried to the next timestep.Formally, the operations that happen in an LSTM cell are given by the following expressions: (4)ft=σ(Wf⋅[ht−1,xt]+bf)it=σ(Wi⋅[ht−1,xt]+bi)C̃t=tanh(Wc⋅[ht−1,xt]+bC)Ct=ft⋅Ct−1+it⋅C̃tot=σ(Wo[ht−1,xt]+bo)ht=ot⋅tanh(Ct)where σ represents a sigmoid function, tanh represents a hyperbolic tangent function, xt represents the input data in time t, ht represents the hidden state in time t, each Wx represents a weight matrix and each bx represents a bias vector.Once we have explained the behaviour of an LSTM network, we can describe how BiLSTM networks work. They are an evolution of LSTM networks based on Bidirectional RNNs, an extension of RNNs. A BiLSTM network contains a forward LSTM network and a backward LSTM network. The forward LSTM uses as input a sequence of values ranging from t−k to t while the backward LSTM uses as input a sequence ranging from t to t−k. The outputs of the forward (h⃗) and backward (h⃖) networks are computed by using the previously described mechanism of a single LSTM. The BiLSTM layer outputs a vector (Yt) that is obtained by using the following equation: (5)Yt=σ(ht⃗,ht⃖)In this case, a sigmoid function unifies the outputs of the single LSTM networks.Note that BiLSTMs use the so-called dropout technique in order to regulate over-fitting. The dropout rate is indeed a hyperparameter to consider in the definition of the model.Finally, we stack a fully-connected layer that returns the prediction output.4.3. Tuning parametersFirst, we set in advance the number of epochs (100) and the batch size (32). Then, we applied an exhaustive search algorithm in order to fine-tune the hyperparameters of the model. The ranges of the possible values of hyperparameters were selected by ensuring to use a reasonable computational time in the execution of the model (at most, 70 seconds per epochs). The obtained hyperparameters were as follows.The convolutional layer contains 256 filters. Our experiments showed that having a higher number of filters achieved a very modest decrease of the error while considerably increasing the computation time. Besides, using a lower number of filters considerably increased the error. As kernel, we use a 2 × 2 matrix. Although this kernel dimension might look too small, we must take into account that the input matrix has also a relatively small dimension (9 × 4). By using a higher dimension kernel, a small number of hidden features would be extracted by the convolutional operation. In the max pooling layer, we use a 2 × 2 window with a stride of 2 to apply the max pooling operation. Again, we use a small window dimension because of the small dimension of the input matrix. We fixed the stride value after several experiments revealing that for higher values, the dimension reduction implies a significant information loss. This pooling operation is made by using a same padding technique, which means that output and input sizes would be the same with stride equal to 1. Therefore, it requires the filter window to slip outside the input matrix. For this, it is necessary to add either a row or a column of zeros. Our experiments showed that the usual no padding technique, which means that only the input matrix will be covered by the filter window with the given stride, obtained slightly worse results than same padding.Focusing now on the BiLSTM block, the BiLSTM layer contains 500 BiLSTM units (each of them is composed by a forward LSTM unit and a backward LSTM unit). We considered 500 BiLSTM units because this number provided a good trade-off between observed error and computational time. The initial weights of the BiLSTM layer were set from a random normal distribution. We also made preliminary experiments with other distributions such as random uniform, truncated normal, zeros distribution and ones distribution. After that, we conclude that random normal gave the best results. Then, we added a dropout layer with a rate of 0.5. As the dropout rate did not seem to have influence in the final results, we decided to use the usually considered best dropout rate value for hidden layers (Baldi and Sadowski, 2013). The fully connected layer has only 1 output neuron with a linear activation function because we predict only one value.We used the Adam optimiser as optimisation algorithm for stochastic gradient descent on the network by using as loss function the mean absolute error.Finally, we would like to explain the order applied to the 36 predictor variables (nine variables by each of the four considered timesteps). It is natural to consider a 9 × 4 input matrix because the relationship between variables with a difference of one timestep will be bigger than the relationship between variables with a bigger difference. For the same reason, we considered natural that the rows should be in ascending time order. However, there is not a by default order for the nine columns corresponding to the predictor variables. We considered a grouping by affinity (see Fig. 2) but it should be analysed whether this order makes sense. In order to show that we are choosing the best order, it would be necessary to produce the 9!=362.880 different combinations and perform the experiments for each of these combinations. Even if this would be feasible, this approach does not scale with the number of predictor variables. Therefore, we decided to perform a different experiment with the goal of checking whether the specific order between columns have a relevant impact. We produced eight random combinations, took one traffic station and one prediction and compared the training curves to see whether there were significant differences between them. As can be seen in Fig. 5, the curves of all the groupings are quite similar, so that we have some evidence to claim that the order of the predictor variables does not significantly affect either the training process or the final loss.Download: Download high-res image (112KB)Download: Download full-size imageFig. 5. Loss function value (y axis) during the epochs (x axis) of the training process, using eight different combinations of the same predictor variables (Arturo Soria, 12 h in advance).We have used Netron (Netron, 2022) to provide a graphical representation of our hybrid model, including the values of the relevant hyperparameters (see Fig. 6). The interested reader is referred to https://github.com/MMH1997/CNN-BiLSTM-network for the complete code of the model.Download: Download high-res image (255KB)Download: Download full-size imageFig. 6. Our proposed hybrid model.5. Baseline modelsIn this section we will briefly describe the baseline models whose performance will be compared with the one of the proposed model. We will specify the hyperparameter values for each baseline model.5.1. Random forest (RF) regressorThe RF Regressor algorithm is a statistical machine learning model whose prediction is based on the output of combined regression trees. These regression trees are trained by randomly sampled subsets of the original training set. The final output value of the RF model will be the mean of the regression trees outputs. The use of several regression trees considerably improves the use of a single one. Specifically, it avoids original handicaps of regression trees such as a high susceptibility to a brief data modification and over-fitting.We will use the following hyperparameters in this algorithm: 100 regression trees, a maximum depth of 10 for each tree and a minimum number of samples required in a node before splitting it of 20. These values were set after performing preliminary experiments. These experiments showed that for more than 100 regression trees, the mean absolute error converges to a similar value. Similarly, maximum depth and minimum number of samples required in a node have been selected after a fine-tuning process.5.2. Linear regression (LR)LR is a statistical machine learning algorithm used to find a linear relationship between a dependent or target variable and several independent or predictor variables. In our case, we do not use any variant such as Ridge or Lasso.5.3. k-Nearest neighbours algorithm (KNN)KNN is a non-parametric and statistical algorithm that bases its output on the mean of the target variable of the k most similar observations. KNN does not have a training phase, that is, it does not learn a specific model. It just makes predictions computing the similarity between observations.After a fine-tuning process, we decided to use the following hyperparameters: 5 as k value, euclidean metric as similarity distance and the weight of each neighbour will be inversely proportional to its distance to the target observation.5.4. Simple recurrent neural networks (SRNN)SRNNs were the first recurrent network. They carry out a recurrent connection between the output of the previous and current inputs. This process develops a temporal feedback in the network. SRNNs were the prelude of the more powerful and accurate LSTM networks and derivatives.In our experiments, after a fine-tuning process, we used an SRNN with 500 units, followed by a dense layer with 1 neuron. In this case, we train the model using a Stochastic Gradient Descent as optimiser, with a learning rate of 0.1.5.5. Convolutional neural network (CNN)CNN is commonly used to work with images and matrices. Its architecture is described in Section 4. In this case, we use the matrix 9 × 4 as input to obtain the corresponding target variable as output. We use a 2D convolutional layer with 256 filters, a kernel size of 2 × 2 and a relu as activation function. This is followed by a MaxPooling layer with a pool size 2 × 2. Then, we add a flatten layer and finally a dense layer with 1 neuron and a linear function as activation.5.6. LSTM networkLSTM network is the most widely used deep learning model in regression and time series tasks. Its architecture has been described in Section 4. In our experiments, we use a unidirectional LSTM layer with 500 units followed by a dropout layer with a rate of 0.5 and by a dense layer with 1 neuron. We use these hyperparameters in order to obtain a model with a similar complexity to our proposed model.5.7. BiLSTM networkBiLSTM is a decisive baseline model. It will show us whether it is worth to add the CNN block to the proposed model. Its architecture is the same as the one described in Section 4, but without including the CNN block. As happens with the previously described baseline, we will use similar hyperparameters to the proposal with the same purpose. The BiLSTM contains 500 BiLSTM units followed by a dropout layer with a rate of 0.5 and by a dense layer with 1 neuron.5.8. CNN-LSTM networkThis is also a decisive baseline model. It will allow us to determine whether it is worth to use a BiLSTM rather than an LSTM in the proposed model. Its architecture is similar to the one described in Section 4. The only difference is that we will use here a unidirectional LSTM network rather than a BiLSTM network. The hyperparameters in the CNN block will be exactly the same as in the proposed model. The LSTM block contains 500 LSTM units followed by a dropout layer with a rate of 0.5 and by a dense layer with 1 neuron.5.9. Discarded alternative baseline modelsIn addition to the considered baseline models, we analysed other possibilities such as multi-layer perceptron neural network (MLP). However, this type of neural networks does not have a temporal component. For example, in order to use MLP it would be necessary a data flattened, which would completely eliminate the temporal dependencies, so that data can be taken by the model as a single vector.6. ExperimentsIn this section, we evaluate the proposed model in the four data-sets corresponding to each target station. We also compare it with the eight baseline models previously described. For each experiment, we use 2/3 of data to train and 1/3 of data to test. We randomly chose the training and testing sets. In order to evaluate the performance of the models, we considered three metrics.•Mean absolute error (MAE). This metric is one of the most used in regression to evaluate the quality of a model predictions. Its definition is: (6)MAE=1n∑i=1n|yˆi−yi|being n the total number of evaluated observations, yˆi the predicted value of the ith observation and yi the actual value of the ith observation.•Accuracy (Acc). This is a classification metric that can be very useful to implement measures by the authorities. For each station, we will classify the values (actual and predicted) in three categories (low volume of traffic, medium volume of traffic and high volume of traffic). Let Pk be the kth percentile of the target variable. The first category includes the observations which are between P0 and P15, the second category includes the observations which are between P15 and P85 and the third category includes the observations which are between P85 and P100. Acc is defined as the ratio between the number of well-classified observations and the total number of observations.•Root mean square error (RMSE). This metric is also widely used in regression. We considered it just in case that it could reveal results different from the ones corresponding to MAE. Its definition is: (7)RMSE=1n∑i=1n(yˆi−yi)2being n the total number of evaluated observations, yˆi the predicted value of the ith observation and yi the actual value of the ith observation.It should be mentioned that we considered another typical metric in regression: mean absolute percentage error (MAPE). However, MAPE has some disadvantages (Goodwin and Lawton, 1999) that hinder its usefulness. For example, when the actual value is close to 0, it yields extremely large percentage errors. In case that actual value is 0 and the predicted value is not 0, it produces undefined errors. We have this situation in our data, particularly, early morning data in working days, when the hourly traffic flow can be close to 0 or even 0. Therefore, we discarded it.Concerning some of the models that we use, note that neural networks and RF models present a random component. Because of this, we evaluated each of these models five times in each station, using different seeds, in order to give consistency to our experiments. Then, the average of the five evaluations by model and station was considered.4 The rest of models (KNN and LR) do not have a random component and, therefore, only one evaluation in each station was performed.We will show and comment the results of the experiments in each target station and from a broad perspective. In the discussion of the results, we will emphasise more the values corresponding to MAE. The rationale is that models are trained to optimise MAE and that our observations show that results corresponding to the three metrics are quite similar.Finally, one remark about the presentation of the results corresponding to the experiments. MAE values obtained by the LR and CNN models are much higher than the ones corresponding to the rest of the models. Therefore, we have decided to remove the LR and CNN models in the graphs because, using a linear scale, it would condition the plot ranges and, therefore, the graphical displays. Nevertheless, these values can be found in the different tables presenting the numeric results.6.1. P/Castellana station resultsIn this station, as we can see in Fig. 7 and Table 2, the best performance is obtained in all time granularities by the proposed model except when predicting results 12 h in advance. The MAE of the proposed model is 1.2% higher than the MAE of BiLSTM in the 12 h granularity. However, the MAE of the proposed model is 3.6% smaller than the MAE of the second best model (CNN-LSTM) in 24 h granularity; 3.2% smaller than the MAE of the second best model (BiLSTM) in 48 h granularity and 1.8% smaller than the MAE of the second best model (BiLSTM) in 72 h granularity. By taking into account the average of all granularities, the MAE of the proposed model is 2.2% smaller than the MAE of the second best model (BiLSTM). Acc behaves similar to MAE: the proposed model obtains the best performance in all granularities except when working 12 h in advance (in this case, BiLSTM obtains the best performance). Considering the RMSE, in all granularities our proposal obtains the best results except when working 24 h in advance. By taking into account the average of all granularities, the RMSE of the proposed model is 2.3% smaller than the RMSE of the second best model (RF). Interestingly enough, in this station, all the models show better results, both for MAE and for Acc (not for RMSE), in 24 h granularity than in 12 h granularity. This phenomenon appears sometimes in some models applied to other stations.Download: Download high-res image (549KB)Download: Download full-size imageFig. 7. Line charts of MAE (top), Acc (centre) and RMSE (bottom) by model and granularity (P/Castellana).Table 2. MAE (top), Acc (centre) and RMSE (bottom) average values by granularity and model (P/Castellana).GranularityMeanMinEmpty Cell12 h24 h48 h72 h12 h24 h48 h72 hRF176.23153.44183.29196.42175.44153.37182.56196.01LR395.33228.38300.63319.03395.33228.38300.63319.03KNN179.99167.45197.78209.18179.99167.45197.78209.18SRNN165,44161.61213.52208.77161.04153.41203.75205.14CNN353.44269.79326.07338.02350.68262.14320.86337.11LSTM157.61153.67187.74197.81152.52151.86182.31192.67CNN-LSTM151.57141.55178.63185.65146.49135.00174.62182.83BiLSTM142.85142.75174.55187.32141.09139.86171.60182.89Proposed model144.22136.51168.92183.95138.62135.09166.83180.38GranularityMeanMaxEmpty Cell12 h24 h48 h72 h12 h24 h48 h72 hRF0.790.8290.7920.7780.7920.8310.7920.779LR0.7070.750.720.7120.7070.750.720.712KNN0.8030.8130.7890.7680.8030.8130.7890.768SRNN0.8070.8130.7820.7730.8170.8270.7860.777CNN0.6870.7430.720.7110.6920.7470.7240.714LSTM0.8260.840.8050.7940.8340.8460.8140.798CNN-LSTM0.8340.8460.8080.8070.8380.8520.8170.811BiLSTM0.840.8470.8170.8070.8460.8530.8240.816Proposed model0.8390.8510.8220.8080.8480.8560.8230.812GranularityMeanMinEmpty Cell12 h24 h48 h72 h12 h24 h48 h72 hRF253.95233.04264.73293.57253.02232.82263.91292.9LR475.13321.28393.11408.41475.13321.28393.11408.41KNN269.28256.02291304.78269.28256.02291304.78SRNN263.98264.76322.02317.52258.03253.57307.1306.58CNN458.45354.49429.96443.1455.09350.72423.65438.9LSTM257.14256.76294.7320.66249.4253.79289.78311.89CNN-LSTM249.96244.93287.29299.11238.66235.92281.15295.82BiLSTM230.42243.28283.49300.93227.97239.9280.84291.76Proposed model224.46241.47277.24291.11219.37226.54273.742856.2. C/Arturo Soria station resultsIn this station, as we can see in Fig. 8 and Table 3, the best results are obtained by the proposed model in all granularities. The MAE of the proposed model is 2.5% smaller than the MAE of the second best model (BiLSTM) in 12 h granularity; the MAE of the proposed model is 13.5% smaller than the MAE of the second best model (BiLSTM) in 24 h granularity; the MAE of the proposed model is 5.6% smaller than the MAE of the second best model (BiLSTM) in 48 h granularity and, finally, 7.7% smaller than the MAE of the second best model (BiLSTM) in 72 h granularity. On average, the MAE of the proposed model is 7.4% smaller than the MAE of the second best model (BiLSTM). The Acc and RMSE of the proposed model also obtain the best performance in all granularities.Download: Download high-res image (538KB)Download: Download full-size imageFig. 8. Line charts of MAE (top), Acc (centre) and RMSE (bottom) by model and granularity (C/Arturo Soria).Table 3. MAE (top), Acc (centre) and RMSE (bottom) average values by granularity and model (C/Arturo Soria).GranularityMeanMinEmpty Cell12 h24 h48 h72 h12 h24 h48 h72 hRF111.4599.00125.45129.76111.2798.93124.76129.60LR242.80150.72198.54204.11242.80150.72198.54204.11KNN107.85108.59129.25134.47107.85108.59129.25134.47SRNN88.3693.74134.43128.0485.1096.20126.72124.10CNN174.79163.33195.42201.25170.30158.47192.90196.61LSTM95.36106.03123.90131.4293.07103.72121.46127.74CNN-LSTM100.02108.57128.73135.0398.35104.60127.65128.79BiLSTM84.1398.68117.97128.1481.8197.08116.60126.44Proposed model82.0685.41111.43118.2179.8784.02108.32117.4GranularityMeanMaxEmpty Cell12 h24 h48 h72 h12 h24 h48 h72 hRF0.8350.8420.8020.7950.8330.840.8010.794LR0.7150.750.7180.7150.7150.750.7180.715KNN0.8150.8160.7920.7840.8150.8160.7920.784SRNN0.8370.8290.7870.80.8440.8340.7970.815CNN0.7140.7240.7370.7330.7220.730.7410.737LSTM0.8280.8160.7980.7930.8040.8070.7960.788CNN-LSTM0.820.8140.7870.790.8170.810.7820.781BiLSTM0.8370.8270.810.8020.8310.8220.8040.793Proposed model0.8530.8520.8250.820.8480.8470.8150.814GranularityMeanMinEmpty Cell12 h24 h48 h72 h12 h24 h48 h72 hRF168.31161.36192.76196.78168.22161.18191.75196.38LR299.36218.53264.21266.91299.36218.53264.21266.91KNN174.38177.06201.65206.95174.38177.06201.65206.95SRNN165.50179.94219.09210.16162.25176.24215.93203.41CNN253.35231.67271.94278.38247.84229.15268.80271.25LSTM154.90172.33193.57200.00152.19169.43192.55196.23CNN-LSTM152.70174.14196.78203.02150.33168.60195.82198.03BiLSTM137.93162.37185.80195.51135.30161.00183.42194.46Proposed model137.05158.12181.02190.34133.08157.44176.24188.046.3. P/Sta María station resultsIn this station, as we can see in Fig. 9 and Table 4, the proposed model obtains again the best performance in all granularities. The MAE of the proposed model is 2.8% smaller than the MAE of the second best model (BiLSTM) in 12 h granularity; the MAE of the proposed model is 5.3% smaller than the MAE of the second best model (RF) in 24 h granularity; the MAE of the proposed model is 2.4% smaller than the MAE of the second best model (BiLSTM) in 48 h granularity and 1.7% smaller than the MAE of the second best model (BiLSTM) in 72 h granularity. On average, the MAE of the proposed model is 3.6% smaller than the MAE of the second best model (BiLSTM). The Acc of the proposed model also obtains the best performance in all granularities. However, the RMSE of the proposed model obtains the best performance in all cases except in 24 h in advance. On average, the RMSE of the proposed model is 2.2% smaller than the RMSE of the second best model (BiLSTM).Download: Download high-res image (529KB)Download: Download full-size imageFig. 9. Line charts of MAE (top), Acc (centre) and RMSE (bottom) by model and granularity (P/Sta María).Table 4. MAE (top), Acc (centre) and RMSE (bottom) average values by granularity and model (P/Sta María).GranularityMeanMinEmpty Cell12 h24 h48 h72 h12 h24 h48 h72 hRF95.44100.46118.01122.1895.20100.19117.72122.00LR212.83138.61175.71185.15212.83138.61175.71185.15KNN124.31107.45120.12127.39124.31107.45120.12127.39SRNN98.32103.50135.83124.1495.0299.12124.72122.70CNN165.34157.29182.85189.79129.64155.41181.70185.53LSTM96.05107.29120.77122.4494.75104.43117.60121.69CNN-LSTM99.49103.65118.86122.4495.48103.21116.54121.14BiLSTM89.75104.11112.97119.1788.13101.64110.97117.83Proposed model87.3096.12110.25117.2085.0593.68105.76113.49GranularityMeanMaxEmpty Cell12 h24 h48 h72 h12 h24 h48 h72 hRF0.8280.8360.7890.7910.830.8380.7890.792LR0.70.7360.710.7060.70.7360.710.706KNN0.7830.820.7950.7810.7830.820.7950.781SRNN0.820.8190.770.790.8290.8230.7930.795CNN0.7070.7290.7250.7210.7130.730.7270.723LSTM0.8280.8160.7910.7880.8370.8210.7960.792CNN-LSTM0.8270.830.8050.7970.8390.8360.8090.8BiLSTM0.8380.8190.8110.80.8460.830.8170.806Proposed model0.8470.8390.8210.8110.8510.8460.8270.818GranularityMeanMinEmpty Cell12 h24 h48 h72 h12 h24 h48 h72 hRF138.84148.12165.65170.90138.49147.91165.23170.66LR250.55189.85223.74235.18250.55189.85223.74235.18KNN174.61160.42170.67267.84174.61160.42170.67267.84SRNN154.09160.69200.05186.55149.65155.05184182.92CNN233.27210.59241.12248.36229.8210.13237.39242.29LSTM141.19155.72169.13169.71136.77153.11167.2168.99CNN-LSTM152.16161.58176.07179.77147.48160.58173.77178.64BiLSTM131.13151.53162.66167.73130.04148.91160.07166.2Proposed model126.78151.36162.44159.12124.23147.42159.34155.026.4. C/Gran Vía station resultsFinally, the MAE results corresponding to C/Gran Vía station show again that our proposed model outperforms all the baseline models for all granularities (see Fig. 10 and Table 5). The MAE of the proposed model is 7.1% smaller than the MAE of the second best model (BiLSTM) in 12 h granularity; the MAE of the proposed model is 4.3% smaller than the MAE of the second best model (RF) in 24 h granularity; the MAE of the proposed model is 7.3% smaller than the MAE of the second best model (RF) in 48 h granularity and 3.1% smaller than the MAE of the second best model (BiLSTM) in 72 h granularity. On average, the MAE of the proposed model is a 6.7% smaller than the MAE of the second best model (BiLSTM). In this station, the Acc of the proposed model is the best in all granularities except in 12 h in advance. In this case, BiLSTM is slightly better than the proposed model. However, the RMSE of the proposed model obtains the best performance in only one granularity (48 h in advance). On the average, the RMSE of the proposed model is 0.8% higher than the obtained in the best model (BiLSTM).Download: Download high-res image (531KB)Download: Download full-size imageFig. 10. Line charts of MAE (top), Acc (centre) and RMSE (bottom) by model and granularity (C/Gran Vía).We assume that anomalies seen in these results can be explained by considering the particularities of the C/Gran Via data mentioned in data analysis Section 3.3.Table 5. MAE (top), Acc (centre) and RMSE (bottom) average values by granularity and model (C/Gran Vía).GranularityMeanMinEmpty Cell12 h24 h48 h72 h12 h24 h48 h72 hRF153.26154.36175.40173.94153.02154.06175.08173.86LR259.88196.55243.91251.27259.88196.55243.91251.27KNN154.19163.22176.77182.84154.19163.22176.77182.84SRNN146.56151.16197.96189.87145.33149.03187.62186.94CNN251.39210.4243.94254.32241.22206.02242.07248.64LSTM151.75164.61185.41186.13150.02163.19181.75182.19CNN-LSTM148.40156.94175.86179.07146.42156.31174.44176.53BiLSTM143.43161.29175.69175.20140.86156.36171.38172.29Proposed model133.22147.7162.66168.61131.38143.91159.29164.86GranularityMeanMaxEmpty Cell12 h24 h48 h72 h12 h24 h48 h72 hRF0.7740.7820.7640.7560.7750.7830.7650.757LR0.70.730.7050.6980.70.730.7050.698KNN0.7740.7740.7560.7460.7740.7740.7560.746SRNN0.7820.7830.7390.7390.7840.7870.7450.743CNN0.6960.7070.6990.6890.7010.710.70.691LSTM0.780.7670.7520.7390.7870.7710.7610.746CNN-LSTM0.7870.7830.7640.7550.7930.7860.7670.761BiLSTM0.8050.780.7580.7430.8090.7830.7640.756Proposed model0.7930.7940.770.7610.8030.80.7830.765GranularityMeanMinEmpty Cell12 h24 h48 h72 h12 h24 h48 h72 hRF227.05237.78264.29263.39226.95237263.79263.23LR330.7283.97333.29338.74330.7283.97333.29338.74KNN231.73250.37265.98267.84231.73250.37265.98267.84SRNN241.14256.25308.66303.99243.79252.88296.72301.48CNN339.19304.66340.29352.37327.48300.05337.17346.3LSTM230.6248.07272.23267.54227.86243.76266.6262.43CNN-LSTM242.61266.54288.49291.55240.53266.54288.5291.55BiLSTM220.92243.05261.21257.62219.13237.2255.77250.2Proposed model224.55239.19261.11266.24217.7230.73258.2261.176.5. Summary of the resultsFinally, we will analyse the general behaviour of each model by taking into account the average value of all the stations. As we can see in Fig. 11 and Table 6, the proposed model obtains the best results in all the granularities and metrics except considering RMSE in 24 h in advance. By granularity, the MAE of the proposed model is 3% smaller than the MAE of the second best model (BiLSTM) in 12 h granularity; 8.1% smaller than the MAE of the second best models (RF and BiLSTM) in 24 h granularity; 4.8% smaller than the MAE of the second best model (BiLSTM) in 48 h granularity and 3.6% smaller than the MAE of the second best model (BiLSTM) in 72 h granularity. On average, the MAE of the proposed model is 4.8% smaller than the MAE of second best model, BiLSTM. Now, we can analyse the impact of the components that we apply in the proposed model. On average, to use a BiLSTM block rather than an LSTM block implies an 8.1% decrease of the MAE while adding a CNN block rather than not adding it implies a decrease of the MAE by 4.8%. Finally, the performance of both improvements implies a decrease of the MAE by 10.3% with respect to a single LSTM model.Download: Download high-res image (433KB)Download: Download full-size imageFig. 11. Line charts of MAE (top), Acc (centre) and RMSE (bottom) by model and granularity (average of four stations).The station in which the proposed model obtains the biggest difference with respect to the baseline models is C/Gran Vía. This may be in part due to the smaller temporal dependence in this station, as we mentioned in Section 3.3. This result shows that the hidden characteristics detected by the CNN block of the proposed model are not only temporal dependencies: it is also able to identify relationships between different predictor variables. On the contrary, the baseline models are not so well suited to detect these relationships. These non-temporal relations are present to a greater or lesser extent in all stations data. Because of this, the proposed model obtains the best results in fifteen out of the sixteen performed experiments.Table 6. MAE, Acc and RMSE values by granularity and model (average of the four stations).GranularityMAEAccRMSEEmpty Cell12 h24 h48 h72 h12 h24 h48 h72 h12 h24 h48 h72 hRF134.1126.817150.542155.5750.8070.8220.7860.78197.04195.09221.86231.16KNN141.585136.678155.98163.470.7940.8060.7830.77212.5210.97232.3339.35SRNN124.64127.5140.44162.70.81150.8110.76950.775206.18215.51265.45254.33LSTM125.197132.897154.455159.450.8150.810.7860.778195.96208.22232.41239.48CNN-LSTM124.87127.677150.52155.5470.8170.8180.7910.787199.36211.79237.16243.36BiLSTM115.04126.713145.295152.4580.830.8180.80.788180.1200.06223.29230.45Proposed model111.69116.43138.32146.990.8330.8340.8090.8178.21197.53220.45226.7Briefly discussing the Acc values, we can mention that it commonly behaves as MAE. The Acc of the proposed model obtains the best results in all granularities. We can highlight that there is a low increase of the Acc in the proposed model (and in others such as RF, CNN-LSTM and KNN) from the 12 to the 24 h granularity. For this metric, the proposed model obtains the best performance in fourteen out of the sixteen experiments.Finally, briefly discussing the RMSE values, we note that, in general, our proposal obtains the best performance in all the granularities except in 24 h in advance. In this case, RF obtains the best results in three out of four stations. However, RF obtains poorer RMSE than our proposal when predicting 12 h in advance, while our proposal obtains similar RMSE to RF in 24 h in advance. Moreover, our proposal obtains the best performance in ten out of the sixteen experiments while the second best model (RF) obtains only the best results in four out of the sixteen experiments. Therefore, our model does not only stand out for its best performance, but also by its higher consistency with respect to baseline models.7. Conclusions and future workIn this paper we presented the development of a hybrid model between two types of neural networks (CNN and BiLSTM) for long-term traffic flow forecasting. We have evaluated the usefulness of our model in four traffic stations of Madrid. In order to assess the quality and performance of the proposed model, we compared it with eight baseline models typically used in time series problems. The results show that, in general, the proposed model obtains the lowest error in all granularities. By granularity, and considering MAE, the proposal takes a higher difference respect to the best baseline models in 24 h granularity, obtaining an MAE 8.1% smaller than the one obtained by the second best model. On the contrary, the lower difference with respect to the best baseline model is achieved in 12 h granularity, obtaining an MAE 3.3% smaller than the one obtained by the second best model. We would also like to point out that, on average, the accuracy obtained in 24 h granularity (0.834) is slightly higher than the one obtained in 12 h granularity (0.833). By station, and considering Acc, the proposal obtains the higher accuracy in C/Arturo Soria station, obtaining an average accuracy of 0.837, while the lower is obtained in C/Gran Vía station (0.78). Our proposal obtains the best RMSE in 10 out of the sixteen experiments. It also confirms the best performance of our proposal with respect to the baseline models. Moreover, the consistency of the proposed model in all the granularities is another advantage to take into account. We can bring out two main conclusions of our work. First, the extraction of characteristics (CNN layer) from a time series matrix is relevant to obtain a better quality of the forecasts. Second, in light of the obtained results, we can claim that a BiLSTM network obtains better results than an LSTM network in time series tasks. By extension, the hybridisation of BiLSTM and CNN is more accurate than the hybridisation between an LSTM and CNN.We consider some lines for future work. First, in order to decrease the error of the model, we would like to perform a detailed analysis about the behaviour of different hyperparameters in the CNN-BiLSTM hybrid model. Second, we would like to export the model to other traffic stations in Madrid and to other cities that could require it. In the second case, we will establish a criterion for processing data as general as possible. Third, we would like to introduce new predictor variables. For example, it would be useful to have a variable that determines the state of traffic restrictions under stressful and unusual conditions such as a pandemic. In this line, we would like to use air quality prediction models (Méndez et al., 2023), in particular, our previous work on ozone concentration (Méndez et al., 2022b), to enhance traffic flow predictions. Finally, we would like to integrate our work with current research on Complex Event Processing (CEP) (Díaz et al., 2020, Roldán et al., 2020, Semlali et al., 2021, Brazález et al., 2022) to implement a system that could automatically raise alarms by taking into account complex events generated from different contexts, specifically, traffic flow and air quality.CRediT authorship contribution statementManuel Méndez: Conceptualization, Software, Validation, Data curation, Writing – original draft, Writing – review & editing, Visualization. Mercedes G. Merayo: Conceptualization, Methodology, Writing – original draft, Writing – review & editing, Supervision, Funding acquisition. Manuel Núñez: Conceptualization, Methodology, Writing – original draft, Writing – review & editing, Supervision, Funding acquisition.Declaration of Competing InterestThe authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.AcknowledgementsWe would like to thank the anonymous reviewers of this paper for the careful reading and their constructive comments, which have helped us to further strengthen the paper.Recommended articlesData availabilityData will be made available on request.ReferencesAbdi et al., 2010Abdi, J., Moshiri, B., Sedigh, A.K., 2010. Comparison of RBF and MLP neural networks in short-term traffic flow forecasting. In: 2010 International Conference on Power, Control and Embedded Systems. pp. 1–4. http://dx.doi.org/10.1109/ICPCES.2010.5698623.Google ScholarAbduljabbar et al., 2021Abduljabbar R., Dia H., Tsai P.-W.Unidirectional and bidirectional LSTM models for short-term traffic predictionJ. Adv. Transp., 2021 (2021), pp. 1-16, 10.1155/2021/5589075Google ScholarAbu Kwaik et al., 2019Abu Kwaik K., Saad M., Chatzikyriakidis S., Dobnik S.LSTM-CNN deep learning model for sentiment analysis of dialectal arabicSmaïli K. (Ed.), Arabic Language Processing: From Theory to Practice, 978-3-030-32959-4, Springer International Publishing, Cham (2019), pp. 108-121, 10.1007/978-3-030-32959-4_8View in ScopusGoogle ScholarAri and Alagoz, 2022Ari D., Alagoz B.B.An effective integrated genetic programming and neural network model for electronic nose calibration of air pollution monitoring applicationNeural Comput. Appl., 34 (2022), pp. 12633-12652, 10.1007/s00521-022-07129-0View in ScopusGoogle ScholarAri and Alagoz, 2023Ari D., Alagoz B.B.DEHypGpOls: a genetic programming with evolutionary hyperparameter optimization and its application for stock market trend predictionSoft Comput., 27 (2023), pp. 2553-2574, 10.1007/s00500-022-07571-1View in ScopusGoogle ScholarBaldi and Sadowski, 2013Baldi, P., Sadowski, P.J., 2013. Understanding Dropout. In: Burges, C.J.C., Bottou, L., Ghahramani, Z., Weinberger, K.Q. (Eds.), Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a Meeting Held December 5-8, 2013, Lake Tahoe, Nevada, United States. pp. 2814–2822.Google ScholarBalwant, 2019Balwant, M.K., 2019. Bidirectional LSTM Based on POS tags and CNN Architecture for Fake News Detection. In: 2019 10th International Conference on Computing, Communication and Networking Technologies. ICCCNT, pp. 1–6. http://dx.doi.org/10.1109/ICCCNT45670.2019.8944460.Google ScholarBelhadi et al., 2020Belhadi A., Djenouri Y., Djenouri D., Lin C.-W.A recurrent neural network for urban long-term traffic flow forecastingAppl. Intell., 50 (2020), 10.1007/s10489-020-01716-1Google ScholarBrazález et al., 2022Brazález E., Macià H., Díaz G., Valero V., Boubeta-Puig J.PITS: an intelligent transportation system in pandemic timesEng. Appl. Artif. Intell., 114 (2022), Article 105154, 10.1016/j.engappai.2022.105154View PDFView articleView in ScopusGoogle ScholarBui Khac Hoai et al., 2022Bui Khac Hoai N., Cho J., Yi H.Spatial-temporal graph neural network for traffic forecasting: An overview and open research issuesAppl. Intell., 52 (2022), 10.1007/s10489-021-02587-wGoogle ScholarChen et al., 2011Chen C., Hu J., Meng Q., Zhang Y.Short-time traffic flow prediction with ARIMA-GARCH modelIEEE Intelligent Vehicles Symposium (IV), 2011, Baden-Baden, Germany, June 5-9, 2011, IEEE (2011), pp. 607-612, 10.1109/IVS.2011.5940418View in ScopusGoogle ScholarChou et al., 2017Chou C., Shie C., Chang F., Chang J., Chang E.Y.Representation learning on large and small data (2017), 10.48550/arXiv.1707.09873Google ScholarChu et al., 2021Chu, Q., Li, G., Zhou, R., Ping, Z., 2021. Traffic Flow Prediction Model Based on LSTM with Finnish Dataset. In: 2021 6th International Conference on Intelligent Computing and Signal Processing. ICSP, pp. 389–392. http://dx.doi.org/10.1109/ICSP51882.2021.9408888.Google ScholarDíaz et al., 2020Díaz G., Macià H., Valero V., Boubeta-Puig J., Cuartero F.An intelligent transportation system to control air pollution and road traffic in cities integrating CEP and colored Petri netsNeural Comput. Appl., 32 (2) (2020), pp. 405-426, 10.1007/s00521-018-3850-1View in ScopusGoogle ScholarGoodwin and Lawton, 1999Goodwin P., Lawton R.On the asymmetry of the symmetric MAPEInt. J. Forecast., 0169-2070, 15 (4) (1999), pp. 405-408, 10.1016/S0169-2070(99)00007-2View PDFView articleView in ScopusGoogle ScholarHong et al., 2015Hong H., Huang W., Zhou X., Du S., Bian K., Xie K.Short-term traffic flow forecasting: Multi-metric KNN with related station discovery12th International Conference on Fuzzy Systems and Knowledge Discovery, FSKD 2015, Zhangjiajie, China, August 15-17, 2015, IEEE (2015), pp. 1670-1675, 10.1109/FSKD.2015.7382196View in ScopusGoogle ScholarHussain et al., 2020Hussain T., Muhammad K., Ullah A., Cao Z., Baik S.W., de Albuquerque V.H.C.Cloud-assisted multiview video summarization using CNN and bidirectional LSTMIEEE Trans. Ind. Inform., 16 (1) (2020), pp. 77-86, 10.1109/TII.2019.2929228View in ScopusGoogle ScholarKang and Zhang, 2020Kang, C., Zhang, Z., 2020. Application of LSTM in Short-term Traffic Flow Prediction. In: 2020 IEEE 5th International Conference on Intelligent Transportation Engineering. ICITE, pp. 98–101. http://dx.doi.org/10.1109/ICITE50838.2020.9231500.Google ScholarKong et al., 2022Kong X., Zhang J., Wei X., Xing W., Lu W.Adaptive spatial-temporal graph attention networks for traffic flow forecastingAppl. Intell., 52 (2022), pp. 1-17, 10.1007/s10489-021-02648-0View in ScopusGoogle ScholarLi, 2020Li D.Predicting short-term traffic flow in urban based on multivariate linear regression modelJ. Intell. Fuzzy Systems, 39 (2) (2020), pp. 1417-1427, 10.3233/JIFS-179916View in ScopusGoogle ScholarLi et al., 2021Li Y., Chai S., Ma Z., Wang G.A hybrid deep learning framework for long-term traffic flow predictionIEEE Access, 9 (2021), pp. 11264-11271, 10.1109/ACCESS.2021.3050836View in ScopusGoogle ScholarLi et al., 2022Li S., Xie Q., Yang J.Daily suspended sediment forecast by an integrated dynamic neural networkJ. Hydrol., 604 (2022), Article 127258, 10.1016/j.jhydrol.2021.127258View PDFView articleView in ScopusGoogle ScholarLi et al., 2018Li, C., Zhan, G., Li, Z., 2018. News Text Classification Based on Improved Bi-LSTM-CNN. In: 2018 9th International Conference on Information Technology in Medicine and Education. ITME, pp. 890–893. http://dx.doi.org/10.1109/ITME.2018.00199.Google ScholarLin and Huang, 2021Lin X., Huang Y.Short-term high-speed traffic flow prediction based on ARIMA-GARCH-M modelWirel. Pers. Commun., 117 (4) (2021), pp. 3421-3430, 10.1007/s11277-021-08085-zView in ScopusGoogle ScholarLin et al., 2019Lin J.C.-W., Shao Y., Zhou Y., Pirouz M., Chen H.-C.A Bi-LSTM mention hypergraph model with encoding schema for mention extractionEng. Appl. Artif. Intell., 0952-1976, 85 (2019), pp. 175-181, 10.1016/j.engappai.2019.06.005URL https://www.sciencedirect.com/science/article/pii/S0952197619301447View PDFView articleView in ScopusGoogle ScholarMakarenkov et al., 2019Makarenkov V., Rokach L., Shapira B.Choosing the right word: Using bidirectional LSTM tagger for writing support systemsEng. Appl. Artif. Intell., 0952-1976, 84 (2019), pp. 1-10, 10.1016/j.engappai.2019.05.003URL https://www.sciencedirect.com/science/article/pii/S0952197619301034View PDFView articleView in ScopusGoogle ScholarMéndez et al., 2022aMéndez M., Ibias A., Núñez M.Using deep learning to detect anomalies in traffic flow14th Asian Conference on Intelligent Information and Database Systems, ACIIDS’22, LNCS 13757, Springer (2022), pp. 299-312CrossrefView in ScopusGoogle ScholarMéndez et al., 2023Méndez M., Merayo M.G., Núñez M.Machine learning algorithms to forecast air quality: a surveyArtif. Intell. Rev. (2023)(in press)Google ScholarMéndez et al., 2022bMéndez M., Montero C., Núñez M.Using deep transformer based models to predict ozone levels14th Asian Conference on Intelligent Information and Database Systems, ACIIDS’22, LNAI 13757, Springer (2022), pp. 169-182CrossrefView in ScopusGoogle ScholarNetron, 2022Netron open source tool (2022)https://netron.app/, accessed: 2022-06-30Google ScholarOzcan et al., 2023Ozcan A., Catal C., Donmez E., Senturk B.A hybrid DNN–LSTM model for detecting phishing URLsNeural Comput. Appl., 35 (2023), pp. 4957-4973, 10.1007/s00521-021-06401-zView in ScopusGoogle ScholarPassricha and Aggarwal, 2020Passricha V., Aggarwal R.K.A hybrid of deep CNN and bidirectional LSTM for automatic speech recognitionJ. Intell. Syst., 29 (1) (2020), pp. 1261-1274, 10.1515/jisys-2018-0372View in ScopusGoogle ScholarPeiris et al., 2022Peiris A.T., Jayasinghe J., Rathnayake U.Forecasting electricity power generation of Pawan Danawi wind farm, Sri Lanka, using Gene Expression ProgrammingAppl. Comput. Intell. Soft Comput., 2022 (2022), Article 7081444, 10.1155/2022/7081444Google ScholarPeng et al., 2021Peng H., Du B., Liu M., Liu M., Ji S., Wang S., Zhang X., He L.Dynamic graph convolutional network for long-term traffic flow prediction with reinforcement learningInform. Sci., 0020-0255, 578 (2021), pp. 401-416, 10.1016/j.ins.2021.07.007URL https://www.sciencedirect.com/science/article/pii/S0020025521006976View PDFView articleView in ScopusGoogle ScholarPoonia and Jain, 2020Poonia, P., Jain, V.K., 2020. Short-Term Traffic Flow Prediction: Using LSTM. In: 2020 International Conference on Emerging Trends in Communication, Control and Computing (ICONC3). pp. 1–4. http://dx.doi.org/10.1109/ICONC345789.2020.9117329.Google ScholarRhanoui et al., 2019Rhanoui M., Mikram M., Yousfi S., Barzali S.A CNN-BiLSTM model for document-level sentiment analysisMach. Learn. Knowl. Extract., 2504-4990, 1 (3) (2019), pp. 832-847, 10.3390/make1030048URL https://www.mdpi.com/2504-4990/1/3/48View in ScopusGoogle ScholarRick and Berton, 2022Rick R., Berton L.Energy forecasting model based on CNN-LSTM-AE for many time series with unequal lengthsEng. Appl. Artif. Intell., 0952-1976, 113 (2022), Article 104998, 10.1016/j.engappai.2022.104998URL https://www.sciencedirect.com/science/article/pii/S0952197622001889View PDFView articleView in ScopusGoogle ScholarRoldán et al., 2020Roldán J., Boubeta-Puig J., Martínez J.L., Ortiz G.Integrating complex event processing and machine learning: An intelligent architecture for detecting IoT security attacksExpert Syst. Appl., 149 (2020), Article 113251, 10.1016/j.eswa.2020.1132511–22View PDFView articleView in ScopusGoogle ScholarSemlali et al., 2021Semlali B.B., Amrani C.E., Ortiz G., Boubeta-Puig J., de Prado A.G.SAT-CEP-monitor: An air quality monitoring software architecture combining complex event processing with satellite remote sensingComput. Electr. Eng., 93 (2021), Article 107257, 10.1016/j.compeleceng.2021.107257View PDFView articleView in ScopusGoogle ScholarShubhangi and Pratibha, 2021Shubhangi, D., Pratibha, A., 2021. Asthma, Alzheimer’s and Dementia Disease Detection based on Voice Recognition using Multi-Layer Perceptron Algorithm. In: 2021 International Conference on Innovative Computing, Intelligent Communication and Smart Electrical Systems. ICSES, pp. 1–7. http://dx.doi.org/10.1109/ICSES52305.2021.9633923.Google ScholarSiami-Namini et al., 2019Siami-Namini, S., Tavakoli, N., Namin, A.S., 2019. The Performance of LSTM and BiLSTM in Forecasting Time Series. In: 2019 IEEE International Conference on Big Data (Big Data). pp. 3285–3292. http://dx.doi.org/10.1109/BigData47090.2019.9005997.Google ScholarSong et al., 2021Song J.W., Park Y.I., Hong J.J., Kim S.G., Kang S.J.Attention-based bidirectional LSTM-CNN model for remaining useful life estimation53rd IEEE Int. Symposium on Circuits and Systems, ISCAS’21, IEEE (2021), pp. 1-5, 10.1109/ISCAS51556.2021.9401572Google ScholarSoon et al., 2019Soon K.L., Lim J.M.-Y., Parthiban R.Extended pheromone-based short-term traffic forecasting models for vehicular systemsEng. Appl. Artif. Intell., 0952-1976, 82 (2019), pp. 60-75, 10.1016/j.engappai.2019.03.017URL https://www.sciencedirect.com/science/article/pii/S0952197619300673View PDFView articleView in ScopusGoogle ScholarWang et al., 2017Wang Y., Li L., Xu X.A piecewise hybrid of ARIMA and SVMs for short-term traffic flow predictionLiu D., Xie S., Li Y., Zhao D., El-Alfy E.M. (Eds.), Neural Information Processing - 24th International Conference, ICONIP 2017, Guangzhou, China, November 14-18, 2017, Proceedings, Part V, Lecture Notes in Computer Science, vol. 10638, Springer (2017), pp. 493-502, 10.1007/978-3-319-70139-4_50View in ScopusGoogle ScholarWang et al., 2021Wang Z., Su X., Ding Z.Long-term traffic prediction based on LSTM encoder-decoder architectureIEEE Trans. Intell. Transp. Syst., 22 (10) (2021), pp. 6561-6571, 10.1109/TITS.2020.2995546View in ScopusGoogle ScholarZameer et al., 2017Zameer A., Arshad J., Khan A., Raja M.A.Z.Intelligent and robust prediction of short term wind power using genetic programming based ensemble of neural networksEnergy Convers. Manage., 134 (2017), pp. 361-372, 10.1016/j.enconman.2016.12.032View PDFView articleView in ScopusGoogle ScholarZarei et al., 2013Zarei N., Ghayour M.A., Hashemi S.Road traffic prediction using context-aware random forest based on volatility nature of traffic flowsSelamat A., Nguyen N.T., Haron H. (Eds.), Intelligent Information and Database Systems - 5th Asian Conference, ACIIDS 2013, Kuala Lumpur, Malaysia, March 18-20, 2013, Proceedings, Part I, Lecture Notes in Computer Science, vol. 7802, Springer (2013), pp. 196-205, 10.1007/978-3-642-36546-1_21View in ScopusGoogle ScholarZhang et al., 2020Zhang Y., Qiao S., Ji S., Li Y.DeepSite: bidirectional LSTM and CNN models for predicting DNA–protein bindingInt. J. Mach. Learn. Cybern., 11 (2020), pp. 841-851, 10.1007/s13042-019-00990-xView in ScopusGoogle ScholarZhang et al., 2021Zhang Y., Yang Y., Zhou W., Wang H., Ouyang X.Multi-city traffic flow forecasting via multi-task learningAppl. Intell., 51 (2021), pp. 1-19, 10.1007/s10489-020-02074-8Google ScholarZhu et al., 2014Zhu J.Z., Cao J.X., Zhu Y.Traffic volume forecasting based on radial basis function neural network with the consideration of traffic flows at the adjacent intersectionsTransp. Res. C, 0968-090X, 47 (2014), pp. 139-154, 10.1016/j.trc.2014.06.011URL https://www.sciencedirect.com/science/article/pii/S0968090X14002010View PDFView articleView in ScopusGoogle ScholarCited by (66)A high-precision crown control strategy for hot-rolled electric steel using theoretical model-guided BO-CNN-BiLSTM framework2024, Applied Soft ComputingShow abstractThe prediction accuracy of strip crown is low under complex industrial data environments to general machine learning models, i.e., lack of reasonable mechanism explanation and spatial dimension dependence, which will directly affect the product quality of hot-rolled electrical steel. Therefore, a high-precision crown prediction model is proposed for electrical steel in hot rolling based on a theoretical model-guided BO-CNN-BiLSTM (Bayesian optimization, Convolution Neural Network, and Bidirectional Long Short-term Memory) framework. The work roll wear model, thermal crown model, and the secondary deformation model of the strip between stands, based on the primary deformation by the loaded gap profile, were constructed. The mechanism parameters and measured parameters are integrated into a dataset as input feature variables. In the TG-BO-CNN-BiLSTM framework, the CNN-BiLSTM model, which can achieve spatial dimension dependence, was used to extract its feature component and sequentially predict the crown using the dataset, simultaneously, the BO module optimizes the hyperparameter of the CNN-BiLSTM model. The advantages of the proposed model are verified by adopting multiple evaluation indicators, which improves running speed and prediction accuracy. The effects of process parameters on the crown with typical upstream and downstream stands were comprehensively analyzed with the proposed model. A high-precision crown control strategy, combining the framework and influence law, for multi-stand and multi-method was proposed to obtain the high-precision crown. The control strategy applied in 1450 mm 4-high hot strip mills shows that the production performance of electrical steel is significantly improved.MFOA-Bi-LSTM: An optimized bidirectional long short-term memory model for short-term traffic flow prediction2024, Physica A: Statistical Mechanics and its ApplicationsShow abstractWithin Intelligent Transportation Systems (ITSs), modeling of traffic flow assumes a pivotal role, as it is crucial for alleviating traffic congestion and reducing carbon emissions. Developing a reliable and sustainable model for predicting traffic flow is highly challenging due to the fluctuations and nonlinear characteristics of traffic flow. In addition to being a practical problem, accurately forecasting traffic flow provides substantial challenges to the researchers working in this area. The aim of this study is to develop a complex framework for forecasting short-term traffic flow with the purpose of improving prediction accuracy. In order to attain this goal, a Bidirectional Long Short-Term Memory (Bi-LSTM) model with a Modified Firefly Optimization Algorithm (MFOA) known as MFOA-Bi-LSTM has been proposed. The MFOA optimization technique has been utilized to optimize the hyperparameters of the deep learning models. Moreover, the optimization of the Bi-LSTM network prediction model is achieved through the application of the MFOA technique, which is renowned for its ability to converge quickly, maintain robustness and perform extensive global searches. To better capture complex patterns in the input data and increase prediction accuracy, model’s architecture makes use of the bidirectional capabilities of Bi-LSTM layers. This combination of Bi-LSTM and FOA is proposed for optimizing complex sequence modeling tasks where selecting the right hyperparameters for the model is challenging and it allows for automated hyperparameter tuning and often leads to improved model performance. In terms of performance metrics including Root Mean Square Error (RMSE), Mean Absolute Error (MAE), Mean Absolute Percentage Error (MAPE) and Correlation Coefficient (r), the proposed model significantly outperforms the other selected models and study findings confirm its appropriateness for the predictive task.A new LASSO-BiLSTM-based ensemble learning approach for exchange rate forecasting2024, Engineering Applications of Artificial IntelligenceShow abstractForeign exchange rate affects many countries' economic status and development. Therefore, it is essential to find the factors affecting the exchange rate price and make reasonable predictions. This paper proposes the novel LASSO-BiLSTM-based ensemble learning method by integrating least absolute shrinkage and selection operator (LASSO) and bidirectional long short-term memory (LSTM) to predict the USD/CNY exchange rate. First, 29 variables are selected to reflect economic activities on market and macroeconomic levels. Then, LASSO-BiLSTM-based ensemble learning approach is adopted with two steps: 1) LASSO is used to select six highly correlated variables with the exchange rate to reduce noises. 2) BiLSTM is employed to forecast the exchange rate with the six chosen variables. Last, to test the effectiveness of BiLSTM, comparisons with four deep learning algorithms, which are extreme learning machine (ELM), kernel extreme learning machine (KELM), long short-term memory (LSTM), and support vector regression (SVR), are conducted. The result shows that LASSO-BiLSTM outperforms the other models in 1-step forecast (MAE: 0.051, RMSE: 0.072, MDA: 0.777). The same conclusion applies to 3-steps and 6-steps forecasts. Overall, the proposed LASSO-BiLSTM-based ensemble learning method demonstrates high potential in time series forecasting.A hierarchical methodology for vessel traffic flow prediction using Bayesian tensor decomposition and similarity grouping2023, Ocean EngineeringShow abstractAccurate vessel traffic flow (VTF) prediction can enhance navigation safety and economic efficiency. To address the challenge of the inherently complex and dynamic growth of the VTF time series, a new hierarchical methodology for VTF prediction is proposed. Firstly, the original VTF data is reconfigured as a three-dimensional tensor by a modified Bayesian Gaussian CANDECOMP/PARAFAC (BGCP) tensor decomposition model. Secondly, the VTF matrix (hour ✕ day) of each week is decomposed into high- and low-frequency matrices using a Bidimensional Empirical Mode Decomposition (BEMD) model to address the non-stationary signals affecting prediction results. Thirdly, the self-similarities between VTF matrices of each week within the high-frequency tensor are utilised to rearrange the matrices as different one-dimensional time series to solve the weak mathematical regularity in the high-frequency matrix. Then, a Dynamic Time Warping (DTW) model is employed to identify grouped segments with high similarities to generate more suitable high-frequency tensors. The experimental results verify that the proposed methodology outperforms the state-of-the-art VTF prediction methods using real Automatic Identification System (AIS) datasets collected from two areas. The methodology can potentially optimise relation operations and manage vessel traffic, benefiting stakeholders such as port authorities, ship operators, and freight forwarders.Advanced series decomposition with a gated recurrent unit and graph convolutional neural network for non-stationary data patterns2024, Journal of Cloud ComputingSpatio-Temporal Residual Graph Convolutional Network for Short-Term Traffic Flow Prediction2023, IEEE AccessView all citing articles on Scopus☆This work has been supported by the State Research Agency (AEI) of the Spanish Ministry of Science and Innovation under grant PID2021-122215NB-C31 (AwESOMe); the Comunidad de Madrid, Spain under grant S2018/TCS-4314 (FORTE-CM) co-funded by EIE Funds of the European Union
 .1https://datos.gob.es/es/catalogo/l01280796-aforos-de-trafico-en-la-ciudad-de-madrid-permanentes1.2AEMET is the state meteorological agency of Spain: https://opendata.aemet.es/centrodedescargas/productosAEMET?.3https://datos.madrid.es/portal/site/egob/.4We think that repeating the experiment five times was enough because the results were very stable and there was a small variance. In fact, average and best results were very close, as can be appreciated in Table 2, Table 3, Table 4, Table 5.© 2023 The Author(s). Published by Elsevier Ltd.Recommended articlesA CNN-BiLSTM based hybrid model for Indian language identificationApplied Acoustics, Volume 182, 2021, Article 108274Himanish Shekhar Das, Pinki RoyView PDFError-distribution-free kernel extreme learning machine for traffic flow forecastingEngineering Applications of Artificial Intelligence, Volume 123, Part C, 2023, Article 106411Keer Wu, …, Teng ZhouView PDFShort-term traffic flow prediction: An ensemble machine learning approachAlexandria Engineering Journal, Volume 74, 2023, pp. 467-480Guowen Dai, …, Wang LuoView PDFGeneric Dynamic Graph Convolutional Network for traffic flow forecastingInformation Fusion, Volume 100, 2023, Article 101946Yi Xu, …, Weifeng LvView PDFA hybrid deep learning model with 1DCNN-LSTM-Attention networks for short-term traffic flow predictionPhysica A: Statistical Mechanics and its Applications, Volume 583, 2021, Article 126293Ke Wang, …, Sheng DongView PDFRPConvformer: A novel Transformer-based deep neural networks for traffic flow predictionExpert Systems with Applications, Volume 218, 2023, Article 119587Yanjie Wen, …, Xiaoyu WangView PDFShow 3 more articlesArticle MetricsCitationsCitation Indexes: 64CapturesReaders: 39View detailsAbout ScienceDirectRemote accessShopping cartAdvertiseContact and supportTerms and conditionsPrivacy policyCookies are used by this site.  Cookie SettingsAll content on this site: Copyright © 2024 Elsevier B.V., its licensors, and contributors. All rights are reserved, including those for text and data mining, AI training, and similar technologies. For all open access content, the Creative Commons licensing terms apply.
















Cookie Preference CenterWe use cookies which are necessary to make our site work. We may also use additional cookies to analyse, improve and personalise our content and your digital experience. For more information, see our Cookie Policy and the list of Google Ad-Tech Vendors.


You may choose not to allow some types of cookies. However, blocking some types may impact your experience of our site and the services we are able to offer. See the different category headings below to find out more or change your settings.

Allow all Manage Consent PreferencesStrictly Necessary CookiesAlways activeThese cookies are necessary for the website to function and cannot be switched off in our systems. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work. These cookies do not store any personally identifiable information.
Cookie Details List‎Functional Cookies  Functional Cookies These cookies enable the website to provide enhanced functionality and personalisation. They may be set by us or by third party providers whose services we have added to our pages. If you do not allow these cookies then some or all of these services may not function properly.Cookie Details List‎Performance Cookies  Performance Cookies These cookies allow us to count visits and traffic sources so we can measure and improve the performance of our site. They help us to know which pages are the most and least popular and see how visitors move around the site.Cookie Details List‎Targeting Cookies  Targeting Cookies These cookies may be set through our site by our advertising partners. They may be used by those companies to build a profile of your interests and show you relevant adverts on other sites. If you do not allow these cookies, you will experience less targeted advertising.Cookie Details List‎Back ButtonCookie List Search IconFilter IconClear checkbox label labelApply CancelConsent Leg.Interest checkbox label label checkbox label label checkbox label label Confirm my choicesYour Privacy [`dialog closed`]