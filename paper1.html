<section aria-label="article masthead" class="app-masthead">
 <div class="app-masthead__container">
  <div class="app-article-masthead u-sans-serif js-context-bar-sticky-point-masthead" data-test="masthead-component" data-track-component="article">
   <div class="app-article-masthead__info">
    <nav aria-label="breadcrumbs" data-test="breadcrumbs">
     <ol class="c-breadcrumbs c-breadcrumbs--contrast" itemscope="" itemtype="https://schema.org/BreadcrumbList">
      <li class="c-breadcrumbs__item" id="breadcrumb0" itemprop="itemListElement" itemscope="" itemtype="https://schema.org/ListItem">
       <a class="c-breadcrumbs__link" data-track="click_breadcrumb" data-track-action="breadcrumbs" data-track-category="article" data-track-context="article page" data-track-label="breadcrumb1" href="/" itemprop="item">
        <span itemprop="name">
         Home
        </span>
       </a>
       <meta content="1" itemprop="position"/>
       <svg aria-hidden="true" class="c-breadcrumbs__chevron" focusable="false" height="10" role="img" viewbox="0 0 10 10" width="10">
        <path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)">
        </path>
       </svg>
      </li>
      <li class="c-breadcrumbs__item" id="breadcrumb1" itemprop="itemListElement" itemscope="" itemtype="https://schema.org/ListItem">
       <a class="c-breadcrumbs__link" data-track="click_breadcrumb" data-track-action="breadcrumbs" data-track-category="article" data-track-context="article page" data-track-label="breadcrumb2" href="/journal/10844" itemprop="item">
        <span itemprop="name">
         Journal of Intelligent Information Systems
        </span>
       </a>
       <meta content="2" itemprop="position"/>
       <svg aria-hidden="true" class="c-breadcrumbs__chevron" focusable="false" height="10" role="img" viewbox="0 0 10 10" width="10">
        <path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)">
        </path>
       </svg>
      </li>
      <li class="c-breadcrumbs__item" id="breadcrumb2" itemprop="itemListElement" itemscope="" itemtype="https://schema.org/ListItem">
       <span itemprop="name">
        Article
       </span>
       <meta content="3" itemprop="position"/>
      </li>
     </ol>
    </nav>
    <h1 class="c-article-title" data-article-title="" data-test="article-title">
     Heterogeneous graphormer for extractive multimodal summarization
    </h1>
    <ul class="c-article-identifiers">
     <li class="c-article-identifiers__item" data-test="article-category">
      Research
     </li>
     <li class="c-article-identifiers__item">
      Published:
      <time datetime="2024-09-30">
       30 September 2024
      </time>
     </li>
    </ul>
    <ul class="c-article-identifiers c-article-identifiers--cite-list">
     <li class="c-article-identifiers__item">
      (
      <span data-test="article-publication-year">
       2024
      </span>
      )
     </li>
     <li class="c-article-identifiers__item c-article-identifiers__item--cite">
      <a data-track="click" data-track-action="cite this article" data-track-category="article body" data-track-label="link" href="#citeas">
       Cite this article
      </a>
     </li>
    </ul>
    <div class="app-article-masthead__buttons" data-test="download-article-link-wrapper" data-track-context="masthead">
     <div class="c-pdf-container">
      <div class="c-pdf-download u-clear-both u-mb-16">
       <a class="u-button u-button--full-width u-button--primary u-justify-content-space-between c-pdf-download__link" data-article-pdf="true" data-draft-ignore="true" data-readcube-pdf-url="true" data-test="pdf-link" data-track="content_download" data-track-action="download pdf" data-track-external="" data-track-label="button" data-track-type="article pdf download" download="" href="/content/pdf/10.1007/s10844-024-00886-5.pdf">
        <span class="c-pdf-download__text">
         Download PDF
        </span>
        <svg aria-hidden="true" class="u-icon" focusable="false" height="16" width="16">
         <use xlink:href="#icon-eds-i-download-medium">
         </use>
        </svg>
       </a>
      </div>
     </div>
     <p class="app-article-masthead__access">
      <svg aria-hidden="true" focusable="false" height="16" role="img" width="16">
       <use xlink:href="#icon-eds-i-check-filled-medium" xmlns:xlink="http://www.w3.org/1999/xlink">
       </use>
      </svg>
      Access provided by Chung Ang University
     </p>
    </div>
   </div>
   <div class="app-article-masthead__brand">
    <a class="app-article-masthead__journal-link" data-track="click_journal_home" data-track-action="journal homepage" data-track-context="article page" data-track-label="link" href="/journal/10844">
     <picture>
      <source height="159" media="(min-width: 768px)" srcset="https://media.springernature.com/w120/springer-static/cover-hires/journal/10844?as=webp,
                                                    https://media.springernature.com/w316/springer-static/cover-hires/journal/10844?as=webp 2x" type="image/webp" width="120"/>
      <img alt="" height="95" src="https://media.springernature.com/w72/springer-static/cover-hires/journal/10844?as=webp" srcset="https://media.springernature.com/w144/springer-static/cover-hires/journal/10844?as=webp 2x" width="72"/>
     </picture>
     <span class="app-article-masthead__journal-title">
      Journal of Intelligent Information Systems
     </span>
    </a>
    <a class="app-article-masthead__submission-link" data-track="click_aims_and_scope" data-track-action="aims and scope" data-track-context="article page" data-track-label="link" href="https://link.springer.com/journal/10844/aims-and-scope">
     Aims and scope
     <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
      <use xlink:href="#icon-eds-i-arrow-right-medium" xmlns:xlink="http://www.w3.org/1999/xlink">
      </use>
     </svg>
    </a>
    <a class="app-article-masthead__submission-link" data-track="click_submit_manuscript" data-track-action="submit manuscript" data-track-context="article masthead on springerlink article page" data-track-label="link" href="https://submission.nature.com/new-submission/10844/3">
     Submit manuscript
     <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
      <use xlink:href="#icon-eds-i-arrow-right-medium" xmlns:xlink="http://www.w3.org/1999/xlink">
      </use>
     </svg>
    </a>
   </div>
  </div>
 </div>
</section>
<section aria-labelledby="Abs1" data-title="Abstract" lang="en">
 <div class="c-article-section" id="Abs1-section">
  <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">
   Abstract
  </h2>
  <div class="c-article-section__content" id="Abs1-content">
   <p>
    Multimodal summarization with multimodal output (MSMO) aims to generate summaries that incorporate both text and images. Existing methods have not effectively leveraged intermodal relationships, such as sentence-image relationships, which are crucial for generating high-quality multimodal summaries. In this paper, we propose a heterogeneous graph-based model for multimodal summarization (HGMS) designed to efficiently leverage intermodal relationships within multimodal data. The model constructs a heterogeneous graph based on the relationships between modalities, containing nodes for words, sentences and images. An enhanced Graphormer is then proposed to update node representations, aiming to more effectively model intricate relationships between multiple modalities. To the best of our knowledge, we are the first to apply Graphormer in the field of graph-based summarization. Experimental results on a large-scale benchmark dataset demonstrate that HGMS achieves state-of-the-art performance in terms of automatic metrics and human evaluations.
   </p>
  </div>
 </div>
</section>
<section aria-labelledby="inline-recommendations" class="c-article-recommendations" data-title="Inline Recommendations" data-track-component="inline-recommendations">
 <h3 class="c-article-recommendations-title" id="inline-recommendations">
  Similar content being viewed by others
 </h3>
 <div class="c-article-recommendations-list">
  <div class="c-article-recommendations-list__item">
   <article class="c-article-recommendations-card" itemscope="" itemtype="http://schema.org/ScholarlyArticle">
    <div class="c-article-recommendations-card__img">
     <img alt="" loading="lazy" src="https://media.springernature.com/w215h120/springer-static/image/art%3A10.1007%2Fs10844-022-00757-x/MediaObjects/10844_2022_757_Fig1_HTML.png"/>
    </div>
    <div class="c-article-recommendations-card__main">
     <h3 class="c-article-recommendations-card__heading" itemprop="name headline">
      <a class="c-article-recommendations-card__link" data-track="select_recommendations_1" data-track-action="click recommendations inline - 1" data-track-context="inline recommendations" data-track-label="10.1007/s10844-022-00757-x" href="https://link.springer.com/10.1007/s10844-022-00757-x?fromPaywallRec=false" itemprop="url">
       Extractive text-image summarization with relation-enhanced graph attention network
      </a>
     </h3>
     <div class="c-article-meta-recommendations" data-test="recommendation-info">
      <span class="c-article-meta-recommendations__item-type">
       Article
      </span>
      <span class="c-article-meta-recommendations__date">
       28 October 2022
      </span>
     </div>
    </div>
   </article>
  </div>
  <div class="c-article-recommendations-list__item">
   <article class="c-article-recommendations-card" itemscope="" itemtype="http://schema.org/ScholarlyArticle">
    <div class="c-article-recommendations-card__img">
     <img alt="" loading="lazy" src="https://media.springernature.com/w92h120/springer-static/cover-hires/book/978-981-99-6207-5?as=webp"/>
    </div>
    <div class="c-article-recommendations-card__main">
     <h3 class="c-article-recommendations-card__heading" itemprop="name headline">
      <a class="c-article-recommendations-card__link" data-track="select_recommendations_2" data-track-action="click recommendations inline - 2" data-track-context="inline recommendations" data-track-label="10.1007/978-981-99-6207-5_17" href="https://link.springer.com/10.1007/978-981-99-6207-5_17?fromPaywallRec=false" itemprop="url">
       MCLS: A Large-Scale Multimodal Cross-Lingual Summarization Dataset
      </a>
     </h3>
     <div class="c-article-meta-recommendations" data-test="recommendation-info">
      <span class="c-article-meta-recommendations__item-type">
       Chapter
      </span>
      <span class="c-article-meta-recommendations__date">
       © 2023
      </span>
     </div>
    </div>
   </article>
  </div>
  <div class="c-article-recommendations-list__item">
   <article class="c-article-recommendations-card" itemscope="" itemtype="http://schema.org/ScholarlyArticle">
    <div class="c-article-recommendations-card__img">
     <img alt="" loading="lazy" src="https://media.springernature.com/w215h120/springer-static/image/art%3A10.1007%2Fs00521-023-08821-5/MediaObjects/521_2023_8821_Fig1_HTML.png"/>
    </div>
    <div class="c-article-recommendations-card__main">
     <h3 class="c-article-recommendations-card__heading" itemprop="name headline">
      <a class="c-article-recommendations-card__link" data-track="select_recommendations_3" data-track-action="click recommendations inline - 3" data-track-context="inline recommendations" data-track-label="10.1007/s00521-023-08821-5" href="https://link.springer.com/10.1007/s00521-023-08821-5?fromPaywallRec=false" itemprop="url">
       Topic-guided abstractive multimodal summarization with multimodal output
      </a>
     </h3>
     <div class="c-article-meta-recommendations" data-test="recommendation-info">
      <span class="c-article-meta-recommendations__item-type">
       Article
      </span>
      <span class="c-article-meta-recommendations__date">
       24 August 2023
      </span>
     </div>
    </div>
   </article>
  </div>
 </div>
</section>
<section aria-labelledby="content-related-subjects" data-test="subject-content">
 <h3 class="c-article__sub-heading" id="content-related-subjects">
  Explore related subjects
 </h3>
 <span class="u-sans-serif u-text-s u-display-block u-mb-24">
  Discover the latest articles, news and stories from top researchers in related subjects.
 </span>
 <ul class="c-article-subject-list" role="list">
  <li class="c-article-subject-list__subject">
   <a data-track="select_related_subject_1" data-track-context="related subjects from content page" data-track-label="Artificial Intelligence" href="/subject/artificial-intelligence">
    Artificial Intelligence
   </a>
  </li>
 </ul>
</section>
<section data-title="Introduction">
 <div class="c-article-section" id="Sec1-section">
  <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">
   <span class="c-article-section__title-number">
    1
   </span>
   Introduction
  </h2>
  <div class="c-article-section__content" id="Sec1-content">
   <p>
    With the rapid development of multimedia data on the Internet, multimodal summarization has attracted widespread attention from researchers. Recently proposed Multimodal Summarization with Multimodal Output (Zhu et al.,
    <a aria-label="Reference 2018" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR45" id="ref-link-section-d206753678e305" title="Zhu, J., Li, H., Liu, T., et al. (2018). MSMO: Multimodal summarization with multimodal output. In: Riloff E, Chiang D, Hockenmaier J, et al (eds) Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Brussels, Belgium, pp 4154–4164. 
                https://doi.org/10.18653/v1/D18-1448
                
              ">
     2018
    </a>
    ) (MSMO) that condenses long multimodal news to a short pictorial version, as shown in Fig.
    <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#Fig1">
     1
    </a>
    . This innovative approach has been substantiated to significantly enhance users’ ability to swiftly grasp key news points, thereby elevating user satisfaction (Zhu et al.,
    <a aria-label="Reference 2018" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR45" id="ref-link-section-d206753678e311" title="Zhu, J., Li, H., Liu, T., et al. (2018). MSMO: Multimodal summarization with multimodal output. In: Riloff E, Chiang D, Hockenmaier J, et al (eds) Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Brussels, Belgium, pp 4154–4164. 
                https://doi.org/10.18653/v1/D18-1448
                
              ">
     2018
    </a>
    ).
   </p>
   <p>
    While several methods have been proposed to tackle the MSMO task (Zhu et al.,
    <a aria-label="Reference 2018" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR45" id="ref-link-section-d206753678e317" title="Zhu, J., Li, H., Liu, T., et al. (2018). MSMO: Multimodal summarization with multimodal output. In: Riloff E, Chiang D, Hockenmaier J, et al (eds) Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Brussels, Belgium, pp 4154–4164. 
                https://doi.org/10.18653/v1/D18-1448
                
              ">
     2018
    </a>
    ,
    <a aria-label="Reference 2020" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR46" id="ref-link-section-d206753678e320" title="Zhu, J., Zhou, Y., Zhang, J., et al. (2020). Multimodal summarization with guidance of multimodal reference. Proceedings of the AAAI Conference on Artificial Intelligence, 34(05), 9749–975. 
                https://doi.org/10.1609/aaai.v34i05.6525
                
              ">
     2020
    </a>
    ; Jiang et al.,
    <a aria-label="Reference 2023" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR8" id="ref-link-section-d206753678e323" title="Jiang, C., Xie, R., Ye, W., et al. (2023). Exploiting pseudo image captions for multimodal summarization. In: Rogers A, Boyd-Graber J, Okazaki N (Eds.), Findings of the association for computational linguistics: ACL 2023. Association for Computational Linguistics, Toronto, Canada, pp 161–175. 
                https://doi.org/10.18653/v1/2023.findings-acl.12
                
              ">
     2023
    </a>
    ), effectively handling the relationship between the image and text modalities remains a challenging problem. Zhang et al. (
    <a aria-label="Reference 2022c" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR43" id="ref-link-section-d206753678e326" title="Zhang, Z., Meng, X., Wang, Y., et al. (2022c). Unims: A unified framework for multimodal summarization with knowledge distillation. Proceedings of the AAAI Conference on Artificial Intelligence 36(10) 11757–11764. 
                https://doi.org/10.1609/aaai.v36i10.21431
                
              ">
     2022c
    </a>
    ) extends the text encoder to a multimodal encoder, which takes the concatenation of textual and visual embeddings as input to obtain their contextualized joint representations. However, experimental results indicate that merely concatenating textual and visual embeddings cannot well capture the intricate relationships between modalities, such as sentence-image relationships. Jiang et al. (
    <a aria-label="Reference 2023" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR8" id="ref-link-section-d206753678e329" title="Jiang, C., Xie, R., Ye, W., et al. (2023). Exploiting pseudo image captions for multimodal summarization. In: Rogers A, Boyd-Graber J, Okazaki N (Eds.), Findings of the association for computational linguistics: ACL 2023. Association for Computational Linguistics, Toronto, Canada, pp 161–175. 
                https://doi.org/10.18653/v1/2023.findings-acl.12
                
              ">
     2023
    </a>
    ) takes sentences related to images as additional input rather than relying directly on image information. Although a image-text alignment mechanism was introduced in the sentence selection stage to leverage the relationships between images and sentences, this valuable relationship was not considered in the summarization stage.
   </p>
   <div class="c-article-section__figure js-c-reading-companion-figures-item" data-container-section="figure" data-test="figure" data-title="Fig. 1" id="figure-1">
    <figure>
     <figcaption>
      <b class="c-article-section__figure-caption" data-test="figure-caption-text" id="Fig1">
       Fig. 1
      </b>
     </figcaption>
     <div class="c-article-section__figure-content">
      <div class="c-article-section__figure-item">
       <a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-action="view figure" data-track-label="image" href="/article/10.1007/s10844-024-00886-5/figures/1" rel="nofollow">
        <picture>
         <source srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10844-024-00886-5/MediaObjects/10844_2024_886_Fig1_HTML.png?as=webp" type="image/webp"/>
         <img alt="figure 1" aria-describedby="Fig1" height="507" loading="lazy" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10844-024-00886-5/MediaObjects/10844_2024_886_Fig1_HTML.png" width="685"/>
        </picture>
       </a>
      </div>
      <div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc">
       <p>
        An example of multimodal summarization with multimodal output task
       </p>
      </div>
     </div>
     <div class="u-text-right u-hide-print">
      <a aria-label="Full size image figure 1" class="c-article__pill-button" data-test="article-link" data-track="click" data-track-action="view figure" data-track-dest="link:Figure1 Full size image" data-track-label="button" href="/article/10.1007/s10844-024-00886-5/figures/1" rel="nofollow">
       <span>
        Full size image
       </span>
       <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
        <use xlink:href="#icon-eds-i-chevron-right-small" xmlns:xlink="http://www.w3.org/1999/xlink">
        </use>
       </svg>
      </a>
     </div>
    </figure>
   </div>
   <p>
    We believe that there are extensive many-to-many relationships between images and sentences. By effectively leveraging both the relevance and irrelevance between sentences and images, we can more accurately extract salient sentences and critical images. Taking Fig.
    <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#Fig1">
     1
    </a>
    as an example, the first sentence succinctly summarizes the main points of the news article: “Eating fruits and vegetables could cut the risk of heart attacks and strokes.” This guides our selection of the vegetable image which is relevant to it. Compared to the third sentence, which is only related to the BMI image, the second sentence mentions both vegetables and BMI. In this case, further selecting the second sentence, which covers different aspects of the topic, can provide a more comprehensive summary.
   </p>
   <p>
    To effectively leverage the many-to-many relationships between sentences and images for multimodal summarization, an intuitive approach is to employ graph structures for modeling these relationships. In recent years, numerous studies (Jia et al.,
    <a aria-label="Reference 2020" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR7" id="ref-link-section-d206753678e359" title="Jia, R., Cao, Y., Tang, H., et al. (2020). Neural extractive summarization with hierarchical attentive heterogeneous graph network. In: Webber B, Cohn T, He Y, et al (Eds.), Proceedings of the 2020 conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics, Online, pp 3622–363. 
                https://doi.org/10.18653/v1/2020.emnlp-main.295
                
              ">
     2020
    </a>
    ; Song &amp; King,
    <a aria-label="Reference 2022" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR30" id="ref-link-section-d206753678e362" title="Song, Z., &amp; King, I. (2022). Hierarchical heterogeneous graph attention network for syntax-aware summarization. Proceedings of the AAAI Conference on Artificial Intelligence, 36(10), 11340–1134. 
                https://doi.org/10.1609/aaai.v36i10.21385
                
              ">
     2022
    </a>
    ) have been dedicated to exploring the application of Graph Neural Networks (GNNs) in the realm of text summarization with impressive results. Specifically, GNNs are able to model complex relationships between semantic units. By building graphs on semantic units, such as sentences, words (Wang et al.,
    <a aria-label="Reference 2020" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR35" id="ref-link-section-d206753678e365" title="Wang, D., Liu, P., Zheng, Y., et al. (2020). Heterogeneous graph neural networks for extractive document summarization. In: Jurafsky D, Chai J, Schluter N, et al (Eds.), Proceedings of the 58th annual meeting of the association for computational linguistics. Association for Computational Linguistics, Online, pp 6209–6219. 
                https://doi.org/10.18653/v1/2020.acl-main.553
                
              ">
     2020
    </a>
    ), latent topics (Cui et al.,
    <a aria-label="Reference 2020" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR5" id="ref-link-section-d206753678e368" title="Cui, P., Hu, L., &amp; Liu, Y. (2020). Enhancing extractive text summarization with topic-aware graph neural networks. In: Scott D, Bel N, Zong C (Eds.), Proceedings of the 28th international conference on computational linguistics. International Committee on Computational Linguistics, Barcelona, Spain (Online), pp 5360–537. 
                https://doi.org/10.18653/v1/2020.coling-main.468
                
              ">
     2020
    </a>
    ), or passages (Phan et al.,
    <a aria-label="Reference 2022" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR27" id="ref-link-section-d206753678e371" title="Phan TA, Nguyen NDN, &amp; Bui KHN. (2022). HeterGraphLongSum: Heterogeneous graph neural network with passage aggregation for extractive long document summarization. In: Calzolari N, Huang CR, Kim H, et al (Eds.), Proceedings of the 29th international conference on computational linguistics. International Committee on Computational Linguistics, Gyeongju, Republic of Korea, pp 6248–6258. 
                https://aclanthology.org/2022.coling-1.545
                
              ">
     2022
    </a>
    ), GNNs can enhance representations of semantic units for text summarization. Different from these works, we propose a novel heterogeneous graph for multimodal summarization. This graph includes nodes representing words, sentences, and images, with edges connecting sentences to both images and words. In this graph, images and sentences can serve as intermediaries for each other, thus enhancing their representations for multimodal summarization.
   </p>
   <p>
    To compute node representations of this graph, we propose the heterogeneous Graphormer (HeterGraphormer for short) by enhancing Graphormer (Ying et al.,
    <a aria-label="Reference 2021" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR39" id="ref-link-section-d206753678e378" title="Ying, C., Cai, T., Luo, S., et al. (2021). Do transformers really perform badly for graph representation? In: Ranzato M, Beygelzimer A, Dauphin YN, et al (Eds.), Advances in Neural Information Processing Systems, pp 28877–28888. 
                https://proceedings.neurips.cc/paper/2021/hash/f1c1592588411002af340cbaedd6fc33-Abstract.html
                
              ">
     2021
    </a>
    ) to effectively model intricate relationships between multiple modalities. Graphormer leverages self-attention to enable attention to all nodes when updating nodes, thereby alleviating the over-smoothing issue caused by traditional GNNs. We enhance Graphormer in the follow three aspect. First, we introduce type embedding and apply distinct spatial and edge embeddings for different heterogeneous edges to more effectively handle the heterogeneity of nodes and edges. Second, the centrality embedding was removed to optimize its performance for document graphs. Third, unconnected nodes are considered during node updates, as unrelated relationships are also valuable.
   </p>
   <p>
    Overall, we propose a heterogeneous graph-based model for multimodal summarization (HGMS). The model first constructs a heterogeneous graph containing nodes for words, sentences and images. Subsequently, HeterGraphormer is employed to iteratively update the representations of nodes in the heterogeneous graph, aiming to more effectively model intricate relationships between nodes. Experimental results show that our model significantly enhances the performance of multimodal summarization. The contributions of our paper are as follows:
   </p>
   <ol class="u-list-style-none">
    <li>
     <span class="u-custom-list-number">
      1.
     </span>
     <p>
      We propose a heterogeneous graph-based model for multimodal summarization to efficiently leverage intermodal relationships through the multimodal graph we constructed.
     </p>
    </li>
    <li>
     <span class="u-custom-list-number">
      2.
     </span>
     <p>
      We propose HeterGraphormer to effectively model intricate relationships between multiple modalities. To the best of our knowledge, we are the first to apply Graphormer in the field of graph-based summarization.
     </p>
    </li>
    <li>
     <span class="u-custom-list-number">
      3.
     </span>
     <p>
      Our proposed model has achieved state-of-the-art (SOTA) performance on the MSMO dataset. Extensive experiments demonstrate the effectiveness of our proposed HeterGraphormer.
     </p>
    </li>
   </ol>
  </div>
 </div>
</section>
<section data-title="Related work">
 <div class="c-article-section" id="Sec2-section">
  <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">
   <span class="c-article-section__title-number">
    2
   </span>
   Related work
  </h2>
  <div class="c-article-section__content" id="Sec2-content">
   <h3 class="c-article__sub-heading" id="Sec3">
    <span class="c-article-section__title-number">
     2.1
    </span>
    Extractive summarization
   </h3>
   <p>
    Text summarization is the task of generating concise summaries for original long documents. Existing text summarization methods can be categorized into extractive and abstractive approaches. Abstractive summarization (Sutskever et al.,
    <a aria-label="Reference 2014" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR31" id="ref-link-section-d206753678e431" title="Sutskever, I., Vinyals, O., &amp; Le, Q.V. (2014). Sequence to sequence learning with neural networks. In: Ghahramani Z, Welling M, Cortes C, et al (Eds.), Advances in Neural Information Processing Systems,[SPACE]
                https://proceedings.neurips.cc/paper/2014/hash/a14ac55a4f27472c5d894ec1c3c743d2-Abstract.html
                
              ">
     2014
    </a>
    ; See et al.,
    <a aria-label="Reference 2017" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR29" id="ref-link-section-d206753678e434" title="See, A., Liu, P.J., Manning, C.D. (2017). Get to the point: Summarization with pointer-generator networks. In: Barzilay R, Kan MY (Eds.), Proceedings of the 55th annual meeting of the association for computational linguistics (Volume 1: Long Papers). Association for Computational Linguistics, Vancouver, Canada, pp 1073–1083. 
                https://doi.org/10.18653/v1/P17-1099
                
              ">
     2017
    </a>
    ; Paulus et al.,
    <a aria-label="Reference 2018" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR25" id="ref-link-section-d206753678e437" title="Paulus, R., Xiong, C., &amp; Socher, R. (2018). A deep reinforced model for abstractive summarization. In: International conference on learning representations, 
                https://openreview.net/forum?id=HkAClQgA-
                
              ">
     2018
    </a>
    ; Zhang et al.,
    <a aria-label="Reference 2020" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR40" id="ref-link-section-d206753678e440" title="Zhang, J., Zhao, Y., Saleh, M., et al. (2020). PEGASUS: pre-training with extracted gap-sentences for abstractive summarization. In: Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, pp 11328–11339. 
                http://proceedings.mlr.press/v119/zhang20ae.html
                
              ">
     2020
    </a>
    ; Liu et al.,
    <a aria-label="Reference 2022" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR19" id="ref-link-section-d206753678e443" title="Liu, Y., Liu, P., Radev, D., et al. (2022). BRIO: Bringing order to abstractive summarization. In: Muresan S, Nakov P, Villavicencio A (Eds.), Proceedings of the 60th annual meeting of the association for computational linguistics (Volume 1: Long Papers). Association for Computational Linguistics, Dublin, Ireland, pp 2890–2903. 
                https://doi.org/10.18653/v1/2022.acl-long.207
                
              ">
     2022
    </a>
    ) generate a summary from scratch, typically with lower redundancy. Extractive summarization (Cheng &amp; Lapata,
    <a aria-label="Reference 2016" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR3" id="ref-link-section-d206753678e447" title="Cheng, J., &amp; Lapata, M. (2016). Neural summarization by extracting sentences and words. In: Erk K, Smith NA (Eds.), Proceedings of the 54th annual meeting of the association for computational linguistics (Volume 1: Long Papers). Association for Computational Linguistics, Berlin, Germany, pp 484–494. 
                https://doi.org/10.18653/v1/P16-1046
                
              ">
     2016
    </a>
    ; An et al.,
    <a aria-label="Reference 2022" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR1" id="ref-link-section-d206753678e450" title="An, C., Zhong, M., Wu, Z., et al. (2022). CoLo: A contrastive learning based re-ranking framework for one-stage summarization. In: Calzolari N, Huang CR, Kim H, et al (Eds.), Proceedings of the 29th international conference on computational linguistics. International Committee on Computational Linguistics, Gyeongju, Republic of Korea, pp 5783–5793. 
                https://aclanthology.org/2022.coling-1.508
                
              ">
     2022
    </a>
    ; Cheng et al.,
    <a aria-label="Reference 2023" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR4" id="ref-link-section-d206753678e453" title="Cheng, X., Shen, Y., &amp; Lu, W. (2023). A set prediction network for extractive summarization. In: Rogers A, Boyd-Graber J, Okazaki N (Eds.), Findings of the association for computational linguistics: ACL 2023. Association for Computational Linguistics, Toronto, Canada, pp 4766–4777. 
                https://doi.org/10.18653/v1/2023.findings-acl.293
                
              ">
     2023
    </a>
    ) aims to select salient sentences from original documents.
   </p>
   <p>
    Recent research work on extractive summarization covers a variety of methods. SummaRuNNer (Nallapati et al.,
    <a aria-label="Reference 2017" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR22" id="ref-link-section-d206753678e459" title="Nallapati R, Zhai F, &amp; Zhou B. (2017) .Summarunner: A recurrent neural network based sequence model for extractive summarization of documents. Proceedings of the AAAI Conference on Artificial Intelligence 31(1). 
                https://doi.org/10.1609/aaai.v31i1.10958
                
              ">
     2017
    </a>
    ) is one of the earliest models to employ an encoder based on Recurrent Neural Networks (RNNs). Narayan et al. (
    <a aria-label="Reference 2018" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR23" id="ref-link-section-d206753678e462" title="Narayan S, Cohen SB, &amp; Lapata M. (2018). Ranking sentences for extractive summarization with reinforcement learning. In: Walker M, Ji H, Stent A (Eds.) Proceedings of the 2018 conference of the North American chapter of the association for computational linguistics: Human language technologies, Volume 1 (Long Papers). Association for Computational Linguistics, New Orleans, Louisiana, pp 1747–1759. 
                https://doi.org/10.18653/v1/N18-1158
                
              ">
     2018
    </a>
    ) proposed a reinforcement learning-based system trained by globally optimizing the ROUGE metric. Liu and Lapata (
    <a aria-label="Reference 2019" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR17" id="ref-link-section-d206753678e465" title="Liu Y, &amp; Lapata M. (2019). Text summarization with pretrained encoders. In: Inui K, Jiang J, Ng V, et al (Eds.), Proceedings of the 2019 conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). Association for Computational Linguistics, Hong Kong, China, pp 3730–3740. 
                https://doi.org/10.18653/v1/D19-1387
                
              ">
     2019
    </a>
    ) leverages the capability of pre-trained language models to effectively capture contextual features, thereby significantly enhancing the performance of summarization. Zhong et al. (
    <a aria-label="Reference 2020" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR44" id="ref-link-section-d206753678e468" title="Zhong, M., Liu, P., Chen, Y., et al. (2020). Extractive summarization as text matching. In: Jurafsky D, Chai J, Schluter N, et al (Eds.) Proceedings of the 58th annual meeting of the association for computational linguistics. Association for Computational Linguistics, Online, pp 6197–6208. 
                https://doi.org/10.18653/v1/2020.acl-main.552
                
              ">
     2020
    </a>
    ) formulates the extractive summarization task as a semantic text matching problem, selecting the set of sentences that most closely match the source document in the semantic space as the summary. Tang et al. (
    <a aria-label="Reference 2022" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR32" id="ref-link-section-d206753678e471" title="Tang, P., Hu, K., Yan, R., et al. (2022). OTExtSum: Extractive text summarisation with optimal transport. In: Carpuat M, de Marneffe MC, Meza Ruiz IV (eds) Findings of the association for computational linguistics: NAACL 2022. Association for Computational Linguistics, Seattle, United States, pp 1128–1141. 
                https://doi.org/10.18653/v1/2022.findings-naacl.85
                
              ">
     2022
    </a>
    ) formulates the extractive summarization task as an Optimal Transport (OT) problem from document to summary.
   </p>
   <h3 class="c-article__sub-heading" id="Sec4">
    <span class="c-article-section__title-number">
     2.2
    </span>
    Multimodal summarization
   </h3>
   <p>
    Multimodal summarization takes multiple modalities of data as input, and outputs either text-only (Li et al.,
    <a aria-label="Reference 2018" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR13" id="ref-link-section-d206753678e482" title="Li, H., Zhu, J., Liu, T., et al. (2018). Multi-modal sentence summarization with modality attention and image filtering. In: Lang J (Ed.), Proceedings of the twenty-seventh international joint conference on artificial intelligence, IJCAI 2018, July 13-19, 2018, Stockholm, Sweden. ijcai.org, Stockholm, Sweden, pp 4152–4158. 
                https://doi.org/10.24963/ijcai.2018/577
                
              ">
     2018
    </a>
    ; Overbay et al.,
    <a aria-label="Reference 2023" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR24" id="ref-link-section-d206753678e485" title="Overbay, K., Ahn, J., Pesaran zadeh, F., et al. (2023). mRedditSum: A multimodal abstractive summarization dataset of Reddit threads with images. In: Bouamor H, Pino J, Bali K (Eds.), Proceedings of the 2023 conference on empirical methods in natural language processing. Association for Computational Linguistics, Singapore, pp 4117–4132. 
                https://doi.org/10.18653/v1/2023.emnlp-main.251
                
              ">
     2023
    </a>
    ; Liang et al.,
    <a aria-label="Reference 2023" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR16" id="ref-link-section-d206753678e488" title="Liang, Y., Meng, F., Xu, J., et al. (2023). Summary-oriented vision modeling for multimodal abstractive summarization. In: Rogers A, Boyd-Graber J, Okazaki N (Eds.), Proceedings of the 61st annual meeting of the association for computational linguistics (Volume 1: Long Papers). Association for Computational Linguistics, Toronto, Canada, pp 2934–2951. 
                https://doi.org/10.18653/v1/2023.acl-long.165
                
              ">
     2023
    </a>
    ) or multimodal (Li et al.,
    <a aria-label="Reference 2020" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR15" id="ref-link-section-d206753678e491" title="Li, M., Chen, X., Gao, S., et al (2020) VMSMO: Learning to generate multimodal summary for video-based news articles. In: Webber B, Cohn T, He Y, et al (Eds.), Proceedings of the 2020 conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics, Online, pp 9360–9369. 
                https://doi.org/10.18653/v1/2020.emnlp-main.752
                
              ">
     2020
    </a>
    ; Zhang et al.,
    <a aria-label="Reference 2022a" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR41" id="ref-link-section-d206753678e494" title="Zhang, L., Zhang, X., &amp; Pan, J. (2022a). Hierarchical cross-modality semantic correlation learning model for multimodal summarization. Proceedings of the AAAI Conference on Artificial Intelligence 36(10) 11676–11684. 
                https://doi.org/10.1609/aaai.v36i10.21422
                
              ">
     2022a
    </a>
    ) summary.
   </p>
   <p>
    For multimodal summarization with text-only output, Chen and Zhuge (
    <a aria-label="Reference 2018" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR2" id="ref-link-section-d206753678e500" title="Chen, J., &amp; Zhuge, H. (2018). Abstractive text-image summarization using multi-modal attentional hierarchical RNN. In: Riloff E, Chiang D, Hockenmaier J, et al (Eds.), Proceedings of the 2018 conference on empirical methods in natural language processing. Association for Computational Linguistics, Brussels, Belgium, pp 4046–4056. 
                https://doi.org/10.18653/v1/D18-1438
                
              ">
     2018
    </a>
    ) proposed an abstractive text-image summarization method using a multi-modal attentional hierarchical RNN, which integrates textual and visual information to produce a textual summary. Zhang et al. (
    <a aria-label="Reference 2022b" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR42" id="ref-link-section-d206753678e503" title="Zhang, M., Zhou, G., Yu, W., et al. (2022b). MAA-PTG: multimodal aspect-aware product title generation. J Intell Inf Syst 59(1) 213–235. 
                https://doi.org/10.1007/S10844-022-00695-8
                
              ">
     2022b
    </a>
    ) introduces an aspect precision score function to assess summary relevance and incorporates it using reinforcement learning. Xiao et al. (
    <a aria-label="Reference 2023" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR37" id="ref-link-section-d206753678e506" title="Xiao M, Zhu J, Lin H, et al. (2023). CFSum coarse-to-fine contribution network for multimodal summarization. In: Rogers A, Boyd-Graber J, Okazaki N (Eds.), Proceedings of the 61st annual meeting of the association for computational linguistics (Volume 1: Long Papers). Association for Computational Linguistics, Toronto, Canada, pp 8538–8553. 
                https://doi.org/10.18653/v1/2023.acl-long.476
                
              ">
     2023
    </a>
    ) proposed a coarse-to-fine contribution network to model different contributions of images for summarization. Jin and Chen (
    <a aria-label="Reference 2024" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR9" id="ref-link-section-d206753678e509" title="Jin, L., &amp; Chen, J. (2024). Self-supervised opinion summarization with multi-modal knowledge graph. J Intell Inf Syst, 62(1), 191–208. 
                https://doi.org/10.1007/S10844-023-00812-1
                
              ">
     2024
    </a>
    ) proposes a multimodal knowledge graph constructed from entities in text and images, using structural knowledge in a self-supervised manner.
   </p>
   <p>
    For multimodal summarization with multimodal output, the task generates summaries that include multiple modalities, such as text and images, to provide richer and more comprehensive information. Zhu et al. (
    <a aria-label="Reference 2018" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR45" id="ref-link-section-d206753678e515" title="Zhu, J., Li, H., Liu, T., et al. (2018). MSMO: Multimodal summarization with multimodal output. In: Riloff E, Chiang D, Hockenmaier J, et al (eds) Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Brussels, Belgium, pp 4154–4164. 
                https://doi.org/10.18653/v1/D18-1448
                
              ">
     2018
    </a>
    ) first proposed generating a pictorial summary given a document and a collection of images. Zhu et al. (
    <a aria-label="Reference 2020" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR46" id="ref-link-section-d206753678e518" title="Zhu, J., Zhou, Y., Zhang, J., et al. (2020). Multimodal summarization with guidance of multimodal reference. Proceedings of the AAAI Conference on Artificial Intelligence, 34(05), 9749–975. 
                https://doi.org/10.1609/aaai.v34i05.6525
                
              ">
     2020
    </a>
    ) further introduced a extra cross-entropy loss for image selection. Zhang et al. (
    <a aria-label="Reference 2022c" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR43" id="ref-link-section-d206753678e521" title="Zhang, Z., Meng, X., Wang, Y., et al. (2022c). Unims: A unified framework for multimodal summarization with knowledge distillation. Proceedings of the AAAI Conference on Artificial Intelligence 36(10) 11757–11764. 
                https://doi.org/10.1609/aaai.v36i10.21431
                
              ">
     2022c
    </a>
    ) utilizes knowledge distillation with a vision-language pre-trained model to help image selection. Xie et al. (
    <a aria-label="Reference 2023" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR38" id="ref-link-section-d206753678e524" title="Xie, F., Chen, J., &amp; Chen, K. (2023). Extractive text-image summarization with relation-enhanced graph attention network. J Intell Inf Syst, 61(2), 325–341. 
                https://doi.org/10.1007/S10844-022-00757-X
                
              ">
     2023
    </a>
    ) introduces a graph structure, leveraging image captions to establish indirect connections between images and sentences via word nodes, thereby enhancing the association between images and text. Jiang et al. (
    <a aria-label="Reference 2023" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR8" id="ref-link-section-d206753678e527" title="Jiang, C., Xie, R., Ye, W., et al. (2023). Exploiting pseudo image captions for multimodal summarization. In: Rogers A, Boyd-Graber J, Okazaki N (Eds.), Findings of the association for computational linguistics: ACL 2023. Association for Computational Linguistics, Toronto, Canada, pp 161–175. 
                https://doi.org/10.18653/v1/2023.findings-acl.12
                
              ">
     2023
    </a>
    ) introduced an image-text alignment mechanism to convert input images into the most relevant sentences to better capture visual knowledge. However, previous studies have not adequately considered the many-to-many relationships between sentences and images. In this study, we model these relationships using a graph structure to more effectively utilize this crucial information.
   </p>
   <h3 class="c-article__sub-heading" id="Sec5">
    <span class="c-article-section__title-number">
     2.3
    </span>
    Graph-based summarization
   </h3>
   <p>
    Early work focused on utilizing content similarity between sentences to construct document graphs and extracting sentences in an unsupervised manner such as TextRank (Mihalcea &amp; Tarau,
    <a aria-label="Reference 2004" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR21" id="ref-link-section-d206753678e538" title="Mihalcea, R., Tarau, P (2004) TextRank: Bringing order into text. In: Lin D, Wu D (Eds.), Proceedings of the 2004 conference on empirical methods in natural language processing. Association for Computational Linguistics, Barcelona, Spain, pp 404–411. 
                https://aclanthology.org/W04-3252
                
              ">
     2004
    </a>
    ) and LexRank (Erkan &amp; Radev,
    <a aria-label="Reference 2004" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR6" id="ref-link-section-d206753678e541" title="Erkan, G., &amp; Radev, D. R. (2004). Lexrank: Graph-based lexical centrality as salience in text summarization. J Artif Intell Res, 22, 457–47. 
                https://doi.org/10.1613/JAIR.1523
                
              ">
     2004
    </a>
    ). Recently, the application of GNNs in text summarization task has attracted widespread attention. Wang et al. (
    <a aria-label="Reference 2020" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR35" id="ref-link-section-d206753678e544" title="Wang, D., Liu, P., Zheng, Y., et al. (2020). Heterogeneous graph neural networks for extractive document summarization. In: Jurafsky D, Chai J, Schluter N, et al (Eds.), Proceedings of the 58th annual meeting of the association for computational linguistics. Association for Computational Linguistics, Online, pp 6209–6219. 
                https://doi.org/10.18653/v1/2020.acl-main.553
                
              ">
     2020
    </a>
    ) constructed a heterogeneous graph, enriching the cross-sentence relations through the word nodes between sentences. Jia et al. (
    <a aria-label="Reference 2020" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR7" id="ref-link-section-d206753678e547" title="Jia, R., Cao, Y., Tang, H., et al. (2020). Neural extractive summarization with hierarchical attentive heterogeneous graph network. In: Webber B, Cohn T, He Y, et al (Eds.), Proceedings of the 2020 conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics, Online, pp 3622–363. 
                https://doi.org/10.18653/v1/2020.emnlp-main.295
                
              ">
     2020
    </a>
    ) proposed a hierarchical heterogeneous graph to extract sentences by simultaneously balancing salience and redundancy. Cui et al. (
    <a aria-label="Reference 2020" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR5" id="ref-link-section-d206753678e550" title="Cui, P., Hu, L., &amp; Liu, Y. (2020). Enhancing extractive text summarization with topic-aware graph neural networks. In: Scott D, Bel N, Zong C (Eds.), Proceedings of the 28th international conference on computational linguistics. International Committee on Computational Linguistics, Barcelona, Spain (Online), pp 5360–537. 
                https://doi.org/10.18653/v1/2020.coling-main.468
                
              ">
     2020
    </a>
    ) incorporates latent topics into graph propagation via a joint neural topic model, facilitating the extraction of crucial information from documents. Jing et al. (
    <a aria-label="Reference 2021" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR10" id="ref-link-section-d206753678e554" title="Jing, B., You, Z., Yang, T., et al. (2021). Multiplex graph neural network for extractive text summarization. In: Moens MF, Huang X, Specia L, et al. (Eds.), Proceedings of the 2021 conference on empirical methods in natural language processing. Association for Computational Linguistics, Online and Punta Cana, Dominican Republic, pp 133–139. 
                https://doi.org/10.18653/v1/2021.emnlp-main.11
                
              ">
     2021
    </a>
    ) proposed to use multiplex graph to model different types of relationships among sentences and words. Song and King (
    <a aria-label="Reference 2022" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR30" id="ref-link-section-d206753678e557" title="Song, Z., &amp; King, I. (2022). Hierarchical heterogeneous graph attention network for syntax-aware summarization. Proceedings of the AAAI Conference on Artificial Intelligence, 36(10), 11340–1134. 
                https://doi.org/10.1609/aaai.v36i10.21385
                
              ">
     2022
    </a>
    ) obtains sentence representations based on constituency trees to leverage syntactic information. Phan et al. (
    <a aria-label="Reference 2022" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR27" id="ref-link-section-d206753678e560" title="Phan TA, Nguyen NDN, &amp; Bui KHN. (2022). HeterGraphLongSum: Heterogeneous graph neural network with passage aggregation for extractive long document summarization. In: Calzolari N, Huang CR, Kim H, et al (Eds.), Proceedings of the 29th international conference on computational linguistics. International Committee on Computational Linguistics, Gyeongju, Republic of Korea, pp 6248–6258. 
                https://aclanthology.org/2022.coling-1.545
                
              ">
     2022
    </a>
    ) introduce passage nodes into the sentence-word graph to enrich the representation of sentences in long documents. In this study, we construct a novel heterogeneous graph for multimodal summarization based on the interrelations between different modalities. Additionally, we propose HeterGraphormer to more effectively model the intricate relationships within the graph.
   </p>
   <div class="c-article-section__figure js-c-reading-companion-figures-item" data-container-section="figure" data-test="figure" data-title="Fig. 2" id="figure-2">
    <figure>
     <figcaption>
      <b class="c-article-section__figure-caption" data-test="figure-caption-text" id="Fig2">
       Fig. 2
      </b>
     </figcaption>
     <div class="c-article-section__figure-content">
      <div class="c-article-section__figure-item">
       <a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-action="view figure" data-track-label="image" href="/article/10.1007/s10844-024-00886-5/figures/2" rel="nofollow">
        <picture>
         <source srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10844-024-00886-5/MediaObjects/10844_2024_886_Fig2_HTML.png?as=webp" type="image/webp"/>
         <img alt="figure 2" aria-describedby="Fig2" height="280" loading="lazy" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10844-024-00886-5/MediaObjects/10844_2024_886_Fig2_HTML.png" width="685"/>
        </picture>
       </a>
      </div>
      <div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc">
       <p>
        Overview of our proposed HGMS
       </p>
      </div>
     </div>
     <div class="u-text-right u-hide-print">
      <a aria-label="Full size image figure 2" class="c-article__pill-button" data-test="article-link" data-track="click" data-track-action="view figure" data-track-dest="link:Figure2 Full size image" data-track-label="button" href="/article/10.1007/s10844-024-00886-5/figures/2" rel="nofollow">
       <span>
        Full size image
       </span>
       <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
        <use xlink:href="#icon-eds-i-chevron-right-small" xmlns:xlink="http://www.w3.org/1999/xlink">
        </use>
       </svg>
      </a>
     </div>
    </figure>
   </div>
  </div>
 </div>
</section>
<section data-title="Method">
 <div class="c-article-section" id="Sec6-section">
  <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec6">
   <span class="c-article-section__title-number">
    3
   </span>
   Method
  </h2>
  <div class="c-article-section__content" id="Sec6-content">
   <h3 class="c-article__sub-heading" id="Sec7">
    <span class="c-article-section__title-number">
     3.1
    </span>
    Problem formulation
   </h3>
   <p>
    Given a multimodal document
    <span class="mathjax-tex">
     \(\{T, P\}\)
    </span>
    , where
    <span class="mathjax-tex">
     \(T=\{s_1,s_2,\cdots ,s_m\}\)
    </span>
    is a text document consisting of
    <i>
     m
    </i>
    sentences and
    <span class="mathjax-tex">
     \(P=\{p_1,p_2,\cdots ,p_k\}\)
    </span>
    is a collection of
    <i>
     k
    </i>
    images, the model summarizes multimodal document into a multimodal summary
    <span class="mathjax-tex">
     \(\mathcal {S}=\{\mathcal {S}_t,\mathcal {S}_p\}\)
    </span>
    , where
    <span class="mathjax-tex">
     \(\mathcal {S}_t\)
    </span>
    denotes the textual summary extracted from
    <i>
     T
    </i>
    and
    <span class="mathjax-tex">
     \(\mathcal {S}_p\)
    </span>
    is a subset of images selected from the image collection
    <i>
     P
    </i>
    .
   </p>
   <h3 class="c-article__sub-heading" id="Sec8">
    <span class="c-article-section__title-number">
     3.2
    </span>
    Graph construction
   </h3>
   <p>
    In order to model the relationship between sentences and images, we construct a heterogeneous graph, as shown in the Fig.
    <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#Fig2">
     2
    </a>
    . In this graph, there are three types of nodes: word, sentence, and image. Each word node corresponds to the identical word appearing multiple times in the document. Each sentence and image node corresponds to the sentences and images in the document.
   </p>
   <p>
    For the connection between nodes, we first connect words and sentences through inclusion relationships. To enhance model efficiency, we retained only those word nodes connected to two or more sentences. Furthermore, we connect all sentence and image pairs, rather than limiting connections to relevant pairs. In order to accurately discern the significance of connections, we introduce TF-IDF and Image-Text Match (ITM) scores as the weights of these two edges respectively.
   </p>
   <p>
    Given a undirected graph
    <span class="mathjax-tex">
     \(G = \{V, E\}\)
    </span>
    ,where
    <i>
     V
    </i>
    denotes the set of nodes and
    <i>
     E
    </i>
    represents edge weights between nodes. Our heterogeneous graph can be formally defined as
    <span class="mathjax-tex">
     \( V=V_w \cup V_s \cup V_p \)
    </span>
    and
    <span class="mathjax-tex">
     \(E=E_{sw} \cup E_{sp} \)
    </span>
    , where
    <span class="mathjax-tex">
     \(V_w\)
    </span>
    ,
    <span class="mathjax-tex">
     \(V_s\)
    </span>
    , and
    <span class="mathjax-tex">
     \(V_p\)
    </span>
    denote the sets of word, sentence, and image nodes, respectively.
    <span class="mathjax-tex">
     \(E_{sw}=\{e^{sw}_{11}, \cdots , e^{sw}_{mu}\}\)
    </span>
    and
    <span class="mathjax-tex">
     \(E_{sp}=\{e^{sp}_{11}, \cdots , e^{sp}_{mk}\}\)
    </span>
    represent sentence-word and sentence-image edge weights respectively. Here
    <span class="mathjax-tex">
     \(e^{sw}_{ij} \in [0, 1]\)
    </span>
    denotes the TF-IDF score between sentence
    <span class="mathjax-tex">
     \(s_i\)
    </span>
    and word
    <span class="mathjax-tex">
     \(w_j\)
    </span>
    , while
    <span class="mathjax-tex">
     \(e^{sp}_{ij} \in [0, 1]\)
    </span>
    represents the ITM score between sentence
    <span class="mathjax-tex">
     \(s_i\)
    </span>
    and image
    <span class="mathjax-tex">
     \(p_j\)
    </span>
    , computed via the BLIP (Li et al.,
    <a aria-label="Reference 2022" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR14" id="ref-link-section-d206753678e1412" title="Li, J., Li, D., Xiong, C., et al. (2022). BLIP: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In: Chaudhuri K, Jegelka S, Song L, et al (Eds.), International conference on machine learning, ICML 2022, 17-23 July 2022, PMLR, Baltimore, Maryland, USA, Proceedings of Machine Learning Research, 162 pp 12888–12900">
     2022
    </a>
    ).
   </p>
   <p>
    For node features, we use GloVe (Pennington et al.,
    <a aria-label="Reference 2014" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR26" id="ref-link-section-d206753678e1418" title="Pennington, J., Socher, R., &amp; Manning, C. (2014). GloVe: Global vectors for word representation. In: Moschitti A, Pang B, Daelemans W (Eds.) Proceedings of the 2014 conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics, Doha, Qatar, pp 1532–1543. 
                https://doi.org/10.3115/v1/D14-1162
                
              ">
     2014
    </a>
    ), RoBERTa (Liu et al.,
    <a aria-label="Reference 2019" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR18" id="ref-link-section-d206753678e1421" title="Liu, Y., Ott, M., Goyal, N., et al. (2019). Roberta: A robustly optimized BERT pretraining approach. 
                arXiv:1907.11692
                
              
                https://arxiv.org/abs/1907.11692
                
              ">
     2019
    </a>
    ), and BLIP to obtain node representations of words, sentences, and images, respectively.
   </p>
   <div class="c-article-equation" id="Equ1">
    <div class="c-article-equation__content">
     <span class="mathjax-tex">
      $$\begin{aligned} x^w_i&amp;= \text {GloVe}(w_i),\end{aligned}$$
     </span>
    </div>
    <div class="c-article-equation__number">
     (1)
    </div>
   </div>
   <div class="c-article-equation" id="Equ2">
    <div class="c-article-equation__content">
     <span class="mathjax-tex">
      $$\begin{aligned} x^s_i&amp;= \text {RoBERTa}(s_i)W_s + \text {PE}(i),\end{aligned}$$
     </span>
    </div>
    <div class="c-article-equation__number">
     (2)
    </div>
   </div>
   <div class="c-article-equation" id="Equ3">
    <div class="c-article-equation__content">
     <span class="mathjax-tex">
      $$\begin{aligned} x^p_i&amp;= \text {BLIP}(p_i)W_p, \end{aligned}$$
     </span>
    </div>
    <div class="c-article-equation__number">
     (3)
    </div>
   </div>
   <p>
    where
    <span class="mathjax-tex">
     \( x^w_i \in \mathbb {R}^d \)
    </span>
    ,
    <span class="mathjax-tex">
     \( W_s \in \mathbb {R}^{d_s \times d} \)
    </span>
    ,
    <span class="mathjax-tex">
     \( W_p \in \mathbb {R}^{d_p \times d} \)
    </span>
    are projection matrices and
    <span class="mathjax-tex">
     \(\text {PE}(i)\)
    </span>
    denotes the Position Encoding proposed by Vaswani et al. (
    <a aria-label="Reference 2017" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR33" id="ref-link-section-d206753678e1817" title="Vaswani A, Shazeer N, Parmar N, et al. (2017). Attention is all you need. In: Guyon I, Luxburg UV, Bengio S, et al (Eds.), Advances in Neural Information Processing Systems,[SPACE]
                https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html
                
              ">
     2017
    </a>
    ).
   </p>
   <h3 class="c-article__sub-heading" id="Sec9">
    <span class="c-article-section__title-number">
     3.3
    </span>
    Graph encoder
   </h3>
   <p>
    Graphormer integrates graph structural information into the Transformer (Vaswani et al.,
    <a aria-label="Reference 2017" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR33" id="ref-link-section-d206753678e1828" title="Vaswani A, Shazeer N, Parmar N, et al. (2017). Attention is all you need. In: Guyon I, Luxburg UV, Bengio S, et al (Eds.), Advances in Neural Information Processing Systems,[SPACE]
                https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html
                
              ">
     2017
    </a>
    ) architecture using centrality embedding, spatial embedding, and edge embedding to effectively learn node representations. To better apply it to our constructed heterogeneous graph, we propose HeterGraphormer, which introduces several enhancements designed to better model the relationships between heterogeneous nodes.
   </p>
   <p>
    Before input, we add type embedding to each node to distinguish different node types. Let
    <i>
     h
    </i>
    denote the node’s hidden state, and the initial hidden state can be defined as
   </p>
   <div class="c-article-equation" id="Equ4">
    <div class="c-article-equation__content">
     <span class="mathjax-tex">
      $$\begin{aligned} h^{\tau }_i = x^{\tau }_i + z_{\tau }, \end{aligned}$$
     </span>
    </div>
    <div class="c-article-equation__number">
     (4)
    </div>
   </div>
   <p>
    where
    <span class="mathjax-tex">
     \(z \in \mathbb {R}^d\)
    </span>
    are learnable embedding vectors specified by the node type
    <span class="mathjax-tex">
     \(\tau \in \{w,s,i\}\)
    </span>
    .
   </p>
   <p>
    It is noteworthy that we did not use centrality embedding, which adds embeddings to node representations based on the degrees of nodes to differentiate their importance, as the degree of nodes in document graphs does not accurately reflect their significance. We prove this assumption via the ablation study in the experiment section.
   </p>
   <h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec10">
    <span class="c-article-section__title-number">
     3.3.1
    </span>
    Attention with spatial embedding
   </h4>
   <p>
    Due to the global receptive field characteristic of the Transformer, it can effectively attend to information from any position when updating node representations. To differentiate nodes at different distances, Graphormer introduces spatial embedding, which assigns a unique embedding for each distance as a weight in the self-attention module. This enables the model to accurately distinguish distance variances between nodes in the graph while maintaining its capacity for global information processing. Following  Ying et al. (
    <a aria-label="Reference 2021" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR39" id="ref-link-section-d206753678e1983" title="Ying, C., Cai, T., Luo, S., et al. (2021). Do transformers really perform badly for graph representation? In: Ranzato M, Beygelzimer A, Dauphin YN, et al (Eds.), Advances in Neural Information Processing Systems, pp 28877–28888. 
                https://proceedings.neurips.cc/paper/2021/hash/f1c1592588411002af340cbaedd6fc33-Abstract.html
                
              ">
     2021
    </a>
    ), we adopt the shortest path distance as the distance metric between nodes, as it is the most direct and widely used approach.
   </p>
   <p>
    Concretely, let
    <span class="mathjax-tex">
     \(\phi (i,j)\)
    </span>
    denote the distance between nodes
    <span class="mathjax-tex">
     \(v_i\)
    </span>
    and
    <span class="mathjax-tex">
     \(v_j\)
    </span>
    . If a path exists between the nodes,
    <span class="mathjax-tex">
     \(\phi \)
    </span>
    outputs the shortest path distance between them; if not, it outputs a special value, i.e., -1. In this paper, we consider both the relevance and irrelevance between different modalities. For each feasible value and the -1 value corresponding to unconnected heterogeneous nodes, we assign a learnable scalar that serves as a weight in the self-attention module. Denote
    <span class="mathjax-tex">
     \(A_{ij}\)
    </span>
    as the (
    <i>
     i
    </i>
    ,
    <i>
     j
    </i>
    )-element of the attention matrix
    <i>
     A
    </i>
    , we have:
   </p>
   <div class="c-article-equation" id="Equ5">
    <div class="c-article-equation__content">
     <span class="mathjax-tex">
      $$\begin{aligned} A_{ij}=\frac{(h_iW_{Q})(h_jW_{K})^T}{\sqrt{d_k}} \cdot b_{\phi (i,j)}, \end{aligned}$$
     </span>
    </div>
    <div class="c-article-equation__number">
     (5)
    </div>
   </div>
   <p>
    where
    <span class="mathjax-tex">
     \(b_{\phi (i,j)}\)
    </span>
    is a learnable scalar indexed by
    <span class="mathjax-tex">
     \(\phi (i,j)\)
    </span>
    , and shared across all layers.
    <span class="mathjax-tex">
     \(W_{Q}\)
    </span>
    and
    <span class="mathjax-tex">
     \(W_{K}\)
    </span>
    are learnable weight matrices used to transform
    <span class="mathjax-tex">
     \(h_i\)
    </span>
    and
    <span class="mathjax-tex">
     \(h_j\)
    </span>
    into query and key vectors, respectively.
    <span class="mathjax-tex">
     \(d_k\)
    </span>
    denotes the dimensionality of the key vectors.
   </p>
   <h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec11">
    <span class="c-article-section__title-number">
     3.3.2
    </span>
    Edge embedding
   </h4>
   <p>
    Edge embedding is proposed to encode edge features, allowing the model to incorporate edge information when updating node representations.
   </p>
   <p>
    We use the product of weights along the shortest path as the weight between two nodes, rather than averaging, because the product more accurately reflects the connectivity of the path. For example, when the shortest path
    <span class="mathjax-tex">
     \(\text {SP}_{ij} = \{1, 0\}\)
    </span>
    , the product is 0, which more accurately represents the connection strength compared to the average value of 0.5. Concretely, let
    <span class="mathjax-tex">
     \(\omega (i,j)\)
    </span>
    denotes the product of weights along one of the shortest paths
    <span class="mathjax-tex">
     \(\text {SP}_{ij}=(e_1,e_2,\cdots ,e_N)\)
    </span>
    from
    <span class="mathjax-tex">
     \(v_i\)
    </span>
    to
    <span class="mathjax-tex">
     \(v_j\)
    </span>
    , expressed as follows:
   </p>
   <div class="c-article-equation" id="Equ6">
    <div class="c-article-equation__content">
     <span class="mathjax-tex">
      $$\begin{aligned} \omega (i,j)&amp;= \prod _{n=1}^{N} e_n. \end{aligned}$$
     </span>
    </div>
    <div class="c-article-equation__number">
     (6)
    </div>
   </div>
   <p>
    For each discretized value of
    <span class="mathjax-tex">
     \(\omega (i,j)\)
    </span>
    , we assign a learnable scalar that serves as a weight in the self-attention module. Equation (
    <a data-track="click" data-track-action="equation anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#Equ5">
     5
    </a>
    ) is further modified as follows:
   </p>
   <div class="c-article-equation" id="Equ7">
    <div class="c-article-equation__content">
     <span class="mathjax-tex">
      $$\begin{aligned} A_{ij}=\frac{(h_iW_{Q})(h_jW_{K})^T}{\sqrt{d_k}} \cdot (b_{\phi (i,j)} + c_{\lfloor \omega (i,j) \cdot a\rfloor }), \end{aligned}$$
     </span>
    </div>
    <div class="c-article-equation__number">
     (7)
    </div>
   </div>
   <p>
    where
    <span class="mathjax-tex">
     \(c_{\lfloor \omega (i,j) \cdot a\rfloor }\)
    </span>
    is a learnable scalar indexed by the floor value of the product of
    <span class="mathjax-tex">
     \(\omega (i,j)\)
    </span>
    and the hyperparameter
    <i>
     a
    </i>
    .
   </p>
   <p>
    Moreover, given the varying significance of heterogeneous connections between nodes, such as sentences to images and sentences to sentences, we employ distinct spatial and edge embeddings for diverse heterogeneous connections.
   </p>
   <h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec12">
    <span class="c-article-section__title-number">
     3.3.3
    </span>
    Heter graphormer layer
   </h4>
   <p>
    The HeterGraphormer layer can be formally characterized as follows:
   </p>
   <div class="c-article-equation" id="Equ8">
    <div class="c-article-equation__content">
     <span class="mathjax-tex">
      $$\begin{aligned} {\begin{matrix} H^{'(l)} &amp; = \text {MHA}(\text {LN}(H^{(l-1)}), E) + H^{(l-1)},\\ H^{(l)} &amp; = \text {FFN}(\text {LN}(H^{'(l)})) + H^{'(l)}, \end{matrix}} \end{aligned}$$
     </span>
    </div>
    <div class="c-article-equation__number">
     (8)
    </div>
   </div>
   <p>
    where
    <span class="mathjax-tex">
     \(H^{'(l)}\)
    </span>
    denotes the node representations output from layer
    <i>
     l
    </i>
    ,
    <span class="mathjax-tex">
     \(\text {LN}\)
    </span>
    signifies layer normalization and
    <span class="mathjax-tex">
     \(\text {FFN}\)
    </span>
    refers to feed-forward network.
    <span class="mathjax-tex">
     \(\text {MHA}\)
    </span>
    represents multi-head self-attention, represented as
   </p>
   <div class="c-article-equation" id="Equ9">
    <div class="c-article-equation__content">
     <span class="mathjax-tex">
      $$\begin{aligned} \begin{aligned} \text {MHA}(H, E) = \text {Concat}(\text {head}_1, \ldots , \text {head}_N) W^O,\\ \text {head}_i = \text {Attention}_i(H, E)=\text {softmax}(A)HW^V_i, \end{aligned} \end{aligned}$$
     </span>
    </div>
    <div class="c-article-equation__number">
     (9)
    </div>
   </div>
   <p>
    where
    <span class="mathjax-tex">
     \(W^O\)
    </span>
    is a learnable weight matrix used to linearly combine the outputs of all attention heads,
    <span class="mathjax-tex">
     \(W^V\)
    </span>
    is a learnable weight matrix used to transform
    <i>
     H
    </i>
    into value vectors.
   </p>
   <p>
    Given the existence of two types of edges on the graph, we sequentially update the sentence-word subgraph and the sentence-image subgraph, formulated as follows:
   </p>
   <div class="c-article-equation" id="Equ10">
    <div class="c-article-equation__content">
     <span class="mathjax-tex">
      $$\begin{aligned} {\begin{matrix} [H^{'(l)}_s, H^{(l)}_w] &amp; = \text {layer}([H^{(l-1)}_s, H^{(l-1)}_w], E_{sw}),\\ [H^{(l)}_s, H^{(l)}_p] &amp; = \text {layer}([H^{'(l)}_s, H^{(l-1)}_p], E_{sp}).\\ \end{matrix}} \end{aligned}$$
     </span>
    </div>
    <div class="c-article-equation__number">
     (10)
    </div>
   </div>
   <h3 class="c-article__sub-heading" id="Sec13">
    <span class="c-article-section__title-number">
     3.4
    </span>
    Multimodal summarizer
   </h3>
   <p>
    After passing
    <i>
     L
    </i>
    HeterGraphormer layers, we obtain the final sentence node representations
    <span class="mathjax-tex">
     \(H^{L}_s = \{h^s_1, h^s_2, \cdots , h^s_m \}\)
    </span>
    and image node representations
    <span class="mathjax-tex">
     \(H^{L}_p = \{h^p_1, h^p_2, \cdots , h^p_k \}\)
    </span>
    . Subsequently, these representations are fed into a single feed-forward layer for label prediction:
   </p>
   <div class="c-article-equation" id="Equ11">
    <div class="c-article-equation__content">
     <span class="mathjax-tex">
      $$\begin{aligned} \begin{aligned} \hat{y}^s_i&amp;= \text {sigmoid}(W_{s1}h^s_i),\\ \hat{y}^p_i&amp;= \text {sigmoid}(W_{p1}h^p_i),\\ \end{aligned} \end{aligned}$$
     </span>
    </div>
    <div class="c-article-equation__number">
     (11)
    </div>
   </div>
   <p>
    where
    <span class="mathjax-tex">
     \(W_{s1}\)
    </span>
    ,
    <span class="mathjax-tex">
     \(W_{p1} \)
    </span>
    are trainable parameters.
   </p>
   <p>
    For sentence extraction, our objective is to learn a binary label
    <span class="mathjax-tex">
     \(y^s_i \in \{0, 1\}\)
    </span>
    for each sentence
    <span class="mathjax-tex">
     \(s_i\)
    </span>
    , where
    <span class="mathjax-tex">
     \(y^s_i = 1\)
    </span>
    means that the sentence
    <span class="mathjax-tex">
     \(s_i\)
    </span>
    should be included in the textual summary
    <span class="mathjax-tex">
     \(\mathcal {S}_t\)
    </span>
    , and
    <span class="mathjax-tex">
     \(y^s_i = 0\)
    </span>
    means that the sentence
    <span class="mathjax-tex">
     \(s_i\)
    </span>
    should be excluded from the summary. The ground truth labels, referred to as ORACLE, is extracted using the greedy algorithm introduced by Nallapati et al. (
    <a aria-label="Reference 2017" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR22" id="ref-link-section-d206753678e4570" title="Nallapati R, Zhai F, &amp; Zhou B. (2017) .Summarunner: A recurrent neural network based sequence model for extractive summarization of documents. Proceedings of the AAAI Conference on Artificial Intelligence 31(1). 
                https://doi.org/10.1609/aaai.v31i1.10958
                
              ">
     2017
    </a>
    ). In detail, the algorithm generates a summary consisting of multiple sentences, which are selected greedily to maximize the ROUGE score against the gold summary
    <sup>
     <a href="#Fn1">
      <span class="u-visually-hidden">
       Footnote
      </span>
      1
     </a>
    </sup>
    .
   </p>
   <p>
    The binary cross-entropy loss of the sentence classifier is expressed as
   </p>
   <div class="c-article-equation" id="Equ12">
    <div class="c-article-equation__content">
     <span class="mathjax-tex">
      $$\begin{aligned} \mathcal {L}_s = -\frac{1}{m}\sum _{i=1}^{m} \left( y^s_i \log (\hat{y}^s_i) + (1 - y^s_i) \log (1 - \hat{y}^s_i)\right) \end{aligned}$$
     </span>
    </div>
    <div class="c-article-equation__number">
     (12)
    </div>
   </div>
   <p>
    Following Zhang et al. (
    <a aria-label="Reference 2022c" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR43" id="ref-link-section-d206753678e4743" title="Zhang, Z., Meng, X., Wang, Y., et al. (2022c). Unims: A unified framework for multimodal summarization with knowledge distillation. Proceedings of the AAAI Conference on Artificial Intelligence 36(10) 11757–11764. 
                https://doi.org/10.1609/aaai.v36i10.21431
                
              ">
     2022c
    </a>
    ), we use BLIP as the teacher model to compute the ITM score between the gold summary
    <span class="mathjax-tex">
     \(\mathcal {T}\)
    </span>
    and each image
    <span class="mathjax-tex">
     \(p_i\)
    </span>
    . The mean squared error (MSE) loss is formulated as follows:
   </p>
   <div class="c-article-equation" id="Equ13">
    <div class="c-article-equation__content">
     <span class="mathjax-tex">
      $$\begin{aligned} \mathcal {L}_p = \frac{1}{k} \sum _{i=1}^{k} (\hat{y}^p_i - \text {match}(\mathcal {T},p_i))^2. \end{aligned}$$
     </span>
    </div>
    <div class="c-article-equation__number">
     (13)
    </div>
   </div>
   <p>
    The final loss of our model is the linear combination of two parts of loss with hyperparameter
    <span class="mathjax-tex">
     \(\lambda \)
    </span>
    to balance their weights:
   </p>
   <div class="c-article-equation" id="Equ14">
    <div class="c-article-equation__content">
     <span class="mathjax-tex">
      $$\begin{aligned} \mathcal {L} = \lambda \mathcal {L}_s + (1-\lambda )\mathcal {L}_p. \end{aligned}$$
     </span>
    </div>
    <div class="c-article-equation__number">
     (14)
    </div>
   </div>
  </div>
 </div>
</section>
<section data-title="Experiments setting">
 <div class="c-article-section" id="Sec14-section">
  <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec14">
   <span class="c-article-section__title-number">
    4
   </span>
   Experiments setting
  </h2>
  <div class="c-article-section__content" id="Sec14-content">
   <h3 class="c-article__sub-heading" id="Sec15">
    <span class="c-article-section__title-number">
     4.1
    </span>
    Dataset
   </h3>
   <div class="c-article-table" data-container-section="table" data-test="inline-table" id="table-1">
    <figure>
     <figcaption class="c-article-table__figcaption">
      <b data-test="table-caption" id="Tab1">
       Table 1 MSMO dataset statistics
      </b>
     </figcaption>
     <div class="u-text-right u-hide-print">
      <a aria-label="Full size table 1" class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" href="/article/10.1007/s10844-024-00886-5/tables/1" rel="nofollow">
       <span>
        Full size table
       </span>
       <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
        <use xlink:href="#icon-eds-i-chevron-right-small" xmlns:xlink="http://www.w3.org/1999/xlink">
        </use>
       </svg>
      </a>
     </div>
    </figure>
   </div>
   <p>
    We use the MSMO dataset build by Zhu et al. (
    <a aria-label="Reference 2018" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR45" id="ref-link-section-d206753678e5158" title="Zhu, J., Li, H., Liu, T., et al. (2018). MSMO: Multimodal summarization with multimodal output. In: Riloff E, Chiang D, Hockenmaier J, et al (eds) Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Brussels, Belgium, pp 4154–4164. 
                https://doi.org/10.18653/v1/D18-1448
                
              ">
     2018
    </a>
    ) for multimodal summarization. It contains news articles from the
    <i>
     Daily Mail
    </i>
    website
    <sup>
     <a href="#Fn2">
      <span class="u-visually-hidden">
       Footnote
      </span>
      2
     </a>
    </sup>
    , each paired with corresponding images and a text summary. The dataset includes 293,965 articles for training, 10,355 articles for validation, and 10,261 articles for testing. In the test set, at most three images are annotated as reference images. Image captions are excluded from the dataset for generalization. More details are illustrated in Table
    <a data-track="click" data-track-action="table anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#Tab1">
     1
    </a>
    .
   </p>
   <h3 class="c-article__sub-heading" id="Sec16">
    <span class="c-article-section__title-number">
     4.2
    </span>
    Implementation details
   </h3>
   <p>
    We use the roberta-base
    <sup>
     <a href="#Fn3">
      <span class="u-visually-hidden">
       Footnote
      </span>
      3
     </a>
    </sup>
    version of RoBERTa and the blip-itm-base-coco
    <sup>
     <a href="#Fn4">
      <span class="u-visually-hidden">
       Footnote
      </span>
      4
     </a>
    </sup>
    version of BLIP to implement our models in all experiments. We conducted fine-tuning on the last 4 layers of RoBERTa while keeping BLIP frozen during the training process. We trained the model on an NVIDIA GeForce RTX 3090, setting the learning rate for BERT at 5e-6 and for other parameters at 5e-5. The batch size was set to 24, and we utilized the Adam optimizer (Kingma &amp; Ba,
    <a aria-label="Reference 2015" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR11" id="ref-link-section-d206753678e5208" title="Kingma, D.P., &amp; Ba, J (2015). Adam: A method for stochastic optimization. In: Bengio Y, LeCun Y (Eds.), 3rd International conference on learning representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings,[SPACE]
                http://arxiv.org/abs/1412.6980
                
              ">
     2015
    </a>
    ). The hidden state dimension
    <i>
     d
    </i>
    of the nodes was set to 256. We constrain the number of sentences and images to 50 and 16, respectively. There are 6 layers of HeterGraphormer, each layer has 32 attention heads, and the dimension of FFN is set to 512. The hyperparameter
    <i>
     a
    </i>
    is set to 10 for discretizing edge weights and
    <span class="mathjax-tex">
     \(\lambda \)
    </span>
    is set to 0.8 to balance the loss of sentence and image selection. We select the best checkpoint according to the validation loss and report the results on the test set. For sentence extraction, we rank sentences in descending order based on their scores, selecting the top-3 while filtering out those with scores below 0.185.
   </p>
   <h3 class="c-article__sub-heading" id="Sec17">
    <span class="c-article-section__title-number">
     4.3
    </span>
    Baselines
   </h3>
   <p>
    We compare our model with existing text and multimodal summarization methods:
   </p>
   <p>
    <b>
     ORACLE
    </b>
    generates a summary consisting of multiple sentences, which are selected by a greedy algorithm (Nallapati et al.,
    <a aria-label="Reference 2017" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR22" id="ref-link-section-d206753678e5248" title="Nallapati R, Zhai F, &amp; Zhou B. (2017) .Summarunner: A recurrent neural network based sequence model for extractive summarization of documents. Proceedings of the AAAI Conference on Artificial Intelligence 31(1). 
                https://doi.org/10.1609/aaai.v31i1.10958
                
              ">
     2017
    </a>
    ) to maximize the ROUGE score against the gold summary.
   </p>
   <p>
    <b>
     LEAD-3
    </b>
    selects the first three sentences of the document as the text summary.
   </p>
   <p>
    <b>
     ATG
    </b>
    /
    <b>
     ATL
    </b>
    /
    <b>
     HAN
    </b>
    are proposed by Zhu et al. (
    <a aria-label="Reference 2018" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR45" id="ref-link-section-d206753678e5267" title="Zhu, J., Li, H., Liu, T., et al. (2018). MSMO: Multimodal summarization with multimodal output. In: Riloff E, Chiang D, Hockenmaier J, et al (eds) Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Brussels, Belgium, pp 4154–4164. 
                https://doi.org/10.18653/v1/D18-1448
                
              ">
     2018
    </a>
    ), incorporating visual information into the pointer generator network (See et al.,
    <a aria-label="Reference 2017" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR29" id="ref-link-section-d206753678e5270" title="See, A., Liu, P.J., Manning, C.D. (2017). Get to the point: Summarization with pointer-generator networks. In: Barzilay R, Kan MY (Eds.), Proceedings of the 55th annual meeting of the association for computational linguistics (Volume 1: Long Papers). Association for Computational Linguistics, Vancouver, Canada, pp 1073–1083. 
                https://doi.org/10.18653/v1/P17-1099
                
              ">
     2017
    </a>
    ) through visual attention on global features, local features, and hierarchical local features of the images, respectively. The visual attention distributions are then utilized to select images.
   </p>
   <p>
    <b>
     MOF
    </b>
    (Zhu et al.,
    <a aria-label="Reference 2020" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR46" id="ref-link-section-d206753678e5279" title="Zhu, J., Zhou, Y., Zhang, J., et al. (2020). Multimodal summarization with guidance of multimodal reference. Proceedings of the AAAI Conference on Artificial Intelligence, 34(05), 9749–975. 
                https://doi.org/10.1609/aaai.v34i05.6525
                
              ">
     2020
    </a>
    ) introduces a multimodal objective function into ATG. Among the four MOF variants, we chose the one with the best overall performance.
   </p>
   <p>
    <b>
     UniMS
    </b>
    (Zhang et al.,
    <a aria-label="Reference 2022c" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR43" id="ref-link-section-d206753678e5287" title="Zhang, Z., Meng, X., Wang, Y., et al. (2022c). Unims: A unified framework for multimodal summarization with knowledge distillation. Proceedings of the AAAI Conference on Artificial Intelligence 36(10) 11757–11764. 
                https://doi.org/10.1609/aaai.v36i10.21431
                
              ">
     2022c
    </a>
    ) extends the text encoder of BART Lewis et al. (
    <a aria-label="Reference 2020" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR12" id="ref-link-section-d206753678e5290" title="Lewis, M., Liu, Y., Goyal, N., et al. (2020). BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In: Jurafsky D, Chai J, Schluter N, et al (Eds.), Proceedings of the 58th annual meeting of the association for computational linguistics. Association for Computational Linguistics, Online, pp 7871–7880. 
                https://doi.org/10.18653/v1/2020.acl-main.703
                
              ">
     2020
    </a>
    ) to a multimodal encoder and utilizes knowledge distillation to help image selection.
   </p>
   <p>
    <b>
     SITA
    </b>
    (Jiang et al.,
    <a aria-label="Reference 2023" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR8" id="ref-link-section-d206753678e5298" title="Jiang, C., Xie, R., Ye, W., et al. (2023). Exploiting pseudo image captions for multimodal summarization. In: Rogers A, Boyd-Graber J, Okazaki N (Eds.), Findings of the association for computational linguistics: ACL 2023. Association for Computational Linguistics, Toronto, Canada, pp 161–175. 
                https://doi.org/10.18653/v1/2023.findings-acl.12
                
              ">
     2023
    </a>
    ) selects sentences most relevant to the images, using them as additional input instead of utilizing image information directly. The selection of images is based on the ROUGE-L of these sentences.
   </p>
   <p>
    <b>
     HGMS(GAT)
    </b>
    : To compare the performance gap between HeterGraphormer and GAT (Velickovic et al.,
    <a aria-label="Reference 2017" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR34" id="ref-link-section-d206753678e5306" title="Velickovic, P., Cucurull, G., Casanova, A., et al. (2017). Graph attention networks. stat, 1050(20), 10–48550.">
     2017
    </a>
    ), we implemented a GAT-based HGMS. Following the method proposed by Wang et al. (
    <a aria-label="Reference 2020" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR35" id="ref-link-section-d206753678e5309" title="Wang, D., Liu, P., Zheng, Y., et al. (2020). Heterogeneous graph neural networks for extractive document summarization. In: Jurafsky D, Chai J, Schluter N, et al (Eds.), Proceedings of the 58th annual meeting of the association for computational linguistics. Association for Computational Linguistics, Online, pp 6209–6219. 
                https://doi.org/10.18653/v1/2020.acl-main.553
                
              ">
     2020
    </a>
    ), the GAT incorporates considerations for both neighboring nodes and edge weights in updating nodes, and the number of layers is 2.
   </p>
   <h3 class="c-article__sub-heading" id="Sec18">
    <span class="c-article-section__title-number">
     4.4
    </span>
    Evaluation metrics
   </h3>
   <p>
    The quality of generated textual summary is evaluated by
    <b>
     ROUGE
    </b>
    . For image selection, image precision(
    <b>
     IP
    </b>
    ) indicates whether the chosen image is present in the image references.
    <span class="mathjax-tex">
     \({\textbf {M}}_{\text {sim}}\)
    </span>
    is an image-text relevance metric which calculates the maximum similarity between the image and each selected sentence by cross-modal retrieval model. For details regarding IP and
    <span class="mathjax-tex">
     \(\text {M}_{\text {sim}}\)
    </span>
    , please refer to the work of Zhu et al. (
    <a aria-label="Reference 2018" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR45" id="ref-link-section-d206753678e5371" title="Zhu, J., Li, H., Liu, T., et al. (2018). MSMO: Multimodal summarization with multimodal output. In: Riloff E, Chiang D, Hockenmaier J, et al (eds) Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Brussels, Belgium, pp 4154–4164. 
                https://doi.org/10.18653/v1/D18-1448
                
              ">
     2018
    </a>
    ).
   </p>
  </div>
 </div>
</section>
<section data-title="Results and analysis">
 <div class="c-article-section" id="Sec19-section">
  <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec19">
   <span class="c-article-section__title-number">
    5
   </span>
   Results and analysis
  </h2>
  <div class="c-article-section__content" id="Sec19-content">
   <h3 class="c-article__sub-heading" id="Sec20">
    <span class="c-article-section__title-number">
     5.1
    </span>
    Automatic evaluation
   </h3>
   <div class="c-article-table" data-container-section="table" data-test="inline-table" id="table-2">
    <figure>
     <figcaption class="c-article-table__figcaption">
      <b data-test="table-caption" id="Tab2">
       Table 2 Automatic evaluation on MSMO
      </b>
     </figcaption>
     <div class="u-text-right u-hide-print">
      <a aria-label="Full size table 2" class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" href="/article/10.1007/s10844-024-00886-5/tables/2" rel="nofollow">
       <span>
        Full size table
       </span>
       <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
        <use xlink:href="#icon-eds-i-chevron-right-small" xmlns:xlink="http://www.w3.org/1999/xlink">
        </use>
       </svg>
      </a>
     </div>
    </figure>
   </div>
   <p>
    The performance of baseline methods and our models is shown in Table
    <a data-track="click" data-track-action="table anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#Tab2">
     2
    </a>
    . The first block contains the ORACLE upper bound and LEAD-3 baseline, and the second block includes the multimodal summarization models. We present our models in the third block.
   </p>
   <p>
    Compared to the baseline methods, our HGMS model has demonstrated significant improvements across all metrics, including text summarization quality, image selection precision, image-text relevance, and the overall quality of multimodal summary.
   </p>
   <p>
    Regarding the image selection precision metric (IP), UniMS achieves a noteworthy advantage over its predecessor baselines by distilling knowledge within a vision-language pre-trained model. SITA achieves further improvements by leveraging sentences most relevant to images in the text as cues for image selection. By leveraging the inherent many-to-many relationships between sentences and images, our model achieves a significant performance boost compared to SITA (e.g., 79.72 vs. 76.41 in the IP metric).
   </p>
   <p>
    In terms of the
    <span class="mathjax-tex">
     \(\text {M}_{\text {sim}}\)
    </span>
    metric for evaluating the relevance between sentences and images, our model achieved a performance improvement of over 40% compared to SITA (e.g., 46.56 vs. 33.47 of
    <span class="mathjax-tex">
     \(\text {M}_{\text {sim}}\)
    </span>
    ). This is mainly attributed to the introduction of ITM scores as weights for the edges between sentences and images, making the model more inclined to select relevant sentence-image pairs.
   </p>
   <p>
    Concerning text summarization quality metrics, our model exhibits notable improvements in both Rouge1 and Rouge2 (e.g., an increase of 1.32 and 1.05, respectively, compared to SITA). However, Rouge-L shows a marginal improvement of only 0.05. This indicates that the introduction of word nodes makes the model focus more on word-level semantic information.
   </p>
   <p>
    For different graph encoders, HGMS based on HeterGraphormer outperforms HGMS im-plemented with GAT across all metrics. This suggests that HeterGraphormer has a significant advantage in more effectively modeling intricate relationships between nodes.
   </p>
   <div class="c-article-table" data-container-section="table" data-test="inline-table" id="table-3">
    <figure>
     <figcaption class="c-article-table__figcaption">
      <b data-test="table-caption" id="Tab3">
       Table 3 Ablation studies on the test set
      </b>
     </figcaption>
     <div class="u-text-right u-hide-print">
      <a aria-label="Full size table 3" class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" href="/article/10.1007/s10844-024-00886-5/tables/3" rel="nofollow">
       <span>
        Full size table
       </span>
       <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
        <use xlink:href="#icon-eds-i-chevron-right-small" xmlns:xlink="http://www.w3.org/1999/xlink">
        </use>
       </svg>
      </a>
     </div>
    </figure>
   </div>
   <h3 class="c-article__sub-heading" id="Sec21">
    <span class="c-article-section__title-number">
     5.2
    </span>
    Ablation study
   </h3>
   <p>
    We perform the ablation study to investigate the potential influence of different components. As shown in Table
    <a data-track="click" data-track-action="table anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#Tab3">
     3
    </a>
    , we design the following experimental settings:
   </p>
   <ol class="u-list-style-none">
    <li>
     <span class="u-custom-list-number">
      (1)
     </span>
     <p>
      We retain the centrality embedding used to represent node degree information in Graphormer.
     </p>
    </li>
    <li>
     <span class="u-custom-list-number">
      (2)
     </span>
     <p>
      The type embedding used to differentiate between different node types was removed.
     </p>
    </li>
    <li>
     <span class="u-custom-list-number">
      (3)
     </span>
     <p>
      We employ identical spatial and edge embeddings for each heterogeneous connection.
     </p>
    </li>
    <li>
     <span class="u-custom-list-number">
      (4)
     </span>
     <p>
      During the update of node features, we no longer consider heterogeneous nodes without connections. Sentence-image pairs are only connected when the ITM score is greater than 0.5.
     </p>
    </li>
    <li>
     <span class="u-custom-list-number">
      (5)
     </span>
     <p>
      We changed the method of calculating the weights between nodes from a product approach to averaging.
     </p>
    </li>
    <li>
     <span class="u-custom-list-number">
      (6)
     </span>
     <p>
      We removed the TF-IDF and ITM weights on the edges used to distinguish the importance of connections.
     </p>
    </li>
    <li>
     <span class="u-custom-list-number">
      (7)
     </span>
     <p>
      We replaced the BLIP weights used in the model with CLIP(Radford et al.,
      <a aria-label="Reference 2021" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR28" id="ref-link-section-d206753678e6275" title="Radford A, Kim JW, Hallacy C, et al (2021) Learning transferable visual models from natural language supervision. In: Meila M, Zhang T (Eds.), Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, pp 8748–8763. 
                http://proceedings.mlr.press/v139/radford21a.html
                
              ">
       2021
      </a>
      ).
     </p>
    </li>
    <li>
     <span class="u-custom-list-number">
      (8)
     </span>
     <p>
      The pre-trained RoBERTa was replaced with a bidirectional LSTM.
     </p>
    </li>
   </ol>
   <p>
    After the removal of edge weights, there is a notable decrease in the IP metric, indicating that the ITM scores on the edges between sentences and images play a crucial role in image selection. Regarding the enhancements made to Graphormer (from 1 to 3), the experimental results substantiate the effectiveness of these enhancements.
   </p>
   <div class="c-article-table" data-container-section="table" data-test="inline-table" id="table-4">
    <figure>
     <figcaption class="c-article-table__figcaption">
      <b data-test="table-caption" id="Tab4">
       Table 4 Experimental results with different methods of building image references
      </b>
     </figcaption>
     <div class="u-text-right u-hide-print">
      <a aria-label="Full size table 4" class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" href="/article/10.1007/s10844-024-00886-5/tables/4" rel="nofollow">
       <span>
        Full size table
       </span>
       <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
        <use xlink:href="#icon-eds-i-chevron-right-small" xmlns:xlink="http://www.w3.org/1999/xlink">
        </use>
       </svg>
      </a>
     </div>
    </figure>
   </div>
   <h3 class="c-article__sub-heading" id="Sec22">
    <span class="c-article-section__title-number">
     5.3
    </span>
    Image reference
   </h3>
   <p>
    To investigate the impact of employing diverse image reference strategies during model training, we additionally incorporated a strategy named ROUGE-Top. This method selects an image with a caption that has the highest ROUGE score for each article as the reference image. As shown in Table
    <a data-track="click" data-track-action="table anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#Tab4">
     4
    </a>
    , compared to the knowledge distillation method, the IP score of the ROUGE-Top strategy has significantly increased by 3.69, indicating the effectiveness of this strategy. It is noteworthy that the IP score achieved through the ROUGE-Top strategy significantly surpassed its label upper bound (79.89), indicating the model’s success in leveraging the intricate relationships between sentences and images to enhance its capability in selecting critical images. However, as the ROUGE-Top strategy heavily relies on the quality of image captions, for the sake of model generalization, we choose to employ knowledge distillation.
   </p>
   <h3 class="c-article__sub-heading" id="Sec23">
    <span class="c-article-section__title-number">
     5.4
    </span>
    Layer number analysis
   </h3>
   <p>
    In order to further analyze the advantages of HeterGraphormer over GAT, we conducted a study on the performance of the models at different numbers of layers. As illustrated in Fig.
    <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#Fig3">
     3
    </a>
    , we compare the Rouge scores of HGMS using HeterGraphormer and GAT as encoders across different layer depths. We observed that with an increase in the number of layers, the performance gap between HeterGraphormer and GAT gradually widened. hough we introduced residual connections between layers in GAT to alleviate the issue of over-smoothing, the optimal number of layers for GAT was only 2, beyond which the performance started to decline. In contrast, HeterGraphormer, with its ability to attend to all nodes during node updates, remained unaffected by over-smoothing issue. With increasing layer depth, its performance also improves, stabilizing after 6 layers. This suggests that HeterGraphormer can enhance its ability to model intricate relationships by increasing the number of layers.
   </p>
   <div class="c-article-section__figure js-c-reading-companion-figures-item" data-container-section="figure" data-test="figure" data-title="Fig. 3" id="figure-3">
    <figure>
     <figcaption>
      <b class="c-article-section__figure-caption" data-test="figure-caption-text" id="Fig3">
       Fig. 3
      </b>
     </figcaption>
     <div class="c-article-section__figure-content">
      <div class="c-article-section__figure-item">
       <a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-action="view figure" data-track-label="image" href="/article/10.1007/s10844-024-00886-5/figures/3" rel="nofollow">
        <picture>
         <source srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10844-024-00886-5/MediaObjects/10844_2024_886_Fig3_HTML.png?as=webp" type="image/webp"/>
         <img alt="figure 3" aria-describedby="Fig3" height="493" loading="lazy" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10844-024-00886-5/MediaObjects/10844_2024_886_Fig3_HTML.png" width="685"/>
        </picture>
       </a>
      </div>
      <div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc">
       <p>
        The ROUGE scores on validation set when the number of layers increases.
        <span class="mathjax-tex">
         \(\hat{R}\)
        </span>
        is the mean of R-1, R-2 and R-L
       </p>
      </div>
     </div>
     <div class="u-text-right u-hide-print">
      <a aria-label="Full size image figure 3" class="c-article__pill-button" data-test="article-link" data-track="click" data-track-action="view figure" data-track-dest="link:Figure3 Full size image" data-track-label="button" href="/article/10.1007/s10844-024-00886-5/figures/3" rel="nofollow">
       <span>
        Full size image
       </span>
       <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
        <use xlink:href="#icon-eds-i-chevron-right-small" xmlns:xlink="http://www.w3.org/1999/xlink">
        </use>
       </svg>
      </a>
     </div>
    </figure>
   </div>
   <div class="c-article-section__figure js-c-reading-companion-figures-item" data-container-section="figure" data-test="figure" data-title="Fig. 4" id="figure-4">
    <figure>
     <figcaption>
      <b class="c-article-section__figure-caption" data-test="figure-caption-text" id="Fig4">
       Fig. 4
      </b>
     </figcaption>
     <div class="c-article-section__figure-content">
      <div class="c-article-section__figure-item">
       <a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-action="view figure" data-track-label="image" href="/article/10.1007/s10844-024-00886-5/figures/4" rel="nofollow">
        <picture>
         <source srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10844-024-00886-5/MediaObjects/10844_2024_886_Fig4_HTML.png?as=webp" type="image/webp"/>
         <img alt="figure 4" aria-describedby="Fig4" height="497" loading="lazy" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10844-024-00886-5/MediaObjects/10844_2024_886_Fig4_HTML.png" width="685"/>
        </picture>
       </a>
      </div>
      <div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc">
       <p>
        Visualization of sentence and image embeddings for 100 documents in the test set
       </p>
      </div>
     </div>
     <div class="u-text-right u-hide-print">
      <a aria-label="Full size image figure 4" class="c-article__pill-button" data-test="article-link" data-track="click" data-track-action="view figure" data-track-dest="link:Figure4 Full size image" data-track-label="button" href="/article/10.1007/s10844-024-00886-5/figures/4" rel="nofollow">
       <span>
        Full size image
       </span>
       <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
        <use xlink:href="#icon-eds-i-chevron-right-small" xmlns:xlink="http://www.w3.org/1999/xlink">
        </use>
       </svg>
      </a>
     </div>
    </figure>
   </div>
   <h3 class="c-article__sub-heading" id="Sec24">
    <span class="c-article-section__title-number">
     5.5
    </span>
    Embedding analysis
   </h3>
   <p>
    To explore the sentence and image embeddings generated by HGMS, we employ the t-SNE (van der Maaten &amp; Hinton,
    <a aria-label="Reference 2008" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR20" id="ref-link-section-d206753678e6502" title="van der Maaten L, &amp; Hinton G (2008) Visualizing data using t-sne. Journal of Machine Learning Research 9(86), 2579–2605. 
                http://jmlr.org/papers/v9/vandermaaten08a.html
                
              ">
     2008
    </a>
    ) algorithm to map the embeddings of the final layer outputs from HeterGraphormer to a two-dimensional space for visualization. We randomly selected 100 documents from the test set, comprising approximately 2500 sentences and 600 images. As illustrated in Fig.
    <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#Fig4">
     4
    </a>
    , we use blue and gray points to represent ground truth and non-ground truth sentences, respectively, while yellow and black points denote reference and non-reference images, respectively.
   </p>
   <p>
    It can be observed that ground truth sentences exhibit a clustering tendency in the bottom-right area, indicating the model’s effectiveness in distinguishing salient sentences. Notably, compared to reference images, non-reference images show a higher degree of clustering, indicating that the model has better recognition ability for irrelevant images.
   </p>
   <h3 class="c-article__sub-heading" id="Sec25">
    <span class="c-article-section__title-number">
     5.6
    </span>
    Human evaluation
   </h3>
   <p>
    In addition to the automatic evaluations, we conducted human evaluations to assess the performance of models. Following Wu et al. (
    <a aria-label="Reference 2021" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR36" id="ref-link-section-d206753678e6519" title="Wu W, Li W, Xiao X, et al. (2021). BASS: Boosting abstractive summarization with unified semantic graph. In: Proceedings of the 59th annual meeting of the association for computational linguistics and the 11th international joint conference on natural language processing (Volume 1: Long Papers). Association for Computational Linguistics, Online, pp 6052–6067.
                https://doi.org/10.18653/v1/2021.acl-long.472
                
              ">
     2021
    </a>
    ), we randomly select 50 samples from the test set of MSMO dataset. Each sample is independently annotated by three different participants, and the rating is the average of their evaluations. Annotators evaluate the overall quality of summaries by ranking them based on the following three criteria simultaneously: (1)
    <i>
     Informativeness
    </i>
    : Does the summary effectively convey the crucial information from the source document? (2)
    <i>
     Succinctness
    </i>
    : Is the summary succinct, avoiding unnecessary details? (3)
    <i>
     Relevance
    </i>
    : Does the selected image match the textual summary? To further distinguish the differences between models, especially when their average rankings are similar, we quantified the rankings. Rankings range from 1 to 4, allowing ties, with scores corresponding to 2, 1, -1, -2, respectively. The final rating for each model is determined by averaging the scores across all test samples.
   </p>
   <div class="c-article-table" data-container-section="table" data-test="inline-table" id="table-5">
    <figure>
     <figcaption class="c-article-table__figcaption">
      <b data-test="table-caption" id="Tab5">
       Table 5 Ranking results of system summaries by human evaluation
      </b>
     </figcaption>
     <div class="u-text-right u-hide-print">
      <a aria-label="Full size table 5" class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" href="/article/10.1007/s10844-024-00886-5/tables/5" rel="nofollow">
       <span>
        Full size table
       </span>
       <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
        <use xlink:href="#icon-eds-i-chevron-right-small" xmlns:xlink="http://www.w3.org/1999/xlink">
        </use>
       </svg>
      </a>
     </div>
    </figure>
   </div>
   <div class="c-article-table" data-container-section="table" data-test="inline-table" id="table-6">
    <figure>
     <figcaption class="c-article-table__figcaption">
      <b data-test="table-caption" id="Tab6">
       Table 6 An example of multimodal summarization generation
      </b>
     </figcaption>
     <div class="u-text-right u-hide-print">
      <a aria-label="Full size table 6" class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" href="/article/10.1007/s10844-024-00886-5/tables/6" rel="nofollow">
       <span>
        Full size table
       </span>
       <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
        <use xlink:href="#icon-eds-i-chevron-right-small" xmlns:xlink="http://www.w3.org/1999/xlink">
        </use>
       </svg>
      </a>
     </div>
    </figure>
   </div>
   <p>
    Table
    <a data-track="click" data-track-action="table anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#Tab5">
     5
    </a>
    presents the human evaluation results on three strong baseline models and our proposed HGMS. The agreement between the annotators was calculated using Fleiss’ Kappa, with a value of 0.538. The results show that HGMS is able to generate higher quality multi-modal summaries compared to other models. Additionally, the human evaluation results further validate the effectiveness of our proposed HeterGraphormer.
   </p>
   <h3 class="c-article__sub-heading" id="Sec26">
    <span class="c-article-section__title-number">
     5.7
    </span>
    Case study
   </h3>
   <p>
    Table
    <a data-track="click" data-track-action="table anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#Tab6">
     6
    </a>
    presents a typical example selected from test set consisting of the input article, the gold summary, the summary generated by the strong baseline SITA, and the summary produced by our proposed HGMS model. The highlighted text corresponds to entities present in the image. As BLIP does not have the ability to detect entities, these entities were manually annotated. Notably, the summary output by our model shows high similarity to the gold summary, effectively summarizing key information related to ”islands” and ”cruise ships”, while accurately selecting the image most relevant to these themes. Compared to the SITA model, which uses Rouge scores between pseudo image captions and textual summary for image selection, our model can select critical images more accurately.
   </p>
   <p>
    Additionally, Table
    <a data-track="click" data-track-action="table anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#Tab7">
     7
    </a>
    present the ITM scores calculated using BLIP between sentences (S1, S2, S3, S4) and images (IMG1, IMG2). It is evident that the many-to-many relationships between sentences and images play an important role in accurately extracting salient sentences and critical images. This further validates the effectiveness of the proposed graph structure.
   </p>
   <div class="c-article-table" data-container-section="table" data-test="inline-table" id="table-7">
    <figure>
     <figcaption class="c-article-table__figcaption">
      <b data-test="table-caption" id="Tab7">
       Table 7 The matching scores between the images and the sentences
      </b>
     </figcaption>
     <div class="u-text-right u-hide-print">
      <a aria-label="Full size table 7" class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" href="/article/10.1007/s10844-024-00886-5/tables/7" rel="nofollow">
       <span>
        Full size table
       </span>
       <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
        <use xlink:href="#icon-eds-i-chevron-right-small" xmlns:xlink="http://www.w3.org/1999/xlink">
        </use>
       </svg>
      </a>
     </div>
    </figure>
   </div>
  </div>
 </div>
</section>
<section data-title="Conclusion">
 <div class="c-article-section" id="Sec27-section">
  <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec27">
   <span class="c-article-section__title-number">
    6
   </span>
   Conclusion
  </h2>
  <div class="c-article-section__content" id="Sec27-content">
   <p>
    This paper presents HGMS for multimodal summarization. HGMS constructs a multimodal heterogeneous graph by considering both related and unrelated relationships between modalities, and applies HeterGraphormer on the graph to effectively model intermodal relationships. Experimental results show HGMS outperforms the current state-of-the-art model on MSMO datasets, which demonstrates the effectiveness of our method. Furthermore, our proposed HeterGraphormer can be extended to other NLP tasks, offering potential performance improvements for such tasks.
   </p>
  </div>
 </div>
</section>
<section data-title="Limitations">
 <div class="c-article-section" id="Sec28-section">
  <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec28">
   <span class="c-article-section__title-number">
    7
   </span>
   Limitations
  </h2>
  <div class="c-article-section__content" id="Sec28-content">
   <p>
    Due to the utilization of Byte Pair Encoding (BPE) tokenization in the pre-trained language model, we opt to employ it only as a sentence encoder, without leveraging its word-level features. This decision stems from the inherent characteristics of BPE tokenization, which may result in the establishment of connections between originally unrelated sentence nodes after tokenization.
   </p>
   <p>
    Another limitation of this study is the lack of diverse datasets. Currently, only one publicly available dataset is accessible for use. Thus, the analysis and conclusions of our study may be influenced by the limitations of the dataset. Future research could address this limitation by gathering additional datasets to enhance the reliability and generalizability of the study findings.
   </p>
  </div>
 </div>
</section>
<section data-title="Data Availability">
 <div class="c-article-section" id="data-availability-section">
  <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="data-availability">
   Data Availability
  </h2>
  <div class="c-article-section__content" id="data-availability-content">
   <p>
    No datasets were generated or analysed during the current study.
   </p>
  </div>
 </div>
</section>
<section data-title="Code Availability">
 <div class="c-article-section" id="code-availability-section">
  <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="code-availability">
   Code Availability
  </h2>
  <div class="c-article-section__content" id="code-availability-content">
   <p>
    Code is available at
    <a href="https://github.com/anonymous272218/HGMS">
     https://github.com/anonymous272218/HGMS
    </a>
   </p>
  </div>
 </div>
</section>
<section data-title="Notes">
 <div class="c-article-section" id="notes-section">
  <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="notes">
   Notes
  </h2>
  <div class="c-article-section__content" id="notes-content">
   <ol class="c-article-footnote c-article-footnote--listed">
    <li class="c-article-footnote--listed__item" data-counter="1." id="Fn1">
     <div class="c-article-footnote--listed__content">
      <p>
       Gold summary refers to the ideal, human-generated summary that serves as a standard for evaluating the quality of automated text summarization systems.
      </p>
     </div>
    </li>
    <li class="c-article-footnote--listed__item" data-counter="2." id="Fn2">
     <div class="c-article-footnote--listed__content">
      <p>
       <a href="http://www.dailymail.co.uk">
        http://www.dailymail.co.uk
       </a>
      </p>
     </div>
    </li>
    <li class="c-article-footnote--listed__item" data-counter="3." id="Fn3">
     <div class="c-article-footnote--listed__content">
      <p>
       <a href="https://huggingface.co/FacebookAI">
        https://huggingface.co/FacebookAI
       </a>
      </p>
     </div>
    </li>
    <li class="c-article-footnote--listed__item" data-counter="4." id="Fn4">
     <div class="c-article-footnote--listed__content">
      <p>
       <a href="https://huggingface.co/Salesforce">
        https://huggingface.co/Salesforce
       </a>
      </p>
     </div>
    </li>
   </ol>
  </div>
 </div>
</section>
<section aria-labelledby="Bib1" data-title="References">
 <div class="c-article-section" id="Bib1-section">
  <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">
   References
  </h2>
  <div class="c-article-section__content" id="Bib1-content">
   <div data-container-section="references">
    <ul class="c-article-references" data-track-component="outbound reference" data-track-context="references section">
     <li class="c-article-references__item js-c-reading-companion-references-item">
      <p class="c-article-references__text" id="ref-CR1">
       An, C., Zhong, M., Wu, Z., et al. (2022). CoLo: A contrastive learning based re-ranking framework for one-stage summarization. In: Calzolari N, Huang CR, Kim H, et al (Eds.), Proceedings of the 29th international conference on computational linguistics.
       <i>
        International Committee on Computational Linguistics
       </i>
       , Gyeongju, Republic of Korea, pp 5783–5793.
       <a data-track="click_references" data-track-action="external reference" data-track-label="https://aclanthology.org/2022.coling-1.508" data-track-value="external reference" href="https://aclanthology.org/2022.coling-1.508">
        https://aclanthology.org/2022.coling-1.508
       </a>
      </p>
     </li>
     <li class="c-article-references__item js-c-reading-companion-references-item">
      <p class="c-article-references__text" id="ref-CR2">
       Chen, J., &amp; Zhuge, H. (2018). Abstractive text-image summarization using multi-modal attentional hierarchical RNN. In: Riloff E, Chiang D, Hockenmaier J, et al (Eds.), Proceedings of the 2018 conference on empirical methods in natural language processing.
       <i>
        Association for Computational Linguistics,
       </i>
       Brussels, Belgium, pp 4046–4056.
       <a data-track="click_references" data-track-action="external reference" data-track-label="10.18653/v1/D18-1438" data-track-value="external reference" href="https://doi.org/10.18653/v1/D18-1438">
        https://doi.org/10.18653/v1/D18-1438
       </a>
      </p>
     </li>
     <li class="c-article-references__item js-c-reading-companion-references-item">
      <p class="c-article-references__text" id="ref-CR3">
       Cheng, J., &amp; Lapata, M. (2016). Neural summarization by extracting sentences and words. In: Erk K, Smith NA (Eds.), Proceedings of the 54th annual meeting of the association for computational linguistics (Volume 1: Long Papers).
       <i>
        Association for Computational Linguistics,
       </i>
       Berlin, Germany, pp 484–494.
       <a data-track="click_references" data-track-action="external reference" data-track-label="10.18653/v1/P16-1046" data-track-value="external reference" href="https://doi.org/10.18653/v1/P16-1046">
        https://doi.org/10.18653/v1/P16-1046
       </a>
      </p>
     </li>
     <li class="c-article-references__item js-c-reading-companion-references-item">
      <p class="c-article-references__text" id="ref-CR4">
       Cheng, X., Shen, Y., &amp; Lu, W. (2023). A set prediction network for extractive summarization. In: Rogers A, Boyd-Graber J, Okazaki N (Eds.), Findings of the association for computational linguistics: ACL 2023.
       <i>
        Association for Computational Linguistics,
       </i>
       Toronto, Canada, pp 4766–4777.
       <a data-track="click_references" data-track-action="external reference" data-track-label="10.18653/v1/2023.findings-acl.293" data-track-value="external reference" href="https://doi.org/10.18653/v1/2023.findings-acl.293">
        https://doi.org/10.18653/v1/2023.findings-acl.293
       </a>
      </p>
     </li>
     <li class="c-article-references__item js-c-reading-companion-references-item">
      <p class="c-article-references__text" id="ref-CR5">
       Cui, P., Hu, L., &amp; Liu, Y. (2020). Enhancing extractive text summarization with topic-aware graph neural networks. In: Scott D, Bel N, Zong C (Eds.), Proceedings of the 28th international conference on computational linguistics.
       <i>
        International Committee on Computational Linguistics,
       </i>
       Barcelona, Spain (Online), pp 5360–537.
       <a data-track="click_references" data-track-action="external reference" data-track-label="10.18653/v1/2020.coling-main.468" data-track-value="external reference" href="https://doi.org/10.18653/v1/2020.coling-main.468">
        https://doi.org/10.18653/v1/2020.coling-main.468
       </a>
      </p>
     </li>
     <li class="c-article-references__item js-c-reading-companion-references-item">
      <p class="c-article-references__text" id="ref-CR6">
       Erkan, G., &amp; Radev, D. R. (2004). Lexrank: Graph-based lexical centrality as salience in text summarization.
       <i>
        J Artif Intell Res,
       </i>
       <i>
        22
       </i>
       , 457–47.
       <a data-track="click_references" data-track-action="external reference" data-track-label="10.1613/JAIR.1523" data-track-value="external reference" href="https://doi.org/10.1613/JAIR.1523">
        https://doi.org/10.1613/JAIR.1523
       </a>
      </p>
      <p class="c-article-references__links u-hide-print">
       <a aria-label="Article reference 6" data-doi="10.1613/JAIR.1523" data-track="click_references" data-track-action="article reference" data-track-item_id="10.1613/JAIR.1523" data-track-label="10.1613/JAIR.1523" data-track-value="article reference" href="https://doi.org/10.1613%2FJAIR.1523" rel="nofollow noopener">
        Article
       </a>
       <a aria-label="Google Scholar reference 6" data-track="click_references" data-track-action="google scholar reference" data-track-item_id="link" data-track-label="link" data-track-value="google scholar reference" href="http://scholar.google.com/scholar_lookup?&amp;title=Lexrank%3A%20Graph-based%20lexical%20centrality%20as%20salience%20in%20text%20summarization&amp;journal=J%20Artif%20Intell%20Res&amp;doi=10.1613%2FJAIR.1523&amp;volume=22&amp;pages=457-47&amp;publication_year=2004&amp;author=Erkan%2CG&amp;author=Radev%2CDR" rel="nofollow noopener">
        Google Scholar
       </a>
      </p>
     </li>
     <li class="c-article-references__item js-c-reading-companion-references-item">
      <p class="c-article-references__text" id="ref-CR7">
       Jia, R., Cao, Y., Tang, H., et al. (2020). Neural extractive summarization with hierarchical attentive heterogeneous graph network. In: Webber B, Cohn T, He Y, et al (Eds.), Proceedings of the 2020 conference on Empirical Methods in Natural Language Processing (EMNLP).
       <i>
        Association for Computational Linguistics,
       </i>
       Online, pp 3622–363.
       <a data-track="click_references" data-track-action="external reference" data-track-label="10.18653/v1/2020.emnlp-main.295" data-track-value="external reference" href="https://doi.org/10.18653/v1/2020.emnlp-main.295">
        https://doi.org/10.18653/v1/2020.emnlp-main.295
       </a>
      </p>
     </li>
     <li class="c-article-references__item js-c-reading-companion-references-item">
      <p class="c-article-references__text" id="ref-CR8">
       Jiang, C., Xie, R., Ye, W., et al. (2023). Exploiting pseudo image captions for multimodal summarization. In: Rogers A, Boyd-Graber J, Okazaki N (Eds.), Findings of the association for computational linguistics: ACL 2023.
       <i>
        Association for Computational Linguistics,
       </i>
       Toronto, Canada, pp 161–175.
       <a data-track="click_references" data-track-action="external reference" data-track-label="10.18653/v1/2023.findings-acl.12" data-track-value="external reference" href="https://doi.org/10.18653/v1/2023.findings-acl.12">
        https://doi.org/10.18653/v1/2023.findings-acl.12
       </a>
      </p>
     </li>
     <li class="c-article-references__item js-c-reading-companion-references-item">
      <p class="c-article-references__text" id="ref-CR9">
       Jin, L., &amp; Chen, J. (2024). Self-supervised opinion summarization with multi-modal knowledge graph.
       <i>
        J Intell Inf Syst,
       </i>
       <i>
        62
       </i>
       (1), 191–208.
       <a data-track="click_references" data-track-action="external reference" data-track-label="10.1007/S10844-023-00812-1" data-track-value="external reference" href="https://doi.org/10.1007/S10844-023-00812-1">
        https://doi.org/10.1007/S10844-023-00812-1
       </a>
      </p>
      <p class="c-article-references__links u-hide-print">
       <a aria-label="Article reference 9" data-doi="10.1007/S10844-023-00812-1" data-track="click_references" data-track-action="article reference" data-track-item_id="10.1007/S10844-023-00812-1" data-track-label="10.1007/S10844-023-00812-1" data-track-value="article reference" href="https://link.springer.com/doi/10.1007/S10844-023-00812-1" rel="noopener">
        Article
       </a>
       <a aria-label="MathSciNet reference 9" data-track="click_references" data-track-action="mathscinet reference" data-track-item_id="link" data-track-label="link" data-track-value="mathscinet reference" href="http://www.ams.org/mathscinet-getitem?mr=4702630" rel="nofollow noopener">
        MathSciNet
       </a>
       <a aria-label="Google Scholar reference 9" data-track="click_references" data-track-action="google scholar reference" data-track-item_id="link" data-track-label="link" data-track-value="google scholar reference" href="http://scholar.google.com/scholar_lookup?&amp;title=Self-supervised%20opinion%20summarization%20with%20multi-modal%20knowledge%20graph&amp;journal=J%20Intell%20Inf%20Syst&amp;doi=10.1007%2FS10844-023-00812-1&amp;volume=62&amp;issue=1&amp;pages=191-208&amp;publication_year=2024&amp;author=Jin%2CL&amp;author=Chen%2CJ" rel="nofollow noopener">
        Google Scholar
       </a>
      </p>
     </li>
     <li class="c-article-references__item js-c-reading-companion-references-item">
      <p class="c-article-references__text" id="ref-CR10">
       Jing, B., You, Z., Yang, T., et al. (2021). Multiplex graph neural network for extractive text summarization. In: Moens MF, Huang X, Specia L, et al. (Eds.), Proceedings of the 2021 conference on empirical methods in natural language processing.
       <i>
        Association for Computational Linguistics,
       </i>
       Online and Punta Cana, Dominican Republic, pp 133–139.
       <a data-track="click_references" data-track-action="external reference" data-track-label="10.18653/v1/2021.emnlp-main.11" data-track-value="external reference" href="https://doi.org/10.18653/v1/2021.emnlp-main.11">
        https://doi.org/10.18653/v1/2021.emnlp-main.11
       </a>
      </p>
     </li>
     <li class="c-article-references__item js-c-reading-companion-references-item">
      <p class="c-article-references__text" id="ref-CR11">
       Kingma, D.P., &amp; Ba, J (2015). Adam: A method for stochastic optimization. In: Bengio Y, LeCun Y (Eds.), 3rd International conference on learning representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015,
       <i>
        Conference Track Proceedings,
       </i>
       [SPACE]
       <a data-track="click_references" data-track-action="external reference" data-track-label="http://arxiv.org/abs/1412.6980" data-track-value="external reference" href="http://arxiv.org/abs/1412.6980">
        http://arxiv.org/abs/1412.6980
       </a>
      </p>
     </li>
     <li class="c-article-references__item js-c-reading-companion-references-item">
      <p class="c-article-references__text" id="ref-CR12">
       Lewis, M., Liu, Y., Goyal, N., et al. (2020). BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In: Jurafsky D, Chai J, Schluter N, et al (Eds.), Proceedings of the 58th annual meeting of the association for computational linguistics.
       <i>
        Association for Computational Linguistics,
       </i>
       Online, pp 7871–7880.
       <a data-track="click_references" data-track-action="external reference" data-track-label="10.18653/v1/2020.acl-main.703" data-track-value="external reference" href="https://doi.org/10.18653/v1/2020.acl-main.703">
        https://doi.org/10.18653/v1/2020.acl-main.703
       </a>
      </p>
     </li>
     <li class="c-article-references__item js-c-reading-companion-references-item">
      <p class="c-article-references__text" id="ref-CR13">
       Li, H., Zhu, J., Liu, T., et al. (2018). Multi-modal sentence summarization with modality attention and image filtering. In: Lang J (Ed.), Proceedings of the twenty-seventh international joint conference on artificial intelligence, IJCAI 2018, July 13-19, 2018, Stockholm, Sweden. ijcai.org, Stockholm, Sweden, pp 4152–4158.
       <a data-track="click_references" data-track-action="external reference" data-track-label="10.24963/ijcai.2018/577" data-track-value="external reference" href="https://doi.org/10.24963/ijcai.2018/577">
        https://doi.org/10.24963/ijcai.2018/577
       </a>
      </p>
     </li>
     <li class="c-article-references__item js-c-reading-companion-references-item">
      <p class="c-article-references__text" id="ref-CR14">
       Li, J., Li, D., Xiong, C., et al. (2022). BLIP: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In: Chaudhuri K, Jegelka S, Song L, et al (Eds.), International conference on machine learning, ICML 2022, 17-23 July 2022, PMLR, Baltimore, Maryland, USA,
       <i>
        Proceedings of Machine Learning Research,
       </i>
       <i>
        162
       </i>
       pp 12888–12900
      </p>
     </li>
     <li class="c-article-references__item js-c-reading-companion-references-item">
      <p class="c-article-references__text" id="ref-CR15">
       Li, M., Chen, X., Gao, S., et al (2020) VMSMO: Learning to generate multimodal summary for video-based news articles. In: Webber B, Cohn T, He Y, et al (Eds.), Proceedings of the 2020 conference on Empirical Methods in Natural Language Processing (EMNLP).
       <i>
        Association for Computational Linguistics,
       </i>
       Online, pp 9360–9369.
       <a data-track="click_references" data-track-action="external reference" data-track-label="10.18653/v1/2020.emnlp-main.752" data-track-value="external reference" href="https://doi.org/10.18653/v1/2020.emnlp-main.752">
        https://doi.org/10.18653/v1/2020.emnlp-main.752
       </a>
      </p>
     </li>
     <li class="c-article-references__item js-c-reading-companion-references-item">
      <p class="c-article-references__text" id="ref-CR16">
       Liang, Y., Meng, F., Xu, J., et al. (2023). Summary-oriented vision modeling for multimodal abstractive summarization. In: Rogers A, Boyd-Graber J, Okazaki N (Eds.), Proceedings of the 61st annual meeting of the association for computational linguistics (Volume 1: Long Papers).
       <i>
        Association for Computational Linguistics,
       </i>
       Toronto, Canada, pp 2934–2951.
       <a data-track="click_references" data-track-action="external reference" data-track-label="10.18653/v1/2023.acl-long.165" data-track-value="external reference" href="https://doi.org/10.18653/v1/2023.acl-long.165">
        https://doi.org/10.18653/v1/2023.acl-long.165
       </a>
      </p>
     </li>
     <li class="c-article-references__item js-c-reading-companion-references-item">
      <p class="c-article-references__text" id="ref-CR17">
       Liu Y, &amp; Lapata M. (2019). Text summarization with pretrained encoders. In: Inui K, Jiang J, Ng V, et al (Eds.), Proceedings of the 2019 conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP).
       <i>
        Association for Computational Linguistics,
       </i>
       Hong Kong, China, pp 3730–3740.
       <a data-track="click_references" data-track-action="external reference" data-track-label="10.18653/v1/D19-1387" data-track-value="external reference" href="https://doi.org/10.18653/v1/D19-1387">
        https://doi.org/10.18653/v1/D19-1387
       </a>
      </p>
     </li>
     <li class="c-article-references__item js-c-reading-companion-references-item">
      <p class="c-article-references__text" id="ref-CR18">
       Liu, Y., Ott, M., Goyal, N., et al. (2019). Roberta: A robustly optimized BERT pretraining approach.
       <a data-track="click_references" data-track-action="external reference" data-track-label="http://arxiv.org/abs/1907.11692" data-track-value="external reference" href="http://arxiv.org/abs/1907.11692">
        arXiv:1907.11692
       </a>
       <a data-track="click_references" data-track-action="external reference" data-track-label="https://arxiv.org/abs/1907.11692" data-track-value="external reference" href="https://arxiv.org/abs/1907.11692">
        https://arxiv.org/abs/1907.11692
       </a>
      </p>
     </li>
     <li class="c-article-references__item js-c-reading-companion-references-item">
      <p class="c-article-references__text" id="ref-CR19">
       Liu, Y., Liu, P., Radev, D., et al. (2022). BRIO: Bringing order to abstractive summarization. In: Muresan S, Nakov P, Villavicencio A (Eds.), Proceedings of the 60th annual meeting of the association for computational linguistics (Volume 1: Long Papers).
       <i>
        Association for Computational Linguistics,
       </i>
       Dublin, Ireland, pp 2890–2903.
       <a data-track="click_references" data-track-action="external reference" data-track-label="10.18653/v1/2022.acl-long.207" data-track-value="external reference" href="https://doi.org/10.18653/v1/2022.acl-long.207">
        https://doi.org/10.18653/v1/2022.acl-long.207
       </a>
      </p>
     </li>
     <li class="c-article-references__item js-c-reading-companion-references-item">
      <p class="c-article-references__text" id="ref-CR20">
       van der Maaten L, &amp; Hinton G (2008) Visualizing data using t-sne.
       <i>
        Journal of Machine Learning Research
       </i>
       <i>
        9
       </i>
       (86), 2579–2605.
       <a data-track="click_references" data-track-action="external reference" data-track-label="http://jmlr.org/papers/v9/vandermaaten08a.html" data-track-value="external reference" href="http://jmlr.org/papers/v9/vandermaaten08a.html">
        http://jmlr.org/papers/v9/vandermaaten08a.html
       </a>
      </p>
     </li>
     <li class="c-article-references__item js-c-reading-companion-references-item">
      <p class="c-article-references__text" id="ref-CR21">
       Mihalcea, R., Tarau, P (2004) TextRank: Bringing order into text. In: Lin D, Wu D (Eds.), Proceedings of the 2004 conference on empirical methods in natural language processing.
       <i>
        Association for Computational Linguistics,
       </i>
       Barcelona, Spain, pp 404–411.
       <a data-track="click_references" data-track-action="external reference" data-track-label="https://aclanthology.org/W04-3252" data-track-value="external reference" href="https://aclanthology.org/W04-3252">
        https://aclanthology.org/W04-3252
       </a>
      </p>
     </li>
     <li class="c-article-references__item js-c-reading-companion-references-item">
      <p class="c-article-references__text" id="ref-CR22">
       Nallapati R, Zhai F, &amp; Zhou B. (2017) .Summarunner: A recurrent neural network based sequence model for extractive summarization of documents.
       <i>
        Proceedings of the AAAI Conference on Artificial Intelligence
       </i>
       <i>
        31
       </i>
       (1).
       <a data-track="click_references" data-track-action="external reference" data-track-label="10.1609/aaai.v31i1.10958" data-track-value="external reference" href="https://doi.org/10.1609/aaai.v31i1.10958">
        https://doi.org/10.1609/aaai.v31i1.10958
       </a>
      </p>
     </li>
     <li class="c-article-references__item js-c-reading-companion-references-item">
      <p class="c-article-references__text" id="ref-CR23">
       Narayan S, Cohen SB, &amp; Lapata M. (2018). Ranking sentences for extractive summarization with reinforcement learning. In: Walker M, Ji H, Stent A (Eds.) Proceedings of the 2018 conference of the North American chapter of the association for computational linguistics: Human language technologies, Volume 1 (Long Papers).
       <i>
        Association for Computational Linguistics,
       </i>
       New Orleans, Louisiana, pp 1747–1759.
       <a data-track="click_references" data-track-action="external reference" data-track-label="10.18653/v1/N18-1158" data-track-value="external reference" href="https://doi.org/10.18653/v1/N18-1158">
        https://doi.org/10.18653/v1/N18-1158
       </a>
      </p>
     </li>
     <li class="c-article-references__item js-c-reading-companion-references-item">
      <p class="c-article-references__text" id="ref-CR24">
       Overbay, K., Ahn, J., Pesaran zadeh, F., et al. (2023). mRedditSum: A multimodal abstractive summarization dataset of Reddit threads with images. In: Bouamor H, Pino J, Bali K (Eds.), Proceedings of the 2023 conference on empirical methods in natural language processing.
       <i>
        Association for Computational Linguistics,
       </i>
       Singapore, pp 4117–4132.
       <a data-track="click_references" data-track-action="external reference" data-track-label="10.18653/v1/2023.emnlp-main.251" data-track-value="external reference" href="https://doi.org/10.18653/v1/2023.emnlp-main.251">
        https://doi.org/10.18653/v1/2023.emnlp-main.251
       </a>
      </p>
     </li>
     <li class="c-article-references__item js-c-reading-companion-references-item">
      <p class="c-article-references__text" id="ref-CR25">
       Paulus, R., Xiong, C., &amp; Socher, R. (2018). A deep reinforced model for abstractive summarization. In: International conference on learning representations,
       <a data-track="click_references" data-track-action="external reference" data-track-label="https://openreview.net/forum?id=HkAClQgA-" data-track-value="external reference" href="https://openreview.net/forum?id=HkAClQgA-">
        https://openreview.net/forum?id=HkAClQgA-
       </a>
      </p>
     </li>
     <li class="c-article-references__item js-c-reading-companion-references-item">
      <p class="c-article-references__text" id="ref-CR26">
       Pennington, J., Socher, R., &amp; Manning, C. (2014). GloVe: Global vectors for word representation. In: Moschitti A, Pang B, Daelemans W (Eds.) Proceedings of the 2014 conference on Empirical Methods in Natural Language Processing (EMNLP).
       <i>
        Association for Computational Linguistics,
       </i>
       Doha, Qatar, pp 1532–1543.
       <a data-track="click_references" data-track-action="external reference" data-track-label="10.3115/v1/D14-1162" data-track-value="external reference" href="https://doi.org/10.3115/v1/D14-1162">
        https://doi.org/10.3115/v1/D14-1162
       </a>
      </p>
     </li>
     <li class="c-article-references__item js-c-reading-companion-references-item">
      <p class="c-article-references__text" id="ref-CR27">
       Phan TA, Nguyen NDN, &amp; Bui KHN. (2022). HeterGraphLongSum: Heterogeneous graph neural network with passage aggregation for extractive long document summarization. In: Calzolari N, Huang CR, Kim H, et al (Eds.), Proceedings of the 29th international conference on computational linguistics.
       <i>
        International Committee on Computational Linguistics,
       </i>
       Gyeongju, Republic of Korea, pp 6248–6258.
       <a data-track="click_references" data-track-action="external reference" data-track-label="https://aclanthology.org/2022.coling-1.545" data-track-value="external reference" href="https://aclanthology.org/2022.coling-1.545">
        https://aclanthology.org/2022.coling-1.545
       </a>
      </p>
     </li>
     <li class="c-article-references__item js-c-reading-companion-references-item">
      <p class="c-article-references__text" id="ref-CR28">
       Radford A, Kim JW, Hallacy C, et al (2021) Learning transferable visual models from natural language supervision. In: Meila M, Zhang T (Eds.),
       <i>
        Proceedings of the 38th International Conference on Machine Learning,
       </i>
       ICML 2021, 18-24 July 2021, Virtual Event, pp 8748–8763.
       <a data-track="click_references" data-track-action="external reference" data-track-label="http://proceedings.mlr.press/v139/radford21a.html" data-track-value="external reference" href="http://proceedings.mlr.press/v139/radford21a.html">
        http://proceedings.mlr.press/v139/radford21a.html
       </a>
      </p>
     </li>
     <li class="c-article-references__item js-c-reading-companion-references-item">
      <p class="c-article-references__text" id="ref-CR29">
       See, A., Liu, P.J., Manning, C.D. (2017). Get to the point: Summarization with pointer-generator networks. In: Barzilay R, Kan MY (Eds.), Proceedings of the 55th annual meeting of the association for computational linguistics (Volume 1: Long Papers).
       <i>
        Association for Computational Linguistics,
       </i>
       Vancouver, Canada, pp 1073–1083.
       <a data-track="click_references" data-track-action="external reference" data-track-label="10.18653/v1/P17-1099" data-track-value="external reference" href="https://doi.org/10.18653/v1/P17-1099">
        https://doi.org/10.18653/v1/P17-1099
       </a>
      </p>
     </li>
     <li class="c-article-references__item js-c-reading-companion-references-item">
      <p class="c-article-references__text" id="ref-CR30">
       Song, Z., &amp; King, I. (2022). Hierarchical heterogeneous graph attention network for syntax-aware summarization.
       <i>
        Proceedings of the AAAI Conference on Artificial Intelligence,
       </i>
       <i>
        36
       </i>
       (10), 11340–1134.
       <a data-track="click_references" data-track-action="external reference" data-track-label="10.1609/aaai.v36i10.21385" data-track-value="external reference" href="https://doi.org/10.1609/aaai.v36i10.21385">
        https://doi.org/10.1609/aaai.v36i10.21385
       </a>
      </p>
      <p class="c-article-references__links u-hide-print">
       <a aria-label="Article reference 30" data-doi="10.1609/aaai.v36i10.21385" data-track="click_references" data-track-action="article reference" data-track-item_id="10.1609/aaai.v36i10.21385" data-track-label="10.1609/aaai.v36i10.21385" data-track-value="article reference" href="https://doi.org/10.1609%2Faaai.v36i10.21385" rel="nofollow noopener">
        Article
       </a>
       <a aria-label="Google Scholar reference 30" data-track="click_references" data-track-action="google scholar reference" data-track-item_id="link" data-track-label="link" data-track-value="google scholar reference" href="http://scholar.google.com/scholar_lookup?&amp;title=Hierarchical%20heterogeneous%20graph%20attention%20network%20for%20syntax-aware%20summarization&amp;journal=Proceedings%20of%20the%20AAAI%20Conference%20on%20Artificial%20Intelligence&amp;doi=10.1609%2Faaai.v36i10.21385&amp;volume=36&amp;issue=10&amp;pages=11340-1134&amp;publication_year=2022&amp;author=Song%2CZ&amp;author=King%2CI" rel="nofollow noopener">
        Google Scholar
       </a>
      </p>
     </li>
     <li class="c-article-references__item js-c-reading-companion-references-item">
      <p class="c-article-references__text" id="ref-CR31">
       Sutskever, I., Vinyals, O., &amp; Le, Q.V. (2014). Sequence to sequence learning with neural networks. In: Ghahramani Z, Welling M, Cortes C, et al (Eds.),
       <i>
        Advances in Neural Information Processing Systems,
       </i>
       [SPACE]
       <a data-track="click_references" data-track-action="external reference" data-track-label="https://proceedings.neurips.cc/paper/2014/hash/a14ac55a4f27472c5d894ec1c3c743d2-Abstract.html" data-track-value="external reference" href="https://proceedings.neurips.cc/paper/2014/hash/a14ac55a4f27472c5d894ec1c3c743d2-Abstract.html">
        https://proceedings.neurips.cc/paper/2014/hash/a14ac55a4f27472c5d894ec1c3c743d2-Abstract.html
       </a>
      </p>
     </li>
     <li class="c-article-references__item js-c-reading-companion-references-item">
      <p class="c-article-references__text" id="ref-CR32">
       Tang, P., Hu, K., Yan, R., et al. (2022). OTExtSum: Extractive text summarisation with optimal transport. In: Carpuat M, de Marneffe MC, Meza Ruiz IV (eds) Findings of the association for computational linguistics: NAACL 2022.
       <i>
        Association for Computational Linguistics,
       </i>
       Seattle, United States, pp 1128–1141.
       <a data-track="click_references" data-track-action="external reference" data-track-label="10.18653/v1/2022.findings-naacl.85" data-track-value="external reference" href="https://doi.org/10.18653/v1/2022.findings-naacl.85">
        https://doi.org/10.18653/v1/2022.findings-naacl.85
       </a>
      </p>
     </li>
     <li class="c-article-references__item js-c-reading-companion-references-item">
      <p class="c-article-references__text" id="ref-CR33">
       Vaswani A, Shazeer N, Parmar N, et al. (2017). Attention is all you need. In: Guyon I, Luxburg UV, Bengio S, et al (Eds.),
       <i>
        Advances in Neural Information Processing Systems,
       </i>
       [SPACE]
       <a data-track="click_references" data-track-action="external reference" data-track-label="https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html" data-track-value="external reference" href="https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html">
        https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html
       </a>
      </p>
     </li>
     <li class="c-article-references__item js-c-reading-companion-references-item">
      <p class="c-article-references__text" id="ref-CR34">
       Velickovic, P., Cucurull, G., Casanova, A., et al. (2017).
       <i>
        Graph attention networks. stat,
       </i>
       <i>
        1050
       </i>
       (20), 10–48550.
      </p>
      <p class="c-article-references__links u-hide-print">
       <a aria-label="Google Scholar reference 34" data-track="click_references" data-track-action="google scholar reference" data-track-item_id="link" data-track-label="link" data-track-value="google scholar reference" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=Graph%20attention%20networks.%20stat&amp;volume=1050&amp;issue=20&amp;pages=10-48550&amp;publication_year=2017&amp;author=Velickovic%2CP&amp;author=Cucurull%2CG&amp;author=Casanova%2CA" rel="nofollow noopener">
        Google Scholar
       </a>
      </p>
     </li>
     <li class="c-article-references__item js-c-reading-companion-references-item">
      <p class="c-article-references__text" id="ref-CR35">
       Wang, D., Liu, P., Zheng, Y., et al. (2020). Heterogeneous graph neural networks for extractive document summarization. In: Jurafsky D, Chai J, Schluter N, et al (Eds.), Proceedings of the 58th annual meeting of the association for computational linguistics.
       <i>
        Association for Computational Linguistics,
       </i>
       Online, pp 6209–6219.
       <a data-track="click_references" data-track-action="external reference" data-track-label="10.18653/v1/2020.acl-main.553" data-track-value="external reference" href="https://doi.org/10.18653/v1/2020.acl-main.553">
        https://doi.org/10.18653/v1/2020.acl-main.553
       </a>
      </p>
     </li>
     <li class="c-article-references__item js-c-reading-companion-references-item">
      <p class="c-article-references__text" id="ref-CR36">
       Wu W, Li W, Xiao X, et al. (2021). BASS: Boosting abstractive summarization with unified semantic graph. In: Proceedings of the 59th annual meeting of the association for computational linguistics and the 11th international joint conference on natural language processing (Volume 1: Long Papers).
       <i>
        Association for Computational Linguistics,
       </i>
       Online, pp 6052–6067.
       <a data-track="click_references" data-track-action="external reference" data-track-label="10.18653/v1/2021.acl-long.472" data-track-value="external reference" href="https://doi.org/10.18653/v1/2021.acl-long.472">
        https://doi.org/10.18653/v1/2021.acl-long.472
       </a>
      </p>
     </li>
     <li class="c-article-references__item js-c-reading-companion-references-item">
      <p class="c-article-references__text" id="ref-CR37">
       Xiao M, Zhu J, Lin H, et al. (2023). CFSum coarse-to-fine contribution network for multimodal summarization. In: Rogers A, Boyd-Graber J, Okazaki N (Eds.), Proceedings of the 61st annual meeting of the association for computational linguistics (Volume 1: Long Papers).
       <i>
        Association for Computational Linguistics,
       </i>
       Toronto, Canada, pp 8538–8553.
       <a data-track="click_references" data-track-action="external reference" data-track-label="10.18653/v1/2023.acl-long.476" data-track-value="external reference" href="https://doi.org/10.18653/v1/2023.acl-long.476">
        https://doi.org/10.18653/v1/2023.acl-long.476
       </a>
      </p>
     </li>
     <li class="c-article-references__item js-c-reading-companion-references-item">
      <p class="c-article-references__text" id="ref-CR38">
       Xie, F., Chen, J., &amp; Chen, K. (2023). Extractive text-image summarization with relation-enhanced graph attention network.
       <i>
        J Intell Inf Syst,
       </i>
       <i>
        61
       </i>
       (2), 325–341.
       <a data-track="click_references" data-track-action="external reference" data-track-label="10.1007/S10844-022-00757-X" data-track-value="external reference" href="https://doi.org/10.1007/S10844-022-00757-X">
        https://doi.org/10.1007/S10844-022-00757-X
       </a>
      </p>
      <p class="c-article-references__links u-hide-print">
       <a aria-label="Article reference 38" data-doi="10.1007/S10844-022-00757-X" data-track="click_references" data-track-action="article reference" data-track-item_id="10.1007/S10844-022-00757-X" data-track-label="10.1007/S10844-022-00757-X" data-track-value="article reference" href="https://link.springer.com/doi/10.1007/S10844-022-00757-X" rel="noopener">
        Article
       </a>
       <a aria-label="Google Scholar reference 38" data-track="click_references" data-track-action="google scholar reference" data-track-item_id="link" data-track-label="link" data-track-value="google scholar reference" href="http://scholar.google.com/scholar_lookup?&amp;title=Extractive%20text-image%20summarization%20with%20relation-enhanced%20graph%20attention%20network&amp;journal=J%20Intell%20Inf%20Syst&amp;doi=10.1007%2FS10844-022-00757-X&amp;volume=61&amp;issue=2&amp;pages=325-341&amp;publication_year=2023&amp;author=Xie%2CF&amp;author=Chen%2CJ&amp;author=Chen%2CK" rel="nofollow noopener">
        Google Scholar
       </a>
      </p>
     </li>
     <li class="c-article-references__item js-c-reading-companion-references-item">
      <p class="c-article-references__text" id="ref-CR39">
       Ying, C., Cai, T., Luo, S., et al. (2021). Do transformers really perform badly for graph representation? In: Ranzato M, Beygelzimer A, Dauphin YN, et al (Eds.),
       <i>
        Advances in Neural Information Processing Systems,
       </i>
       pp 28877–28888.
       <a data-track="click_references" data-track-action="external reference" data-track-label="https://proceedings.neurips.cc/paper/2021/hash/f1c1592588411002af340cbaedd6fc33-Abstract.html" data-track-value="external reference" href="https://proceedings.neurips.cc/paper/2021/hash/f1c1592588411002af340cbaedd6fc33-Abstract.html">
        https://proceedings.neurips.cc/paper/2021/hash/f1c1592588411002af340cbaedd6fc33-Abstract.html
       </a>
      </p>
     </li>
     <li class="c-article-references__item js-c-reading-companion-references-item">
      <p class="c-article-references__text" id="ref-CR40">
       Zhang, J., Zhao, Y., Saleh, M., et al. (2020). PEGASUS: pre-training with extracted gap-sentences for abstractive summarization. In: Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, pp 11328–11339.
       <a data-track="click_references" data-track-action="external reference" data-track-label="http://proceedings.mlr.press/v119/zhang20ae.html" data-track-value="external reference" href="http://proceedings.mlr.press/v119/zhang20ae.html">
        http://proceedings.mlr.press/v119/zhang20ae.html
       </a>
      </p>
     </li>
     <li class="c-article-references__item js-c-reading-companion-references-item">
      <p class="c-article-references__text" id="ref-CR41">
       Zhang, L., Zhang, X., &amp; Pan, J. (2022a). Hierarchical cross-modality semantic correlation learning model for multimodal summarization.
       <i>
        Proceedings of the AAAI Conference on Artificial Intelligence
       </i>
       <i>
        36
       </i>
       (10) 11676–11684.
       <a data-track="click_references" data-track-action="external reference" data-track-label="10.1609/aaai.v36i10.21422" data-track-value="external reference" href="https://doi.org/10.1609/aaai.v36i10.21422">
        https://doi.org/10.1609/aaai.v36i10.21422
       </a>
      </p>
     </li>
     <li class="c-article-references__item js-c-reading-companion-references-item">
      <p class="c-article-references__text" id="ref-CR42">
       Zhang, M., Zhou, G., Yu, W., et al. (2022b). MAA-PTG: multimodal aspect-aware product title generation.
       <i>
        J Intell Inf Syst
       </i>
       <i>
        59
       </i>
       (1) 213–235.
       <a data-track="click_references" data-track-action="external reference" data-track-label="10.1007/S10844-022-00695-8" data-track-value="external reference" href="https://doi.org/10.1007/S10844-022-00695-8">
        https://doi.org/10.1007/S10844-022-00695-8
       </a>
      </p>
     </li>
     <li class="c-article-references__item js-c-reading-companion-references-item">
      <p class="c-article-references__text" id="ref-CR43">
       Zhang, Z., Meng, X., Wang, Y., et al. (2022c). Unims: A unified framework for multimodal summarization with knowledge distillation.
       <i>
        Proceedings of the AAAI Conference on Artificial Intelligence
       </i>
       <i>
        36
       </i>
       (10) 11757–11764.
       <a data-track="click_references" data-track-action="external reference" data-track-label="10.1609/aaai.v36i10.21431" data-track-value="external reference" href="https://doi.org/10.1609/aaai.v36i10.21431">
        https://doi.org/10.1609/aaai.v36i10.21431
       </a>
      </p>
     </li>
     <li class="c-article-references__item js-c-reading-companion-references-item">
      <p class="c-article-references__text" id="ref-CR44">
       Zhong, M., Liu, P., Chen, Y., et al. (2020). Extractive summarization as text matching. In: Jurafsky D, Chai J, Schluter N, et al (Eds.) Proceedings of the 58th annual meeting of the association for computational linguistics.
       <i>
        Association for Computational Linguistics,
       </i>
       Online, pp 6197–6208.
       <a data-track="click_references" data-track-action="external reference" data-track-label="10.18653/v1/2020.acl-main.552" data-track-value="external reference" href="https://doi.org/10.18653/v1/2020.acl-main.552">
        https://doi.org/10.18653/v1/2020.acl-main.552
       </a>
      </p>
     </li>
     <li class="c-article-references__item js-c-reading-companion-references-item">
      <p class="c-article-references__text" id="ref-CR45">
       Zhu, J., Li, H., Liu, T., et al. (2018). MSMO: Multimodal summarization with multimodal output. In: Riloff E, Chiang D, Hockenmaier J, et al (eds) Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing.
       <i>
        Association for Computational Linguistics,
       </i>
       Brussels, Belgium, pp 4154–4164.
       <a data-track="click_references" data-track-action="external reference" data-track-label="10.18653/v1/D18-1448" data-track-value="external reference" href="https://doi.org/10.18653/v1/D18-1448">
        https://doi.org/10.18653/v1/D18-1448
       </a>
      </p>
     </li>
     <li class="c-article-references__item js-c-reading-companion-references-item">
      <p class="c-article-references__text" id="ref-CR46">
       Zhu, J., Zhou, Y., Zhang, J., et al. (2020). Multimodal summarization with guidance of multimodal reference.
       <i>
        Proceedings of the AAAI Conference on Artificial Intelligence,
       </i>
       <i>
        34
       </i>
       (05), 9749–975.
       <a data-track="click_references" data-track-action="external reference" data-track-label="10.1609/aaai.v34i05.6525" data-track-value="external reference" href="https://doi.org/10.1609/aaai.v34i05.6525">
        https://doi.org/10.1609/aaai.v34i05.6525
       </a>
      </p>
      <p class="c-article-references__links u-hide-print">
       <a aria-label="Article reference 46" data-doi="10.1609/aaai.v34i05.6525" data-track="click_references" data-track-action="article reference" data-track-item_id="10.1609/aaai.v34i05.6525" data-track-label="10.1609/aaai.v34i05.6525" data-track-value="article reference" href="https://doi.org/10.1609%2Faaai.v34i05.6525" rel="nofollow noopener">
        Article
       </a>
       <a aria-label="Google Scholar reference 46" data-track="click_references" data-track-action="google scholar reference" data-track-item_id="link" data-track-label="link" data-track-value="google scholar reference" href="http://scholar.google.com/scholar_lookup?&amp;title=Multimodal%20summarization%20with%20guidance%20of%20multimodal%20reference&amp;journal=Proceedings%20of%20the%20AAAI%20Conference%20on%20Artificial%20Intelligence&amp;doi=10.1609%2Faaai.v34i05.6525&amp;volume=34&amp;issue=05&amp;pages=9749-975&amp;publication_year=2020&amp;author=Zhu%2CJ&amp;author=Zhou%2CY&amp;author=Zhang%2CJ" rel="nofollow noopener">
        Google Scholar
       </a>
      </p>
     </li>
    </ul>
    <p class="c-article-references__download u-hide-print">
     <a data-track="click" data-track-action="download citation references" data-track-label="link" href="https://citation-needed.springer.com/v2/references/10.1007/s10844-024-00886-5?format=refman&amp;flavour=references" rel="nofollow">
      Download references
      <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
       <use xlink:href="#icon-eds-i-download-medium" xmlns:xlink="http://www.w3.org/1999/xlink">
       </use>
      </svg>
     </a>
    </p>
   </div>
  </div>
 </div>
</section>
<section data-title="Funding">
 <div class="c-article-section" id="Fun-section">
  <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Fun">
   Funding
  </h2>
  <div class="c-article-section__content" id="Fun-content">
   <p>
    This research was sponsored by the National Natural Science Foundation of China (No.61806101).
   </p>
  </div>
 </div>
</section>
<section aria-labelledby="author-information" data-title="Author information">
 <div class="c-article-section" id="author-information-section">
  <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">
   Author information
  </h2>
  <div class="c-article-section__content" id="author-information-content">
   <span class="c-article-author-information__subtitle u-visually-hidden" id="author-notes">
    Author notes
   </span>
   <ol class="c-article-author-information__list">
    <li class="c-article-author-information__item" id="na1">
     <p>
      Xiankai Jiang and Jingqiang Chen are both equally contributed.
     </p>
    </li>
   </ol>
   <h3 class="c-article__sub-heading" id="affiliations">
    Authors and Affiliations
   </h3>
   <ol class="c-article-author-affiliation__list">
    <li id="Aff1">
     <p class="c-article-author-affiliation__address">
      School of Computer Science, Nanjing University of Posts and Telecommunications, Nanjing, 210049, Jiangsu, China
     </p>
     <p class="c-article-author-affiliation__authors-list">
      Xiankai Jiang &amp; Jingqiang Chen
     </p>
    </li>
   </ol>
   <div class="u-js-hide u-hide-print" data-test="author-info">
    <span class="c-article__sub-heading">
     Authors
    </span>
    <ol class="c-article-authors-search u-list-reset">
     <li id="auth-Xiankai-Jiang-Aff1">
      <span class="c-article-authors-search__title u-h3 js-search-name">
       Xiankai Jiang
      </span>
      <div class="c-article-authors-search__list">
       <div class="c-article-authors-search__item c-article-authors-search__list-item--left">
        <a class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" href="/search?dc.creator=Xiankai%20Jiang" rel="nofollow">
         View author publications
        </a>
       </div>
       <div class="c-article-authors-search__item c-article-authors-search__list-item--right">
        <p class="search-in-title-js c-article-authors-search__text">
         You can also search for this author in
         <span class="c-article-identifiers">
          <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - pubmed" data-track-label="link" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Xiankai%20Jiang" rel="nofollow">
           PubMed
          </a>
          <span class="u-hide">
          </span>
          <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - scholar" data-track-label="link" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Xiankai%20Jiang%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" rel="nofollow">
           Google Scholar
          </a>
         </span>
        </p>
       </div>
      </div>
     </li>
     <li id="auth-Jingqiang-Chen-Aff1">
      <span class="c-article-authors-search__title u-h3 js-search-name">
       Jingqiang Chen
      </span>
      <div class="c-article-authors-search__list">
       <div class="c-article-authors-search__item c-article-authors-search__list-item--left">
        <a class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" href="/search?dc.creator=Jingqiang%20Chen" rel="nofollow">
         View author publications
        </a>
       </div>
       <div class="c-article-authors-search__item c-article-authors-search__list-item--right">
        <p class="search-in-title-js c-article-authors-search__text">
         You can also search for this author in
         <span class="c-article-identifiers">
          <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - pubmed" data-track-label="link" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Jingqiang%20Chen" rel="nofollow">
           PubMed
          </a>
          <span class="u-hide">
          </span>
          <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - scholar" data-track-label="link" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Jingqiang%20Chen%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" rel="nofollow">
           Google Scholar
          </a>
         </span>
        </p>
       </div>
      </div>
     </li>
    </ol>
   </div>
   <h3 class="c-article__sub-heading" id="contributions">
    Contributions
   </h3>
   <p>
    X.J. and J.C. wrote the manuscript . All authors reviewed the manuscript.
   </p>
   <h3 class="c-article__sub-heading" id="corresponding-author">
    Corresponding author
   </h3>
   <p id="corresponding-author-list">
    Correspondence to
    <a href="mailto:cjq@njupt.edu.cn" id="corresp-c1">
     Jingqiang Chen
    </a>
    .
   </p>
  </div>
 </div>
</section>
<section data-title="Ethics declarations">
 <div class="c-article-section" id="ethics-section">
  <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="ethics">
   Ethics declarations
  </h2>
  <div class="c-article-section__content" id="ethics-content">
   <h3 class="c-article__sub-heading" id="FPar1">
    Competing Interests
   </h3>
   <p>
    The authors declare no competing interests.
   </p>
   <h3 class="c-article__sub-heading" id="FPar2">
    Ethics Approval and Consent to Participate
   </h3>
   <p>
    Not Applicable.
   </p>
   <h3 class="c-article__sub-heading" id="FPar3">
    Consent for Publication
   </h3>
   <p>
    The authors declare that they consent for publication.
   </p>
  </div>
 </div>
</section>
<section data-title="Additional information">
 <div class="c-article-section" id="additional-information-section">
  <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="additional-information">
   Additional information
  </h2>
  <div class="c-article-section__content" id="additional-information-content">
   <h3 class="c-article__sub-heading">
    Publisher's Note
   </h3>
   <p>
    Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.
   </p>
  </div>
 </div>
</section>
<section data-title="Rights and permissions">
 <div class="c-article-section" id="rightslink-section">
  <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">
   Rights and permissions
  </h2>
  <div class="c-article-section__content" id="rightslink-content">
   <p>
    Springer Nature or its licensor (e.g. a society or other partner) holds exclusive rights to this article under a publishing agreement with the author(s) or other rightsholder(s); author self-archiving of the accepted manuscript version of this article is solely governed by the terms of such publishing agreement and applicable law.
   </p>
   <p class="c-article-rights">
    <a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Heterogeneous%20graphormer%20for%20extractive%20multimodal%20summarization&amp;author=Xiankai%20Jiang%20et%20al&amp;contentID=10.1007%2Fs10844-024-00886-5&amp;copyright=The%20Author%28s%29%2C%20under%20exclusive%20licence%20to%20Springer%20Science%2BBusiness%20Media%2C%20LLC%2C%20part%20of%20Springer%20Nature&amp;publication=0925-9902&amp;publicationDate=2024-09-30&amp;publisherName=SpringerNature&amp;orderBeanReset=true">
     Reprints and permissions
    </a>
   </p>
  </div>
 </div>
</section>
<section aria-labelledby="article-info" data-title="About this article">
 <div class="c-article-section" id="article-info-section">
  <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">
   About this article
  </h2>
  <div class="c-article-section__content" id="article-info-content">
   <div class="c-bibliographic-information">
    <div class="u-hide-print c-bibliographic-information__column c-bibliographic-information__column--border">
     <a data-crossmark="10.1007/s10844-024-00886-5" data-test="crossmark" data-track="click" data-track-action="Click Crossmark" data-track-label="link" href="https://crossmark.crossref.org/dialog/?doi=10.1007/s10844-024-00886-5" rel="noopener" target="_blank">
      <img alt="Check for updates. Verify currency and authenticity via CrossMark" height="81" loading="lazy" src="data:image/svg+xml;base64,<svg height="81" width="57" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m17.35 35.45 21.3-14.2v-17.03h-21.3" fill="#989898"/><path d="m38.65 35.45-21.3-14.2v-17.03h21.3" fill="#747474"/><path d="m28 .5c-12.98 0-23.5 10.52-23.5 23.5s10.52 23.5 23.5 23.5 23.5-10.52 23.5-23.5c0-6.23-2.48-12.21-6.88-16.62-4.41-4.4-10.39-6.88-16.62-6.88zm0 41.25c-9.8 0-17.75-7.95-17.75-17.75s7.95-17.75 17.75-17.75 17.75 7.95 17.75 17.75c0 4.71-1.87 9.22-5.2 12.55s-7.84 5.2-12.55 5.2z" fill="#535353"/><path d="m41 36c-5.81 6.23-15.23 7.45-22.43 2.9-7.21-4.55-10.16-13.57-7.03-21.5l-4.92-3.11c-4.95 10.7-1.19 23.42 8.78 29.71 9.97 6.3 23.07 4.22 30.6-4.86z" fill="#9c9c9c"/><path d="m.2 58.45c0-.75.11-1.42.33-2.01s.52-1.09.91-1.5c.38-.41.83-.73 1.34-.94.51-.22 1.06-.32 1.65-.32.56 0 1.06.11 1.51.35.44.23.81.5 1.1.81l-.91 1.01c-.24-.24-.49-.42-.75-.56-.27-.13-.58-.2-.93-.2-.39 0-.73.08-1.05.23-.31.16-.58.37-.81.66-.23.28-.41.63-.53 1.04-.13.41-.19.88-.19 1.39 0 1.04.23 1.86.68 2.46.45.59 1.06.88 1.84.88.41 0 .77-.07 1.07-.23s.59-.39.85-.68l.91 1c-.38.43-.8.76-1.28.99-.47.22-1 .34-1.58.34-.59 0-1.13-.1-1.64-.31-.5-.2-.94-.51-1.31-.91-.38-.4-.67-.9-.88-1.48-.22-.59-.33-1.26-.33-2.02zm8.4-5.33h1.61v2.54l-.05 1.33c.29-.27.61-.51.96-.72s.76-.31 1.24-.31c.73 0 1.27.23 1.61.71.33.47.5 1.14.5 2.02v4.31h-1.61v-4.1c0-.57-.08-.97-.25-1.21-.17-.23-.45-.35-.83-.35-.3 0-.56.08-.79.22-.23.15-.49.36-.78.64v4.8h-1.61zm7.37 6.45c0-.56.09-1.06.26-1.51.18-.45.42-.83.71-1.14.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.36c.07.62.29 1.1.65 1.44.36.33.82.5 1.38.5.29 0 .57-.04.83-.13s.51-.21.76-.37l.55 1.01c-.33.21-.69.39-1.09.53-.41.14-.83.21-1.26.21-.48 0-.92-.08-1.34-.25-.41-.16-.76-.4-1.07-.7-.31-.31-.55-.69-.72-1.13-.18-.44-.26-.95-.26-1.52zm4.6-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.07.45-.31.29-.5.73-.58 1.3zm2.5.62c0-.57.09-1.08.28-1.53.18-.44.43-.82.75-1.13s.69-.54 1.1-.71c.42-.16.85-.24 1.31-.24.45 0 .84.08 1.17.23s.61.34.85.57l-.77 1.02c-.19-.16-.38-.28-.56-.37-.19-.09-.39-.14-.61-.14-.56 0-1.01.21-1.35.63-.35.41-.52.97-.52 1.67 0 .69.17 1.24.51 1.66.34.41.78.62 1.32.62.28 0 .54-.06.78-.17.24-.12.45-.26.64-.42l.67 1.03c-.33.29-.69.51-1.08.65-.39.15-.78.23-1.18.23-.46 0-.9-.08-1.31-.24-.4-.16-.75-.39-1.05-.7s-.53-.69-.7-1.13c-.17-.45-.25-.96-.25-1.53zm6.91-6.45h1.58v6.17h.05l2.54-3.16h1.77l-2.35 2.8 2.59 4.07h-1.75l-1.77-2.98-1.08 1.23v1.75h-1.58zm13.69 1.27c-.25-.11-.5-.17-.75-.17-.58 0-.87.39-.87 1.16v.75h1.34v1.27h-1.34v5.6h-1.61v-5.6h-.92v-1.2l.92-.07v-.72c0-.35.04-.68.13-.98.08-.31.21-.57.4-.79s.42-.39.71-.51c.28-.12.63-.18 1.04-.18.24 0 .48.02.69.07.22.05.41.1.57.17zm.48 5.18c0-.57.09-1.08.27-1.53.17-.44.41-.82.72-1.13.3-.31.65-.54 1.04-.71.39-.16.8-.24 1.23-.24s.84.08 1.24.24c.4.17.74.4 1.04.71s.54.69.72 1.13c.19.45.28.96.28 1.53s-.09 1.08-.28 1.53c-.18.44-.42.82-.72 1.13s-.64.54-1.04.7-.81.24-1.24.24-.84-.08-1.23-.24-.74-.39-1.04-.7c-.31-.31-.55-.69-.72-1.13-.18-.45-.27-.96-.27-1.53zm1.65 0c0 .69.14 1.24.43 1.66.28.41.68.62 1.18.62.51 0 .9-.21 1.19-.62.29-.42.44-.97.44-1.66 0-.7-.15-1.26-.44-1.67-.29-.42-.68-.63-1.19-.63-.5 0-.9.21-1.18.63-.29.41-.43.97-.43 1.67zm6.48-3.44h1.33l.12 1.21h.05c.24-.44.54-.79.88-1.02.35-.24.7-.36 1.07-.36.32 0 .59.05.78.14l-.28 1.4-.33-.09c-.11-.01-.23-.02-.38-.02-.27 0-.56.1-.86.31s-.55.58-.77 1.1v4.2h-1.61zm-47.87 15h1.61v4.1c0 .57.08.97.25 1.2.17.24.44.35.81.35.3 0 .57-.07.8-.22.22-.15.47-.39.73-.73v-4.7h1.61v6.87h-1.32l-.12-1.01h-.04c-.3.36-.63.64-.98.86-.35.21-.76.32-1.24.32-.73 0-1.27-.24-1.61-.71-.33-.47-.5-1.14-.5-2.02zm9.46 7.43v2.16h-1.61v-9.59h1.33l.12.72h.05c.29-.24.61-.45.97-.63.35-.17.72-.26 1.1-.26.43 0 .81.08 1.15.24.33.17.61.4.84.71.24.31.41.68.53 1.11.13.42.19.91.19 1.44 0 .59-.09 1.11-.25 1.57-.16.47-.38.85-.65 1.16-.27.32-.58.56-.94.73-.35.16-.72.25-1.1.25-.3 0-.6-.07-.9-.2s-.59-.31-.87-.56zm0-2.3c.26.22.5.37.73.45.24.09.46.13.66.13.46 0 .84-.2 1.15-.6.31-.39.46-.98.46-1.77 0-.69-.12-1.22-.35-1.61-.23-.38-.61-.57-1.13-.57-.49 0-.99.26-1.52.77zm5.87-1.69c0-.56.08-1.06.25-1.51.16-.45.37-.83.65-1.14.27-.3.58-.54.93-.71s.71-.25 1.08-.25c.39 0 .73.07 1 .2.27.14.54.32.81.55l-.06-1.1v-2.49h1.61v9.88h-1.33l-.11-.74h-.06c-.25.25-.54.46-.88.64-.33.18-.69.27-1.06.27-.87 0-1.56-.32-2.07-.95s-.76-1.51-.76-2.65zm1.67-.01c0 .74.13 1.31.4 1.7.26.38.65.58 1.15.58.51 0 .99-.26 1.44-.77v-3.21c-.24-.21-.48-.36-.7-.45-.23-.08-.46-.12-.7-.12-.45 0-.82.19-1.13.59-.31.39-.46.95-.46 1.68zm6.35 1.59c0-.73.32-1.3.97-1.71.64-.4 1.67-.68 3.08-.84 0-.17-.02-.34-.07-.51-.05-.16-.12-.3-.22-.43s-.22-.22-.38-.3c-.15-.06-.34-.1-.58-.1-.34 0-.68.07-1 .2s-.63.29-.93.47l-.59-1.08c.39-.24.81-.45 1.28-.63.47-.17.99-.26 1.54-.26.86 0 1.51.25 1.93.76s.63 1.25.63 2.21v4.07h-1.32l-.12-.76h-.05c-.3.27-.63.48-.98.66s-.73.27-1.14.27c-.61 0-1.1-.19-1.48-.56-.38-.36-.57-.85-.57-1.46zm1.57-.12c0 .3.09.53.27.67.19.14.42.21.71.21.28 0 .54-.07.77-.2s.48-.31.73-.56v-1.54c-.47.06-.86.13-1.18.23-.31.09-.57.19-.76.31s-.33.25-.41.4c-.09.15-.13.31-.13.48zm6.29-3.63h-.98v-1.2l1.06-.07.2-1.88h1.34v1.88h1.75v1.27h-1.75v3.28c0 .8.32 1.2.97 1.2.12 0 .24-.01.37-.04.12-.03.24-.07.34-.11l.28 1.19c-.19.06-.4.12-.64.17-.23.05-.49.08-.76.08-.4 0-.74-.06-1.02-.18-.27-.13-.49-.3-.67-.52-.17-.21-.3-.48-.37-.78-.08-.3-.12-.64-.12-1.01zm4.36 2.17c0-.56.09-1.06.27-1.51s.41-.83.71-1.14c.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.37c.08.62.29 1.1.65 1.44.36.33.82.5 1.38.5.3 0 .58-.04.84-.13.25-.09.51-.21.76-.37l.54 1.01c-.32.21-.69.39-1.09.53s-.82.21-1.26.21c-.47 0-.92-.08-1.33-.25-.41-.16-.77-.4-1.08-.7-.3-.31-.54-.69-.72-1.13-.17-.44-.26-.95-.26-1.52zm4.61-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.08.45-.31.29-.5.73-.57 1.3zm3.01 2.23c.31.24.61.43.92.57.3.13.63.2.98.2.38 0 .65-.08.83-.23s.27-.35.27-.6c0-.14-.05-.26-.13-.37-.08-.1-.2-.2-.34-.28-.14-.09-.29-.16-.47-.23l-.53-.22c-.23-.09-.46-.18-.69-.3-.23-.11-.44-.24-.62-.4s-.33-.35-.45-.55c-.12-.21-.18-.46-.18-.75 0-.61.23-1.1.68-1.49.44-.38 1.06-.57 1.83-.57.48 0 .91.08 1.29.25s.71.36.99.57l-.74.98c-.24-.17-.49-.32-.73-.42-.25-.11-.51-.16-.78-.16-.35 0-.6.07-.76.21-.17.15-.25.33-.25.54 0 .14.04.26.12.36s.18.18.31.26c.14.07.29.14.46.21l.54.19c.23.09.47.18.7.29s.44.24.64.4c.19.16.34.35.46.58.11.23.17.5.17.82 0 .3-.06.58-.17.83-.12.26-.29.48-.51.68-.23.19-.51.34-.84.45-.34.11-.72.17-1.15.17-.48 0-.95-.09-1.41-.27-.46-.19-.86-.41-1.2-.68z" fill="#535353"/></g></svg>" width="57"/>
     </a>
    </div>
    <div class="c-bibliographic-information__column">
     <h3 class="c-article__sub-heading" id="citeas">
      Cite this article
     </h3>
     <p class="c-bibliographic-information__citation">
      Jiang, X., Chen, J. Heterogeneous graphormer for extractive multimodal summarization.
      <i>
       J Intell Inf Syst
      </i>
      (2024). https://doi.org/10.1007/s10844-024-00886-5
     </p>
     <p class="c-bibliographic-information__download-citation u-hide-print">
      <a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-external="" data-track-label="link" href="https://citation-needed.springer.com/v2/references/10.1007/s10844-024-00886-5?format=refman&amp;flavour=citation" rel="nofollow">
       Download citation
       <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
        <use xlink:href="#icon-eds-i-download-medium" xmlns:xlink="http://www.w3.org/1999/xlink">
        </use>
       </svg>
      </a>
     </p>
     <ul class="c-bibliographic-information__list" data-test="publication-history">
      <li class="c-bibliographic-information__list-item">
       <p>
        Received
        <span class="u-hide">
         :
        </span>
        <span class="c-bibliographic-information__value">
         <time datetime="2024-04-14">
          14 April 2024
         </time>
        </span>
       </p>
      </li>
      <li class="c-bibliographic-information__list-item">
       <p>
        Revised
        <span class="u-hide">
         :
        </span>
        <span class="c-bibliographic-information__value">
         <time datetime="2024-09-04">
          04 September 2024
         </time>
        </span>
       </p>
      </li>
      <li class="c-bibliographic-information__list-item">
       <p>
        Accepted
        <span class="u-hide">
         :
        </span>
        <span class="c-bibliographic-information__value">
         <time datetime="2024-09-05">
          05 September 2024
         </time>
        </span>
       </p>
      </li>
      <li class="c-bibliographic-information__list-item">
       <p>
        Published
        <span class="u-hide">
         :
        </span>
        <span class="c-bibliographic-information__value">
         <time datetime="2024-09-30">
          30 September 2024
         </time>
        </span>
       </p>
      </li>
      <li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--full-width">
       <p>
        <abbr title="Digital Object Identifier">
         DOI
        </abbr>
        <span class="u-hide">
         :
        </span>
        <span class="c-bibliographic-information__value">
         https://doi.org/10.1007/s10844-024-00886-5
        </span>
       </p>
      </li>
     </ul>
     <div data-component="share-box">
      <div class="c-article-share-box u-display-block">
       <h3 class="c-article__sub-heading">
        Share this article
       </h3>
       <p class="c-article-share-box__description">
        Anyone you share the following link with will be able to read this content:
       </p>
       <button class="js-get-share-url c-article-share-box__button" data-track="click" data-track-action="get shareable link" data-track-external="" data-track-label="button" id="get-share-url" type="button">
        Get shareable link
       </button>
       <div class="js-no-share-url-container u-display-none" hidden="">
        <p class="js-c-article-share-box__no-sharelink-info c-article-share-box__no-sharelink-info">
         Sorry, a shareable link is not currently available for this article.
        </p>
       </div>
       <div class="js-share-url-container u-display-none" hidden="">
        <p class="js-share-url c-article-share-box__only-read-input" data-track="click" data-track-action="select share url" data-track-label="button" id="share-url">
        </p>
        <button class="js-copy-share-url c-article-share-box__button--link-like" data-track="click" data-track-action="copy share url" data-track-external="" data-track-label="button" id="copy-share-url" type="button">
         Copy to clipboard
        </button>
       </div>
       <p class="js-c-article-share-box__additional-info c-article-share-box__additional-info">
        Provided by the Springer Nature SharedIt content-sharing initiative
       </p>
      </div>
     </div>
     <h3 class="c-article__sub-heading">
      Keywords
     </h3>
     <ul class="c-article-subject-list">
      <li class="c-article-subject-list__subject">
       <span>
        <a data-track="click" data-track-action="view keyword" data-track-label="link" href='/search?query=Summarization&amp;facet-discipline="Computer%20Science"'>
         Summarization
        </a>
       </span>
      </li>
      <li class="c-article-subject-list__subject">
       <span>
        <a data-track="click" data-track-action="view keyword" data-track-label="link" href='/search?query=Extractive%20summarization&amp;facet-discipline="Computer%20Science"'>
         Extractive summarization
        </a>
       </span>
      </li>
      <li class="c-article-subject-list__subject">
       <span>
        <a data-track="click" data-track-action="view keyword" data-track-label="link" href='/search?query=Multi-modal%20summarization&amp;facet-discipline="Computer%20Science"'>
         Multi-modal summarization
        </a>
       </span>
      </li>
      <li class="c-article-subject-list__subject">
       <span>
        <a data-track="click" data-track-action="view keyword" data-track-label="link" href='/search?query=Graph%20neural%20networks&amp;facet-discipline="Computer%20Science"'>
         Graph neural networks
        </a>
       </span>
      </li>
     </ul>
     <div data-component="article-info-list">
     </div>
    </div>
   </div>
  </div>
 </div>
</section>
