	Opens in a new window Opens an external website Opens an external website in a new window       Close this dialog        This website utilizes technologies such as cookies to enable essential site functionality, as well as for analytics, personalization, and targeted advertising. You may change your settings at any time or accept the default settings. You may close this banner to continue with only essential cookies.   Privacy Policy   Storage Preferences     Targeted Advertising  Personalization  Analytics       Save   Accept All   Reject All                          Close Cookie Preferences             


















An Automated Hyperparameter Search-Based Deep Learning Model for Highway Traffic Prediction | IEEE Journals & Magazine | IEEE Xplore












































Skip to Main ContentIEEE.orgIEEE XploreIEEE SAIEEE SpectrumMore SitesDonateCartCreate AccountPersonal Sign In Browse  My Settings  Help Access provided by:Chung-ang UnivSign OutAccess provided by:Chung-ang UnivSign OutAllBooksConferencesCoursesJournals & MagazinesStandardsAuthorsCitationsImages (Beta)ADVANCED SEARCH Journals & Magazines >IEEE Transactions on Intellig... >Volume: 22 Issue: 9An Automated Hyperparameter Search-Based Deep Learning Model for Highway Traffic PredictionPublisher: IEEECite ThisPDFHongsuk Yi; Khac-Hoai Nam BuiAll Authors View Document 49Cites inPapers1661FullText ViewsAlertsAlerts Manage Content Alerts  Add to Citation Alerts AbstractDocument SectionsI.IntroductionII.Literature ReviewIII.HyperNet Framework for Automated Hyperparameter SearchIV.Hypernet Framework-Based Deep RNN-LSTM Model for Traffic Prediction on Highway SystemV.ImplementationShow Full OutlineAuthorsFiguresReferencesCitationsKeywordsMetricsMore Like ThisFootnotes Download PDF  Download References  Request Permissions  Save to  Alerts  Abstract:Auto machine learning recently has been introduced as a trending technique for learning applications, including smart transportation. In this study, we focus on applying ...View more Metadata  Abstract:Auto machine learning recently has been introduced as a trending technique for learning applications, including smart transportation. In this study, we focus on applying auto-machine learning for hyperparameter tuning to learn traffic datasets at the main regions of highway systems. Particularly, deep learning models have been recently introduced as emergent methods for traffic prediction. However, training deep learning models requires expensive works (e.g., time-consuming and human expertise), especially in terms of determining the configurations of hyperparameters in the models. In this regard, this paper introduces an automated framework for hyperparameter tuning to learn traffic datasets at an ecosystem in terms of reducing time-consuming tasks. Specifically, we first propose the HyperNet framework, using advanced data science techniques (e.g., Bayesian optimization and meta-learning) for the automated hyperparameter search process. Then, a deep learning model with the long short term memory network based on the HyperNet framework has presented for learning the temporal variation of traffic datasets at main regions of highway traffic systems. Regarding the experiment, we take data from the Korean highway system into account as a case study to evaluate the proposed approach. The evaluation indicates promising results of the proposed framework for learning multiple datasets of the traffic highway systems.Published in: IEEE Transactions on Intelligent Transportation Systems ( Volume: 22, Issue: 9, September 2021) Page(s):  5486 - 5495Date of Publication: 24 April 2020  ISSN Information: DOI: 10.1109/TITS.2020.2987614Publisher: IEEEFunding Agency:  Contents 


																SECTION I.IntroductionAs the ever-increasing traffic demand, cities around the world have to face the traffic congestion problem. Traffic flow on the highway system is no exception. For instance, traffic congestion has occurred on the main roads at rush hours such as weekend morning and evening [1]. Hence, with the development of the Intelligent Transportation System (ITS), where a large amount of data generated every day, many approaches have been proposed to deal with traffic congestion problems based on data collection from ITS [2], [3]. In the case of highway systems, traffic flow prediction becomes the main component for improving the traffic flow [4]. However, the rapid growth of generated data becomes an emergent challenge that the traditional processing systems are not able to deal with the data analytics requirements [5].Recently, Deep Learning (DL), an evolution of Machine Learning (ML), which contains multiple hidden layers, has been introduced as a promising method to improve the accuracy of traffic prediction [6]. Specifically, some well-known DL models (e.g., Deep Belief Networks (DBNs), Stacked Autoencoder (SAE), Convolution Neural Networks (CNN), and Long-Short Term Memory (LSTM)), a variant of Recurrent Neural Network (RNN), have demonstrated capabilities for traffic prediction. For instance, Yisheng et al. [7] have proposed a DL model for traffic flow prediction using SAE to train generic features of traffic data. In [8], Huang et al. have introduced a DBN model for multitask learning with promising results. Furthermore, the work in [9] indicates that LSTM networks can achieve good performance for time series forecasting of traffic data. Nevertheless, configuring neural networks is still a challenging problem in which we have to explore different configurations to provide an appropriate model. Moreover, with the integration of Artificial Intelligence (AI), DL methods emerge as potential solutions in terms of shifting to computational intelligence approaches which are able to automatically turn hyperparameters with the best configurations [10]. Those aforementioned issues are still open researches in this field.Particularly, the successful applications of DL are currently far from the fully automated process since according to No Free Lunch theorems, there are no algorithms/configurations that can archive good performance with all possible problems. Therefore, human experts play a critical role for the development of ML applications (e.g., Feature extraction, model/algorithm selections, and hyperparameter tuning) [11]. Furthermore, training DL requires expensive works since the models involve a highly iterative process (e.g., high-dimensional hyperparameters) which are frustratingly difficult and time-consuming tasks [12]. In this regard, the trend research focuses on the automated training process to determine appropriate values of these hyperparameters for training DL models with acceptable time [13]. Specifically, Fig. 1 depicts the process of Auto Machine Learning (AutoML) for the training process.
Fig. 1. AutoML for training process.Show AllIn order to overcome the aforementioned problems to apply DL models for training the temporal variation of traffic data on highway systems, this study focuses on developing an automated training process for predicting traffic conditions at the main regions of a certain highway system. In particular, we take the automated hyperparameters tuning into account to determine the optimal configuration for the LSTM model to analyze highway traffic time-series datasets. Specifically, several advanced techniques such as Bayesian and meta-learning have been applied for drastically speeding up the hyperparameter search process [14]. To the best of our knowledge, this paper is the first study to apply automated hyperparameter tuning problems for training traffic datasets. Specifically, our contributions include as follows:
We collect and analyze data of the Korean highway system, which are important routes of public transportation in Korea, to predict the congestion in each particular region. Specifically, in highway systems, the traffic flow patterns (e.g., rush and non-rush hours) are quite similar at several specific locations. Therefore, a promising meta-learning method can be applied in order to reduce significantly the time consumption for learning traffic datasets at highway systems [15].We propose an automated framework, which is named HyperNet for hyperparameter tuning to train large-scale traffic datasets in terms of enabling automated processes and reducing time-consuming tasks. Particularly, we apply state-of-the-art data analytics techniques such as Bayesian optimization and meta-learning for tuning hyperparameters of LSTM, a recent DL model which has proved the capabilities for training time-series traffic datasets [16]. In addition, the framework can be extended for other DL models which depends on the characteristic of the input datasets and the target of the predictions (e.g., large-scale traffic forecasting at an urban area).The implementations indicate that the proposed approach is able to provide promising results for the traffic congestion prediction at the main regions (entrance/exit area) of the highway systems. Technically, the proposed approach is an important step to develop the AutoML for learning applications which is a state-of-the-art technique in this research area [17].The rest of this paper is organized as follows: In Section 2, we present some literature reviews of traffic prediction using DL models. Moreover, the hyperparameter optimization problem has been introduced in this section. In Section 3, we propose HyperNet framework for short term traffic speed prediction. Consequently, applying the proposed framework for traffic prediction on highway systems is described in Section 4. The experimental results compared with previous works are shown in Section 5. Section 6 includes some discussions and future works of this study.

																SECTION II.Literature ReviewA. Long-Short Term Memory for Time Series PredictionTraffic prediction has been regarded as the main component in ITS. The ARIMA model first introduced to predict the short-term freeway traffic flow. Since then, many models have been proposed for traffic prediction [4]. Recent studies reveal that DL models, which include multi-network layers work together, are promising approaches for traffic prediction. Specifically, It depends on the assumptions of training datasets, there are several well-known DL methods that can use to develop time series forecasting such as Multilayer Perceptrons(MLP), CNN, and LSTM models [7], [9]. Furthermore, hybrids models (e.g., CNN-LSTM and ConvLSTM) have been introduced for traffic prediction in which traffic data have processed as images for the input [18], [19]. Recently, Graph Neural Networks (GNN) has proposed as a promising concept for improving the ability to learn hidden patterns from spatial-temporal graphs, which is an important issue for various applications such as human action recognition and traffic prediction [20].In this study, we use the LSTM model, which have been proved the capability for learning the temporal variation in ITS [21], to train the collected datasets on highway systems. Technically, LSTM model is a variant of RNN architecture that is able to learn information with long time spans and determine the optimal time lags in an automatic manner. Particularly, the LSTM cell comprises an input gate x
, an output gate o
, and a forget gate f
 for regulating the flow of information (e.i., into and out) of the cell. Therefore, LSTM is able to overcome the vanishing gradient problem that occurs with most RNN models. Specifically, Fig. 2 depicts the difference between LSTM and standard RNN structures.
Fig. 2. The difference between traditional RNN and LSTM cell.Show AllSupporting the input X
 is the historical traffic data which is denoted as follows:X={x1,x2,…,xn}(1)
View Source\begin{equation*} X = \{x_{1}, x_{2}, \ldots, x_{n}\} \tag{1}\end{equation*}
 The hidden state of memory cells H
 and predict output time series Y
 are respectively determined as follows:H=Y={h1,h2,…,hn}{y1,y2,…,yn}(2)
View Source\begin{align*} H=&\{h_{1}, h_{2}, \ldots, h_{n}\} \\ Y=&\{y_{1}, y_{2}, \ldots, y_{n}\}\tag{2}\end{align*}
 where each ht∈H
 and yt∈Y
 can be sequentially calculated as follow:ht=yt=H(Whxt+Wh′ht−1+bh)Wyht+by(3)
View Source\begin{align*} h_{t}=&H(W_{h}x_{t} + W_{h'}h_{t-1} + b_{h}) \\ y_{t}=&W_{y}h_{t} + b_{y}\tag{3}\end{align*}
 where W
 and b
 denotes weight matrices and bias vectors, respectively. Consequently, the hidden state of memory cells H
 is computed based on the formulations of each gate in the cell:ft=it=ot=ht=σ(Wf⋅[ht−1,xt]+bf)σ(Wi⋅[ht−1,xt]+bi)σ(Wo⋅[ht−1,xt]+bo)ot∗tanh(Ct)(4)
View Source\begin{align*} f_{t}=&\sigma (W_{f} \cdot [h_{t-1}, x_{t}] + b_{f}) \\ i_{t}=&\sigma (W_{i} \cdot [h_{t-1}, x_{t}] + b_{i}) \\ o_{t}=&\sigma (W_{o} \cdot [h_{t-1}, x_{t}] + b_{o}) \\ h_{t}=&o_{t} * tanh(C_{t}) \tag{4}\end{align*}
 where σ
 and C
 denote the sigmoid function and activation vectors, respectively, which can be sequentially calculated as follows:σ(x)=Ct=11+e−xft∗Ct−1+it∗Cˆt(5)
View Source\begin{align*} \sigma (x)=&\frac {1}{1 + e^{-x}} \\ C_{t}=&f_{t} * C_{t-1} + i_{t} * \widehat {C}_{t}\tag{5}\end{align*}
 where Cˆ
 denotes the candidate value in each computed process of the cell which can be calculated as follows:Cˆt=tanh(WC⋅[ht−1,xt]+bC)(6)
View Source\begin{equation*} \widehat {C}_{t} = tanh (W_{C} \cdot [h_{t-1}, x_{t}] + b_{C})\tag{6}\end{equation*}
Nevertheless, the limitation of LSTM model is that there is no direction from Constant Error Carousels (CECs) to LSTM cell for supporting to control which means the gate layers are not able to look at the cell state. Hence, in this study, we apply the work in [22] which has added ‘peepholes connection’ to all the gates. Therefore, the gates of the LSTM cell in Eq. 4 can be re-calculated as follows:it=ft=ot=σ(Wi⋅[ht−1,xt,Ct−1]+bi)σ(Wf⋅[ht−1,xt,Ct−1]+bf)σ(Wo⋅[ht−1,xt,Ct]+bo)(7)
View Source\begin{align*} i_{t}=&\sigma (W_{i} \cdot [h_{t-1}, x_{t}, C_{t-1}] + b_{i}) \\ f_{t}=&\sigma (W_{f} \cdot [h_{t-1}, x_{t}, C_{t-1}] + b_{f}) \\ o_{t}=&\sigma (W_{o} \cdot [h_{t-1}, x_{t}, C_{t}] + b_{o})\tag{7}\end{align*}
B. Hyperparameters Optimization in Deep Learning ModelsDetermining the best configuration is one of the most challenges in implementing machine learning (ML) solutions. Normally, we minimize the testing error by regularly modifying the model during programming as shown in Fig. 3. However, applying this issue for DL models is computationally expensive since difference with conventional ML, DL involves many hyperparameters. Recently, automated hyperparameter optimization (HPO) for DL models has been introduced with promising results [23]. Although there has not a clear definition, hyperparameters are generally described as follows:
Fig. 3. The predictive analytics process.Show AllDefinition 1:In machine learning, hyperparameters are variables that define network structure and determine how the model is trained. They are specified before beginning the learning process.Technically, searching for the best configuration of hyperparameters is an iterative process. Currently, there have four well-known strategies for searching the configuration which include: i) Manual Search; ii) Grid Search; iii) Random Search; and iv) Bayesian Search. Specifically, Tab. I shows the properties of each strategy [24]. In our framework, we apply Bayesian optimization approach for automated searching of the hyperparameters due to following reasons:
The approach has been proved the efficient and robust automated capability for hyperparameter tuning in recent works [23].Grid and Random searches have a limitation in which the new prediction is independent with previous runs. In this regard, using the Bayesian approach is able to take less training steps and achieve a comparable result with a sufficiently high number of experiments.TABLE I 
Search Strategies for Hyperparameters OptimizationC. Meta Learning for Fast Automated Machine LearningHPO using the Bayesian approach has been shown good performance for the hyperparameter tuning problem, especially in case of high dimensions of the hyperparameters [25]. However, applying this approach to real-life problems still remain several challenging issues since Bayesian hyperparameter optimization requires a substantial number of evaluations to detect high-performance regions when started on a new optimization problem. Especially, in case of the transportation domains, where multiple datasets have collected and processed from various sources in the system (e.g., Sensors on the road). In this regard, meta-learning (learning to learn) has introduced to address the aforementioned problems in which ML algorithms are developed to learn from data without being explicitly programmed [26]. Consequently, recent works have successfully applied meta-learning for DL models (i.e., Neural Architecture Search (NAS) and hyperparameter tuning) to automate the learning process and reduce time-consuming tasks [27], [28].In this study, we take hyperparameter tuning using meta-learning into account for multiple traffic datasets on highway systems. Specifically, the key concept by applying meta-learning for the hyperparameter search is to suggest the configuration for a new similar dataset based on the configurations that are known from previous evaluations. This concept is suitable for the datasets from highway traffic systems in which multiple traffic datasets have collected from different sensors locating on the same route and the traffic flow patterns are quite similar at several specific locations. In this regard, although this paper implements the LSTM model as the case study, however, we are able to apply the proposed framework for several state-of-the-art DL models of traffic forecasting such as ConvLSTM and GNN. The main difference is that based on the characteristic of each model, we have to determine the number of hyperparameters and their search spaces for turning as the input of the framework.

																SECTION III.HyperNet Framework for Automated Hyperparameter SearchA. Problem FormulationFig. 4 illustrates the overview of our proposed framework for traffic prediction in ITS. Specifically, according to the Def. 1, hyperparameters are automatically set before training regarding weight optimization.
Fig. 4. Proposed system for traffic prediction using HyperNet framework.Show AllSupporting θ=θ1,θ2,…θm
 and Θ=Θ1×Θ2×…Θm
 denote the set and configuration space of the hyperparameters, respectively, the objective function of HyperNet is to find the hyperparameters of a given model that return the best performance as measured on a validation set. In this regard, the objective function can be formulated as follows:minθ∈RΘf(θ,Dval)(8)
View Source\begin{equation*} \underset {\theta \in \mathbb {R}^{\Theta }}{min} f(\theta, \mathcal {D}_{val})\tag{8}\end{equation*}
 where f(Φ,Dval)
 represents the objective score to minimize regarding validation dataset Dval
. Therefore, the hyperparameter optimization problem can be formulated as follows:θ∗=argminθ∈Θf(θ,Dtrain)(9)
View Source\begin{equation*} \theta ^{*} = \underset {\theta \in \Theta }{argmin} f(\theta, \mathcal {D}_{train})\tag{9}\end{equation*}
 which means that we need to determine the maximum value θ∗
 from training datasets Dtrain
.B. Bayesian Hyperparameter Optimization for a Single DatasetFor solving objective function in Eq. 9, we apply Bayesian Optimization (BO) which has recently introduced as a promising approach for the hyperparameter optimization problems. Specifically, Sequential Model-Based Optimization (SMBO), a formalization of BO, has emerged as a successful method for hyperparameter tuning [23]. Technically, the idea of this approach is to place a surrogate model M
 over the target black-box, and update it sequentially by querying f(θ)
 at new points that optimize an acquisition function by using the expected Positive Improvement (EI) which can be calculated as follows:EIy∗(θ,M)=∫y∗−∞max(y∗−y,0)pM(y|θ)dy(10)
View Source\begin{equation*} EI_{y*}(\theta, \mathcal {M}) = \int _{-\infty }^{y*} max(y^{*} - y, 0)p_{\mathcal {M}}(y|\theta)dy\tag{10}\end{equation*}
 where y
 and y∗
 denotes the actual and threshold values, respectively. More detail of the generic SMBO is demonstrated in Algorithm 1.
Algorithm 1 Sequential Model-Based Optimization SMBO(f,θ,Θ,T
)Show AllC. Exploring Tunable Hyperparameters via Meta Learning for Multiple DatasetsSince SMBO is regarded as as a generic function optimization framework, scalability becomes a serious problem, especially in case the transportation domain which involves multiple datasets from various source. In this regard, we propose a new approach for training multiple datasets using meta-learning in which focus on determining the initialization of turntable hyperparameters in the SMBO algorithm [15] in which we refer to the new algorithm for training multiple datasets as M-SMBO.Considering a set of similar datasets D1
, D2
…Dn
 (e.g., datasets from an ecosystem) in which each dataset divides into a target variable Y
 and a feature vector X
. In this regard, a predictive model has produced for learning the function relationship between X
 and Y
. Particularly, the function is defined by a performance metric that is controlled by m-dimension hyperparameter configuration θ
. Therefore, the objective of M-SMBO is to find the best hyperparameter configuration for a new dataset Dk+1
 based on the set of benchmark datasets D1
, D2
…Dk
. Specifically, this approach includes three steps as follows:1) Benchmark Datasets:In order to apply meta-learning for hyperparameter tuning, we first gather meta-data that evaluate on given k
 (k∈n
) datasets. Each dataset is supported to optimize hyperparameters using the SMBO function (Algorithm 1). Specifically, given a dataset Di
 (i∈k
), the hyperparameter configurations is to minimize the loss function L
 as follows:θ∗,i=argminθ∈Θ L(θ,Di)(11)
View Source\begin{equation*} \theta ^{*,i} = \underset {\theta \in \Theta }{argmin} ~\mathcal {L}(\theta, \mathcal {D}^{i})\tag{11}\end{equation*}
 where L(θ,Di)
 indicates the loss function of dataset Di
 using hyperparameter θ
. In this study, we determine the benchmark datasets based on several analysis techniques on all datasets. Furthermore, since BO contains random factors, we execute 10 times on each benchmark dataset.2) Default Hyperparameter Configuration:Default configurations refer to the hyperparameters that work well across multiple datasets. Normally, they have provided by software packages (e.g., Auto-Weka [29]). In this study, we define the default hyperparameters based on the empirical experiments from benchmarks datasets. Specifically, a subset hyperparameter θs
 (θs∈θ
) is determined as a default configuration in case of satisfying the following equation:θs=argminθ∈Θ h¯¯¯(L(θ∗,1,D1),L(θ∗,2,D2),…,L(θ∗,k,Dk))(12)
View Source\begin{align*} \theta ^{s} = \underset {\theta \in \Theta }{argmin} ~\overline {h}(\mathcal {L}(\theta ^{*,1}, \mathcal {D}^{1}), \mathcal {L}(\theta ^{*,2}, \mathcal {D}^{2}), \ldots, \mathcal {L}(\theta ^{*,k}, \mathcal {D}^{k})) \\\tag{12}\end{align*}
where the function h¯¯¯
 (e.g, median) has used to perform the single scalar value [30].3) Search Space for Hyperparameter Tuning:BO has attracted many studies comparing with other approaches such as grid search and random search since BO can use the information gained by all the previous tries to explore the search space of hyperparameters in an efficient way [25]. However, this process is still expensive work. In this regard, we try to determine the reasonable search space Θ∗
 (Θ∗∈Θ
) by optimal the configuration of each hyperparameter θk
 with high probability for the new datasets. Therefore, we apply the quantile function to determine the threshold of the search space [31]. In this regard, supporting qi,p
 denotes the quantile of the discrete distribution of hyperparameter θi
, the optimal search domain Θ∗
 can be defined as follows:Θ∗={θ∈Θ∣∀i∈1…m:qi,p1≤θi≤qi,p2}(13)
View Source\begin{equation*} \Theta ^{*} = \{\theta \in \Theta \mid \forall i \in {1\ldots m}: q_{i,p_{1}} \leq \theta _{i} \leq q_{i,p_{2}}\}\tag{13}\end{equation*}
where p1
 and p2
 denote the upper and lower quartile values, respectively. Note that this concept can be applied for both numerical and categorical hyperparameters. Particularly, Algorithm 2 depicts the pseudo-code of the M-SMBO algorithm for hyperparameter search across multiple datasets.Algorithm 2 Meta-Learning-Based Hyperparameter Tuning for Traffic Datasets M-SMBO(Dk+1,f,θ∗,1:k,θ,Θ,T
)Data: New Dataset Dk+1
, Target function f
, Hyperparameter θ
, Best Hyperparameters of benchmark Datasets θ∗,1:k
 = θ∗1,θ∗2…θ∗k
,Search Space Θ
, Iteration T
, upper and lower quantile values p1,p2
Result: Best Hyperparameters θ∗,k+1
1Set θs,k+1=argminθ∈Θh¯¯¯(L(θ∗,1,D1),L(θ∗,2,D2),…,L(θ∗,k,Dk))
;2θnew,k+1=θ/θs,k+1
;3Set Θnew,k+1={θnew,k+1∈Θ∣∀i∈1…m:qi,p1≤θi≤qi,p2}
4Return θ∗,k+1←θs,k+1∧SMBO(f,θnew,k+1,Θnew,k+1,T)


																SECTION IV.Hypernet Framework-Based Deep RNN-LSTM Model for Traffic Prediction on Highway SystemA. System ArchitectureFig. 5 depicts the system architecture of our approach for traffic prediction of highway system based on HyperNet framework. Specifically, our approach includes three main processes: i) Data Processing; ii) HyperNet Framework-based Deep LSTM-RNN Model; iii) Predictive Analytics.
Fig. 5. System architecture for traffic prediction on highway system.Show All1) Data Processing:This process is proposed for understanding traffic data that are collected from the highway system. Among having many features, we focus on three important issues as follows:
Data pre-processing: performing statistical checks for the quality of the collected data in terms of avoiding serial correlation where error terms in a time series transfer from one period to another.Features selection: defining features (e,g., main points) that has explanatory power to predict and using them for the training in DL models.Meta-features extraction: since we only consider training DL model for traffic datasets at a certain area of highway systems, we assume that simple meta-features such as the number of features, traffic patterns (e.g., time interval and rush hours), and structure of all considered datasets are similar. Therefore, defining benchmark datasets and calculating the similarity among datasets also include in this process. For instance, the input datasets from sensors which are located on the same route with the same level of traffic condition can be regarded as similar datasets. More detail of this concept is explained in the following section with the real data that we have collected in this study.2) HyperNet Framework-Based Deep LSTM-RNN Model:In this study, we apply an LSTM network which has been proved the capabilities for predicting traffic data [9], [16]. However, the difference with previous works, instead of manually configuration, we proposed a framework for training the LSTM model in which the hyperparameters are automatically turned. In this regard, this process is able to improve the performance of the training model in terms of accuracy and time complexity.3) Predictive Analytics:This process demonstrates the performances of the proposed model in terms of minimizing the square error. Specifically, we use 70% daily data to train (train data), 10% of the validation set and predict the next 20% (test data). Then, we will compare the predicted results with the test data.B. Objective FunctionAs we mentioned above, by applying the automated hyperparameters tuning problem, the objective function of this study is to minimize the square error for predicting traffic flow, which is formulated as follows:e=∑t=1n(yt−pt)2(14)
View Source\begin{equation*} e = \sum _{t = 1}^{n}(y_{t} - p_{t})^{2}\tag{14}\end{equation*}
where e
 denotes the square error value and p
 is the predicted traffic flow value. Consequently, two performance indexes for evaluating the model are taken into consideration which include: i) mean square error (MSE); and ii) mean absolute percentage error (MAPE). Specifically, the performance indexes are sequentially calculated as follows:MSE=MAPE=1n∑i=1n(fi−fiˆ)21n∑i=1n∣fi−fiˆ∣fi(15)
View Source\begin{align*} MSE=&\frac {1}{n}\sum _{i = 1}^{n}{(f_{i} - \widehat {f_{i}})}^{2} \\ MAPE=&\frac {1}{n}\sum _{i = 1}^{n}\frac {\mid f_{i} - \widehat {f_{i}} \mid }{f_{i}}\tag{15}\end{align*}
where f
 and fˆ
 are observation and prediction values, respectively.C. LSTM Hyperparameters for TuningThe challenging issue by using DL models is that they include many hyperparameters. Specifically, adjusting all hyperparameters can make a complicated process. Moreover, search algorithms (Tab. I) treat hyperparameters equality to find the best results which can make time-consuming and computationally expensive problems [32].Recently, several methods have introduced the importance of hyperparameters for the training process [27], [33]. In this study, regarding the traffic data from the highway system, based on the experience in our previous work [16], [34], five hyperparameters are considered for tuning in LSTM models such as learning rate, number of LSTM layers, number of neurons, batch size, and dropout. Particularly, Tab. II depicts the explanations and search spaces of the aforementioned hyperparameters.TABLE II 
Hyperparameters Tuning for Training Big Traffic Ata

																SECTION V.ImplementationA. Data Description and ProcessingRegarding the data for the evaluation, we collected data from four main routes of Korea highway systems such as Gyeongbu (SB), Gyeongchun (SC), Seohaean (SM), and Yeongdong (IG) routes. Specifically, in each route, we take four major regions, which include Northbound and Southbound of two terminus, into account. For instance, Fig. 61 depicts the map and main regions of SB highway that we consider in this study.
Fig. 6. Frequency of TPI values in each region.Show AllHence, our evaluation totally includes 16 input datasets. Consequently, each dataset obtains the average traffic speed every 5 minutes which have collected from May to July, 2017 (3 months). Therefore, the usage data includes 23616 average traffic speed data which means that 288 average traffic speed data had aggregated every day and stored them for 82 days excepting holidays. In particular, Tab. III depicts the format of highway traffic data that we have processed before training.TABLE III 
Format of Highway Traffic DatasetsSpecifically, since there have several posted limited speed from 80 KPH (Kilometers per hour) to 110 KPH, the 5 minutes average speed has converted to the Traffic Performance Index (TPI) [35], which is calculated as follows:TPIt=vf−vtvf(16)
View Source\begin{equation*} TPI_{t} = \frac {v_{f} - v_{t}}{v_{f}} \tag{16}\end{equation*}
where vf
, vt
 represents the maximum speed and the average traffic speed in each 5 minutes, respectively. Particularly, the maximum speed is determined by calculating the speed of a vehicle without interruption by any surrounding vehicles. Basically, the maximum speed can be calculated as follows:vf=c∗vl(17)
View Source\begin{equation*} v_{f} = c * v_{l} \tag{17}\end{equation*}
where vl
 denotes the posted limited speed of a specific link and c
 is a constant number which can be varied from 1.2 to 1.5. Consequently, by converting the 5 minutes average speed to TPI values, the data is logically normalized which is used as the input data for the prediction.Fundamental analysis from data collection shows the traffic conditions in each main region of a route. For instance, in the case of SB highway, traffic congestion has occurred at regions that are nearby Seoul station (SB-R1 and SB-R3) as shown in Fig. 6. Specifically, Fig. 7 shows the frequency of TPI values in each region of SB highway. Technically, in continuous traffic flow such as highway systems, the TPI values less than 0.4 and greater than 0.8 represents non-congested condition and serious congested condition, respectively.
Fig. 7. Frequency of TPI values in each region of SB route.Show AllFor more detail, Tab. IV indicates the traffic conditions of all datasets in each route that we take into count for the evaluation. In this regard, for training multiple datasets, we will take two regions that are in different traffic condition level in each route (e.g., R1 and R4) for the benchmark datasets.TABLE IV 
Traffic Conditions of All Considered RegionsB. Experimental SettingsFor the implementation, we use Keras,2 a python DL library for developing our model. The experiments work well by a PC with Core i7 16-GB CPU and 32GB GPU memories in which we used the GPU for acceleration. Specifically, our implementation includes three main processes as follows:
Hyperparameter tuning: We first implement an auto hyperparameter search based on Bayesian optimization for determining hyperparameters values which are regarded as the best configuration.Traffic Speed Prediction: By applying a Deep LSTM-RNN model with the aforementioned hyperparameters, the results are the prediction values of TPI for the next time interval in each considered region.Comparison: For evaluating the effectiveness of the proposed approach, we compare Bayesian Optimization with Random Search in terms of both accuracy and time-consuming tasks. Furthermore, we also evaluate our proposed framework for hyperparameter search across multiple datasets to show the capability of the scalability of the proposed approach for traffic prediction on highway systems.More detail of the experimental settings are described as follows:
Input and Output: The input includes the average traffic speed of vehicles in previous time intervals. The output is the traffic speed prediction in the next time interval (short-term prediction) of each certain region.Training and Testing Data: Data have been collected for 82 days (excepting holidays) in 2017. In this regard, for each dataset, we use the data of the first 66 days as the training set (including 8 days for the validation) and the last 16 days as the testing set.Evaluation Matrix: In this study, we use MAPE for error measurement. Moreover, MSE (Loss) has been considered for the overall performance evaluation to imply the accuracy of the proposed model.C. Single Dataset Result AnalysisIn order to evaluate the performance, we first implement the proposed framework for each single benchmark dataset. Specifically, we consider 50 iterations (T=50
) per each run to find the best optimal values. For instance, Fig. 8 demonstrates the result of the best-found values after 50 optimizations for training SB-R4 dataset. Furthermore, we compare the results with Random Search which is another well-known approach for hyperparameter tuning [36]. Fig. 9 demonstrates the distribution of 50 optimizations between two algorithms. In particular, the dashed lines, which indicate the medians of the distributions, denote that Bayesian Search is better than Random Search regarding the considered datasets.
Fig. 8. Best found values of SB-R4 datasets using Bayesian hyperparameter optimization.Show All
Fig. 9. Distribution of best values over 50 optimizations.Show AllFor more detail, Tab. V depicts the summary results by applying the two approaches for all benchmark datasets in which BS, L, and TS denote the Best Score, the training step (Iteration) to find Best Score, and the Test Score, respectively. As results, using Bayesian Search for hyperparameter optimization outperforms Random Search, especially in the case of the Iteration to find the best configuration. Specifically, although there are no big differences between the loss values, however, as we mentioned above, the advantage of Bayesian optimization is able to take fewer training steps in order to achieve comparable results. Particularly, this is an important concept for the further development of AutoML for learning applications in the transportation domain.TABLE V 
The Comparison Between Random Search and HyperNet for Training Benchmark DatasetsFor more detail, Fig 10 demonstrates the graph convergence to find the optimum between two approaches on SB-R4 dataset. We can see that the Bayesian Search has been able to converge better than the Random Search. Consequently, Fig. 11 demonstrates the TPI predictions of SB-R1 (congestion) and SB-R4 (non-congestion) of the SB route based on the best-found hyperparameter values by applying the proposed framework.
Fig. 10. Convergence graph for finding the optimum value of SB-R4 datasets.Show All
Fig. 11. TPI predictions.Show AllD. Multiple Datasets Result AnalysisThe main disadvantage of applying the Bayesian approach (SMBO) is the time-consuming problem. Specifically, it is difficult to extend this approach with a large number of datasets in which we have to train each dataset independently.Therefore, in this study, we propose the hyperparameter optimization across multiple datasets using meta-learning for highway traffic system. The main reason is that in each route of highway systems, based on traffic conditions, we are able to classify similar datasets in which we do not need to train similar datasets from scratch. In this regard, Tab. VI demonstrate the result of our implementation for the proposed framework comparing with using SMBO for training each dataset independently. Specifically, by applying the proposed framework, we are able to achieve comparable results with the best values. However, the advantage of the proposed framework is that we are able to reduce significantly time consumption with acceptable results. In particular, Fig. 12 depicts the results in terms of accuracy and time consumption between two approaches, respectively. Note that for training multiple datasets, to simplify the problem, we assume that the input datasets are trained in sequence.TABLE VI 
The Comparison Between SMBO and HyperNet for Hyperparameter Optimization Across Multiple Datasets
Fig. 12. Loss and time consumption.Show All

																SECTION VI.Conclusion and Future WorkIn ITS, DL for time series prediction has exploited the effectiveness in terms of analyzing data sequence and extracting traffic flow features. In this regard, this paper focuses on implementing a DL method using LSTM-RNN models for predicting the congestion in highway systems. Specifically, the main challenge of applying DL models is that it requires the human expert to determine the appropriate model for traffic prediction. In this regard, this study proposes the HyperNet framework for improving the automated training process in terms of optimizing the configuration of hyperparameters across multiple datasets at the main regions of a certain highway system. Furthermore, we take the Korean highway traffic system into account as the case study of our work. The experiment indicates promising results with the highway traffic datasets that we have collected from four main routes in the Korea transportation system. For the future work of this study, we are going to extend the proposed framework for other applications of the traffic forecasting (e.g., traffic flow at urban areas or multi-channels predictions) by using another DL model (e.g., ConvLSTM or Graph Neural Networks). Specifically, based on the objective and problem definition of the DL models, the input datasets and the hyperparameters need to pre-process and determine, respectively. Moreover, the exploration in more detail for the hierarchy of the hyperparameters to train models also need to take into account. Specifically, based on our experiences, the hyperparameters should not be treated equally by applying search algorithms which can make the process more complex. There are interesting issues that we are considering regarding this research area.

AuthorsFiguresReferencesCitationsKeywordsMetricsFootnotes More Like This Efficient Deep Learning Hyperparameter Tuning Using Cloud Infrastructure: Intelligent Distributed Hyperparameter Tuning with Bayesian Optimization in the Cloud2019 IEEE 12th International Conference on Cloud Computing (CLOUD)Published: 2019Deep Learning on Active Sonar Data Using Bayesian Optimization for Hyperparameter Tuning2020 25th International Conference on Pattern Recognition (ICPR)Published: 2021Show MoreReferencesReferences is not available for this document.IEEE Personal AccountChange username/passwordPurchase DetailsPayment OptionsView Purchased DocumentsProfile InformationCommunications PreferencesProfession and EducationTechnical interestsNeed Help? US & Canada: +1 800 678 4333  Worldwide: +1 732 981 0060  Contact & Support FollowAbout IEEE Xplore | Contact Us | Help | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics Reporting | Sitemap | IEEE Privacy Policy A not-for-profit organization, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity.  © Copyright 2024 IEEE - All rights reserved, including rights for text and data mining and training of artificial intelligence and similar technologies. 


 





IEEE Account

Change Username/Password
Update Address



Purchase Details

Payment Options
Order History
View Purchased Documents



Profile Information

Communications Preferences
Profession and Education
Technical Interests



Need Help?

US & Canada: +1 800 678 4333
Worldwide:  +1 732 981 0060

Contact & Support








About IEEE Xplore
Contact Us
Help
Accessibility
Terms of Use
Nondiscrimination Policy
Sitemap
Privacy & Opting Out of Cookies


							A not-for-profit organization, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity.© Copyright 2024 IEEE - All rights reserved. Use of this web site signifies your agreement to the terms and conditions.
						




































































