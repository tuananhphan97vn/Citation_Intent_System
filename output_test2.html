<html class="js" lang="en"><head>
<meta charset="utf-8"/>
<meta content="IE=edge" http-equiv="X-UA-Compatible"/>
<meta content="pc,mobile" name="applicable-device"/>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="max-image-preview:large" name="robots"/>
<meta content="Yes" name="access"/>
<meta content="1268d79b5e96aecf3ff2a7dac04ad990" name="360-site-verification"/>
<title>Heterogeneous graphormer for extractive multimodal summarization | Journal of Intelligent Information Systems</title>
<meta content="@SpringerLink" name="twitter:site"/>
<meta content="summary_large_image" name="twitter:card"/>
<meta content="Content cover image" name="twitter:image:alt"/>
<meta content="Heterogeneous graphormer for extractive multimodal summarization" name="twitter:title"/>
<meta content="Journal of Intelligent Information Systems - Multimodal summarization with multimodal output (MSMO) aims to generate summaries that incorporate both text and images. Existing methods have not..." name="twitter:description"/>
<meta content="https://static-content.springer.com/image/art%3A10.1007%2Fs10844-024-00886-5/MediaObjects/10844_2024_886_Fig1_HTML.png" name="twitter:image"/>
<meta content="10844" name="journal_id"/>
<meta content="Heterogeneous graphormer for extractive multimodal summarization" name="dc.title"/>
<meta content="Journal of Intelligent Information Systems 2024" name="dc.source"/>
<meta content="text/html" name="dc.format"/>
<meta content="Springer" name="dc.publisher"/>
<meta content="2024-09-30" name="dc.date"/>
<meta content="OriginalPaper" name="dc.type"/>
<meta content="En" name="dc.language"/>
<meta content="2024 The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature" name="dc.copyright"/>
<meta content="2024 The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature" name="dc.rights"/>
<meta content="journalpermissions@springernature.com" name="dc.rightsAgent"/>
<meta content="Multimodal summarization with multimodal output (MSMO) aims to generate summaries that incorporate both text and images. Existing methods have not effectively leveraged intermodal relationships, such as sentence-image relationships, which are crucial for generating high-quality multimodal summaries. In this paper, we propose a heterogeneous graph-based model for multimodal summarization (HGMS) designed to efficiently leverage intermodal relationships within multimodal data. The model constructs a heterogeneous graph based on the relationships between modalities, containing nodes for words, sentences and images. An enhanced Graphormer is then proposed to update node representations, aiming to more effectively model intricate relationships between multiple modalities. To the best of our knowledge, we are the first to apply Graphormer in the field of graph-based summarization. Experimental results on a large-scale benchmark dataset demonstrate that HGMS achieves state-of-the-art performance in terms of automatic metrics and human evaluations." name="dc.description"/>
<meta content="1573-7675" name="prism.issn"/>
<meta content="Journal of Intelligent Information Systems" name="prism.publicationName"/>
<meta content="2024-09-30" name="prism.publicationDate"/>
<meta content="OriginalPaper" name="prism.section"/>
<meta content="1" name="prism.startingPage"/>
<meta content="19" name="prism.endingPage"/>
<meta content="2024 The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature" name="prism.copyright"/>
<meta content="journalpermissions@springernature.com" name="prism.rightsAgent"/>
<meta content="https://link.springer.com/article/10.1007/s10844-024-00886-5" name="prism.url"/>
<meta content="doi:10.1007/s10844-024-00886-5" name="prism.doi"/>
<meta content="https://link.springer.com/content/pdf/10.1007/s10844-024-00886-5.pdf" name="citation_pdf_url"/>
<meta content="https://link.springer.com/article/10.1007/s10844-024-00886-5" name="citation_fulltext_html_url"/>
<meta content="Journal of Intelligent Information Systems" name="citation_journal_title"/>
<meta content="J Intell Inf Syst" name="citation_journal_abbrev"/>
<meta content="Springer US" name="citation_publisher"/>
<meta content="1573-7675" name="citation_issn"/>
<meta content="Heterogeneous graphormer for extractive multimodal summarization" name="citation_title"/>
<meta content="2024/09/30" name="citation_online_date"/>
<meta content="1" name="citation_firstpage"/>
<meta content="19" name="citation_lastpage"/>
<meta content="Research" name="citation_article_type"/>
<meta content="en" name="citation_language"/>
<meta content="doi:10.1007/s10844-024-00886-5" name="dc.identifier"/>
<meta content="10.1007/s10844-024-00886-5" name="DOI"/>
<meta content="250995" name="size"/>
<meta content="10.1007/s10844-024-00886-5" name="citation_doi"/>
<meta content="http://api.springer.com/xmldata/jats?q=doi:10.1007/s10844-024-00886-5&amp;api_key=" name="citation_springer_api_url"/>
<meta content="Multimodal summarization with multimodal output (MSMO) aims to generate summaries that incorporate both text and images. Existing methods have not effectiv" name="description"/>
<meta content="Jiang, Xiankai" name="dc.creator"/>
<meta content="Chen, Jingqiang" name="dc.creator"/>
<meta content="Information Storage and Retrieval" name="dc.subject"/>
<meta content="Data Structures and Information Theory" name="dc.subject"/>
<meta content="Artificial Intelligence" name="dc.subject"/>
<meta content="IT in Business" name="dc.subject"/>
<meta content="Natural Language Processing (NLP)" name="dc.subject"/>
<meta content="An, C., Zhong, M., Wu, Z., et al. (2022). CoLo: A contrastive learning based re-ranking framework for one-stage summarization. In: Calzolari N, Huang CR, Kim H, et al (Eds.), Proceedings of the 29th international conference on computational linguistics. International Committee on Computational Linguistics, Gyeongju, Republic of Korea, pp 5783–5793. 
                https://aclanthology.org/2022.coling-1.508
                
              " name="citation_reference"/>
<meta content="Chen, J., &amp; Zhuge, H. (2018). Abstractive text-image summarization using multi-modal attentional hierarchical RNN. In: Riloff E, Chiang D, Hockenmaier J, et al (Eds.), Proceedings of the 2018 conference on empirical methods in natural language processing. Association for Computational Linguistics, Brussels, Belgium, pp 4046–4056. 
                https://doi.org/10.18653/v1/D18-1438
                
              " name="citation_reference"/>
<meta content="Cheng, J., &amp; Lapata, M. (2016). Neural summarization by extracting sentences and words. In: Erk K, Smith NA (Eds.), Proceedings of the 54th annual meeting of the association for computational linguistics (Volume 1: Long Papers). Association for Computational Linguistics, Berlin, Germany, pp 484–494. 
                https://doi.org/10.18653/v1/P16-1046
                
              " name="citation_reference"/>
<meta content="Cheng, X., Shen, Y., &amp; Lu, W. (2023). A set prediction network for extractive summarization. In: Rogers A, Boyd-Graber J, Okazaki N (Eds.), Findings of the association for computational linguistics: ACL 2023. Association for Computational Linguistics, Toronto, Canada, pp 4766–4777. 
                https://doi.org/10.18653/v1/2023.findings-acl.293
                
              " name="citation_reference"/>
<meta content="Cui, P., Hu, L., &amp; Liu, Y. (2020). Enhancing extractive text summarization with topic-aware graph neural networks. In: Scott D, Bel N, Zong C (Eds.), Proceedings of the 28th international conference on computational linguistics. International Committee on Computational Linguistics, Barcelona, Spain (Online), pp 5360–537. 
                https://doi.org/10.18653/v1/2020.coling-main.468
                
              " name="citation_reference"/>
<meta content="citation_journal_title=J Artif Intell Res; citation_title=Lexrank: Graph-based lexical centrality as salience in text summarization; citation_author=G Erkan, DR Radev; citation_volume=22; citation_publication_date=2004; citation_pages=457-47; citation_doi=10.1613/JAIR.1523; citation_id=CR6" name="citation_reference"/>
<meta content="Jia, R., Cao, Y., Tang, H., et al. (2020). Neural extractive summarization with hierarchical attentive heterogeneous graph network. In: Webber B, Cohn T, He Y, et al (Eds.), Proceedings of the 2020 conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics, Online, pp 3622–363. 
                https://doi.org/10.18653/v1/2020.emnlp-main.295
                
              " name="citation_reference"/>
<meta content="Jiang, C., Xie, R., Ye, W., et al. (2023). Exploiting pseudo image captions for multimodal summarization. In: Rogers A, Boyd-Graber J, Okazaki N (Eds.), Findings of the association for computational linguistics: ACL 2023. Association for Computational Linguistics, Toronto, Canada, pp 161–175. 
                https://doi.org/10.18653/v1/2023.findings-acl.12
                
              " name="citation_reference"/>
<meta content="citation_journal_title=J Intell Inf Syst; citation_title=Self-supervised opinion summarization with multi-modal knowledge graph; citation_author=L Jin, J Chen; citation_volume=62; citation_issue=1; citation_publication_date=2024; citation_pages=191-208; citation_doi=10.1007/S10844-023-00812-1; citation_id=CR9" name="citation_reference"/>
<meta content="Jing, B., You, Z., Yang, T., et al. (2021). Multiplex graph neural network for extractive text summarization. In: Moens MF, Huang X, Specia L, et al. (Eds.), Proceedings of the 2021 conference on empirical methods in natural language processing. Association for Computational Linguistics, Online and Punta Cana, Dominican Republic, pp 133–139. 
                https://doi.org/10.18653/v1/2021.emnlp-main.11
                
              " name="citation_reference"/>
<meta content="Kingma, D.P., &amp; Ba, J (2015). Adam: A method for stochastic optimization. In: Bengio Y, LeCun Y (Eds.), 3rd International conference on learning representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings,[SPACE]
                http://arxiv.org/abs/1412.6980
                
              " name="citation_reference"/>
<meta content="Lewis, M., Liu, Y., Goyal, N., et al. (2020). BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In: Jurafsky D, Chai J, Schluter N, et al (Eds.), Proceedings of the 58th annual meeting of the association for computational linguistics. Association for Computational Linguistics, Online, pp 7871–7880. 
                https://doi.org/10.18653/v1/2020.acl-main.703
                
              " name="citation_reference"/>
<meta content="Li, H., Zhu, J., Liu, T., et al. (2018). Multi-modal sentence summarization with modality attention and image filtering. In: Lang J (Ed.), Proceedings of the twenty-seventh international joint conference on artificial intelligence, IJCAI 2018, July 13-19, 2018, Stockholm, Sweden. ijcai.org, Stockholm, Sweden, pp 4152–4158. 
                https://doi.org/10.24963/ijcai.2018/577
                
              " name="citation_reference"/>
<meta content="Li, J., Li, D., Xiong, C., et al. (2022). BLIP: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In: Chaudhuri K, Jegelka S, Song L, et al (Eds.), International conference on machine learning, ICML 2022, 17-23 July 2022, PMLR, Baltimore, Maryland, USA, Proceedings of Machine Learning Research, 162 pp 12888–12900" name="citation_reference"/>
<meta content="Li, M., Chen, X., Gao, S., et al (2020) VMSMO: Learning to generate multimodal summary for video-based news articles. In: Webber B, Cohn T, He Y, et al (Eds.), Proceedings of the 2020 conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics, Online, pp 9360–9369. 
                https://doi.org/10.18653/v1/2020.emnlp-main.752
                
              " name="citation_reference"/>
<meta content="Liang, Y., Meng, F., Xu, J., et al. (2023). Summary-oriented vision modeling for multimodal abstractive summarization. In: Rogers A, Boyd-Graber J, Okazaki N (Eds.), Proceedings of the 61st annual meeting of the association for computational linguistics (Volume 1: Long Papers). Association for Computational Linguistics, Toronto, Canada, pp 2934–2951. 
                https://doi.org/10.18653/v1/2023.acl-long.165
                
              " name="citation_reference"/>
<meta content="Liu Y, &amp; Lapata M. (2019). Text summarization with pretrained encoders. In: Inui K, Jiang J, Ng V, et al (Eds.), Proceedings of the 2019 conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). Association for Computational Linguistics, Hong Kong, China, pp 3730–3740. 
                https://doi.org/10.18653/v1/D19-1387
                
              " name="citation_reference"/>
<meta content="Liu, Y., Ott, M., Goyal, N., et al. (2019). Roberta: A robustly optimized BERT pretraining approach. 
                arXiv:1907.11692
                
              
                https://arxiv.org/abs/1907.11692
                
              " name="citation_reference"/>
<meta content="Liu, Y., Liu, P., Radev, D., et al. (2022). BRIO: Bringing order to abstractive summarization. In: Muresan S, Nakov P, Villavicencio A (Eds.), Proceedings of the 60th annual meeting of the association for computational linguistics (Volume 1: Long Papers). Association for Computational Linguistics, Dublin, Ireland, pp 2890–2903. 
                https://doi.org/10.18653/v1/2022.acl-long.207
                
              " name="citation_reference"/>
<meta content="van der Maaten L, &amp; Hinton G (2008) Visualizing data using t-sne. Journal of Machine Learning Research 9(86), 2579–2605. 
                http://jmlr.org/papers/v9/vandermaaten08a.html
                
              " name="citation_reference"/>
<meta content="Mihalcea, R., Tarau, P (2004) TextRank: Bringing order into text. In: Lin D, Wu D (Eds.), Proceedings of the 2004 conference on empirical methods in natural language processing. Association for Computational Linguistics, Barcelona, Spain, pp 404–411. 
                https://aclanthology.org/W04-3252
                
              " name="citation_reference"/>
<meta content="Nallapati R, Zhai F, &amp; Zhou B. (2017) .Summarunner: A recurrent neural network based sequence model for extractive summarization of documents. Proceedings of the AAAI Conference on Artificial Intelligence 31(1). 
                https://doi.org/10.1609/aaai.v31i1.10958
                
              " name="citation_reference"/>
<meta content="Narayan S, Cohen SB, &amp; Lapata M. (2018). Ranking sentences for extractive summarization with reinforcement learning. In: Walker M, Ji H, Stent A (Eds.) Proceedings of the 2018 conference of the North American chapter of the association for computational linguistics: Human language technologies, Volume 1 (Long Papers). Association for Computational Linguistics, New Orleans, Louisiana, pp 1747–1759. 
                https://doi.org/10.18653/v1/N18-1158
                
              " name="citation_reference"/>
<meta content="Overbay, K., Ahn, J., Pesaran zadeh, F., et al. (2023). mRedditSum: A multimodal abstractive summarization dataset of Reddit threads with images. In: Bouamor H, Pino J, Bali K (Eds.), Proceedings of the 2023 conference on empirical methods in natural language processing. Association for Computational Linguistics, Singapore, pp 4117–4132. 
                https://doi.org/10.18653/v1/2023.emnlp-main.251
                
              " name="citation_reference"/>
<meta content="Paulus, R., Xiong, C., &amp; Socher, R. (2018). A deep reinforced model for abstractive summarization. In: International conference on learning representations, 
                https://openreview.net/forum?id=HkAClQgA-
                
              " name="citation_reference"/>
<meta content="Pennington, J., Socher, R., &amp; Manning, C. (2014). GloVe: Global vectors for word representation. In: Moschitti A, Pang B, Daelemans W (Eds.) Proceedings of the 2014 conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics, Doha, Qatar, pp 1532–1543. 
                https://doi.org/10.3115/v1/D14-1162
                
              " name="citation_reference"/>
<meta content="Phan TA, Nguyen NDN, &amp; Bui KHN. (2022). HeterGraphLongSum: Heterogeneous graph neural network with passage aggregation for extractive long document summarization. In: Calzolari N, Huang CR, Kim H, et al (Eds.), Proceedings of the 29th international conference on computational linguistics. International Committee on Computational Linguistics, Gyeongju, Republic of Korea, pp 6248–6258. 
                https://aclanthology.org/2022.coling-1.545
                
              " name="citation_reference"/>
<meta content="Radford A, Kim JW, Hallacy C, et al (2021) Learning transferable visual models from natural language supervision. In: Meila M, Zhang T (Eds.), Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, pp 8748–8763. 
                http://proceedings.mlr.press/v139/radford21a.html
                
              " name="citation_reference"/>
<meta content="See, A., Liu, P.J., Manning, C.D. (2017). Get to the point: Summarization with pointer-generator networks. In: Barzilay R, Kan MY (Eds.), Proceedings of the 55th annual meeting of the association for computational linguistics (Volume 1: Long Papers). Association for Computational Linguistics, Vancouver, Canada, pp 1073–1083. 
                https://doi.org/10.18653/v1/P17-1099
                
              " name="citation_reference"/>
<meta content="citation_journal_title=Proceedings of the AAAI Conference on Artificial Intelligence; citation_title=Hierarchical heterogeneous graph attention network for syntax-aware summarization; citation_author=Z Song, I King; citation_volume=36; citation_issue=10; citation_publication_date=2022; citation_pages=11340-1134; citation_doi=10.1609/aaai.v36i10.21385; citation_id=CR30" name="citation_reference"/>
<meta content="Sutskever, I., Vinyals, O., &amp; Le, Q.V. (2014). Sequence to sequence learning with neural networks. In: Ghahramani Z, Welling M, Cortes C, et al (Eds.), Advances in Neural Information Processing Systems,[SPACE]
                https://proceedings.neurips.cc/paper/2014/hash/a14ac55a4f27472c5d894ec1c3c743d2-Abstract.html
                
              " name="citation_reference"/>
<meta content="Tang, P., Hu, K., Yan, R., et al. (2022). OTExtSum: Extractive text summarisation with optimal transport. In: Carpuat M, de Marneffe MC, Meza Ruiz IV (eds) Findings of the association for computational linguistics: NAACL 2022. Association for Computational Linguistics, Seattle, United States, pp 1128–1141. 
                https://doi.org/10.18653/v1/2022.findings-naacl.85
                
              " name="citation_reference"/>
<meta content="Vaswani A, Shazeer N, Parmar N, et al. (2017). Attention is all you need. In: Guyon I, Luxburg UV, Bengio S, et al (Eds.), Advances in Neural Information Processing Systems,[SPACE]
                https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html
                
              " name="citation_reference"/>
<meta content="citation_journal_title=Graph attention networks. stat; citation_author=P Velickovic, G Cucurull, A Casanova; citation_volume=1050; citation_issue=20; citation_publication_date=2017; citation_pages=10-48550; citation_id=CR34" name="citation_reference"/>
<meta content="Wang, D., Liu, P., Zheng, Y., et al. (2020). Heterogeneous graph neural networks for extractive document summarization. In: Jurafsky D, Chai J, Schluter N, et al (Eds.), Proceedings of the 58th annual meeting of the association for computational linguistics. Association for Computational Linguistics, Online, pp 6209–6219. 
                https://doi.org/10.18653/v1/2020.acl-main.553
                
              " name="citation_reference"/>
<meta content="Wu W, Li W, Xiao X, et al. (2021). BASS: Boosting abstractive summarization with unified semantic graph. In: Proceedings of the 59th annual meeting of the association for computational linguistics and the 11th international joint conference on natural language processing (Volume 1: Long Papers). Association for Computational Linguistics, Online, pp 6052–6067.
                https://doi.org/10.18653/v1/2021.acl-long.472
                
              " name="citation_reference"/>
<meta content="Xiao M, Zhu J, Lin H, et al. (2023). CFSum coarse-to-fine contribution network for multimodal summarization. In: Rogers A, Boyd-Graber J, Okazaki N (Eds.), Proceedings of the 61st annual meeting of the association for computational linguistics (Volume 1: Long Papers). Association for Computational Linguistics, Toronto, Canada, pp 8538–8553. 
                https://doi.org/10.18653/v1/2023.acl-long.476
                
              " name="citation_reference"/>
<meta content="citation_journal_title=J Intell Inf Syst; citation_title=Extractive text-image summarization with relation-enhanced graph attention network; citation_author=F Xie, J Chen, K Chen; citation_volume=61; citation_issue=2; citation_publication_date=2023; citation_pages=325-341; citation_doi=10.1007/S10844-022-00757-X; citation_id=CR38" name="citation_reference"/>
<meta content="Ying, C., Cai, T., Luo, S., et al. (2021). Do transformers really perform badly for graph representation? In: Ranzato M, Beygelzimer A, Dauphin YN, et al (Eds.), Advances in Neural Information Processing Systems, pp 28877–28888. 
                https://proceedings.neurips.cc/paper/2021/hash/f1c1592588411002af340cbaedd6fc33-Abstract.html
                
              " name="citation_reference"/>
<meta content="Zhang, J., Zhao, Y., Saleh, M., et al. (2020). PEGASUS: pre-training with extracted gap-sentences for abstractive summarization. In: Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, pp 11328–11339. 
                http://proceedings.mlr.press/v119/zhang20ae.html
                
              " name="citation_reference"/>
<meta content="Zhang, L., Zhang, X., &amp; Pan, J. (2022a). Hierarchical cross-modality semantic correlation learning model for multimodal summarization. Proceedings of the AAAI Conference on Artificial Intelligence 36(10) 11676–11684. 
                https://doi.org/10.1609/aaai.v36i10.21422
                
              " name="citation_reference"/>
<meta content="Zhang, M., Zhou, G., Yu, W., et al. (2022b). MAA-PTG: multimodal aspect-aware product title generation. J Intell Inf Syst 59(1) 213–235. 
                https://doi.org/10.1007/S10844-022-00695-8
                
              " name="citation_reference"/>
<meta content="Zhang, Z., Meng, X., Wang, Y., et al. (2022c). Unims: A unified framework for multimodal summarization with knowledge distillation. Proceedings of the AAAI Conference on Artificial Intelligence 36(10) 11757–11764. 
                https://doi.org/10.1609/aaai.v36i10.21431
                
              " name="citation_reference"/>
<meta content="Zhong, M., Liu, P., Chen, Y., et al. (2020). Extractive summarization as text matching. In: Jurafsky D, Chai J, Schluter N, et al (Eds.) Proceedings of the 58th annual meeting of the association for computational linguistics. Association for Computational Linguistics, Online, pp 6197–6208. 
                https://doi.org/10.18653/v1/2020.acl-main.552
                
              " name="citation_reference"/>
<meta content="Zhu, J., Li, H., Liu, T., et al. (2018). MSMO: Multimodal summarization with multimodal output. In: Riloff E, Chiang D, Hockenmaier J, et al (eds) Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Brussels, Belgium, pp 4154–4164. 
                https://doi.org/10.18653/v1/D18-1448
                
              " name="citation_reference"/>
<meta content="citation_journal_title=Proceedings of the AAAI Conference on Artificial Intelligence; citation_title=Multimodal summarization with guidance of multimodal reference; citation_author=J Zhu, Y Zhou, J Zhang; citation_volume=34; citation_issue=05; citation_publication_date=2020; citation_pages=9749-975; citation_doi=10.1609/aaai.v34i05.6525; citation_id=CR46" name="citation_reference"/>
<meta content="Jiang, Xiankai" name="citation_author"/>
<meta content="1222045619@njupt.edu.cn" name="citation_author_email"/>
<meta content="School of Computer Science, Nanjing University of Posts and Telecommunications, Nanjing, China" name="citation_author_institution"/>
<meta content="Chen, Jingqiang" name="citation_author"/>
<meta content="cjq@njupt.edu.cn" name="citation_author_email"/>
<meta content="School of Computer Science, Nanjing University of Posts and Telecommunications, Nanjing, China" name="citation_author_institution"/>
<meta content="telephone=no" name="format-detection"/>
<meta content="https://link.springer.com/article/10.1007/s10844-024-00886-5" property="og:url"/>
<meta content="article" property="og:type"/>
<meta content="SpringerLink" property="og:site_name"/>
<meta content="Heterogeneous graphormer for extractive multimodal summarization - Journal of Intelligent Information Systems" property="og:title"/>
<meta content="Multimodal summarization with multimodal output (MSMO) aims to generate summaries that incorporate both text and images. Existing methods have not effectively leveraged intermodal relationships, such as sentence-image relationships, which are crucial for generating high-quality multimodal summaries. In this paper, we propose a heterogeneous graph-based model for multimodal summarization (HGMS) designed to efficiently leverage intermodal relationships within multimodal data. The model constructs a heterogeneous graph based on the relationships between modalities, containing nodes for words, sentences and images. An enhanced Graphormer is then proposed to update node representations, aiming to more effectively model intricate relationships between multiple modalities. To the best of our knowledge, we are the first to apply Graphormer in the field of graph-based summarization. Experimental results on a large-scale benchmark dataset demonstrate that HGMS achieves state-of-the-art performance in terms of automatic metrics and human evaluations." property="og:description"/>
<meta content="https://static-content.springer.com/image/art%3A10.1007%2Fs10844-024-00886-5/MediaObjects/10844_2024_886_Fig1_HTML.png" property="og:image"/>
<meta content="telephone=no" name="format-detection"/>
<link href="/oscar-static/img/favicons/darwin/apple-touch-icon-92e819bf8a.png" rel="apple-touch-icon" sizes="180x180"/>
<link href="/oscar-static/img/favicons/darwin/android-chrome-192x192-6f081ca7e5.png" rel="icon" sizes="192x192" type="image/png"/>
<link href="/oscar-static/img/favicons/darwin/favicon-32x32-1435da3e82.png" rel="icon" sizes="32x32" type="image/png"/>
<link href="/oscar-static/img/favicons/darwin/favicon-16x16-ed57f42bd2.png" rel="icon" sizes="16x16" type="image/png"/>
<link data-test="shortcut-icon" href="/oscar-static/img/favicons/darwin/favicon-c6d59aafac.ico" rel="shortcut icon"/>
<meta content="#e6e6e6" name="theme-color"/>
<!-- Please see discussion: https://github.com/springernature/frontend-open-space/issues/316-->
<!--TODO: Implement alternative to CTM in here if the discussion concludes we do not continue with CTM as a practice-->
<link href="/oscar-static/app-springerlink/css/print-b8af42253b.css" media="print" rel="stylesheet"/>
<style> html{text-size-adjust:100%;line-height:1.15}body{font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;line-height:1.8;margin:0}details,main{display:block}h1{font-size:2em;margin:.67em 0}a{background-color:transparent;color:#025e8d}sub{bottom:-.25em;font-size:75%;line-height:0;position:relative;vertical-align:baseline}img{border:0;height:auto;max-width:100%;vertical-align:middle}button,input{font-family:inherit;font-size:100%;line-height:1.15;margin:0;overflow:visible}button{text-transform:none}[type=button],[type=submit],button{-webkit-appearance:button}[type=search]{-webkit-appearance:textfield;outline-offset:-2px}summary{display:list-item}[hidden]{display:none}button{cursor:pointer}svg{height:1rem;width:1rem} </style>
<style>@media only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark) {  body{background:#fff;color:#222;font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;line-height:1.8;min-height:100%}a{color:#025e8d;text-decoration:underline;text-decoration-skip-ink:auto}button{cursor:pointer}img{border:0;height:auto;max-width:100%;vertical-align:middle}html{box-sizing:border-box;font-size:100%;height:100%;overflow-y:scroll}h1{font-size:2.25rem}h2{font-size:1.75rem}h1,h2,h4{font-weight:700;line-height:1.2}h4{font-size:1.25rem}body{font-size:1.125rem}*{box-sizing:inherit}p{margin-bottom:2rem;margin-top:0}p:last-of-type{margin-bottom:0}.c-ad{text-align:center}@media only screen and (min-width:480px){.c-ad{padding:8px}}.c-ad--728x90{display:none}.c-ad--728x90 .c-ad__inner{min-height:calc(1.5em + 94px)}@media only screen and (min-width:876px){.js .c-ad--728x90{display:none}}.c-ad__label{color:#333;font-size:.875rem;font-weight:400;line-height:1.5;margin-bottom:4px}.c-ad__label,.c-status-message{font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif}.c-status-message{align-items:center;box-sizing:border-box;display:flex;position:relative;width:100%}.c-status-message :last-child{margin-bottom:0}.c-status-message--boxed{background-color:#fff;border:1px solid #ccc;line-height:1.4;padding:16px}.c-status-message__heading{font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;font-size:.875rem;font-weight:700}.c-status-message__icon{fill:currentcolor;display:inline-block;flex:0 0 auto;height:1.5em;margin-right:8px;transform:translate(0);vertical-align:text-top;width:1.5em}.c-status-message__icon--top{align-self:flex-start}.c-status-message--info .c-status-message__icon{color:#003f8d}.c-status-message--boxed.c-status-message--info{border-bottom:4px solid #003f8d}.c-status-message--error .c-status-message__icon{color:#c40606}.c-status-message--boxed.c-status-message--error{border-bottom:4px solid #c40606}.c-status-message--success .c-status-message__icon{color:#00b8b0}.c-status-message--boxed.c-status-message--success{border-bottom:4px solid #00b8b0}.c-status-message--warning .c-status-message__icon{color:#edbc53}.c-status-message--boxed.c-status-message--warning{border-bottom:4px solid #edbc53}.eds-c-header{background-color:#fff;border-bottom:2px solid #01324b;font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;font-size:1rem;line-height:1.5;padding:8px 0 0}.eds-c-header__container{align-items:center;display:flex;flex-wrap:nowrap;gap:8px 16px;justify-content:space-between;margin:0 auto 8px;max-width:1280px;padding:0 8px;position:relative}.eds-c-header__nav{border-top:2px solid #c5e0f4;padding-top:4px;position:relative}.eds-c-header__nav-container{align-items:center;display:flex;flex-wrap:wrap;margin:0 auto 4px;max-width:1280px;padding:0 8px;position:relative}.eds-c-header__nav-container>:not(:last-child){margin-right:32px}.eds-c-header__link-container{align-items:center;display:flex;flex:1 0 auto;gap:8px 16px;justify-content:space-between}.eds-c-header__list{list-style:none;margin:0;padding:0}.eds-c-header__list-item{font-weight:700;margin:0 auto;max-width:1280px;padding:8px}.eds-c-header__list-item:not(:last-child){border-bottom:2px solid #c5e0f4}.eds-c-header__item{color:inherit}@media only screen and (min-width:768px){.eds-c-header__item--menu{display:none;visibility:hidden}.eds-c-header__item--menu:first-child+*{margin-block-start:0}}.eds-c-header__item--inline-links{display:none;visibility:hidden}@media only screen and (min-width:768px){.eds-c-header__item--inline-links{display:flex;gap:16px 16px;visibility:visible}}.eds-c-header__item--divider:before{border-left:2px solid #c5e0f4;content:"";height:calc(100% - 16px);margin-left:-15px;position:absolute;top:8px}.eds-c-header__brand{padding:16px 8px}.eds-c-header__brand a{display:block;line-height:1;text-decoration:none}.eds-c-header__brand img{height:1.5rem;width:auto}.eds-c-header__link{color:inherit;display:inline-block;font-weight:700;padding:16px 8px;position:relative;text-decoration-color:transparent;white-space:nowrap;word-break:normal}.eds-c-header__icon{fill:currentcolor;display:inline-block;font-size:1.5rem;height:1em;transform:translate(0);vertical-align:bottom;width:1em}.eds-c-header__icon+*{margin-left:8px}.eds-c-header__expander{background-color:#f0f7fc}.eds-c-header__search{display:block;padding:24px 0}@media only screen and (min-width:768px){.eds-c-header__search{max-width:70%}}.eds-c-header__search-container{position:relative}.eds-c-header__search-label{color:inherit;display:inline-block;font-weight:700;margin-bottom:8px}.eds-c-header__search-input{background-color:#fff;border:1px solid #000;padding:8px 48px 8px 8px;width:100%}.eds-c-header__search-button{background-color:transparent;border:0;color:inherit;height:100%;padding:0 8px;position:absolute;right:0}.has-tethered.eds-c-header__expander{border-bottom:2px solid #01324b;left:0;margin-top:-2px;top:100%;width:100%;z-index:10}@media only screen and (min-width:768px){.has-tethered.eds-c-header__expander--menu{display:none;visibility:hidden}}.has-tethered .eds-c-header__heading{display:none;visibility:hidden}.has-tethered .eds-c-header__heading:first-child+*{margin-block-start:0}.has-tethered .eds-c-header__search{margin:auto}.eds-c-header__heading{margin:0 auto;max-width:1280px;padding:16px 16px 0}.eds-c-pagination{align-items:center;display:flex;flex-wrap:wrap;font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;font-size:.875rem;gap:16px 0;justify-content:center;line-height:1.4;list-style:none;margin:0;padding:32px 0}@media only screen and (min-width:480px){.eds-c-pagination{padding:32px 16px}}.eds-c-pagination__item{margin-right:8px}.eds-c-pagination__item--prev{margin-right:16px}.eds-c-pagination__item--next .eds-c-pagination__link,.eds-c-pagination__item--prev .eds-c-pagination__link{padding:16px 8px}.eds-c-pagination__item--next{margin-left:8px}.eds-c-pagination__item:last-child{margin-right:0}.eds-c-pagination__link{align-items:center;color:#222;cursor:pointer;display:inline-block;font-size:1rem;margin:0;padding:16px 24px;position:relative;text-align:center;transition:all .2s ease 0s}.eds-c-pagination__link:visited{color:#222}.eds-c-pagination__link--disabled{border-color:#555;color:#555;cursor:default}.eds-c-pagination__link--active{background-color:#01324b;background-image:none;border-radius:8px;color:#fff}.eds-c-pagination__link--active:focus,.eds-c-pagination__link--active:hover,.eds-c-pagination__link--active:visited{color:#fff}.eds-c-pagination__link-container{align-items:center;display:flex}.eds-c-pagination__icon{fill:#222;height:1.5rem;width:1.5rem}.eds-c-pagination__icon--disabled{fill:#555}.eds-c-pagination__visually-hidden{clip:rect(0,0,0,0);border:0;clip-path:inset(50%);height:1px;overflow:hidden;padding:0;position:absolute!important;white-space:nowrap;width:1px}.c-breadcrumbs{color:#333;font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;font-size:1rem;list-style:none;margin:0;padding:0}.c-breadcrumbs>li{display:inline}svg.c-breadcrumbs__chevron{fill:#333;height:10px;margin:0 .25rem;width:10px}.c-breadcrumbs--contrast,.c-breadcrumbs--contrast .c-breadcrumbs__link{color:#fff}.c-breadcrumbs--contrast svg.c-breadcrumbs__chevron{fill:#fff}@media only screen and (max-width:479px){.c-breadcrumbs .c-breadcrumbs__item{display:none}.c-breadcrumbs .c-breadcrumbs__item:last-child,.c-breadcrumbs .c-breadcrumbs__item:nth-last-child(2){display:inline}}.c-skip-link{background:#01324b;bottom:auto;color:#fff;font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;font-size:1rem;padding:8px;position:absolute;text-align:center;transform:translateY(-100%);width:100%;z-index:9999}@media (prefers-reduced-motion:reduce){.c-skip-link{transition:top .3s ease-in-out 0s}}@media print{.c-skip-link{display:none}}.c-skip-link:active,.c-skip-link:hover,.c-skip-link:link,.c-skip-link:visited{color:#fff}.c-skip-link:focus{transform:translateY(0)}.l-with-sidebar{display:flex;flex-wrap:wrap}.l-with-sidebar>*{margin:0}.l-with-sidebar__sidebar{flex-basis:var(--with-sidebar--basis,400px);flex-grow:1}.l-with-sidebar>:not(.l-with-sidebar__sidebar){flex-basis:0px;flex-grow:999;min-width:var(--with-sidebar--min,53%)}.l-with-sidebar>:first-child{padding-right:4rem}@supports (gap:1em){.l-with-sidebar>:first-child{padding-right:0}.l-with-sidebar{gap:var(--with-sidebar--gap,4rem)}}.c-header__link{color:inherit;display:inline-block;font-weight:700;padding:16px 8px;position:relative;text-decoration-color:transparent;white-space:nowrap;word-break:normal}.app-masthead__colour-4{--background-color:#ff9500;--gradient-light:rgba(0,0,0,.5);--gradient-dark:rgba(0,0,0,.8)}.app-masthead{background:var(--background-color,#0070a8);position:relative}.app-masthead:after{background:radial-gradient(circle at top right,var(--gradient-light,rgba(0,0,0,.4)),var(--gradient-dark,rgba(0,0,0,.7)));bottom:0;content:"";left:0;position:absolute;right:0;top:0}@media only screen and (max-width:479px){.app-masthead:after{background:linear-gradient(225deg,var(--gradient-light,rgba(0,0,0,.4)),var(--gradient-dark,rgba(0,0,0,.7)))}}.app-masthead__container{color:var(--masthead-color,#fff);margin:0 auto;max-width:1280px;padding:0 16px;position:relative;z-index:1}.u-button{align-items:center;background-color:#01324b;background-image:none;border:4px solid transparent;border-radius:32px;cursor:pointer;display:inline-flex;font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;font-size:.875rem;font-weight:700;justify-content:center;line-height:1.3;margin:0;padding:16px 32px;position:relative;transition:all .2s ease 0s;width:auto}.u-button svg,.u-button--contrast svg,.u-button--primary svg,.u-button--secondary svg,.u-button--tertiary svg{fill:currentcolor}.u-button,.u-button:visited{color:#fff}.u-button,.u-button:hover{box-shadow:0 0 0 1px #01324b;text-decoration:none}.u-button:hover{border:4px solid #fff}.u-button:focus{border:4px solid #fc0;box-shadow:none;outline:0;text-decoration:none}.u-button:focus,.u-button:hover{background-color:#fff;background-image:none;color:#01324b}.app-masthead--pastel .c-pdf-download .u-button--primary:focus svg path,.app-masthead--pastel .c-pdf-download .u-button--primary:hover svg path,.c-context-bar--sticky .c-context-bar__container .c-pdf-download .u-button--primary:focus svg path,.c-context-bar--sticky .c-context-bar__container .c-pdf-download .u-button--primary:hover svg path,.u-button--primary:focus svg path,.u-button--primary:hover svg path,.u-button:focus svg path,.u-button:hover svg path{fill:#01324b}.u-button--primary{background-color:#01324b;background-image:none;border:4px solid transparent;box-shadow:0 0 0 1px #01324b;color:#fff;font-weight:700}.u-button--primary:visited{color:#fff}.u-button--primary:hover{border:4px solid #fff;box-shadow:0 0 0 1px #01324b;text-decoration:none}.u-button--primary:focus{border:4px solid #fc0;box-shadow:none;outline:0;text-decoration:none}.u-button--primary:focus,.u-button--primary:hover{background-color:#fff;background-image:none;color:#01324b}.u-button--secondary{background-color:#fff;border:4px solid #fff;color:#01324b;font-weight:700}.u-button--secondary:visited{color:#01324b}.u-button--secondary:hover{border:4px solid #01324b;box-shadow:none}.u-button--secondary:focus,.u-button--secondary:hover{background-color:#01324b;color:#fff}.app-masthead--pastel .c-pdf-download .u-button--secondary:focus svg path,.app-masthead--pastel .c-pdf-download .u-button--secondary:hover svg path,.c-context-bar--sticky .c-context-bar__container .c-pdf-download .u-button--secondary:focus svg path,.c-context-bar--sticky .c-context-bar__container .c-pdf-download .u-button--secondary:hover svg path,.u-button--secondary:focus svg path,.u-button--secondary:hover svg path,.u-button--tertiary:focus svg path,.u-button--tertiary:hover svg path{fill:#fff}.u-button--tertiary{background-color:#ebf1f5;border:4px solid transparent;box-shadow:none;color:#666;font-weight:700}.u-button--tertiary:visited{color:#666}.u-button--tertiary:hover{border:4px solid #01324b;box-shadow:none}.u-button--tertiary:focus,.u-button--tertiary:hover{background-color:#01324b;color:#fff}.u-button--contrast{background-color:transparent;background-image:none;color:#fff;font-weight:400}.u-button--contrast:visited{color:#fff}.u-button--contrast,.u-button--contrast:focus,.u-button--contrast:hover{border:4px solid #fff}.u-button--contrast:focus,.u-button--contrast:hover{background-color:#fff;background-image:none;color:#000}.u-button--contrast:focus svg path,.u-button--contrast:hover svg path{fill:#000}.u-button--disabled,.u-button:disabled{background-color:transparent;background-image:none;border:4px solid #ccc;color:#000;cursor:default;font-weight:400;opacity:.7}.u-button--disabled svg,.u-button:disabled svg{fill:currentcolor}.u-button--disabled:visited,.u-button:disabled:visited{color:#000}.u-button--disabled:focus,.u-button--disabled:hover,.u-button:disabled:focus,.u-button:disabled:hover{border:4px solid #ccc;text-decoration:none}.u-button--disabled:focus,.u-button--disabled:hover,.u-button:disabled:focus,.u-button:disabled:hover{background-color:transparent;background-image:none;color:#000}.u-button--disabled:focus svg path,.u-button--disabled:hover svg path,.u-button:disabled:focus svg path,.u-button:disabled:hover svg path{fill:#000}.u-button--small,.u-button--xsmall{font-size:.875rem;padding:2px 8px}.u-button--small{padding:8px 16px}.u-button--large{font-size:1.125rem;padding:10px 35px}.u-button--full-width{display:flex;width:100%}.u-button--icon-left svg{margin-right:8px}.u-button--icon-right svg{margin-left:8px}.u-clear-both{clear:both}.u-container{margin:0 auto;max-width:1280px;padding:0 16px}.u-justify-content-space-between{justify-content:space-between}.u-display-none{display:none}.js .u-js-hide,.u-hide{display:none;visibility:hidden}.u-visually-hidden{clip:rect(0,0,0,0);border:0;clip-path:inset(50%);height:1px;overflow:hidden;padding:0;position:absolute!important;white-space:nowrap;width:1px}.u-icon{fill:currentcolor;display:inline-block;height:1em;transform:translate(0);vertical-align:text-top;width:1em}.u-list-reset{list-style:none;margin:0;padding:0}.u-ma-16{margin:16px}.u-mt-0{margin-top:0}.u-mt-24{margin-top:24px}.u-mt-32{margin-top:32px}.u-mb-8{margin-bottom:8px}.u-mb-32{margin-bottom:32px}.u-button-reset{background-color:transparent;border:0;padding:0}.u-sans-serif{font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif}.u-serif{font-family:Merriweather,serif}h1,h2,h4{-webkit-font-smoothing:antialiased}p{overflow-wrap:break-word;word-break:break-word}.u-h4{font-size:1.25rem;font-weight:700;line-height:1.2}.u-mbs-0{margin-block-start:0!important}.c-article-header{font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif}.c-article-identifiers{color:#6f6f6f;display:flex;flex-wrap:wrap;font-size:1rem;line-height:1.3;list-style:none;margin:0 0 8px;padding:0}.c-article-identifiers__item{border-right:1px solid #6f6f6f;list-style:none;margin-right:8px;padding-right:8px}.c-article-identifiers__item:last-child{border-right:0;margin-right:0;padding-right:0}@media only screen and (min-width:876px){.c-article-title{font-size:1.875rem;line-height:1.2}}.c-article-author-list{display:inline;font-size:1rem;list-style:none;margin:0 8px 0 0;padding:0;width:100%}.c-article-author-list__item{display:inline;padding-right:0}.c-article-author-list__show-more{display:none;margin-right:4px}.c-article-author-list__button,.js .c-article-author-list__item--hide,.js .c-article-author-list__show-more{display:none}.js .c-article-author-list--long .c-article-author-list__show-more,.js .c-article-author-list--long+.c-article-author-list__button{display:inline}@media only screen and (max-width:767px){.js .c-article-author-list__item--hide-small-screen{display:none}.js .c-article-author-list--short .c-article-author-list__show-more,.js .c-article-author-list--short+.c-article-author-list__button{display:inline}}#uptodate-client,.js .c-article-author-list--expanded .c-article-author-list__show-more{display:none!important}.js .c-article-author-list--expanded .c-article-author-list__item--hide-small-screen{display:inline!important}.c-article-author-list__button,.c-button-author-list{background:#ebf1f5;border:4px solid #ebf1f5;border-radius:20px;color:#666;font-size:.875rem;line-height:1.4;padding:2px 11px 2px 8px;text-decoration:none}.c-article-author-list__button svg,.c-button-author-list svg{margin:1px 4px 0 0}.c-article-author-list__button:hover,.c-button-author-list:hover{background:#025e8d;border-color:transparent;color:#fff}.c-article-body .c-article-access-provider{padding:8px 16px}.c-article-body .c-article-access-provider,.c-notes{border:1px solid #d5d5d5;border-image:initial;border-left:none;border-right:none;margin:24px 0}.c-article-body .c-article-access-provider__text{color:#555}.c-article-body .c-article-access-provider__text,.c-notes__text{font-size:1rem;margin-bottom:0;padding-bottom:2px;padding-top:2px;text-align:center}.c-article-body .c-article-author-affiliation__address{color:inherit;font-weight:700;margin:0}.c-article-body .c-article-author-affiliation__authors-list{list-style:none;margin:0;padding:0}.c-article-body .c-article-author-affiliation__authors-item{display:inline;margin-left:0}.c-article-authors-search{margin-bottom:24px;margin-top:0}.c-article-authors-search__item,.c-article-authors-search__title{font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif}.c-article-authors-search__title{color:#626262;font-size:1.05rem;font-weight:700;margin:0;padding:0}.c-article-authors-search__item{font-size:1rem}.c-article-authors-search__text{margin:0}.c-code-block{border:1px solid #fff;font-family:monospace;margin:0 0 24px;padding:20px}.c-code-block__heading{font-weight:400;margin-bottom:16px}.c-code-block__line{display:block;overflow-wrap:break-word;white-space:pre-wrap}.c-article-share-box{font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;margin-bottom:24px}.c-article-share-box__description{font-size:1rem;margin-bottom:8px}.c-article-share-box__no-sharelink-info{font-size:.813rem;font-weight:700;margin-bottom:24px;padding-top:4px}.c-article-share-box__only-read-input{border:1px solid #d5d5d5;box-sizing:content-box;display:inline-block;font-size:.875rem;font-weight:700;height:24px;margin-bottom:8px;padding:8px 10px}.c-article-share-box__additional-info{color:#626262;font-size:.813rem}.c-article-share-box__button{background:#fff;box-sizing:content-box;text-align:center}.c-article-share-box__button--link-like{background-color:transparent;border:0;color:#025e8d;cursor:pointer;font-size:.875rem;margin-bottom:8px;margin-left:10px}.c-article-associated-content__container .c-article-associated-content__collection-label{font-size:.875rem;line-height:1.4}.c-article-associated-content__container .c-article-associated-content__collection-title{line-height:1.3}.c-reading-companion{clear:both;min-height:389px}.c-reading-companion__figures-list,.c-reading-companion__references-list{list-style:none;min-height:389px;padding:0}.c-reading-companion__references-list--numeric{list-style:decimal inside}.c-reading-companion__figure-item{border-top:1px solid #d5d5d5;font-size:1rem;padding:16px 8px 16px 0}.c-reading-companion__figure-item:first-child{border-top:none;padding-top:8px}.c-reading-companion__reference-item{font-size:1rem}.c-reading-companion__reference-item:first-child{border-top:none}.c-reading-companion__reference-item a{word-break:break-word}.c-reading-companion__reference-citation{display:inline}.c-reading-companion__reference-links{font-size:.813rem;font-weight:700;list-style:none;margin:8px 0 0;padding:0;text-align:right}.c-reading-companion__reference-links>a{display:inline-block;padding-left:8px}.c-reading-companion__reference-links>a:first-child{display:inline-block;padding-left:0}.c-reading-companion__figure-title{display:block;font-size:1.25rem;font-weight:700;line-height:1.2;margin:0 0 8px}.c-reading-companion__figure-links{display:flex;justify-content:space-between;margin:8px 0 0}.c-reading-companion__figure-links>a{align-items:center;display:flex}.c-article-section__figure-caption{display:block;margin-bottom:8px;word-break:break-word}.c-article-section__figure .video,p.app-article-masthead__access--above-download{margin:0 0 16px}.c-article-section__figure-description{font-size:1rem}.c-article-section__figure-description>*{margin-bottom:0}.c-cod{display:block;font-size:1rem;width:100%}.c-cod__form{background:#ebf0f3}.c-cod__prompt{font-size:1.125rem;line-height:1.3;margin:0 0 24px}.c-cod__label{display:block;margin:0 0 4px}.c-cod__row{display:flex;margin:0 0 16px}.c-cod__row:last-child{margin:0}.c-cod__input{border:1px solid #d5d5d5;border-radius:2px;flex-shrink:0;margin:0;padding:13px}.c-cod__input--submit{background-color:#025e8d;border:1px solid #025e8d;color:#fff;flex-shrink:1;margin-left:8px;transition:background-color .2s ease-out 0s,color .2s ease-out 0s}.c-cod__input--submit-single{flex-basis:100%;flex-shrink:0;margin:0}.c-cod__input--submit:focus,.c-cod__input--submit:hover{background-color:#fff;color:#025e8d}.save-data .c-article-author-institutional-author__sub-division,.save-data .c-article-equation__number,.save-data .c-article-figure-description,.save-data .c-article-fullwidth-content,.save-data .c-article-main-column,.save-data .c-article-satellite-article-link,.save-data .c-article-satellite-subtitle,.save-data .c-article-table-container,.save-data .c-blockquote__body,.save-data .c-code-block__heading,.save-data .c-reading-companion__figure-title,.save-data .c-reading-companion__reference-citation,.save-data .c-site-messages--nature-briefing-email-variant .serif,.save-data .c-site-messages--nature-briefing-email-variant.serif,.save-data .serif,.save-data .u-serif,.save-data h1,.save-data h2,.save-data h3{font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif}.c-pdf-download__link{display:flex;flex:1 1 0%;padding:13px 24px}.c-pdf-download__link:hover{text-decoration:none}@media only screen and (min-width:768px){.c-context-bar--sticky .c-pdf-download__link{align-items:center;flex:1 1 183px}}@media only screen and (max-width:320px){.c-context-bar--sticky .c-pdf-download__link{padding:16px}}.c-article-body .c-article-recommendations-list,.c-book-body .c-article-recommendations-list{display:flex;flex-direction:row;gap:16px 16px;margin:0;max-width:100%;padding:16px 0 0}.c-article-body .c-article-recommendations-list__item,.c-book-body .c-article-recommendations-list__item{flex:1 1 0%}@media only screen and (max-width:767px){.c-article-body .c-article-recommendations-list,.c-book-body .c-article-recommendations-list{flex-direction:column}}.c-article-body .c-article-recommendations-card__authors{display:none;font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;font-size:.875rem;line-height:1.5;margin:0 0 8px}@media only screen and (max-width:767px){.c-article-body .c-article-recommendations-card__authors{display:block;margin:0}}.c-article-body .c-article-history{margin-top:24px}.app-article-metrics-bar p{margin:0}.app-article-masthead{display:flex;flex-direction:column;gap:16px 16px;padding:16px 0 24px}.app-article-masthead__info{display:flex;flex-direction:column;flex-grow:1}.app-article-masthead__brand{border-top:1px solid hsla(0,0%,100%,.8);display:flex;flex-direction:column;flex-shrink:0;gap:8px 8px;min-height:96px;padding:16px 0 0}.app-article-masthead__brand img{border:1px solid #fff;border-radius:8px;box-shadow:0 4px 15px 0 hsla(0,0%,50%,.25);height:auto;left:0;position:absolute;width:72px}.app-article-masthead__journal-link{display:block;font-size:1.125rem;font-weight:700;margin:0 0 8px;max-width:400px;padding:0 0 0 88px;position:relative}.app-article-masthead__journal-title{-webkit-box-orient:vertical;-webkit-line-clamp:3;display:-webkit-box;overflow:hidden}.app-article-masthead__submission-link{align-items:center;display:flex;font-size:1rem;gap:4px 4px;margin:0 0 0 88px}.app-article-masthead__access{align-items:center;display:flex;flex-wrap:wrap;font-size:.875rem;font-weight:300;gap:4px 4px;margin:0}.app-article-masthead__buttons{display:flex;flex-flow:column wrap;gap:16px 16px}.app-article-masthead__access svg,.app-masthead--pastel .c-pdf-download .u-button--primary svg,.app-masthead--pastel .c-pdf-download .u-button--secondary svg,.c-context-bar--sticky .c-context-bar__container .c-pdf-download .u-button--primary svg,.c-context-bar--sticky .c-context-bar__container .c-pdf-download .u-button--secondary svg{fill:currentcolor}.app-article-masthead a{color:#fff}.app-masthead--pastel .c-pdf-download .u-button--primary,.c-context-bar--sticky .c-context-bar__container .c-pdf-download .u-button--primary{background-color:#025e8d;background-image:none;border:2px solid transparent;box-shadow:none;color:#fff;font-weight:700}.app-masthead--pastel .c-pdf-download .u-button--primary:visited,.c-context-bar--sticky .c-context-bar__container .c-pdf-download .u-button--primary:visited{color:#fff}.app-masthead--pastel .c-pdf-download .u-button--primary:hover,.c-context-bar--sticky .c-context-bar__container .c-pdf-download .u-button--primary:hover{text-decoration:none}.app-masthead--pastel .c-pdf-download .u-button--primary:focus,.c-context-bar--sticky .c-context-bar__container .c-pdf-download .u-button--primary:focus{border:4px solid #fc0;box-shadow:none;outline:0;text-decoration:none}.app-masthead--pastel .c-pdf-download .u-button--primary:focus,.app-masthead--pastel .c-pdf-download .u-button--primary:hover,.c-context-bar--sticky .c-context-bar__container .c-pdf-download .u-button--primary:focus,.c-context-bar--sticky .c-context-bar__container .c-pdf-download .u-button--primary:hover{background-color:#fff;background-image:none;color:#01324b}.app-masthead--pastel .c-pdf-download .u-button--primary:hover,.c-context-bar--sticky .c-context-bar__container .c-pdf-download .u-button--primary:hover{background:0 0;border:2px solid #025e8d;box-shadow:none;color:#025e8d}.app-masthead--pastel .c-pdf-download .u-button--secondary,.c-context-bar--sticky .c-context-bar__container .c-pdf-download .u-button--secondary{background:0 0;border:2px solid #025e8d;color:#025e8d;font-weight:700}.app-masthead--pastel .c-pdf-download .u-button--secondary:visited,.c-context-bar--sticky .c-context-bar__container .c-pdf-download .u-button--secondary:visited{color:#01324b}.app-masthead--pastel .c-pdf-download .u-button--secondary:hover,.c-context-bar--sticky .c-context-bar__container .c-pdf-download .u-button--secondary:hover{background-color:#01324b;background-color:#025e8d;border:2px solid transparent;box-shadow:none;color:#fff}.app-masthead--pastel .c-pdf-download .u-button--secondary:focus,.c-context-bar--sticky .c-context-bar__container .c-pdf-download .u-button--secondary:focus{background-color:#fff;background-image:none;border:4px solid #fc0;color:#01324b}@media only screen and (min-width:768px){.app-article-masthead{flex-direction:row;gap:64px 64px;padding:24px 0}.app-article-masthead__brand{border:0;padding:0}.app-article-masthead__brand img{height:auto;position:static;width:auto}.app-article-masthead__buttons{align-items:center;flex-direction:row;margin-top:auto}.app-article-masthead__journal-link{display:flex;flex-direction:column;gap:24px 24px;margin:0 0 8px;padding:0}.app-article-masthead__submission-link{margin:0}}@media only screen and (min-width:1024px){.app-article-masthead__brand{flex-basis:400px}}.app-article-masthead .c-article-identifiers{font-size:.875rem;font-weight:300;line-height:1;margin:0 0 8px;overflow:hidden;padding:0}.app-article-masthead .c-article-identifiers--cite-list{margin:0 0 16px}.app-article-masthead .c-article-identifiers *{color:#fff}.app-article-masthead .c-cod{display:none}.app-article-masthead .c-article-identifiers__item{border-left:1px solid #fff;border-right:0;margin:0 17px 8px -9px;padding:0 0 0 8px}.app-article-masthead .c-article-identifiers__item--cite{border-left:0}.app-article-metrics-bar{display:flex;flex-wrap:wrap;font-size:1rem;padding:16px 0 0;row-gap:24px}.app-article-metrics-bar__item{padding:0 16px 0 0}.app-article-metrics-bar__count{font-weight:700}.app-article-metrics-bar__label{font-weight:400;padding-left:4px}.app-article-metrics-bar__icon{height:auto;margin-right:4px;margin-top:-4px;width:auto}.app-article-metrics-bar__arrow-icon{margin:4px 0 0 4px}.app-article-metrics-bar a{color:#000}.app-article-metrics-bar .app-article-metrics-bar__item--metrics{padding-right:0}.app-overview-section .c-article-author-list,.app-overview-section__authors{line-height:2}.app-article-metrics-bar{margin-top:8px}.c-book-toc-pagination+.c-book-section__back-to-top{margin-top:0}.c-article-body .c-article-access-provider__text--chapter{color:#222;font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;padding:20px 0}.c-article-body .c-article-access-provider__text--chapter svg.c-status-message__icon{fill:#003f8d;vertical-align:middle}.c-article-body-section__content--separator{padding-top:40px}.c-pdf-download__link{max-height:44px}.app-article-access .u-button--primary,.app-article-access .u-button--primary:visited{color:#fff}.c-article-sidebar{display:none}@media only screen and (min-width:1024px){.c-article-sidebar{display:block}}.c-cod__form{border-radius:12px}.c-cod__label{font-size:.875rem}.c-cod .c-status-message{align-items:center;justify-content:center;margin-bottom:16px;padding-bottom:16px}@media only screen and (min-width:1024px){.c-cod .c-status-message{align-items:inherit}}.c-cod .c-status-message__icon{margin-top:4px}.c-cod .c-cod__prompt{font-size:1rem;margin-bottom:16px}.c-article-body .app-article-access,.c-book-body .app-article-access{display:block}@media only screen and (min-width:1024px){.c-article-body .app-article-access,.c-book-body .app-article-access{display:none}}.c-article-body .app-card-service{margin-bottom:32px}@media only screen and (min-width:1024px){.c-article-body .app-card-service{display:none}}.app-article-access .buybox__buy .u-button--secondary,.app-article-access .u-button--primary,.c-cod__row .u-button--primary{background-color:#025e8d;border:2px solid #025e8d;box-shadow:none;font-size:1rem;font-weight:700;gap:8px 8px;justify-content:center;line-height:1.5;padding:8px 24px}.app-article-access .buybox__buy .u-button--secondary,.app-article-access .u-button--primary:hover,.c-cod__row .u-button--primary:hover{background-color:#fff;color:#025e8d}.app-article-access .buybox__buy .u-button--secondary:hover{background-color:#025e8d;color:#fff}.buybox__buy .c-notes__text{color:#666;font-size:.875rem;padding:0 16px 8px}.c-cod__input{flex-basis:auto;width:100%}.c-article-title{font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;font-size:2.25rem;font-weight:700;line-height:1.2;margin:12px 0}.c-reading-companion__figure-item figure{margin:0}@media only screen and (min-width:768px){.c-article-title{margin:16px 0}}.app-article-access{border:1px solid #c5e0f4;border-radius:12px}.app-article-access__heading{border-bottom:1px solid #c5e0f4;font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;font-size:1.125rem;font-weight:700;margin:0;padding:16px;text-align:center}.app-article-access .buybox__info svg{vertical-align:middle}.c-article-body .app-article-access p{margin-bottom:0}.app-article-access .buybox__info{font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;font-size:1rem;margin:0}.app-article-access{margin:0 0 32px}@media only screen and (min-width:1024px){.app-article-access{margin:0 0 24px}}.c-status-message{font-size:1rem}.c-article-body{font-size:1.125rem}.c-article-body dl,.c-article-body ol,.c-article-body p,.c-article-body ul{margin-bottom:32px;margin-top:0}.c-article-access-provider__text:last-of-type,.c-article-body .c-notes__text:last-of-type{margin-bottom:0}.c-article-body ol p,.c-article-body ul p{margin-bottom:16px}.c-article-section__figure-caption{font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif}.c-reading-companion__figure-item{border-top-color:#c5e0f4}.c-reading-companion__sticky{max-width:400px}.c-article-section .c-article-section__figure-description>*{font-size:1rem;margin-bottom:16px}.c-reading-companion__reference-item{border-top:1px solid #d5d5d5;padding:16px 0}.c-reading-companion__reference-item:first-child{padding-top:0}.c-article-share-box__button,.js .c-article-authors-search__item .c-article-button{background:0 0;border:2px solid #025e8d;border-radius:32px;box-shadow:none;color:#025e8d;font-size:1rem;font-weight:700;line-height:1.5;margin:0;padding:8px 24px;transition:all .2s ease 0s}.c-article-authors-search__item .c-article-button{width:100%}.c-pdf-download .u-button{background-color:#fff;border:2px solid #fff;color:#01324b;justify-content:center}.c-context-bar__container .c-pdf-download .u-button svg,.c-pdf-download .u-button svg{fill:currentcolor}.c-pdf-download .u-button:visited{color:#01324b}.c-pdf-download .u-button:hover{border:4px solid #01324b;box-shadow:none}.c-pdf-download .u-button:focus,.c-pdf-download .u-button:hover{background-color:#01324b}.c-pdf-download .u-button:focus svg path,.c-pdf-download .u-button:hover svg path{fill:#fff}.c-context-bar__container .c-pdf-download .u-button{background-image:none;border:2px solid;color:#fff}.c-context-bar__container .c-pdf-download .u-button:visited{color:#fff}.c-context-bar__container .c-pdf-download .u-button:hover{text-decoration:none}.c-context-bar__container .c-pdf-download .u-button:focus{box-shadow:none;outline:0;text-decoration:none}.c-context-bar__container .c-pdf-download .u-button:focus,.c-context-bar__container .c-pdf-download .u-button:hover{background-color:#fff;background-image:none;color:#01324b}.c-context-bar__container .c-pdf-download .u-button:focus svg path,.c-context-bar__container .c-pdf-download .u-button:hover svg path{fill:#01324b}.c-context-bar__container .c-pdf-download .u-button,.c-pdf-download .u-button{box-shadow:none;font-size:1rem;font-weight:700;line-height:1.5;padding:8px 24px}.c-context-bar__container .c-pdf-download .u-button{background-color:#025e8d}.c-pdf-download .u-button:hover{border:2px solid #fff}.c-pdf-download .u-button:focus,.c-pdf-download .u-button:hover{background:0 0;box-shadow:none;color:#fff}.c-context-bar__container .c-pdf-download .u-button:hover{border:2px solid #025e8d;box-shadow:none;color:#025e8d}.c-context-bar__container .c-pdf-download .u-button:focus,.c-pdf-download .u-button:focus{border:2px solid #025e8d}.c-article-share-box__button:focus:focus,.c-article__pill-button:focus:focus,.c-context-bar__container .c-pdf-download .u-button:focus:focus,.c-pdf-download .u-button:focus:focus{outline:3px solid #08c;will-change:transform}.c-pdf-download__link .u-icon{padding-top:0}.c-bibliographic-information__column button{margin-bottom:16px}.c-article-body .c-article-author-affiliation__list p,.c-article-body .c-article-author-information__list p,figure{margin:0}.c-article-share-box__button{margin-right:16px}.c-status-message--boxed{border-radius:12px}.c-article-associated-content__collection-title{font-size:1rem}.app-card-service__description,.c-article-body .app-card-service__description{color:#222;margin-bottom:0;margin-top:8px}.app-article-access__subscriptions a,.app-article-access__subscriptions a:visited,.app-book-series-listing__item a,.app-book-series-listing__item a:hover,.app-book-series-listing__item a:visited,.c-article-author-list a,.c-article-author-list a:visited,.c-article-buy-box a,.c-article-buy-box a:visited,.c-article-peer-review a,.c-article-peer-review a:visited,.c-article-satellite-subtitle a,.c-article-satellite-subtitle a:visited,.c-breadcrumbs__link,.c-breadcrumbs__link:hover,.c-breadcrumbs__link:visited{color:#000}.c-article-author-list svg{height:24px;margin:0 0 0 6px;width:24px}.c-article-header{margin-bottom:32px}@media only screen and (min-width:876px){.js .c-ad--conditional{display:block}}.u-lazy-ad-wrapper{background-color:#fff;display:none;min-height:149px}@media only screen and (min-width:876px){.u-lazy-ad-wrapper{display:block}}p.c-ad__label{margin-bottom:4px}.c-ad--728x90{background-color:#fff;border-bottom:2px solid #cedbe0} } </style>
<style>@media only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark) {  .eds-c-header__brand img{height:24px;width:203px}.app-article-masthead__journal-link img{height:93px;width:72px}@media only screen and (min-width:769px){.app-article-masthead__journal-link img{height:161px;width:122px}} } </style>
<link data-inline-css-source="critical-css" data-test="critical-css-handler" href="/oscar-static/app-springerlink/css/core-darwin-3c86549cfc.css" media="all" onload="this.media='all';this.onload=null" rel="stylesheet"/>
<link data-inline-css-source="critical-css" data-test="critical-css-handler" href="/oscar-static/app-springerlink/css/enhanced-darwin-article-0fd044d255.css" media="only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark)" onload="this.media='only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark)';this.onload=null" rel="stylesheet"/>
<script async="" src="//cdn.jsdelivr.net/npm/mathjax@2.7.5/MathJax.js?config=TeX-AMS-MML_SVG.js"></script><script async="" src="https://pagead2.googlesyndication.com/tag/js/gpt.js" type="text/javascript"></script><script async="" src="https://www.googletagmanager.com/gtm.js?id=GTM-MRVXSHQ"></script><script type="text/javascript">
        config = {
            env: 'live',
            site: '10844.springer.com',
            siteWithPath: '10844.springer.com' + window.location.pathname,
            twitterHashtag: '10844',
            cmsPrefix: 'https://studio-cms.springernature.com/studio/',
            
            
            
            
            publisherBrand: 'Springer',
            mustardcut: false
        };
    </script><script onload="initGTM(window,document,'script','dataLayer','GTM-MRVXSHQ')" src="https://cmp.springer.com/production_live/en/consent-bundle-17-52.js"></script>
<script>
        window.dataLayer = [{"GA Key":"UA-26408784-1","DOI":"10.1007/s10844-024-00886-5","Page":"article","springerJournal":true,"Publishing Model":"Hybrid Access","page":{"attributes":{"environment":"live"}},"Country":"KR","japan":false,"doi":"10.1007-s10844-024-00886-5","Journal Id":10844,"Journal Title":"Journal of Intelligent Information Systems","imprint":"Springer","Keywords":"Summarization, Extractive summarization, Multi-modal summarization, Graph neural networks","kwrd":["Summarization","Extractive_summarization","Multi-modal_summarization","Graph_neural_networks"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["cobranding"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["3001948335","3000120396","3000256740","3000522537","3000171421","3000699960","3991460179","3000251005","3000175460"],"businessPartnerIDString":"3001948335|3000120396|3000256740|3000522537|3000171421|3000699960|3991460179|3000251005|3000175460"}},"Access Type":"subscription","Bpids":"3001948335, 3000120396, 3000256740, 3000522537, 3000171421, 3000699960, 3991460179, 3000251005, 3000175460","Bpnames":"South Korea Trial Consortium, Chung Ang University, 7029 SpringerLink South Korea KESLI Korean Journal Package, South Korea Trial Consortium, 5539 SpringerLink South Korea KESLI Full OJA Consortium -, 9823  SpringerLink South Korea Shinwon, South Korea Full eJournal Con - Academic 2015-2017, 8064 South Korea KESLI Korean Consortium, 5539 SpringerLink South Korea KESLI Full eJournal Consortium - Academic","BPID":["3001948335","3000120396","3000256740","3000522537","3000171421","3000699960","3991460179","3000251005","3000175460"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10844-024-00886-5","Full HTML":"Y","Subject Codes":["SCI","SCI18032","SCI15009","SCI21000","SC522000","SCI21040"],"pmc":["I","I18032","I15009","I21000","522000","I21040"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1573-7675","pissn":"0925-9902"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Information Storage and Retrieval","2":"Data Structures and Information Theory","3":"Artificial Intelligence","4":"IT in Business","5":"Natural Language Processing (NLP)"},"secondarySubjectCodes":{"1":"I18032","2":"I15009","3":"I21000","4":"522000","5":"I21040"}},"sucode":"SC6","articleType":"Research"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article"}];
    </script>
<script data-test="springer-link-article-datalayer">
        window.dataLayer = window.dataLayer || [];
        window.dataLayer.push({
            ga4MeasurementId: 'G-B3E4QL2TPR',
            ga360TrackingId: 'UA-26408784-1',
            twitterId: 'o47a7',
            baiduId: 'aef3043f025ccf2305af8a194652d70b',
            ga4ServerUrl: 'https://collect.springer.com',
            imprint: 'springerlink',
                page: {
                    attributes:{
                        featureFlags: [{ name: 'darwin-orion', active: true }, { name: 'chapter-books-recs', active: true } ],
                        darwinAvailable: true
                    }
                }
            
        });
    </script>
<script>
    (function(w, d) {
        w.config = w.config || {};
        w.config.mustardcut = false;

        
        if (w.matchMedia && w.matchMedia('only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark)').matches) {
            w.config.mustardcut = true;
            d.classList.add('js');
            d.classList.remove('grade-c');
            d.classList.remove('no-js');
        }
    })(window, document.documentElement);
</script>
<script class="js-entry">
    if (window.config.mustardcut) {
        (function(w, d) {
            
            
            
                window.Component = {};
                window.suppressShareButton = false;
                window.onArticlePage = true;
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                {'src': '/oscar-static/js/polyfill-es5-bundle-9246e1269f.js', 'async': false}
            ];

            var bodyScripts = [
                
                    {'src': '/oscar-static/js/global-article-es5-bundle-dad1690b0d.js', 'async': false, 'module': false},
                    {'src': '/oscar-static/js/global-article-es6-bundle-e7d03c4cb3.js', 'async': false, 'module': true}
                
                
                    
                
                
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i = 0; i < headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i = 0; i < bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        })(window, document);
    }
</script><script src="/oscar-static/js/polyfill-es5-bundle-9246e1269f.js"></script>
<script data-cc-script="C03" data-src="https://cdn.optimizely.com/js/27195530232.js"></script>
<script data-test="gtm-head">
    window.initGTM = function() {
        if (window.config.mustardcut) {
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                    j = d.createElement(s),
                    dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-MRVXSHQ');
        }
    }
</script>
<script>
(function (w, d, t) {
    function cc() {
        var h = w.location.hostname;
        var e = d.createElement(t),
        s = d.getElementsByTagName(t)[0];

        
        if (h.indexOf('springer.com') > -1 && h.indexOf('biomedcentral.com') === -1 && h.indexOf('springeropen.com') === -1) {
            if (h.indexOf('link-qa.springer.com') > -1 || h.indexOf('test-www.springer.com') > -1) {
                e.src = 'https://cmp.springer.com/production_live/en/consent-bundle-17-52.js';
                e.setAttribute('onload', "initGTM(window,document,'script','dataLayer','GTM-MRVXSHQ')");
            } else {
                e.src = 'https://cmp.springer.com/production_live/en/consent-bundle-17-52.js';
                e.setAttribute('onload', "initGTM(window,document,'script','dataLayer','GTM-MRVXSHQ')");
            }
        } else if (h.indexOf('biomedcentral.com') > -1) {
            if (h.indexOf('biomedcentral.com.qa') > -1) {
                e.src = 'https://cmp.biomedcentral.com/production_live/en/consent-bundle-15-36.js';
                e.setAttribute('onload', "initGTM(window,document,'script','dataLayer','GTM-MRVXSHQ')");
            } else {
                e.src = 'https://cmp.biomedcentral.com/production_live/en/consent-bundle-15-36.js';
                e.setAttribute('onload', "initGTM(window,document,'script','dataLayer','GTM-MRVXSHQ')");
            }
        } else if (h.indexOf('springeropen.com') > -1) {
            if (h.indexOf('springeropen.com.qa') > -1) {
                e.src = 'https://cmp.springernature.com/production_live/en/consent-bundle-16-34.js';
                e.setAttribute('onload', "initGTM(window,document,'script','dataLayer','GTM-MRVXSHQ')");
            } else {
                e.src = 'https://cmp.springernature.com/production_live/en/consent-bundle-16-34.js';
                e.setAttribute('onload', "initGTM(window,document,'script','dataLayer','GTM-MRVXSHQ')");
            }
        } else if (h.indexOf('springernature.com') > -1) {
            if (h.indexOf('beta-qa.springernature.com') > -1) {
                e.src = 'https://cmp.springernature.com/production_live/en/consent-bundle-49-43.js';
                e.setAttribute('onload', "initGTM(window,document,'script','dataLayer','GTM-NK22KLS')");
            } else {
                e.src = 'https://cmp.springernature.com/production_live/en/consent-bundle-49-43.js';
                e.setAttribute('onload', "initGTM(window,document,'script','dataLayer','GTM-NK22KLS')");
            }
        } else {
            e.src = '/oscar-static/js/cookie-consent-es5-bundle-cb57c2c98a.js';
            e.setAttribute('data-consent', h);
        }
        s.insertAdjacentElement('afterend', e);
    }

    cc();
})(window, document, 'script');
</script>
<link href="https://link.springer.com/article/10.1007/s10844-024-00886-5" rel="canonical"/>
<script type="application/ld+json">{"mainEntity":{"headline":"Heterogeneous graphormer for extractive multimodal summarization","description":"Multimodal summarization with multimodal output (MSMO) aims to generate summaries that incorporate both text and images. Existing methods have not effectively leveraged intermodal relationships, such as sentence-image relationships, which are crucial for generating high-quality multimodal summaries. In this paper, we propose a heterogeneous graph-based model for multimodal summarization (HGMS) designed to efficiently leverage intermodal relationships within multimodal data. The model constructs a heterogeneous graph based on the relationships between modalities, containing nodes for words, sentences and images. An enhanced Graphormer is then proposed to update node representations, aiming to more effectively model intricate relationships between multiple modalities. To the best of our knowledge, we are the first to apply Graphormer in the field of graph-based summarization. Experimental results on a large-scale benchmark dataset demonstrate that HGMS achieves state-of-the-art performance in terms of automatic metrics and human evaluations.","datePublished":"2024-09-30T00:00:00Z","dateModified":"2024-09-30T00:00:00Z","pageStart":"1","pageEnd":"19","sameAs":"https://doi.org/10.1007/s10844-024-00886-5","keywords":["Summarization","Extractive summarization","Multi-modal summarization","Graph neural networks","Information Storage and Retrieval","Data Structures and Information Theory","Artificial Intelligence","IT in Business","Natural Language Processing (NLP)"],"image":["https://media.springernature.com/lw1200/springer-static/image/art%3A10.1007%2Fs10844-024-00886-5/MediaObjects/10844_2024_886_Fig1_HTML.png","https://media.springernature.com/lw1200/springer-static/image/art%3A10.1007%2Fs10844-024-00886-5/MediaObjects/10844_2024_886_Fig2_HTML.png","https://media.springernature.com/lw1200/springer-static/image/art%3A10.1007%2Fs10844-024-00886-5/MediaObjects/10844_2024_886_Fig3_HTML.png","https://media.springernature.com/lw1200/springer-static/image/art%3A10.1007%2Fs10844-024-00886-5/MediaObjects/10844_2024_886_Fig4_HTML.png"],"isPartOf":{"name":"Journal of Intelligent Information Systems","issn":["1573-7675","0925-9902"],"@type":["Periodical"]},"publisher":{"name":"Springer US","logo":{"url":"https://www.springernature.com/app-sn/public/images/logo-springernature.png","@type":"ImageObject"},"@type":"Organization"},"author":[{"name":"Xiankai Jiang","affiliation":[{"name":"Nanjing University of Posts and Telecommunications","address":{"name":"School of Computer Science, Nanjing University of Posts and Telecommunications, Nanjing, China","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Jingqiang Chen","affiliation":[{"name":"Nanjing University of Posts and Telecommunications","address":{"name":"School of Computer Science, Nanjing University of Posts and Telecommunications, Nanjing, China","@type":"PostalAddress"},"@type":"Organization"}],"email":"cjq@njupt.edu.cn","@type":"Person"}],"isAccessibleForFree":false,"hasPart":{"isAccessibleForFree":false,"cssSelector":".main-content","@type":"WebPageElement"},"@type":"ScholarlyArticle"},"@context":"https://schema.org","@type":"WebPage"}</script>
<style type="text/css">.cc-banner{background-color:#01324b;border:none!important;bottom:0;box-sizing:border-box;color:#fff!important;left:0;line-height:1.3;margin:auto 0 0;max-width:100%;outline:0;overflow:visible;padding:0;position:fixed;right:0;width:100%;z-index:99999}.cc-banner::backdrop{background-color:#0000004d}.cc-banner *{color:inherit!important}.cc-banner:focus{box-shadow:none;outline:0}.cc-banner a{color:#fff!important;text-decoration:underline}.cc-banner a:active,.cc-banner a:focus,.cc-banner a:hover{color:inherit;text-decoration:none}.cc-banner a:focus{outline:3px solid #08c!important}.cc-banner h2,.cc-banner h3,.cc-banner h4,.cc-banner h5,.cc-banner h6{font-family:sans-serif;font-style:normal;font-weight:700;margin:0 0 .5em}.cc-banner .cc-h2,.cc-banner h2{font-size:18px}.cc-banner .cc-h3,.cc-banner h3{font-size:16px}.cc-banner .cc-h4,.cc-banner .cc-h5,.cc-banner .cc-h6,.cc-banner h4,.cc-banner h5,.cc-banner h6{font-size:14px}.cc-banner .cc-button{font-size:16px}.cc-banner__content{background-color:#01324b;display:flex;flex-direction:column;margin:0 auto;max-height:90vh;max-width:100%;padding:16px;position:relative}.cc-banner__content:focus{outline:0}@media (min-width:680px){.cc-banner__content{padding:12px}}@media (min-width:980px){.cc-banner__content{max-height:60vh;padding-bottom:20px;padding-top:20px}}@media (min-width:1320px){.cc-banner__content{padding-bottom:40px;padding-top:40px}}.cc-banner__container{display:flex;flex-direction:column;margin:auto;max-width:1320px;overflow:auto}.cc-banner__title{background:none!important;flex-shrink:0;font-size:18px!important;font-size:22px!important;font-weight:700!important;letter-spacing:normal;margin:0 0 12px!important}@media (min-width:680px){.cc-banner__title{font-size:24px!important;margin:0 0 16px!important}}@media (min-width:1320px){.cc-banner__title{font-size:26px!important;margin:0 0 24px!important}}.cc-banner__body{display:flex;flex-direction:column;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif!important;overflow-x:hidden;overflow-y:auto;padding:3px 3px 16px}@media (min-width:980px){.cc-banner__body{flex-direction:row}}.cc-banner__policy p{font-size:16px!important;margin:0;max-width:none}.cc-banner__policy p:not(:last-child){margin:0 0 16px}@media (min-width:980px){.cc-banner__policy p:not(:last-child){margin:0 0 24px}}.cc-banner__policy p a{font-size:16px!important;font-weight:700}.cc-banner__footer{box-shadow:none;flex-shrink:0;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif !important!important;margin:0;padding:12px 0 3px 3px;position:relative}@media (min-width:380px){.cc-banner__footer{align-items:stretch;display:flex;flex-wrap:wrap}}@media (min-width:680px){.cc-banner__footer{box-shadow:none;flex-wrap:nowrap;width:40%}}.cc-banner__footer .cc-banner__button{flex:1 1 auto;font-weight:700;overflow:hidden;padding:.5em 1em;width:100%}@media (min-width:680px){.cc-banner__footer .cc-banner__button{max-width:275px}}.cc-banner__footer .cc-banner__button:not(:last-child){margin-bottom:12px}@media (min-width:680px){.cc-banner__footer .cc-banner__button:not(:last-child){margin-bottom:0;margin-right:16px}}@media (min-width:980px){.cc-banner__footer .cc-banner__button:not(:last-child){margin-right:24px}}.cc-banner__button-preferences{padding-left:0;padding-right:0}@media (min-width:380px){.cc-banner__button-preferences{flex:0 0 auto;margin:auto}}@media (min-width:680px){.cc-banner__button-preferences{margin:0}}@media (min-width:380px) and (max-width:680px){.cc-banner__button-break{display:block}}@media (min-width:680px){.cc-banner--is-tcf .cc-banner__footer{width:auto}}@media (min-width:980px){.cc-banner--is-tcf .cc-banner__footer{padding-right:48px;width:66%}}.cc-banner--is-tcf .cc-banner__stacks-intro{font-size:16px!important}.cc-banner--is-tcf .cc-banner__stacks{margin:16px 0 0}@media (min-width:980px){.cc-banner--is-tcf .cc-banner__stacks{display:flex;flex:0 0 33%;flex-direction:column;margin:0 0 0 48px;overflow:auto}}.cc-banner--is-tcf .cc-banner__stacks-intro{font-size:16px;margin:0 0 12px}.cc-banner--is-tcf .cc-banner__stacks-details{padding:0 3px}.cc-banner--is-tcf .cc-banner .cc-details{border-color:#fff3!important}.cc-box{border-radius:10px;padding:12px}.cc-box--info{background-color:#eff6fb}.cc-box--light{background-color:#faf9f6}.cc-button{border:2px solid #0000;border-radius:32px!important;color:inherit;cursor:pointer;font-size:14px;font-weight:700!important;left:auto;line-height:1.2;margin:0;padding:.5em 1em;right:auto;transition:all .2s}@media (min-width:680px){.cc-button{font-size:14px;padding:.75em 1em}}.cc-button--sm{font-size:12px;padding:2px 8px!important}.cc-button--primary{background-color:#025e8d!important;border-color:#025e8d!important;color:#fff!important}.cc-button--primary:focus{outline:3px solid #08c}.cc-button--primary:focus,.cc-button--primary:hover,.cc-button--secondary{background-color:#fff!important;border-color:#025e8d!important;color:#025e8d!important}.cc-button--secondary:focus{background-color:#025e8d!important;border-color:#fff!important;color:#fff!important;outline:3px solid #08c!important}.cc-button--secondary:hover{background-color:#025e8d!important;border-color:#025e8d!important;color:#fff!important}.cc-button--secondary:active{border:2px solid #01324b!important}.cc-button--secondary.cc-button--contrast{border-color:#fff!important}.cc-button--secondary.cc-button--contrast:hover{background-color:initial!important;color:#fff!important}.cc-button--tertiary{background-color:#f0f7fc!important;background-image:none;border:2px solid #f0f7fc;color:#025e8d!important;text-decoration:underline!important;text-underline-offset:.25em}.cc-button--tertiary:focus{outline:3px solid #08c!important}.cc-button--tertiary:hover{text-decoration-thickness:.25em}.cc-button--tertiary:active{color:#013c5b!important}.cc-button--link{background-color:initial!important;border-color:#0000!important;border-radius:0!important;color:inherit!important;padding:0!important;text-decoration:underline!important}.cc-button--link:focus{outline:3px solid #08c}.cc-button--link:hover{background-color:initial!important;box-shadow:none;text-decoration:none}.cc-button--text{border-radius:0;padding:0}.cc-button--details{padding-right:24px!important;position:relative}.cc-button--details:after,.cc-button--details:before{background-color:currentColor;content:"";position:absolute;transition:transform .25s ease-out}.cc-button--details:before{height:10px;margin-top:-5px;right:12px;top:50%;width:2px}.cc-button--details:after{height:2px;margin-top:-1px;right:8px;top:50%;width:10px}.cc-button--details.cc-active:before{transform:rotate(90deg);transform-origin:center}.cc-button--details.cc-active:after{display:none}.cc-details{border-bottom:1px solid #0000001a;padding:12px 0;position:relative;width:100%}.cc-details__summary{align-items:center;display:flex;font-size:14px;font-weight:700;list-style-type:none}.cc-details__summary:focus,.cc-details__summary:focus-visible{outline:3px solid #08c!important;will-change:auto}.cc-details__title{align-items:baseline;display:flex}.cc-details__title h2,.cc-details__title h3,.cc-details__title h4,.cc-details__title h5{line-height:1.4;margin:0!important}.cc-details__title svg{flex-shrink:0;margin-right:8px;position:relative;top:-1px;transition:all .2s}.cc-details[open] .cc-details__title svg{top:2px;transform:rotate(90deg)}.cc-details__switch{margin-left:auto}.cc-details__section{padding:16px 0 0 18px}.cc-details__section p{margin:0}.cc-details__section p:not(:last-child){margin-bottom:12px}details summary::-webkit-details-marker{display:none}.cc-radio{align-items:center;display:flex;position:relative}.cc-radio *{cursor:pointer}.cc-radio__input{height:22px;left:0;position:absolute;top:0;width:22px}.cc-radio__input:focus{outline:none}.cc-radio__label{color:inherit;font-size:14px;font-weight:700;line-height:23px;margin:0;padding-left:28px}.cc-radio__label:after,.cc-radio__label:before{background-color:#fff;content:"";display:block;position:absolute;transition:transform .25s ease-out}.cc-radio__label:before{border:1px solid #777;border-radius:50%;height:22px;left:0;top:0;width:22px}.cc-radio__label:after{border:7px solid #025e8d;border-radius:50%;height:0;left:4px;opacity:0;top:4px;width:0}.cc-radio__input:focus+.cc-radio__label:before{box-shadow:0 0 0 2px #08c;outline:none}.cc-radio__label--hidden{display:none}.cc-radio__input:checked+.cc-radio__label:after{opacity:1}.cc-radio__input:disabled{cursor:default}.cc-radio__input:disabled+.cc-radio__label{cursor:default;opacity:.5}.cc-switch *{cursor:pointer}.cc-switch{align-items:center;display:flex;line-height:1}.cc-switch__label{cursor:pointer;display:inline-block;font-size:14px;font-weight:700;margin:0;-webkit-user-select:none;user-select:none}.cc-switch__input{-webkit-appearance:none;appearance:none;background-color:#fff;background-image:url("data:image/svg+xml;charset=utf-8,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='-4 -4 8 8'%3E%3Ccircle r='3' fill='rgba(0, 0, 0, 0.25)'/%3E%3C/svg%3E");background-position:0;background-repeat:no-repeat;border:1px solid #777;border-radius:16px;height:18px!important;margin:0 8px 0 0;transition:background-position .2s ease-in-out;width:32px!important}.cc-switch__input:focus{background-image:url("data:image/svg+xml;charset=utf-8,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='-4 -4 8 8'%3E%3Ccircle r='3' fill='%2380b3cc'/%3E%3C/svg%3E")}.cc-switch__input:checked{background-color:#025e8d;background-image:url("data:image/svg+xml;charset=utf-8,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='-4 -4 8 8'%3E%3Ccircle r='3' fill='%23fff'/%3E%3C/svg%3E");background-position:100%}.cc-switch__input:checked~.cc-switch__label>.cc-switch__text-off,.cc-switch__input:not(:checked)~.cc-switch__label>.cc-switch__text-on{display:none}.cc-switch__input:hover{cursor:pointer;outline:none}.cc-switch__input:focus-visible{cursor:pointer;outline:3px solid #fc0}.cc-list>ul,ul.cc-list{list-style-type:disc;margin-left:0;padding-left:0}.cc-list>ul>li,ul.cc-list>li{margin-left:1em}.cc-list>ul>li:not(:last-child),ul.cc-list>li:not(:last-child){margin-bottom:4px}.cc-list__title{margin-bottom:8px!important}.cc-list--inline>ul,ul.cc-list--inline{display:flex;flex-wrap:wrap;list-style-type:none}.cc-list--inline>ul>li,ul.cc-list--inline>li{margin:0 .5em .5em 0}.cc-overlay{background-color:#0000004d;z-index:99998}.cc-overlay,.cc-preferences{bottom:0;left:0;position:fixed;right:0;top:0}.cc-preferences{background-color:#050a14f2!important;border:0;box-sizing:border-box;color:#111;font-family:sans-serif!important;line-height:1.4;margin:auto;max-height:100vh;overflow:auto;padding:0;z-index:100000}.cc-preferences:focus{outline:none}.cc-preferences *,.cc-preferences :after,.cc-preferences :before{box-sizing:inherit!important}.cc-preferences h2,.cc-preferences h3,.cc-preferences h4,.cc-preferences h5,.cc-preferences h6{font-family:sans-serif;font-style:normal;font-weight:700;margin:0 0 .5em}.cc-preferences .cc-h2,.cc-preferences h2{font-size:18px}.cc-preferences .cc-h3,.cc-preferences h3{font-size:16px}.cc-preferences .cc-h4,.cc-preferences .cc-h5,.cc-preferences .cc-h6,.cc-preferences h4,.cc-preferences h5,.cc-preferences h6{font-size:14px}.cc-preferences a{color:#025e8d;text-decoration:underline}.cc-preferences a:hover{color:inherit;text-decoration:none}.cc-preferences h3{background:none;color:#111;text-transform:none}dialog.cc-preferences{background-color:initial}dialog.cc-preferences::backdrop{background-color:#000000e6}.cc-preferences__dialog{display:flex;flex-direction:column;margin:auto;max-height:100vh;max-width:860px;padding:12px;position:relative}.cc-preferences__dialog>:last-child{border-bottom-left-radius:10px;border-bottom-right-radius:10px}@media (min-width:980px){.cc-preferences__dialog{padding:16px}}.cc-preferences__close{background:#0000!important;border:1px solid #ececec;border-radius:50%;color:#111!important;cursor:pointer;font-family:Times New Roman,serif;font-size:40px;height:40px;left:auto;line-height:1;margin-top:-20px;padding:0!important;position:absolute;right:20px;top:50%;width:40px}.cc-preferences__close:focus{outline:3px solid #08c}.cc-preferences__close-label{border:0;clip:rect(0,0,0,0);height:1px;margin:-100%;overflow:hidden;padding:0;position:absolute!important;width:1px}.cc-preferences__header{background:#fff!important;border-bottom:1px solid #0000001a;border-top-left-radius:10px;border-top-right-radius:10px;padding:16px;position:relative;text-align:center}.cc-preferences__title{background:none!important;color:#111!important;font-family:sans-serif!important;font-size:18px!important;font-weight:700!important;margin:0!important;padding-right:16px!important}@media (min-width:480px){.cc-preferences__title{padding-right:0!important}}@media (min-width:980px){.cc-preferences__title{font-size:22px!important}}.cc-preferences__body{background:#fff!important;flex:1 1 auto;min-height:200px;overflow-x:hidden;overflow-y:auto;padding:16px}.cc-preferences__footer{background:#fff!important;border-top:1px solid #d0d0d0;box-shadow:0 0 5px 0 #0003;margin-bottom:0;padding:12px;position:relative}@media (min-width:480px){.cc-preferences__footer{align-items:stretch;display:flex}}.cc-preferences__footer>.cc-button{display:block;width:100%}@media (min-width:480px){.cc-preferences__footer>.cc-button{flex:1 1 auto}}@media (min-width:980px){.cc-preferences__footer>.cc-button{flex-basis:auto}}@media (min-width:480px){.cc-preferences__footer>.cc-button:not(:first-child){margin-left:12px}}.cc-preferences__footer>.cc-button:not(:last-child){margin-bottom:8px}@media (min-width:480px){.cc-preferences__footer>.cc-button:not(:last-child){margin-bottom:0}}.cc-preferences__categories{list-style:none;margin:0;padding:0}.cc-preferences__category:not(:last-child){border-bottom:1px solid #0000001a;margin-bottom:12px;padding-bottom:12px}.cc-preferences__category-description{font-size:14px;margin:0 0 8px}.cc-preferences__category-footer{align-items:center;display:flex;justify-content:space-between}.cc-preferences__status{border:0;clip:rect(0,0,0,0);height:1px;margin:-100%;overflow:hidden;padding:0;position:absolute!important;width:1px}.cc-preferences__controls{display:flex;margin:0}.cc-preferences__controls>:not(:last-child){margin-right:12px}.cc-preferences__always-on{font-size:14px;font-weight:700;padding-left:26px;position:relative}.cc-preferences__always-on:before{background:#0000!important;border:solid;border-top-color:#0000;border-width:0 0 4px 4px;content:"";display:block;height:10px;left:0;position:absolute;top:2px;transform:rotate(-45deg);width:18px}.cc-preferences__details{background-color:#eff6fb!important;border-radius:10px;font-size:14px;margin:12px 0 0;padding:12px}.cc-preferences__cookie-list,.cc-preferences__provider-list{list-style:none;margin:0;padding:0}.cc-preferences__provider-list{columns:170px}.cc-preferences__cookie-title{font-size:1em;margin:0}.cc-preferences__cookie-description{font-size:1em;margin:0 0 8px}.cc-preferences__cookie-domain,.cc-preferences__cookie-lifespan{border-left:1px solid #999;color:#666;margin-left:8px;padding-left:4px}body.cc-has-preferences-open{overflow:hidden;position:relative}.cc-table{border-collapse:collapse;width:100%}.cc-table tbody tr{border-top:1px solid #0000001a}.cc-table td,.cc-table th{font-size:14px;padding:4px 8px;vertical-align:top}.cc-table+.cc-table{margin-top:8px}.cc-table--purposes td:first-child,.cc-table--purposes th:first-child{width:70%}@media (min-width:680px){.cc-table--purposes td:first-child,.cc-table--purposes th:first-child{width:80%}}.cc-table--purposes td:last-child,.cc-table--purposes th:last-child{padding-left:8px;width:40%}@media (min-width:680px){.cc-table--purposes td:last-child,.cc-table--purposes th:last-child{width:30%}}.cc-tabs,.cc-tabs>ul{background-color:#faf9f6;display:flex;list-style:none;margin:0;padding:0}.cc-tabs__panel{border-top:1px solid #ececec;margin-top:-1px}.cc-tabs__button{background-color:initial;border:0;border-left:1px solid #0000;border-right:1px solid #0000;font-size:16px;font-weight:700;padding:15px 16px 12px;position:relative}.cc-tabs__button:before{background-color:initial;content:"";display:block;height:3px;left:0;position:absolute;top:0;width:100%}.cc-tabs__button.cc-active{background-color:#fff;border-color:#025e8d #ececec #ececec}.cc-tabs__button.cc-active:before{background-color:#025e8d}.cc-tabs__content{background-color:#fff;display:none;font-size:14px;padding:16px}.cc-tabs__section:not(:first-child){padding-top:24px}.cc-tcf{font-size:14px;margin-top:16px}.cc-tcf__list{list-style:none;margin:0;padding:0}.cc-vendor-count{color:#666;font-size:16px;margin:0 0 8px}.cc-hide{display:none!important}.cc-show{display:block!important}.cc-external-link{background-color:#ececec;background-image:url("data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyMCIgaGVpZ2h0PSIyMCI+PHBhdGggZD0iTTcgMGExIDEgMCAxIDEgMCAySDIuNmMtLjM3MSAwLS42LjIwOS0uNi41djE1YzAgLjI5MS4yMjkuNS42LjVoMTQuOGMuMzcxIDAgLjYtLjIwOS42LS41VjEzYTEgMSAwIDAgMSAyIDB2NC41YzAgMS40MzgtMS4xNjIgMi41LTIuNiAyLjVIMi42QzEuMTYyIDIwIDAgMTguOTM4IDAgMTcuNXYtMTVDMCAxLjA2MiAxLjE2MiAwIDIuNiAwem02IDBoNmwuMDc1LjAwMy4xMjYuMDE3LjExMS4wMy4xMTEuMDQ0LjA5OC4wNTIuMDk2LjA2Ny4wOS4wOGExIDEgMCAwIDEgLjA5Ny4xMTJsLjA3MS4xMS4wNTQuMTE0LjAzNS4xMDUuMDMuMTQ4TDIwIDF2NmExIDEgMCAwIDEtMiAwVjMuNDE0bC02LjY5MyA2LjY5M2ExIDEgMCAwIDEtMS40MTQtMS40MTRMMTYuNTg0IDJIMTNhMSAxIDAgMCAxLS45OTMtLjg4M0wxMiAxYTEgMSAwIDAgMSAxLTEiLz48L3N2Zz4=");background-position:right 8px center;background-repeat:no-repeat;background-size:10px auto;border-radius:3px;box-shadow:0 1px 0 #0003;color:#111!important;display:inline-block;font-size:12px;padding:4px 26px 4px 8px;text-decoration:none!important}.cc-external-link:hover{text-decoration:underline!important}.cc-m-0{margin:0!important}.cc-grey{color:#666}</style><style type="text/css">.MathJax_Hover_Frame {border-radius: .25em; -webkit-border-radius: .25em; -moz-border-radius: .25em; -khtml-border-radius: .25em; box-shadow: 0px 0px 15px #83A; -webkit-box-shadow: 0px 0px 15px #83A; -moz-box-shadow: 0px 0px 15px #83A; -khtml-box-shadow: 0px 0px 15px #83A; border: 1px solid #A6D ! important; display: inline-block; position: absolute}
.MathJax_Menu_Button .MathJax_Hover_Arrow {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 4px; -webkit-border-radius: 4px; -moz-border-radius: 4px; -khtml-border-radius: 4px; font-family: 'Courier New',Courier; font-size: 9px; color: #F0F0F0}
.MathJax_Menu_Button .MathJax_Hover_Arrow span {display: block; background-color: #AAA; border: 1px solid; border-radius: 3px; line-height: 0; padding: 4px}
.MathJax_Hover_Arrow:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_Hover_Arrow:hover span {background-color: #CCC!important}
</style><style type="text/css">#MathJax_About {position: fixed; left: 50%; width: auto; text-align: center; border: 3px outset; padding: 1em 2em; background-color: #DDDDDD; color: black; cursor: default; font-family: message-box; font-size: 120%; font-style: normal; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 15px; -webkit-border-radius: 15px; -moz-border-radius: 15px; -khtml-border-radius: 15px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_About.MathJax_MousePost {outline: none}
.MathJax_Menu {position: absolute; background-color: white; color: black; width: auto; padding: 5px 0px; border: 1px solid #CCCCCC; margin: 0; cursor: default; font: menu; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 5px; -webkit-border-radius: 5px; -moz-border-radius: 5px; -khtml-border-radius: 5px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
.MathJax_MenuItem {padding: 1px 2em; background: transparent}
.MathJax_MenuArrow {position: absolute; right: .5em; padding-top: .25em; color: #666666; font-size: .75em}
.MathJax_MenuActive .MathJax_MenuArrow {color: white}
.MathJax_MenuArrow.RTL {left: .5em; right: auto}
.MathJax_MenuCheck {position: absolute; left: .7em}
.MathJax_MenuCheck.RTL {right: .7em; left: auto}
.MathJax_MenuRadioCheck {position: absolute; left: .7em}
.MathJax_MenuRadioCheck.RTL {right: .7em; left: auto}
.MathJax_MenuLabel {padding: 1px 2em 3px 1.33em; font-style: italic}
.MathJax_MenuRule {border-top: 1px solid #DDDDDD; margin: 4px 3px}
.MathJax_MenuDisabled {color: GrayText}
.MathJax_MenuActive {background-color: #606872; color: white}
.MathJax_MenuDisabled:focus, .MathJax_MenuLabel:focus {background-color: #E8E8E8}
.MathJax_ContextMenu:focus {outline: none}
.MathJax_ContextMenu .MathJax_MenuItem:focus {outline: none}
#MathJax_AboutClose {top: .2em; right: .2em}
.MathJax_Menu .MathJax_MenuClose {top: -10px; left: -10px}
.MathJax_MenuClose {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; font-family: 'Courier New',Courier; font-size: 24px; color: #F0F0F0}
.MathJax_MenuClose span {display: block; background-color: #AAA; border: 1.5px solid; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; line-height: 0; padding: 8px 0 6px}
.MathJax_MenuClose:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_MenuClose:hover span {background-color: #CCC!important}
.MathJax_MenuClose:hover:focus {outline: none}
</style><style type="text/css">.MJX_Assistive_MathML {position: absolute!important; top: 0; left: 0; clip: rect(1px, 1px, 1px, 1px); padding: 1px 0 0 0!important; border: 0!important; height: 1px!important; width: 1px!important; overflow: hidden!important; display: block!important; -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none}
.MJX_Assistive_MathML.MJX_Assistive_MathML_Block {width: 100%!important}
</style><style type="text/css">#MathJax_Zoom {position: absolute; background-color: #F0F0F0; overflow: auto; display: block; z-index: 301; padding: .5em; border: 1px solid black; margin: 0; font-weight: normal; font-style: normal; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; -webkit-box-sizing: content-box; -moz-box-sizing: content-box; box-sizing: content-box; box-shadow: 5px 5px 15px #AAAAAA; -webkit-box-shadow: 5px 5px 15px #AAAAAA; -moz-box-shadow: 5px 5px 15px #AAAAAA; -khtml-box-shadow: 5px 5px 15px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_ZoomOverlay {position: absolute; left: 0; top: 0; z-index: 300; display: inline-block; width: 100%; height: 100%; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
#MathJax_ZoomFrame {position: relative; display: inline-block; height: 0; width: 0}
#MathJax_ZoomEventTrap {position: absolute; left: 0; top: 0; z-index: 302; display: inline-block; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
</style><style type="text/css">.MathJax_Preview {color: #888}
#MathJax_Message {position: fixed; left: 1em; bottom: 1.5em; background-color: #E6E6E6; border: 1px solid #959595; margin: 0px; padding: 2px 8px; z-index: 102; color: black; font-size: 80%; width: auto; white-space: nowrap}
#MathJax_MSIE_Frame {position: absolute; top: 0; left: 0; width: 0px; z-index: 101; border: 0px; margin: 0px; padding: 0px}
.MathJax_Error {color: #CC0000; font-style: italic}
</style><style type="text/css">.MJXp-script {font-size: .8em}
.MJXp-right {-webkit-transform-origin: right; -moz-transform-origin: right; -ms-transform-origin: right; -o-transform-origin: right; transform-origin: right}
.MJXp-bold {font-weight: bold}
.MJXp-italic {font-style: italic}
.MJXp-scr {font-family: MathJax_Script,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-frak {font-family: MathJax_Fraktur,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-sf {font-family: MathJax_SansSerif,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-cal {font-family: MathJax_Caligraphic,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-mono {font-family: MathJax_Typewriter,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-largeop {font-size: 150%}
.MJXp-largeop.MJXp-int {vertical-align: -.2em}
.MJXp-math {display: inline-block; line-height: 1.2; text-indent: 0; font-family: 'Times New Roman',Times,STIXGeneral,serif; white-space: nowrap; border-collapse: collapse}
.MJXp-display {display: block; text-align: center; margin: 1em 0}
.MJXp-math span {display: inline-block}
.MJXp-box {display: block!important; text-align: center}
.MJXp-box:after {content: " "}
.MJXp-rule {display: block!important; margin-top: .1em}
.MJXp-char {display: block!important}
.MJXp-mo {margin: 0 .15em}
.MJXp-mfrac {margin: 0 .125em; vertical-align: .25em}
.MJXp-denom {display: inline-table!important; width: 100%}
.MJXp-denom > * {display: table-row!important}
.MJXp-surd {vertical-align: top}
.MJXp-surd > * {display: block!important}
.MJXp-script-box > *  {display: table!important; height: 50%}
.MJXp-script-box > * > * {display: table-cell!important; vertical-align: top}
.MJXp-script-box > *:last-child > * {vertical-align: bottom}
.MJXp-script-box > * > * > * {display: block!important}
.MJXp-mphantom {visibility: hidden}
.MJXp-munderover, .MJXp-munder {display: inline-table!important}
.MJXp-over {display: inline-block!important; text-align: center}
.MJXp-over > * {display: block!important}
.MJXp-munderover > *, .MJXp-munder > * {display: table-row!important}
.MJXp-mtable {vertical-align: .25em; margin: 0 .125em}
.MJXp-mtable > * {display: inline-table!important; vertical-align: middle}
.MJXp-mtr {display: table-row!important}
.MJXp-mtd {display: table-cell!important; text-align: center; padding: .5em 0 0 .5em}
.MJXp-mtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-mlabeledtr {display: table-row!important}
.MJXp-mlabeledtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mlabeledtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MJXp-scale0 {-webkit-transform: scaleX(.0); -moz-transform: scaleX(.0); -ms-transform: scaleX(.0); -o-transform: scaleX(.0); transform: scaleX(.0)}
.MJXp-scale1 {-webkit-transform: scaleX(.1); -moz-transform: scaleX(.1); -ms-transform: scaleX(.1); -o-transform: scaleX(.1); transform: scaleX(.1)}
.MJXp-scale2 {-webkit-transform: scaleX(.2); -moz-transform: scaleX(.2); -ms-transform: scaleX(.2); -o-transform: scaleX(.2); transform: scaleX(.2)}
.MJXp-scale3 {-webkit-transform: scaleX(.3); -moz-transform: scaleX(.3); -ms-transform: scaleX(.3); -o-transform: scaleX(.3); transform: scaleX(.3)}
.MJXp-scale4 {-webkit-transform: scaleX(.4); -moz-transform: scaleX(.4); -ms-transform: scaleX(.4); -o-transform: scaleX(.4); transform: scaleX(.4)}
.MJXp-scale5 {-webkit-transform: scaleX(.5); -moz-transform: scaleX(.5); -ms-transform: scaleX(.5); -o-transform: scaleX(.5); transform: scaleX(.5)}
.MJXp-scale6 {-webkit-transform: scaleX(.6); -moz-transform: scaleX(.6); -ms-transform: scaleX(.6); -o-transform: scaleX(.6); transform: scaleX(.6)}
.MJXp-scale7 {-webkit-transform: scaleX(.7); -moz-transform: scaleX(.7); -ms-transform: scaleX(.7); -o-transform: scaleX(.7); transform: scaleX(.7)}
.MJXp-scale8 {-webkit-transform: scaleX(.8); -moz-transform: scaleX(.8); -ms-transform: scaleX(.8); -o-transform: scaleX(.8); transform: scaleX(.8)}
.MJXp-scale9 {-webkit-transform: scaleX(.9); -moz-transform: scaleX(.9); -ms-transform: scaleX(.9); -o-transform: scaleX(.9); transform: scaleX(.9)}
.MathJax_PHTML .noError {vertical-align: ; font-size: 90%; text-align: left; color: black; padding: 1px 3px; border: 1px solid}
</style><meta content="AlK2UR5SkAlj8jjdEc9p3F3xuFYlF6LYjAML3EOqw1g26eCwWPjdmecULvBH5MVPoqKYrOfPhYVL71xAXI1IBQoAAAB8eyJvcmlnaW4iOiJodHRwczovL2RvdWJsZWNsaWNrLm5ldDo0NDMiLCJmZWF0dXJlIjoiV2ViVmlld1hSZXF1ZXN0ZWRXaXRoRGVwcmVjYXRpb24iLCJleHBpcnkiOjE3NTgwNjcxOTksImlzU3ViZG9tYWluIjp0cnVlfQ==" http-equiv="origin-trial"/><meta content="Amm8/NmvvQfhwCib6I7ZsmUxiSCfOxWxHayJwyU1r3gRIItzr7bNQid6O8ZYaE1GSQTa69WwhPC9flq/oYkRBwsAAACCeyJvcmlnaW4iOiJodHRwczovL2dvb2dsZXN5bmRpY2F0aW9uLmNvbTo0NDMiLCJmZWF0dXJlIjoiV2ViVmlld1hSZXF1ZXN0ZWRXaXRoRGVwcmVjYXRpb24iLCJleHBpcnkiOjE3NTgwNjcxOTksImlzU3ViZG9tYWluIjp0cnVlfQ==" http-equiv="origin-trial"/><meta content="A9wSqI5i0iwGdf6L1CERNdmsTPgVu44ewj8QxTBYgsv1LCPUVF7YmWOvTappqB1139jAymxUW/RO8zmMqo4zlAAAAACNeyJvcmlnaW4iOiJodHRwczovL2RvdWJsZWNsaWNrLm5ldDo0NDMiLCJmZWF0dXJlIjoiRmxlZGdlQmlkZGluZ0FuZEF1Y3Rpb25TZXJ2ZXIiLCJleHBpcnkiOjE3MzY4MTI4MDAsImlzU3ViZG9tYWluIjp0cnVlLCJpc1RoaXJkUGFydHkiOnRydWV9" http-equiv="origin-trial"/><meta content="A+d7vJfYtay4OUbdtRPZA3y7bKQLsxaMEPmxgfhBGqKXNrdkCQeJlUwqa6EBbSfjwFtJWTrWIioXeMW+y8bWAgQAAACTeyJvcmlnaW4iOiJodHRwczovL2dvb2dsZXN5bmRpY2F0aW9uLmNvbTo0NDMiLCJmZWF0dXJlIjoiRmxlZGdlQmlkZGluZ0FuZEF1Y3Rpb25TZXJ2ZXIiLCJleHBpcnkiOjE3MzY4MTI4MDAsImlzU3ViZG9tYWluIjp0cnVlLCJpc1RoaXJkUGFydHkiOnRydWV9" http-equiv="origin-trial"/><script async="" src="https://pagead2.googlesyndication.com/pagead/managed/js/gpt/m202410080101/pubads_impl.js"></script><script async="" src="https://cdn.pbgrd.com/core-spl.js"></script></head>
<body class="">Your privacy, your choiceWe use essential cookies to make sure the site can function. We also use optional cookies for advertising, personalisation of content, usage analysis, and social media.By accepting optional cookies, you consent to the processing of your personal data - including transfers to third parties. Some third parties are outside of the European Economic Area, with varying standards of data protection.See ourprivacy policyfor more information on the use of your personal data.privacy policyManage preferencesfor further information and to change your choices.Accept all cookiesSkip to main contentLog inMenuFind a journalPublish with usTrack your researchSearchCartSearchSearch by keyword or authorSearchNavigationFind a journalPublish with usTrack your researchHomeJournal of Intelligent Information SystemsArticleHeterogeneous graphormer for extractive multimodal summarizationResearchPublished:30 September 2024(2024)Cite this articleDownload PDFAccess provided by Chung Ang UniversityJournal of Intelligent Information SystemsAims and scopeSubmit manuscriptHeterogeneous graphormer for extractive multimodal summarizationDownload PDFXiankai Jiang1na1&amp;Jingqiang Chen165AccessesExplore all metricsAbstractMultimodal summarization with multimodal output (MSMO) aims to generate summaries that incorporate both text and images. Existing methods have not effectively leveraged intermodal relationships, such as sentence-image relationships, which are crucial for generating high-quality multimodal summaries. In this paper, we propose a heterogeneous graph-based model for multimodal summarization (HGMS) designed to efficiently leverage intermodal relationships within multimodal data. The model constructs a heterogeneous graph based on the relationships between modalities, containing nodes for words, sentences and images. An enhanced Graphormer is then proposed to update node representations, aiming to more effectively model intricate relationships between multiple modalities. To the best of our knowledge, we are the first to apply Graphormer in the field of graph-based summarization. Experimental results on a large-scale benchmark dataset demonstrate that HGMS achieves state-of-the-art performance in terms of automatic metrics and human evaluations.Similar content being viewed by othersExtractive text-image summarization with relation-enhanced graph attention networkArticle28 October 2022MCLS: A Large-Scale Multimodal Cross-Lingual Summarization DatasetChapter© 2023Topic-guided abstractive multimodal summarization with multimodal outputArticle24 August 2023Explore related subjectsDiscover the latest articles, news and stories from top researchers in related subjects.Artificial IntelligenceUse our pre-submission checklistAvoid common mistakes on your manuscript.1IntroductionWith the rapid development of multimedia data on the Internet, multimodal summarization has attracted widespread attention from researchers. Recently proposed Multimodal Summarization with Multimodal Output (Zhu et al.,2018) (MSMO) that condenses long multimodal news to a short pictorial version, as shown in Fig.1. This innovative approach has been substantiated to significantly enhance users’ ability to swiftly grasp key news points, thereby elevating user satisfaction (Zhu et al.,2018).While several methods have been proposed to tackle the MSMO task (Zhu et al.,2018,2020; Jiang et al.,2023), effectively handling the relationship between the image and text modalities remains a challenging problem. Zhang et al. (2022c) extends the text encoder to a multimodal encoder, which takes the concatenation of textual and visual embeddings as input to obtain their contextualized joint representations. However, experimental results indicate that merely concatenating textual and visual embeddings cannot well capture the intricate relationships between modalities, such as sentence-image relationships. Jiang et al. (2023) takes sentences related to images as additional input rather than relying directly on image information. Although a image-text alignment mechanism was introduced in the sentence selection stage to leverage the relationships between images and sentences, this valuable relationship was not considered in the summarization stage.Fig. 1An example of multimodal summarization with multimodal output taskFull size imageWe believe that there are extensive many-to-many relationships between images and sentences. By effectively leveraging both the relevance and irrelevance between sentences and images, we can more accurately extract salient sentences and critical images. Taking Fig.1as an example, the first sentence succinctly summarizes the main points of the news article: “Eating fruits and vegetables could cut the risk of heart attacks and strokes.” This guides our selection of the vegetable image which is relevant to it. Compared to the third sentence, which is only related to the BMI image, the second sentence mentions both vegetables and BMI. In this case, further selecting the second sentence, which covers different aspects of the topic, can provide a more comprehensive summary.To effectively leverage the many-to-many relationships between sentences and images for multimodal summarization, an intuitive approach is to employ graph structures for modeling these relationships. In recent years, numerous studies (Jia et al.,2020; Song &amp; King,2022) have been dedicated to exploring the application of Graph Neural Networks (GNNs) in the realm of text summarization with impressive results. Specifically, GNNs are able to model complex relationships between semantic units. By building graphs on semantic units, such as sentences, words (Wang et al.,2020), latent topics (Cui et al.,2020), or passages (Phan et al.,2022), GNNs can enhance representations of semantic units for text summarization. Different from these works, we propose a novel heterogeneous graph for multimodal summarization. This graph includes nodes representing words, sentences, and images, with edges connecting sentences to both images and words. In this graph, images and sentences can serve as intermediaries for each other, thus enhancing their representations for multimodal summarization.To compute node representations of this graph, we propose the heterogeneous Graphormer (HeterGraphormer for short) by enhancing Graphormer (Ying et al.,2021) to effectively model intricate relationships between multiple modalities. Graphormer leverages self-attention to enable attention to all nodes when updating nodes, thereby alleviating the over-smoothing issue caused by traditional GNNs. We enhance Graphormer in the follow three aspect. First, we introduce type embedding and apply distinct spatial and edge embeddings for different heterogeneous edges to more effectively handle the heterogeneity of nodes and edges. Second, the centrality embedding was removed to optimize its performance for document graphs. Third, unconnected nodes are considered during node updates, as unrelated relationships are also valuable.Overall, we propose a heterogeneous graph-based model for multimodal summarization (HGMS). The model first constructs a heterogeneous graph containing nodes for words, sentences and images. Subsequently, HeterGraphormer is employed to iteratively update the representations of nodes in the heterogeneous graph, aiming to more effectively model intricate relationships between nodes. Experimental results show that our model significantly enhances the performance of multimodal summarization. The contributions of our paper are as follows:1.We propose a heterogeneous graph-based model for multimodal summarization to efficiently leverage intermodal relationships through the multimodal graph we constructed.2.We propose HeterGraphormer to effectively model intricate relationships between multiple modalities. To the best of our knowledge, we are the first to apply Graphormer in the field of graph-based summarization.3.Our proposed model has achieved state-of-the-art (SOTA) performance on the MSMO dataset. Extensive experiments demonstrate the effectiveness of our proposed HeterGraphormer.2Related work2.1Extractive summarizationText summarization is the task of generating concise summaries for original long documents. Existing text summarization methods can be categorized into extractive and abstractive approaches. Abstractive summarization (Sutskever et al.,2014; See et al.,2017; Paulus et al.,2018; Zhang et al.,2020; Liu et al.,2022) generate a summary from scratch, typically with lower redundancy. Extractive summarization (Cheng &amp; Lapata,2016; An et al.,2022; Cheng et al.,2023) aims to select salient sentences from original documents.Recent research work on extractive summarization covers a variety of methods. SummaRuNNer (Nallapati et al.,2017) is one of the earliest models to employ an encoder based on Recurrent Neural Networks (RNNs). Narayan et al. (2018) proposed a reinforcement learning-based system trained by globally optimizing the ROUGE metric. Liu and Lapata (2019) leverages the capability of pre-trained language models to effectively capture contextual features, thereby significantly enhancing the performance of summarization. Zhong et al. (2020) formulates the extractive summarization task as a semantic text matching problem, selecting the set of sentences that most closely match the source document in the semantic space as the summary. Tang et al. (2022) formulates the extractive summarization task as an Optimal Transport (OT) problem from document to summary.2.2Multimodal summarizationMultimodal summarization takes multiple modalities of data as input, and outputs either text-only (Li et al.,2018; Overbay et al.,2023; Liang et al.,2023) or multimodal (Li et al.,2020; Zhang et al.,2022a) summary.For multimodal summarization with text-only output, Chen and Zhuge (2018) proposed an abstractive text-image summarization method using a multi-modal attentional hierarchical RNN, which integrates textual and visual information to produce a textual summary. Zhang et al. (2022b) introduces an aspect precision score function to assess summary relevance and incorporates it using reinforcement learning. Xiao et al. (2023) proposed a coarse-to-fine contribution network to model different contributions of images for summarization. Jin and Chen (2024) proposes a multimodal knowledge graph constructed from entities in text and images, using structural knowledge in a self-supervised manner.For multimodal summarization with multimodal output, the task generates summaries that include multiple modalities, such as text and images, to provide richer and more comprehensive information. Zhu et al. (2018) first proposed generating a pictorial summary given a document and a collection of images. Zhu et al. (2020) further introduced a extra cross-entropy loss for image selection. Zhang et al. (2022c) utilizes knowledge distillation with a vision-language pre-trained model to help image selection. Xie et al. (2023) introduces a graph structure, leveraging image captions to establish indirect connections between images and sentences via word nodes, thereby enhancing the association between images and text. Jiang et al. (2023) introduced an image-text alignment mechanism to convert input images into the most relevant sentences to better capture visual knowledge. However, previous studies have not adequately considered the many-to-many relationships between sentences and images. In this study, we model these relationships using a graph structure to more effectively utilize this crucial information.2.3Graph-based summarizationEarly work focused on utilizing content similarity between sentences to construct document graphs and extracting sentences in an unsupervised manner such as TextRank (Mihalcea &amp; Tarau,2004) and LexRank (Erkan &amp; Radev,2004). Recently, the application of GNNs in text summarization task has attracted widespread attention. Wang et al. (2020) constructed a heterogeneous graph, enriching the cross-sentence relations through the word nodes between sentences. Jia et al. (2020) proposed a hierarchical heterogeneous graph to extract sentences by simultaneously balancing salience and redundancy. Cui et al. (2020) incorporates latent topics into graph propagation via a joint neural topic model, facilitating the extraction of crucial information from documents. Jing et al. (2021) proposed to use multiplex graph to model different types of relationships among sentences and words. Song and King (2022) obtains sentence representations based on constituency trees to leverage syntactic information. Phan et al. (2022) introduce passage nodes into the sentence-word graph to enrich the representation of sentences in long documents. In this study, we construct a novel heterogeneous graph for multimodal summarization based on the interrelations between different modalities. Additionally, we propose HeterGraphormer to more effectively model the intricate relationships within the graph.Fig. 2Overview of our proposed HGMSFull size image3Method3.1Problem formulationGiven a multimodal document\(\{T, P\}\), where\(T=\{s_1,s_2,\cdots ,s_m\}\)is a text document consisting ofmsentences and\(P=\{p_1,p_2,\cdots ,p_k\}\)is a collection ofkimages, the model summarizes multimodal document into a multimodal summary\(\mathcal {S}=\{\mathcal {S}_t,\mathcal {S}_p\}\), where\(\mathcal {S}_t\)denotes the textual summary extracted fromTand\(\mathcal {S}_p\)is a subset of images selected from the image collectionP.3.2Graph constructionIn order to model the relationship between sentences and images, we construct a heterogeneous graph, as shown in the Fig.2. In this graph, there are three types of nodes: word, sentence, and image. Each word node corresponds to the identical word appearing multiple times in the document. Each sentence and image node corresponds to the sentences and images in the document.For the connection between nodes, we first connect words and sentences through inclusion relationships. To enhance model efficiency, we retained only those word nodes connected to two or more sentences. Furthermore, we connect all sentence and image pairs, rather than limiting connections to relevant pairs. In order to accurately discern the significance of connections, we introduce TF-IDF and Image-Text Match (ITM) scores as the weights of these two edges respectively.Given a undirected graph\(G = \{V, E\}\),whereVdenotes the set of nodes andErepresents edge weights between nodes. Our heterogeneous graph can be formally defined as\( V=V_w \cup V_s \cup V_p \)and\(E=E_{sw} \cup E_{sp} \), where\(V_w\),\(V_s\), and\(V_p\)denote the sets of word, sentence, and image nodes, respectively.\(E_{sw}=\{e^{sw}_{11}, \cdots , e^{sw}_{mu}\}\)and\(E_{sp}=\{e^{sp}_{11}, \cdots , e^{sp}_{mk}\}\)represent sentence-word and sentence-image edge weights respectively. Here\(e^{sw}_{ij} \in [0, 1]\)denotes the TF-IDF score between sentence\(s_i\)and word\(w_j\), while\(e^{sp}_{ij} \in [0, 1]\)represents the ITM score between sentence\(s_i\)and image\(p_j\), computed via the BLIP (Li et al.,2022).For node features, we use GloVe (Pennington et al.,2014), RoBERTa (Liu et al.,2019), and BLIP to obtain node representations of words, sentences, and images, respectively.$$\begin{aligned} x^w_i&amp;= \text {GloVe}(w_i),\end{aligned}$$(1)$$\begin{aligned} x^s_i&amp;= \text {RoBERTa}(s_i)W_s + \text {PE}(i),\end{aligned}$$(2)$$\begin{aligned} x^p_i&amp;= \text {BLIP}(p_i)W_p, \end{aligned}$$(3)where\( x^w_i \in \mathbb {R}^d \),\( W_s \in \mathbb {R}^{d_s \times d} \),\( W_p \in \mathbb {R}^{d_p \times d} \)are projection matrices and\(\text {PE}(i)\)denotes the Position Encoding proposed by Vaswani et al. (2017).3.3Graph encoderGraphormer integrates graph structural information into the Transformer (Vaswani et al.,2017) architecture using centrality embedding, spatial embedding, and edge embedding to effectively learn node representations. To better apply it to our constructed heterogeneous graph, we propose HeterGraphormer, which introduces several enhancements designed to better model the relationships between heterogeneous nodes.Before input, we add type embedding to each node to distinguish different node types. Lethdenote the node’s hidden state, and the initial hidden state can be defined as$$\begin{aligned} h^{\tau }_i = x^{\tau }_i + z_{\tau }, \end{aligned}$$(4)where\(z \in \mathbb {R}^d\)are learnable embedding vectors specified by the node type\(\tau \in \{w,s,i\}\).It is noteworthy that we did not use centrality embedding, which adds embeddings to node representations based on the degrees of nodes to differentiate their importance, as the degree of nodes in document graphs does not accurately reflect their significance. We prove this assumption via the ablation study in the experiment section.3.3.1Attention with spatial embeddingDue to the global receptive field characteristic of the Transformer, it can effectively attend to information from any position when updating node representations. To differentiate nodes at different distances, Graphormer introduces spatial embedding, which assigns a unique embedding for each distance as a weight in the self-attention module. This enables the model to accurately distinguish distance variances between nodes in the graph while maintaining its capacity for global information processing. Following  Ying et al. (2021), we adopt the shortest path distance as the distance metric between nodes, as it is the most direct and widely used approach.Concretely, let\(\phi (i,j)\)denote the distance between nodes\(v_i\)and\(v_j\). If a path exists between the nodes,\(\phi \)outputs the shortest path distance between them; if not, it outputs a special value, i.e., -1. In this paper, we consider both the relevance and irrelevance between different modalities. For each feasible value and the -1 value corresponding to unconnected heterogeneous nodes, we assign a learnable scalar that serves as a weight in the self-attention module. Denote\(A_{ij}\)as the (i,j)-element of the attention matrixA, we have:$$\begin{aligned} A_{ij}=\frac{(h_iW_{Q})(h_jW_{K})^T}{\sqrt{d_k}} \cdot b_{\phi (i,j)}, \end{aligned}$$(5)where\(b_{\phi (i,j)}\)is a learnable scalar indexed by\(\phi (i,j)\), and shared across all layers.\(W_{Q}\)and\(W_{K}\)are learnable weight matrices used to transform\(h_i\)and\(h_j\)into query and key vectors, respectively.\(d_k\)denotes the dimensionality of the key vectors.3.3.2Edge embeddingEdge embedding is proposed to encode edge features, allowing the model to incorporate edge information when updating node representations.We use the product of weights along the shortest path as the weight between two nodes, rather than averaging, because the product more accurately reflects the connectivity of the path. For example, when the shortest path\(\text {SP}_{ij} = \{1, 0\}\), the product is 0, which more accurately represents the connection strength compared to the average value of 0.5. Concretely, let\(\omega (i,j)\)denotes the product of weights along one of the shortest paths\(\text {SP}_{ij}=(e_1,e_2,\cdots ,e_N)\)from\(v_i\)to\(v_j\), expressed as follows:$$\begin{aligned} \omega (i,j)&amp;= \prod _{n=1}^{N} e_n. \end{aligned}$$(6)For each discretized value of\(\omega (i,j)\), we assign a learnable scalar that serves as a weight in the self-attention module. Equation (5) is further modified as follows:$$\begin{aligned} A_{ij}=\frac{(h_iW_{Q})(h_jW_{K})^T}{\sqrt{d_k}} \cdot (b_{\phi (i,j)} + c_{\lfloor \omega (i,j) \cdot a\rfloor }), \end{aligned}$$(7)where\(c_{\lfloor \omega (i,j) \cdot a\rfloor }\)is a learnable scalar indexed by the floor value of the product of\(\omega (i,j)\)and the hyperparametera.Moreover, given the varying significance of heterogeneous connections between nodes, such as sentences to images and sentences to sentences, we employ distinct spatial and edge embeddings for diverse heterogeneous connections.3.3.3Heter graphormer layerThe HeterGraphormer layer can be formally characterized as follows:$$\begin{aligned} {\begin{matrix} H^{'(l)} &amp; = \text {MHA}(\text {LN}(H^{(l-1)}), E) + H^{(l-1)},\\ H^{(l)} &amp; = \text {FFN}(\text {LN}(H^{'(l)})) + H^{'(l)}, \end{matrix}} \end{aligned}$$(8)where\(H^{'(l)}\)denotes the node representations output from layerl,\(\text {LN}\)signifies layer normalization and\(\text {FFN}\)refers to feed-forward network.\(\text {MHA}\)represents multi-head self-attention, represented as$$\begin{aligned} \begin{aligned} \text {MHA}(H, E) = \text {Concat}(\text {head}_1, \ldots , \text {head}_N) W^O,\\ \text {head}_i = \text {Attention}_i(H, E)=\text {softmax}(A)HW^V_i, \end{aligned} \end{aligned}$$(9)where\(W^O\)is a learnable weight matrix used to linearly combine the outputs of all attention heads,\(W^V\)is a learnable weight matrix used to transformHinto value vectors.Given the existence of two types of edges on the graph, we sequentially update the sentence-word subgraph and the sentence-image subgraph, formulated as follows:$$\begin{aligned} {\begin{matrix} [H^{'(l)}_s, H^{(l)}_w] &amp; = \text {layer}([H^{(l-1)}_s, H^{(l-1)}_w], E_{sw}),\\ [H^{(l)}_s, H^{(l)}_p] &amp; = \text {layer}([H^{'(l)}_s, H^{(l-1)}_p], E_{sp}).\\ \end{matrix}} \end{aligned}$$(10)3.4Multimodal summarizerAfter passingLHeterGraphormer layers, we obtain the final sentence node representations\(H^{L}_s = \{h^s_1, h^s_2, \cdots , h^s_m \}\)and image node representations\(H^{L}_p = \{h^p_1, h^p_2, \cdots , h^p_k \}\). Subsequently, these representations are fed into a single feed-forward layer for label prediction:$$\begin{aligned} \begin{aligned} \hat{y}^s_i&amp;= \text {sigmoid}(W_{s1}h^s_i),\\ \hat{y}^p_i&amp;= \text {sigmoid}(W_{p1}h^p_i),\\ \end{aligned} \end{aligned}$$(11)where\(W_{s1}\),\(W_{p1} \)are trainable parameters.For sentence extraction, our objective is to learn a binary label\(y^s_i \in \{0, 1\}\)for each sentence\(s_i\), where\(y^s_i = 1\)means that the sentence\(s_i\)should be included in the textual summary\(\mathcal {S}_t\), and\(y^s_i = 0\)means that the sentence\(s_i\)should be excluded from the summary. The ground truth labels, referred to as ORACLE, is extracted using the greedy algorithm introduced by Nallapati et al. (2017). In detail, the algorithm generates a summary consisting of multiple sentences, which are selected greedily to maximize the ROUGE score against the gold summaryFootnote1.The binary cross-entropy loss of the sentence classifier is expressed as$$\begin{aligned} \mathcal {L}_s = -\frac{1}{m}\sum _{i=1}^{m} \left( y^s_i \log (\hat{y}^s_i) + (1 - y^s_i) \log (1 - \hat{y}^s_i)\right) \end{aligned}$$(12)Following Zhang et al. (2022c), we use BLIP as the teacher model to compute the ITM score between the gold summary\(\mathcal {T}\)and each image\(p_i\). The mean squared error (MSE) loss is formulated as follows:$$\begin{aligned} \mathcal {L}_p = \frac{1}{k} \sum _{i=1}^{k} (\hat{y}^p_i - \text {match}(\mathcal {T},p_i))^2. \end{aligned}$$(13)The final loss of our model is the linear combination of two parts of loss with hyperparameter\(\lambda \)to balance their weights:$$\begin{aligned} \mathcal {L} = \lambda \mathcal {L}_s + (1-\lambda )\mathcal {L}_p. \end{aligned}$$(14)4Experiments setting4.1DatasetTable 1 MSMO dataset statisticsFull size tableWe use the MSMO dataset build by Zhu et al. (2018) for multimodal summarization. It contains news articles from theDaily MailwebsiteFootnote2, each paired with corresponding images and a text summary. The dataset includes 293,965 articles for training, 10,355 articles for validation, and 10,261 articles for testing. In the test set, at most three images are annotated as reference images. Image captions are excluded from the dataset for generalization. More details are illustrated in Table1.4.2Implementation detailsWe use the roberta-baseFootnote3version of RoBERTa and the blip-itm-base-cocoFootnote4version of BLIP to implement our models in all experiments. We conducted fine-tuning on the last 4 layers of RoBERTa while keeping BLIP frozen during the training process. We trained the model on an NVIDIA GeForce RTX 3090, setting the learning rate for BERT at 5e-6 and for other parameters at 5e-5. The batch size was set to 24, and we utilized the Adam optimizer (Kingma &amp; Ba,2015). The hidden state dimensiondof the nodes was set to 256. We constrain the number of sentences and images to 50 and 16, respectively. There are 6 layers of HeterGraphormer, each layer has 32 attention heads, and the dimension of FFN is set to 512. The hyperparameterais set to 10 for discretizing edge weights and\(\lambda \)is set to 0.8 to balance the loss of sentence and image selection. We select the best checkpoint according to the validation loss and report the results on the test set. For sentence extraction, we rank sentences in descending order based on their scores, selecting the top-3 while filtering out those with scores below 0.185.4.3BaselinesWe compare our model with existing text and multimodal summarization methods:ORACLEgenerates a summary consisting of multiple sentences, which are selected by a greedy algorithm (Nallapati et al.,2017) to maximize the ROUGE score against the gold summary.LEAD-3selects the first three sentences of the document as the text summary.ATG/ATL/HANare proposed by Zhu et al. (2018), incorporating visual information into the pointer generator network (See et al.,2017) through visual attention on global features, local features, and hierarchical local features of the images, respectively. The visual attention distributions are then utilized to select images.MOF(Zhu et al.,2020) introduces a multimodal objective function into ATG. Among the four MOF variants, we chose the one with the best overall performance.UniMS(Zhang et al.,2022c) extends the text encoder of BART Lewis et al. (2020) to a multimodal encoder and utilizes knowledge distillation to help image selection.SITA(Jiang et al.,2023) selects sentences most relevant to the images, using them as additional input instead of utilizing image information directly. The selection of images is based on the ROUGE-L of these sentences.HGMS(GAT): To compare the performance gap between HeterGraphormer and GAT (Velickovic et al.,2017), we implemented a GAT-based HGMS. Following the method proposed by Wang et al. (2020), the GAT incorporates considerations for both neighboring nodes and edge weights in updating nodes, and the number of layers is 2.4.4Evaluation metricsThe quality of generated textual summary is evaluated byROUGE. For image selection, image precision(IP) indicates whether the chosen image is present in the image references.\({\textbf {M}}_{\text {sim}}\)is an image-text relevance metric which calculates the maximum similarity between the image and each selected sentence by cross-modal retrieval model. For details regarding IP and\(\text {M}_{\text {sim}}\), please refer to the work of Zhu et al. (2018).5Results and analysis5.1Automatic evaluationTable 2 Automatic evaluation on MSMOFull size tableThe performance of baseline methods and our models is shown in Table2. The first block contains the ORACLE upper bound and LEAD-3 baseline, and the second block includes the multimodal summarization models. We present our models in the third block.Compared to the baseline methods, our HGMS model has demonstrated significant improvements across all metrics, including text summarization quality, image selection precision, image-text relevance, and the overall quality of multimodal summary.Regarding the image selection precision metric (IP), UniMS achieves a noteworthy advantage over its predecessor baselines by distilling knowledge within a vision-language pre-trained model. SITA achieves further improvements by leveraging sentences most relevant to images in the text as cues for image selection. By leveraging the inherent many-to-many relationships between sentences and images, our model achieves a significant performance boost compared to SITA (e.g., 79.72 vs. 76.41 in the IP metric).In terms of the\(\text {M}_{\text {sim}}\)metric for evaluating the relevance between sentences and images, our model achieved a performance improvement of over 40% compared to SITA (e.g., 46.56 vs. 33.47 of\(\text {M}_{\text {sim}}\)). This is mainly attributed to the introduction of ITM scores as weights for the edges between sentences and images, making the model more inclined to select relevant sentence-image pairs.Concerning text summarization quality metrics, our model exhibits notable improvements in both Rouge1 and Rouge2 (e.g., an increase of 1.32 and 1.05, respectively, compared to SITA). However, Rouge-L shows a marginal improvement of only 0.05. This indicates that the introduction of word nodes makes the model focus more on word-level semantic information.For different graph encoders, HGMS based on HeterGraphormer outperforms HGMS im-plemented with GAT across all metrics. This suggests that HeterGraphormer has a significant advantage in more effectively modeling intricate relationships between nodes.Table 3 Ablation studies on the test setFull size table5.2Ablation studyWe perform the ablation study to investigate the potential influence of different components. As shown in Table3, we design the following experimental settings:(1)We retain the centrality embedding used to represent node degree information in Graphormer.(2)The type embedding used to differentiate between different node types was removed.(3)We employ identical spatial and edge embeddings for each heterogeneous connection.(4)During the update of node features, we no longer consider heterogeneous nodes without connections. Sentence-image pairs are only connected when the ITM score is greater than 0.5.(5)We changed the method of calculating the weights between nodes from a product approach to averaging.(6)We removed the TF-IDF and ITM weights on the edges used to distinguish the importance of connections.(7)We replaced the BLIP weights used in the model with CLIP(Radford et al.,2021).(8)The pre-trained RoBERTa was replaced with a bidirectional LSTM.After the removal of edge weights, there is a notable decrease in the IP metric, indicating that the ITM scores on the edges between sentences and images play a crucial role in image selection. Regarding the enhancements made to Graphormer (from 1 to 3), the experimental results substantiate the effectiveness of these enhancements.Table 4 Experimental results with different methods of building image referencesFull size table5.3Image referenceTo investigate the impact of employing diverse image reference strategies during model training, we additionally incorporated a strategy named ROUGE-Top. This method selects an image with a caption that has the highest ROUGE score for each article as the reference image. As shown in Table4, compared to the knowledge distillation method, the IP score of the ROUGE-Top strategy has significantly increased by 3.69, indicating the effectiveness of this strategy. It is noteworthy that the IP score achieved through the ROUGE-Top strategy significantly surpassed its label upper bound (79.89), indicating the model’s success in leveraging the intricate relationships between sentences and images to enhance its capability in selecting critical images. However, as the ROUGE-Top strategy heavily relies on the quality of image captions, for the sake of model generalization, we choose to employ knowledge distillation.5.4Layer number analysisIn order to further analyze the advantages of HeterGraphormer over GAT, we conducted a study on the performance of the models at different numbers of layers. As illustrated in Fig.3, we compare the Rouge scores of HGMS using HeterGraphormer and GAT as encoders across different layer depths. We observed that with an increase in the number of layers, the performance gap between HeterGraphormer and GAT gradually widened. hough we introduced residual connections between layers in GAT to alleviate the issue of over-smoothing, the optimal number of layers for GAT was only 2, beyond which the performance started to decline. In contrast, HeterGraphormer, with its ability to attend to all nodes during node updates, remained unaffected by over-smoothing issue. With increasing layer depth, its performance also improves, stabilizing after 6 layers. This suggests that HeterGraphormer can enhance its ability to model intricate relationships by increasing the number of layers.Fig. 3The ROUGE scores on validation set when the number of layers increases.\(\hat{R}\)is the mean of R-1, R-2 and R-LFull size imageFig. 4Visualization of sentence and image embeddings for 100 documents in the test setFull size image5.5Embedding analysisTo explore the sentence and image embeddings generated by HGMS, we employ the t-SNE (van der Maaten &amp; Hinton,2008) algorithm to map the embeddings of the final layer outputs from HeterGraphormer to a two-dimensional space for visualization. We randomly selected 100 documents from the test set, comprising approximately 2500 sentences and 600 images. As illustrated in Fig.4, we use blue and gray points to represent ground truth and non-ground truth sentences, respectively, while yellow and black points denote reference and non-reference images, respectively.It can be observed that ground truth sentences exhibit a clustering tendency in the bottom-right area, indicating the model’s effectiveness in distinguishing salient sentences. Notably, compared to reference images, non-reference images show a higher degree of clustering, indicating that the model has better recognition ability for irrelevant images.5.6Human evaluationIn addition to the automatic evaluations, we conducted human evaluations to assess the performance of models. Following Wu et al. (2021), we randomly select 50 samples from the test set of MSMO dataset. Each sample is independently annotated by three different participants, and the rating is the average of their evaluations. Annotators evaluate the overall quality of summaries by ranking them based on the following three criteria simultaneously: (1)Informativeness: Does the summary effectively convey the crucial information from the source document? (2)Succinctness: Is the summary succinct, avoiding unnecessary details? (3)Relevance: Does the selected image match the textual summary? To further distinguish the differences between models, especially when their average rankings are similar, we quantified the rankings. Rankings range from 1 to 4, allowing ties, with scores corresponding to 2, 1, -1, -2, respectively. The final rating for each model is determined by averaging the scores across all test samples.Table 5 Ranking results of system summaries by human evaluationFull size tableTable 6 An example of multimodal summarization generationFull size tableTable5presents the human evaluation results on three strong baseline models and our proposed HGMS. The agreement between the annotators was calculated using Fleiss’ Kappa, with a value of 0.538. The results show that HGMS is able to generate higher quality multi-modal summaries compared to other models. Additionally, the human evaluation results further validate the effectiveness of our proposed HeterGraphormer.5.7Case studyTable6presents a typical example selected from test set consisting of the input article, the gold summary, the summary generated by the strong baseline SITA, and the summary produced by our proposed HGMS model. The highlighted text corresponds to entities present in the image. As BLIP does not have the ability to detect entities, these entities were manually annotated. Notably, the summary output by our model shows high similarity to the gold summary, effectively summarizing key information related to ”islands” and ”cruise ships”, while accurately selecting the image most relevant to these themes. Compared to the SITA model, which uses Rouge scores between pseudo image captions and textual summary for image selection, our model can select critical images more accurately.Additionally, Table7present the ITM scores calculated using BLIP between sentences (S1, S2, S3, S4) and images (IMG1, IMG2). It is evident that the many-to-many relationships between sentences and images play an important role in accurately extracting salient sentences and critical images. This further validates the effectiveness of the proposed graph structure.Table 7 The matching scores between the images and the sentencesFull size table6ConclusionThis paper presents HGMS for multimodal summarization. HGMS constructs a multimodal heterogeneous graph by considering both related and unrelated relationships between modalities, and applies HeterGraphormer on the graph to effectively model intermodal relationships. Experimental results show HGMS outperforms the current state-of-the-art model on MSMO datasets, which demonstrates the effectiveness of our method. Furthermore, our proposed HeterGraphormer can be extended to other NLP tasks, offering potential performance improvements for such tasks.7LimitationsDue to the utilization of Byte Pair Encoding (BPE) tokenization in the pre-trained language model, we opt to employ it only as a sentence encoder, without leveraging its word-level features. This decision stems from the inherent characteristics of BPE tokenization, which may result in the establishment of connections between originally unrelated sentence nodes after tokenization.Another limitation of this study is the lack of diverse datasets. Currently, only one publicly available dataset is accessible for use. Thus, the analysis and conclusions of our study may be influenced by the limitations of the dataset. Future research could address this limitation by gathering additional datasets to enhance the reliability and generalizability of the study findings.Data AvailabilityNo datasets were generated or analysed during the current study.Code AvailabilityCode is available athttps://github.com/anonymous272218/HGMSNotesGold summary refers to the ideal, human-generated summary that serves as a standard for evaluating the quality of automated text summarization systems.http://www.dailymail.co.ukhttps://huggingface.co/FacebookAIhttps://huggingface.co/SalesforceReferencesAn, C., Zhong, M., Wu, Z., et al. (2022). CoLo: A contrastive learning based re-ranking framework for one-stage summarization. In: Calzolari N, Huang CR, Kim H, et al (Eds.), Proceedings of the 29th international conference on computational linguistics.International Committee on Computational Linguistics, Gyeongju, Republic of Korea, pp 5783–5793.https://aclanthology.org/2022.coling-1.508Chen, J., &amp; Zhuge, H. (2018). Abstractive text-image summarization using multi-modal attentional hierarchical RNN. In: Riloff E, Chiang D, Hockenmaier J, et al (Eds.), Proceedings of the 2018 conference on empirical methods in natural language processing.Association for Computational Linguistics,Brussels, Belgium, pp 4046–4056.https://doi.org/10.18653/v1/D18-1438Cheng, J., &amp; Lapata, M. (2016). Neural summarization by extracting sentences and words. In: Erk K, Smith NA (Eds.), Proceedings of the 54th annual meeting of the association for computational linguistics (Volume 1: Long Papers).Association for Computational Linguistics,Berlin, Germany, pp 484–494.https://doi.org/10.18653/v1/P16-1046Cheng, X., Shen, Y., &amp; Lu, W. (2023). A set prediction network for extractive summarization. In: Rogers A, Boyd-Graber J, Okazaki N (Eds.), Findings of the association for computational linguistics: ACL 2023.Association for Computational Linguistics,Toronto, Canada, pp 4766–4777.https://doi.org/10.18653/v1/2023.findings-acl.293Cui, P., Hu, L., &amp; Liu, Y. (2020). Enhancing extractive text summarization with topic-aware graph neural networks. In: Scott D, Bel N, Zong C (Eds.), Proceedings of the 28th international conference on computational linguistics.International Committee on Computational Linguistics,Barcelona, Spain (Online), pp 5360–537.https://doi.org/10.18653/v1/2020.coling-main.468Erkan, G., &amp; Radev, D. R. (2004). Lexrank: Graph-based lexical centrality as salience in text summarization.J Artif Intell Res,22, 457–47.https://doi.org/10.1613/JAIR.1523ArticleGoogle ScholarJia, R., Cao, Y., Tang, H., et al. (2020). Neural extractive summarization with hierarchical attentive heterogeneous graph network. In: Webber B, Cohn T, He Y, et al (Eds.), Proceedings of the 2020 conference on Empirical Methods in Natural Language Processing (EMNLP).Association for Computational Linguistics,Online, pp 3622–363.https://doi.org/10.18653/v1/2020.emnlp-main.295Jiang, C., Xie, R., Ye, W., et al. (2023). Exploiting pseudo image captions for multimodal summarization. In: Rogers A, Boyd-Graber J, Okazaki N (Eds.), Findings of the association for computational linguistics: ACL 2023.Association for Computational Linguistics,Toronto, Canada, pp 161–175.https://doi.org/10.18653/v1/2023.findings-acl.12Jin, L., &amp; Chen, J. (2024). Self-supervised opinion summarization with multi-modal knowledge graph.J Intell Inf Syst,62(1), 191–208.https://doi.org/10.1007/S10844-023-00812-1ArticleMathSciNetGoogle ScholarJing, B., You, Z., Yang, T., et al. (2021). Multiplex graph neural network for extractive text summarization. In: Moens MF, Huang X, Specia L, et al. (Eds.), Proceedings of the 2021 conference on empirical methods in natural language processing.Association for Computational Linguistics,Online and Punta Cana, Dominican Republic, pp 133–139.https://doi.org/10.18653/v1/2021.emnlp-main.11Kingma, D.P., &amp; Ba, J (2015). Adam: A method for stochastic optimization. In: Bengio Y, LeCun Y (Eds.), 3rd International conference on learning representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015,Conference Track Proceedings,[SPACE]http://arxiv.org/abs/1412.6980Lewis, M., Liu, Y., Goyal, N., et al. (2020). BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In: Jurafsky D, Chai J, Schluter N, et al (Eds.), Proceedings of the 58th annual meeting of the association for computational linguistics.Association for Computational Linguistics,Online, pp 7871–7880.https://doi.org/10.18653/v1/2020.acl-main.703Li, H., Zhu, J., Liu, T., et al. (2018). Multi-modal sentence summarization with modality attention and image filtering. In: Lang J (Ed.), Proceedings of the twenty-seventh international joint conference on artificial intelligence, IJCAI 2018, July 13-19, 2018, Stockholm, Sweden. ijcai.org, Stockholm, Sweden, pp 4152–4158.https://doi.org/10.24963/ijcai.2018/577Li, J., Li, D., Xiong, C., et al. (2022). BLIP: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In: Chaudhuri K, Jegelka S, Song L, et al (Eds.), International conference on machine learning, ICML 2022, 17-23 July 2022, PMLR, Baltimore, Maryland, USA,Proceedings of Machine Learning Research,162pp 12888–12900Li, M., Chen, X., Gao, S., et al (2020) VMSMO: Learning to generate multimodal summary for video-based news articles. In: Webber B, Cohn T, He Y, et al (Eds.), Proceedings of the 2020 conference on Empirical Methods in Natural Language Processing (EMNLP).Association for Computational Linguistics,Online, pp 9360–9369.https://doi.org/10.18653/v1/2020.emnlp-main.752Liang, Y., Meng, F., Xu, J., et al. (2023). Summary-oriented vision modeling for multimodal abstractive summarization. In: Rogers A, Boyd-Graber J, Okazaki N (Eds.), Proceedings of the 61st annual meeting of the association for computational linguistics (Volume 1: Long Papers).Association for Computational Linguistics,Toronto, Canada, pp 2934–2951.https://doi.org/10.18653/v1/2023.acl-long.165Liu Y, &amp; Lapata M. (2019). Text summarization with pretrained encoders. In: Inui K, Jiang J, Ng V, et al (Eds.), Proceedings of the 2019 conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP).Association for Computational Linguistics,Hong Kong, China, pp 3730–3740.https://doi.org/10.18653/v1/D19-1387Liu, Y., Ott, M., Goyal, N., et al. (2019). Roberta: A robustly optimized BERT pretraining approach.arXiv:1907.11692https://arxiv.org/abs/1907.11692Liu, Y., Liu, P., Radev, D., et al. (2022). BRIO: Bringing order to abstractive summarization. In: Muresan S, Nakov P, Villavicencio A (Eds.), Proceedings of the 60th annual meeting of the association for computational linguistics (Volume 1: Long Papers).Association for Computational Linguistics,Dublin, Ireland, pp 2890–2903.https://doi.org/10.18653/v1/2022.acl-long.207van der Maaten L, &amp; Hinton G (2008) Visualizing data using t-sne.Journal of Machine Learning Research9(86), 2579–2605.http://jmlr.org/papers/v9/vandermaaten08a.htmlMihalcea, R., Tarau, P (2004) TextRank: Bringing order into text. In: Lin D, Wu D (Eds.), Proceedings of the 2004 conference on empirical methods in natural language processing.Association for Computational Linguistics,Barcelona, Spain, pp 404–411.https://aclanthology.org/W04-3252Nallapati R, Zhai F, &amp; Zhou B. (2017) .Summarunner: A recurrent neural network based sequence model for extractive summarization of documents.Proceedings of the AAAI Conference on Artificial Intelligence31(1).https://doi.org/10.1609/aaai.v31i1.10958Narayan S, Cohen SB, &amp; Lapata M. (2018). Ranking sentences for extractive summarization with reinforcement learning. In: Walker M, Ji H, Stent A (Eds.) Proceedings of the 2018 conference of the North American chapter of the association for computational linguistics: Human language technologies, Volume 1 (Long Papers).Association for Computational Linguistics,New Orleans, Louisiana, pp 1747–1759.https://doi.org/10.18653/v1/N18-1158Overbay, K., Ahn, J., Pesaran zadeh, F., et al. (2023). mRedditSum: A multimodal abstractive summarization dataset of Reddit threads with images. In: Bouamor H, Pino J, Bali K (Eds.), Proceedings of the 2023 conference on empirical methods in natural language processing.Association for Computational Linguistics,Singapore, pp 4117–4132.https://doi.org/10.18653/v1/2023.emnlp-main.251Paulus, R., Xiong, C., &amp; Socher, R. (2018). A deep reinforced model for abstractive summarization. In: International conference on learning representations,https://openreview.net/forum?id=HkAClQgA-Pennington, J., Socher, R., &amp; Manning, C. (2014). GloVe: Global vectors for word representation. In: Moschitti A, Pang B, Daelemans W (Eds.) Proceedings of the 2014 conference on Empirical Methods in Natural Language Processing (EMNLP).Association for Computational Linguistics,Doha, Qatar, pp 1532–1543.https://doi.org/10.3115/v1/D14-1162Phan TA, Nguyen NDN, &amp; Bui KHN. (2022). HeterGraphLongSum: Heterogeneous graph neural network with passage aggregation for extractive long document summarization. In: Calzolari N, Huang CR, Kim H, et al (Eds.), Proceedings of the 29th international conference on computational linguistics.International Committee on Computational Linguistics,Gyeongju, Republic of Korea, pp 6248–6258.https://aclanthology.org/2022.coling-1.545Radford A, Kim JW, Hallacy C, et al (2021) Learning transferable visual models from natural language supervision. In: Meila M, Zhang T (Eds.),Proceedings of the 38th International Conference on Machine Learning,ICML 2021, 18-24 July 2021, Virtual Event, pp 8748–8763.http://proceedings.mlr.press/v139/radford21a.htmlSee, A., Liu, P.J., Manning, C.D. (2017). Get to the point: Summarization with pointer-generator networks. In: Barzilay R, Kan MY (Eds.), Proceedings of the 55th annual meeting of the association for computational linguistics (Volume 1: Long Papers).Association for Computational Linguistics,Vancouver, Canada, pp 1073–1083.https://doi.org/10.18653/v1/P17-1099Song, Z., &amp; King, I. (2022). Hierarchical heterogeneous graph attention network for syntax-aware summarization.Proceedings of the AAAI Conference on Artificial Intelligence,36(10), 11340–1134.https://doi.org/10.1609/aaai.v36i10.21385ArticleGoogle ScholarSutskever, I., Vinyals, O., &amp; Le, Q.V. (2014). Sequence to sequence learning with neural networks. In: Ghahramani Z, Welling M, Cortes C, et al (Eds.),Advances in Neural Information Processing Systems,[SPACE]https://proceedings.neurips.cc/paper/2014/hash/a14ac55a4f27472c5d894ec1c3c743d2-Abstract.htmlTang, P., Hu, K., Yan, R., et al. (2022). OTExtSum: Extractive text summarisation with optimal transport. In: Carpuat M, de Marneffe MC, Meza Ruiz IV (eds) Findings of the association for computational linguistics: NAACL 2022.Association for Computational Linguistics,Seattle, United States, pp 1128–1141.https://doi.org/10.18653/v1/2022.findings-naacl.85Vaswani A, Shazeer N, Parmar N, et al. (2017). Attention is all you need. In: Guyon I, Luxburg UV, Bengio S, et al (Eds.),Advances in Neural Information Processing Systems,[SPACE]https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.htmlVelickovic, P., Cucurull, G., Casanova, A., et al. (2017).Graph attention networks. stat,1050(20), 10–48550.Google ScholarWang, D., Liu, P., Zheng, Y., et al. (2020). Heterogeneous graph neural networks for extractive document summarization. In: Jurafsky D, Chai J, Schluter N, et al (Eds.), Proceedings of the 58th annual meeting of the association for computational linguistics.Association for Computational Linguistics,Online, pp 6209–6219.https://doi.org/10.18653/v1/2020.acl-main.553Wu W, Li W, Xiao X, et al. (2021). BASS: Boosting abstractive summarization with unified semantic graph. In: Proceedings of the 59th annual meeting of the association for computational linguistics and the 11th international joint conference on natural language processing (Volume 1: Long Papers).Association for Computational Linguistics,Online, pp 6052–6067.https://doi.org/10.18653/v1/2021.acl-long.472Xiao M, Zhu J, Lin H, et al. (2023). CFSum coarse-to-fine contribution network for multimodal summarization. In: Rogers A, Boyd-Graber J, Okazaki N (Eds.), Proceedings of the 61st annual meeting of the association for computational linguistics (Volume 1: Long Papers).Association for Computational Linguistics,Toronto, Canada, pp 8538–8553.https://doi.org/10.18653/v1/2023.acl-long.476Xie, F., Chen, J., &amp; Chen, K. (2023). Extractive text-image summarization with relation-enhanced graph attention network.J Intell Inf Syst,61(2), 325–341.https://doi.org/10.1007/S10844-022-00757-XArticleGoogle ScholarYing, C., Cai, T., Luo, S., et al. (2021). Do transformers really perform badly for graph representation? In: Ranzato M, Beygelzimer A, Dauphin YN, et al (Eds.),Advances in Neural Information Processing Systems,pp 28877–28888.https://proceedings.neurips.cc/paper/2021/hash/f1c1592588411002af340cbaedd6fc33-Abstract.htmlZhang, J., Zhao, Y., Saleh, M., et al. (2020). PEGASUS: pre-training with extracted gap-sentences for abstractive summarization. In: Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, pp 11328–11339.http://proceedings.mlr.press/v119/zhang20ae.htmlZhang, L., Zhang, X., &amp; Pan, J. (2022a). Hierarchical cross-modality semantic correlation learning model for multimodal summarization.Proceedings of the AAAI Conference on Artificial Intelligence36(10) 11676–11684.https://doi.org/10.1609/aaai.v36i10.21422Zhang, M., Zhou, G., Yu, W., et al. (2022b). MAA-PTG: multimodal aspect-aware product title generation.J Intell Inf Syst59(1) 213–235.https://doi.org/10.1007/S10844-022-00695-8Zhang, Z., Meng, X., Wang, Y., et al. (2022c). Unims: A unified framework for multimodal summarization with knowledge distillation.Proceedings of the AAAI Conference on Artificial Intelligence36(10) 11757–11764.https://doi.org/10.1609/aaai.v36i10.21431Zhong, M., Liu, P., Chen, Y., et al. (2020). Extractive summarization as text matching. In: Jurafsky D, Chai J, Schluter N, et al (Eds.) Proceedings of the 58th annual meeting of the association for computational linguistics.Association for Computational Linguistics,Online, pp 6197–6208.https://doi.org/10.18653/v1/2020.acl-main.552Zhu, J., Li, H., Liu, T., et al. (2018). MSMO: Multimodal summarization with multimodal output. In: Riloff E, Chiang D, Hockenmaier J, et al (eds) Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing.Association for Computational Linguistics,Brussels, Belgium, pp 4154–4164.https://doi.org/10.18653/v1/D18-1448Zhu, J., Zhou, Y., Zhang, J., et al. (2020). Multimodal summarization with guidance of multimodal reference.Proceedings of the AAAI Conference on Artificial Intelligence,34(05), 9749–975.https://doi.org/10.1609/aaai.v34i05.6525ArticleGoogle ScholarDownload referencesFundingThis research was sponsored by the National Natural Science Foundation of China (No.61806101).Author informationAuthor notesXiankai Jiang and Jingqiang Chen are both equally contributed.Authors and AffiliationsSchool of Computer Science, Nanjing University of Posts and Telecommunications, Nanjing, 210049, Jiangsu, ChinaXiankai Jiang &amp; Jingqiang ChenAuthorsXiankai JiangView author publicationsYou can also search for this author inPubMedGoogle ScholarJingqiang ChenView author publicationsYou can also search for this author inPubMedGoogle ScholarContributionsX.J. and J.C. wrote the manuscript . All authors reviewed the manuscript.Corresponding authorCorrespondence toJingqiang Chen.Ethics declarationsCompeting InterestsThe authors declare no competing interests.Ethics Approval and Consent to ParticipateNot Applicable.Consent for PublicationThe authors declare that they consent for publication.Additional informationPublisher's NoteSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.Rights and permissionsSpringer Nature or its licensor (e.g. a society or other partner) holds exclusive rights to this article under a publishing agreement with the author(s) or other rightsholder(s); author self-archiving of the accepted manuscript version of this article is solely governed by the terms of such publishing agreement and applicable law.Reprints and permissionsAbout this articleCite this articleJiang, X., Chen, J. Heterogeneous graphormer for extractive multimodal summarization.J Intell Inf Syst(2024). https://doi.org/10.1007/s10844-024-00886-5Download citationReceived:14 April 2024Revised:04 September 2024Accepted:05 September 2024Published:30 September 2024DOI:https://doi.org/10.1007/s10844-024-00886-5Share this articleAnyone you share the following link with will be able to read this content:Get shareable linkSorry, a shareable link is not currently available for this article.Copy to clipboardProvided by the Springer Nature SharedIt content-sharing initiativeKeywordsSummarizationExtractive summarizationMulti-modal summarizationGraph neural networksUse our pre-submission checklistAvoid common mistakes on your manuscript.AdvertisementDiscover contentJournals A-ZBooks A-ZPublish with usJournal finderPublish your researchOpen access publishingProducts and servicesOur productsLibrariansSocietiesPartners and advertisersOur imprintsSpringerNature PortfolioBMCPalgrave MacmillanApressYour privacy choices/Manage cookiesYour US state privacy rightsAccessibility statementTerms and conditionsPrivacy policyHelp and supportCancel contracts here165.194.104.233South Korea Trial Consortium (3001948335)  - Chung Ang University (3000120396)  - 7029 SpringerLink South Korea KESLI Korean Journal Package (3000256740)  - South Korea Trial Consortium (3000522537)  - 5539 SpringerLink South Korea KESLI Full OJA Consortium - (3000171421)  - 9823  SpringerLink South Korea Shinwon (3000699960)  - South Korea Full eJournal Con - Academic 2015-2017 (3991460179)  - 8064 South Korea KESLI Korean Consortium (3000251005)  - 5539 SpringerLink South Korea KESLI Full eJournal Consortium - Academic (3000175460)© 2024 Springer Nature<a class="c-skip-link" href="#main">Skip to main content</a></body></html>