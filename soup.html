<html class="js" lang="en"><head>
<meta charset="utf-8"/>
<meta content="IE=edge" http-equiv="X-UA-Compatible"/>
<meta content="pc,mobile" name="applicable-device"/>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="max-image-preview:large" name="robots"/>
<meta content="Yes" name="access"/>
<meta content="1268d79b5e96aecf3ff2a7dac04ad990" name="360-site-verification"/>
<title>Continual variational dropout: a view of auxiliary local variables in continual learning | Machine Learning</title>
<meta content="@SpringerLink" name="twitter:site"/>
<meta content="summary_large_image" name="twitter:card"/>
<meta content="Content cover image" name="twitter:image:alt"/>
<meta content="Continual variational dropout: a view of auxiliary local variables in continual learning" name="twitter:title"/>
<meta content="Machine Learning - Regularization/prior-based approach appears to be one of the critical strategies in continual learning, considering its mechanism for preserving and preventing forgetting the..." name="twitter:description"/>
<meta content="https://static-content.springer.com/image/art%3A10.1007%2Fs10994-023-06487-7/MediaObjects/10994_2023_6487_Fig1_HTML.png" name="twitter:image"/>
<meta content="10994" name="journal_id"/>
<meta content="Continual variational dropout: a view of auxiliary local variables in continual learning" name="dc.title"/>
<meta content="Machine Learning 2023 113:1" name="dc.source"/>
<meta content="text/html" name="dc.format"/>
<meta content="Springer" name="dc.publisher"/>
<meta content="2023-12-07" name="dc.date"/>
<meta content="OriginalPaper" name="dc.type"/>
<meta content="En" name="dc.language"/>
<meta content="2023 The Author(s), under exclusive licence to Springer Science+Business Media LLC, part of Springer Nature" name="dc.copyright"/>
<meta content="2023 The Author(s), under exclusive licence to Springer Science+Business Media LLC, part of Springer Nature" name="dc.rights"/>
<meta content="journalpermissions@springernature.com" name="dc.rightsAgent"/>
<meta content="Regularization/prior-based approach appears to be one of the critical strategies in continual learning, considering its mechanism for preserving and preventing forgetting the learned knowledge. Without any retraining on previous data or extending the network architecture, the mechanism works by setting a constraint on the important weights of previous tasks when learning the current task. Regularization/prior approach, on the other hand, suffers the challenge of weights being moved intensely to the parameter region, in which the model achieves good performance for the latest task but poor ones for earlier tasks. To that end, we suggest a novel solution to this problem by continually applying variational dropout (CVD), thereby generating task-specific local variables that work as modifying factors for the global variables to fit the task. In particular, as we impose a variational distribution on the auxiliary local variables employed as multiplicative noise to the layers’ input, the model enables the global variables to be retained in a good region for all tasks and reduces the forgetting phenomenon. Furthermore, we obtained theoretical properties that are currently unavailable in existing methods: (1) uncorrelated likelihoods between different data instances reduce the high variance of stochastic gradient variational Bayes; (2) correlated pre-activation improves the representation ability for each task; and (3) data-dependent regularization assures the global variables to be preserved in a good region for all tasks. Throughout our extensive results, adding the local variables shows its significant advantage in enhancing the performance of regularization/prior-based methods by considerable magnitudes on numerous datasets. Specifically, it brings several standard baselines closer to state-of-the-art results." name="dc.description"/>
<meta content="1573-0565" name="prism.issn"/>
<meta content="Machine Learning" name="prism.publicationName"/>
<meta content="2023-12-07" name="prism.publicationDate"/>
<meta content="113" name="prism.volume"/>
<meta content="1" name="prism.number"/>
<meta content="OriginalPaper" name="prism.section"/>
<meta content="281" name="prism.startingPage"/>
<meta content="323" name="prism.endingPage"/>
<meta content="2023 The Author(s), under exclusive licence to Springer Science+Business Media LLC, part of Springer Nature" name="prism.copyright"/>
<meta content="journalpermissions@springernature.com" name="prism.rightsAgent"/>
<meta content="https://link.springer.com/article/10.1007/s10994-023-06487-7" name="prism.url"/>
<meta content="doi:10.1007/s10994-023-06487-7" name="prism.doi"/>
<meta content="https://link.springer.com/content/pdf/10.1007/s10994-023-06487-7.pdf" name="citation_pdf_url"/>
<meta content="https://link.springer.com/article/10.1007/s10994-023-06487-7" name="citation_fulltext_html_url"/>
<meta content="Machine Learning" name="citation_journal_title"/>
<meta content="Mach Learn" name="citation_journal_abbrev"/>
<meta content="Springer US" name="citation_publisher"/>
<meta content="1573-0565" name="citation_issn"/>
<meta content="Continual variational dropout: a view of auxiliary local variables in continual learning" name="citation_title"/>
<meta content="113" name="citation_volume"/>
<meta content="1" name="citation_issue"/>
<meta content="2024/01" name="citation_publication_date"/>
<meta content="2023/12/07" name="citation_online_date"/>
<meta content="281" name="citation_firstpage"/>
<meta content="323" name="citation_lastpage"/>
<meta content="Article" name="citation_article_type"/>
<meta content="en" name="citation_language"/>
<meta content="doi:10.1007/s10994-023-06487-7" name="dc.identifier"/>
<meta content="10.1007/s10994-023-06487-7" name="DOI"/>
<meta content="1103752" name="size"/>
<meta content="10.1007/s10994-023-06487-7" name="citation_doi"/>
<meta content="http://api.springer.com/xmldata/jats?q=doi:10.1007/s10994-023-06487-7&amp;api_key=" name="citation_springer_api_url"/>
<meta content="Regularization/prior-based approach appears to be one of the critical strategies in continual learning, considering its mechanism for preserving and preven" name="description"/>
<meta content="Hai, Nam Le" name="dc.creator"/>
<meta content="Nguyen, Trang" name="dc.creator"/>
<meta content="Van, Linh Ngo" name="dc.creator"/>
<meta content="Nguyen, Thien Huu" name="dc.creator"/>
<meta content="Than, Khoat" name="dc.creator"/>
<meta content="Machine Learning" name="dc.subject"/>
<meta content="Control, Robotics, Mechatronics" name="dc.subject"/>
<meta content="Artificial Intelligence" name="dc.subject"/>
<meta content="Simulation and Modeling" name="dc.subject"/>
<meta content="Natural Language Processing (NLP)" name="dc.subject"/>
<meta content="Ahn, H., Cha, S., Lee, D., &amp; Moon, T. (2019). Uncertainty-based continual learning with adaptive regularization. In Advances in Neural Information Processing Systems (pp. 4392–4402)." name="citation_reference"/>
<meta content="Aljundi, R., Babiloni, F., Elhoseiny, M., Rohrbach, M., &amp; Tuytelaars, T. (2018). Memory aware synapses: Learning what (not) to forget. In Proceedings of the European Conference on Computer Vision (ECCV) (pp. 139–154)." name="citation_reference"/>
<meta content="citation_journal_title=IEEE Transactions on Knowledge and Data Engineering; citation_title=Dynamic transformation of prior knowledge into Bayesian models for data streams; citation_author=TX Bach, ND Anh, NV Linh, K Than; citation_volume=35; citation_issue=4; citation_publication_date=2023; citation_pages=3742-3750; citation_doi=10.1109/TKDE.2021.3139469; citation_id=CR3" name="citation_reference"/>
<meta content="Benzing, F. (2020). Understanding regularisation methods for continual learning. In Workshop of Advances in Neural Information Processing Systems." name="citation_reference"/>
<meta content="Blundell, C., Cornebise, J., Kavukcuoglu, K., &amp; Wierstra, D. (2015). Weight uncertainty in neural network. In International conference on machine learning (pp. 1613–1622). PMLR." name="citation_reference"/>
<meta content="Boluki, S., Ardywibowo, R., Dadaneh, S. Z., Zhou, M., &amp; Qian, X. (2020). Learnable Bernoulli dropout for bayesian deep learning. In The International Conference on Artificial Intelligence and Statistics, AISTATS (pp. 3905–3916)." name="citation_reference"/>
<meta content="Cha, S., Hsu, H., Hwang, T., Calmon, F. P., &amp; Moon, T. (2021). CPR: Classifier-projection regularization for continual learning. In 9th International Conference on Learning Representations, ICLR." name="citation_reference"/>
<meta content="citation_journal_title=IEEE Signal Processing Magazine; citation_title=The MNIST database of handwritten digit images for machine learning research [best of the web]; citation_author=L Deng; citation_volume=29; citation_issue=6; citation_publication_date=2012; citation_pages=141-142; citation_doi=10.1109/MSP.2012.2211477; citation_id=CR8" name="citation_reference"/>
<meta content="De Lange, M., Aljundi, R., Masana, M., Parisot, S., Jia, X., Leonardis, A., Slabaugh, G., &amp; Tuytelaars, T. (2021). A continual learning survey: Defying forgetting in classification tasks. IEEE Transactions on Pattern Analysis and Machine Intelligence" name="citation_reference"/>
<meta content="Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., &amp;  Houlsby, N. (2020). An image is worth 16x16 words: Transformers for image recognition at scale. In International conference on learning representations" name="citation_reference"/>
<meta content="Farquhar, S., &amp; Gal, Y. (2018). A unifying bayesian view of continual learning. In The Bayesian deep learning workshop at neural information processing systems" name="citation_reference"/>
<meta content="Gal, Y., Hron, J., &amp; Kendall, A. (2017). Concrete dropout. In Advances in Neural Information Processing Systems (pp. 3581–3590)." name="citation_reference"/>
<meta content="Ghahramani, Z., &amp; Attias, H. (2000). Online variational Bayesian learning. In Slides from talk presented at NIPS workshop on online learning." name="citation_reference"/>
<meta content="Goodfellow, I. J., Mirza, M., Xiao, D., Courville, A., &amp; Bengio, Y. (2013). An empirical investigation of catastrophic forgetting in gradient-based neural networks. arXiv preprint 
                  arXiv:1312.6211
                  
                " name="citation_reference"/>
<meta content="Graves, A. (2011). Practical variational inference for neural networks. In Advances in Neural Information Processing Systems (pp. 2348–2356). Citeseer." name="citation_reference"/>
<meta content="citation_journal_title=International Journal of Approximate Reasoning; citation_title=Eliminating overfitting of probabilistic topic models on short and noisy text: The role of dropout; citation_author=C Ha, V-D Tran, LN Van, K Than; citation_volume=112; citation_publication_date=2019; citation_pages=85-104; citation_doi=10.1016/j.ijar.2019.05.010; citation_id=CR16" name="citation_reference"/>
<meta content="Hendrycks, D., Basart, S., Mu, N., Kadavath, S., Wang, F., Dorundo, E., Desai, R., Zhu, T., Parajuli, S., Guo, M., Song, D., Steinhardt, J., &amp; Gilmer, J. (2021). The many faces of robustness: A critical analysis of out-of-distribution generalization. In Proceedings of the IEEE/CVF international conference on computer vision (pp. 8340–8349)." name="citation_reference"/>
<meta content="Henning, C., Cervera, M., D’Angelo, F., Von Oswald, J., Traber, R., Ehret, B., Kobayashi, S., Grewe, B. F., &amp; Sacramento, J. (2021). Posterior meta-replay for continual learning. In Advances in neural information processing systems (Vol. 34)." name="citation_reference"/>
<meta content="Jung, S., Ahn, H., Cha, S., &amp; Moon, T. (2020). Continual learning with node-importance based adaptive group sparse regularization. In Advances in neural information processing systems" name="citation_reference"/>
<meta content="citation_journal_title=Advances in Neural Information Processing Systems; citation_title=Variational dropout and the local reparameterization trick; citation_author=DP Kingma, T Salimans, M Welling; citation_volume=28; citation_publication_date=2015; citation_pages=2575-2583; citation_id=CR20" name="citation_reference"/>
<meta content="Kingma, D. P., &amp; Welling, M. (2014). Auto-encoding variational bayes. In: Bengio, Y., LeCun, Y. (eds.) 2nd international conference on learning representations, ICLR." name="citation_reference"/>
<meta content="citation_journal_title=Proceedings of the National Academy of Sciences; citation_title=Overcoming catastrophic forgetting in neural networks; citation_author=J Kirkpatrick, R Pascanu, N Rabinowitz, J Veness, G Desjardins, AA Rusu, K Milan, J Quan, T Ramalho, A Grabska-Barwinska; citation_volume=114; citation_issue=13; citation_publication_date=2017; citation_pages=3521-3526; citation_doi=10.1073/pnas.1611835114; citation_id=CR22" name="citation_reference"/>
<meta content="Krizhevsky, A. (2009). Learning multiple layers of features from tiny images. Technical report, University of Toronto." name="citation_reference"/>
<meta content="citation_journal_title=IEEE Transactions on Pattern Analysis and Machine Intelligence; citation_title=Learning without forgetting; citation_author=Z Li, D Hoiem; citation_volume=40; citation_issue=12; citation_publication_date=2017; citation_pages=2935-2947; citation_doi=10.1109/TPAMI.2017.2773081; citation_id=CR24" name="citation_reference"/>
<meta content="citation_journal_title=Neurocomputing; citation_title=A graph convolutional topic model for short and noisy text streams; citation_author=N Linh, TX Bach, K Than; citation_volume=468; citation_publication_date=2022; citation_pages=345-359; citation_doi=10.1016/j.neucom.2021.10.047; citation_id=CR25" name="citation_reference"/>
<meta content="Liu, Y., Dong, W., Zhang, L., Gong, D., &amp; Shi, Q. (2019). Variational bayesian dropout with a hierarchical prior. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 7124–7133)." name="citation_reference"/>
<meta content="Loo, N., Swaroop, S., &amp; Turner, R. E. (2021). Generalized variational continual learning. In International conference on learning representation" name="citation_reference"/>
<meta content="citation_journal_title=Neural Computation; citation_title=A practical Bayesian framework for backpropagation networks; citation_author=DJC MacKay; citation_volume=4; citation_issue=3; citation_publication_date=1992; citation_pages=448-472; citation_doi=10.1162/neco.1992.4.3.448; citation_id=CR28" name="citation_reference"/>
<meta content="Mirzadeh, S., Farajtabar, M., Pascanu, R., &amp; Ghasemzadeh, H. (2020). Understanding the role of training regimes in continual learning. In Advances in neural information processing systems" name="citation_reference"/>
<meta content="Mirzadeh, S. I., Farajtabar, M., &amp; Ghasemzadeh, H. (2020). Dropout as an implicit gating mechanism for continual learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops (pp. 232–233)." name="citation_reference"/>
<meta content="Molchanov, D., Ashukha, A., &amp; Vetrov, D. (2017). Variational dropout sparsifies deep neural networks. In International conference on machine learning (pp. 2498–2507)." name="citation_reference"/>
<meta content="citation_title=Machine learning: A probabilistic perspective; citation_publication_date=2012; citation_id=CR32; citation_author=KP Murphy; citation_publisher=MIT Press" name="citation_reference"/>
<meta content="citation_title=Bayesian learning for neural networks; citation_publication_date=1996; citation_id=CR33; citation_author=RM Neal; citation_publisher=Springer" name="citation_reference"/>
<meta content="citation_journal_title=Neurocomputing; citation_title=Balancing stability and plasticity when learning topic models from short and noisy text streams; citation_author=T Nguyen, T Mai, N Nguyen, LN Van, K Than; citation_volume=505; citation_publication_date=2022; citation_pages=30-43; citation_doi=10.1016/j.neucom.2022.07.019; citation_id=CR34" name="citation_reference"/>
<meta content="citation_journal_title=Advances in Neural Information Processing Systems; citation_title=Structured dropout variational inference for Bayesian neural networks; citation_author=S Nguyen, D Nguyen, K Nguyen, K Than, H Bui, N Ho; citation_volume=34; citation_publication_date=2021; citation_pages=15188-15202; citation_id=CR35" name="citation_reference"/>
<meta content="citation_journal_title=Machine Learning; citation_title=Adaptive infinite dropout for noisy and sparse data streams; citation_author=H Nguyen, H Pham, S Nguyen, N Linh, K Than; citation_volume=111; citation_issue=8; citation_publication_date=2022; citation_pages=3025-3060; citation_doi=10.1007/s10994-022-06169-w; citation_id=CR36" name="citation_reference"/>
<meta content="Nguyen, C. V., Li, Y., Bui, T. D., &amp; Turner, R. E. (2018). Variational continual learning. In International conference on learning representation." name="citation_reference"/>
<meta content="Nguyen, V.-S., Nguyen, D.-T., Van, L.N., &amp; Than, K. (2019). Infinite dropout for training bayesian models from data streams. In IEEE international conference on big data (Big Data) (pp. 125–134). IEEE" name="citation_reference"/>
<meta content="Oh, C., Adamczewski, K., &amp; Park, M. (2020). Radial and directional posteriors for Bayesian deep learning. In The thirty-fourth conference on artificial intelligence, AAAI (pp. 5298–5305)" name="citation_reference"/>
<meta content="Paisley, J. W., Blei, D. M., &amp; Jordan, M. I. (2012). Variational bayesian inference with stochastic search. In Proceedings of the 29th international conference on machine learning, ICML" name="citation_reference"/>
<meta content="Phan, H., Tuan, A. P., Nguyen, S., Linh, N. V., &amp; Than, K. (2022). Reducing catastrophic forgetting in neural networks via Gaussian mixture approximation. In Pacific-Asia Conference on Knowledge Discovery and Data Mining (pp. 106–117). Springer: Berlin" name="citation_reference"/>
<meta content="citation_journal_title=Neural Computation; citation_title=Online model selection based on the variational bayes; citation_author=M-A Sato; citation_volume=13; citation_issue=7; citation_publication_date=2001; citation_pages=1649-1681; citation_doi=10.1162/089976601750265045; citation_id=CR42" name="citation_reference"/>
<meta content="citation_journal_title=Advances in Neural Information Processing Systems; citation_title=Overcoming catastrophic forgetting in incremental few-shot learning by finding flat minima; citation_author=G Shi, J Chen, W Zhang, L-M Zhan, X-M Wu; citation_volume=34; citation_publication_date=2021; citation_pages=6747-6761; citation_id=CR43" name="citation_reference"/>
<meta content="citation_journal_title=The Journal of Machine Learning Research; citation_title=Dropout: A simple way to prevent neural networks from overfitting; citation_author=N Srivastava, G Hinton, A Krizhevsky, I Sutskever, R Salakhutdinov; citation_volume=15; citation_issue=1; citation_publication_date=2014; citation_pages=1929-1958; citation_id=CR44" name="citation_reference"/>
<meta content="Swaroop, S., Nguyen, C.V., Bui, T. D., &amp; Turner, R. E. (2018). Improving and understanding variational continual learning. In NeurIPS Continual Learning Workshop." name="citation_reference"/>
<meta content="Swiatkowski, J., Roth, K., Veeling, B., Tran, L., Dillon, J., Snoek, J., Mandt, S., Salimans, T., Jenatton, R., &amp; Nowozin, S. (2020). The k-tied normal distribution: A compact parameterization of Gaussian mean field posteriors in Bayesian neural networks. In International conference on machine learning (pp. 9289–9299). PMLR." name="citation_reference"/>
<meta content="Van, L.N., Hai, N.L., Pham, H., &amp; Than, K. (2022). Auxiliary local variables for improving regularization/prior approach in continual learning. In  Pacific-Asia conference on knowledge discovery and data mining (pp. 16–28). Springer: Berlin" name="citation_reference"/>
<meta content="Van de Ven, G. M., &amp; Tolias, A. S. (2019). Three scenarios for continual learning. In NeurIPS—Continual learning workshop" name="citation_reference"/>
<meta content="Wah, C., Branson, S., Welinder, P., Perona, P., &amp; Belongie, S. (2011). The Caltech-UCSD Birds-200-2011 dataset." name="citation_reference"/>
<meta content="Wei, C., Kakade, S., &amp; Ma, T. (2020). The implicit and explicit regularization effects of dropout. In International conference on machine learning (pp. 10181–10192). PMLR." name="citation_reference"/>
<meta content="Yin, D., Farajtabar, M., &amp; Li, A. (2020). Sola: Continual learning with second-order loss approximation. In Workshop of advances in neural information processing systems" name="citation_reference"/>
<meta content="citation_journal_title=Proceedings of Machine Learning Research; citation_title=Continual learning through synaptic intelligence; citation_author=F Zenke, B Poole, S Ganguli; citation_volume=70; citation_publication_date=2017; citation_pages=3987; citation_id=CR52" name="citation_reference"/>
<meta content="Hai, Nam Le" name="citation_author"/>
<meta content="namlh35@fpt.com" name="citation_author_email"/>
<meta content="School of Information and Communication Technology, Hanoi University of Science and Technology, Hanoi, Vietnam" name="citation_author_institution"/>
<meta content="FPT Software AI Center, Hanoi, Vietnam" name="citation_author_institution"/>
<meta content="Nguyen, Trang" name="citation_author"/>
<meta content="thientrangngv@gmail.com" name="citation_author_email"/>
<meta content="School of Information and Communication Technology, Hanoi University of Science and Technology, Hanoi, Vietnam" name="citation_author_institution"/>
<meta content="Van, Linh Ngo" name="citation_author"/>
<meta content="linhnv@soict.hust.edu.vn" name="citation_author_email"/>
<meta content="School of Information and Communication Technology, Hanoi University of Science and Technology, Hanoi, Vietnam" name="citation_author_institution"/>
<meta content="Nguyen, Thien Huu" name="citation_author"/>
<meta content="thien@cs.uoregon.edu" name="citation_author_email"/>
<meta content="University of Oregon, Eugene, USA" name="citation_author_institution"/>
<meta content="Than, Khoat" name="citation_author"/>
<meta content="khoattq@soict.hust.edu.vn" name="citation_author_email"/>
<meta content="School of Information and Communication Technology, Hanoi University of Science and Technology, Hanoi, Vietnam" name="citation_author_institution"/>
<meta content="telephone=no" name="format-detection"/>
<meta content="2024/01/01" name="citation_cover_date"/>
<meta content="https://link.springer.com/article/10.1007/s10994-023-06487-7" property="og:url"/>
<meta content="article" property="og:type"/>
<meta content="SpringerLink" property="og:site_name"/>
<meta content="Continual variational dropout: a view of auxiliary local variables in continual learning - Machine Learning" property="og:title"/>
<meta content="Regularization/prior-based approach appears to be one of the critical strategies in continual learning, considering its mechanism for preserving and preventing forgetting the learned knowledge. Without any retraining on previous data or extending the network architecture, the mechanism works by setting a constraint on the important weights of previous tasks when learning the current task. Regularization/prior approach, on the other hand, suffers the challenge of weights being moved intensely to the parameter region, in which the model achieves good performance for the latest task but poor ones for earlier tasks. To that end, we suggest a novel solution to this problem by continually applying variational dropout (CVD), thereby generating task-specific local variables that work as modifying factors for the global variables to fit the task. In particular, as we impose a variational distribution on the auxiliary local variables employed as multiplicative noise to the layers’ input, the model enables the global variables to be retained in a good region for all tasks and reduces the forgetting phenomenon. Furthermore, we obtained theoretical properties that are currently unavailable in existing methods: (1) uncorrelated likelihoods between different data instances reduce the high variance of stochastic gradient variational Bayes; (2) correlated pre-activation improves the representation ability for each task; and (3) data-dependent regularization assures the global variables to be preserved in a good region for all tasks. Throughout our extensive results, adding the local variables shows its significant advantage in enhancing the performance of regularization/prior-based methods by considerable magnitudes on numerous datasets. Specifically, it brings several standard baselines closer to state-of-the-art results." property="og:description"/>
<meta content="https://static-content.springer.com/image/art%3A10.1007%2Fs10994-023-06487-7/MediaObjects/10994_2023_6487_Fig1_HTML.png" property="og:image"/>
<meta content="telephone=no" name="format-detection"/>
<link href="/oscar-static/img/favicons/darwin/apple-touch-icon-92e819bf8a.png" rel="apple-touch-icon" sizes="180x180"/>
<link href="/oscar-static/img/favicons/darwin/android-chrome-192x192-6f081ca7e5.png" rel="icon" sizes="192x192" type="image/png"/>
<link href="/oscar-static/img/favicons/darwin/favicon-32x32-1435da3e82.png" rel="icon" sizes="32x32" type="image/png"/>
<link href="/oscar-static/img/favicons/darwin/favicon-16x16-ed57f42bd2.png" rel="icon" sizes="16x16" type="image/png"/>
<link data-test="shortcut-icon" href="/oscar-static/img/favicons/darwin/favicon-c6d59aafac.ico" rel="shortcut icon"/>
<meta content="#e6e6e6" name="theme-color"/>
<!-- Please see discussion: https://github.com/springernature/frontend-open-space/issues/316-->
<!--TODO: Implement alternative to CTM in here if the discussion concludes we do not continue with CTM as a practice-->
<link href="/oscar-static/app-springerlink/css/print-b8af42253b.css" media="print" rel="stylesheet"/>
<style> html{text-size-adjust:100%;line-height:1.15}body{font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;line-height:1.8;margin:0}details,main{display:block}h1{font-size:2em;margin:.67em 0}a{background-color:transparent;color:#025e8d}sub{bottom:-.25em;font-size:75%;line-height:0;position:relative;vertical-align:baseline}img{border:0;height:auto;max-width:100%;vertical-align:middle}button,input{font-family:inherit;font-size:100%;line-height:1.15;margin:0;overflow:visible}button{text-transform:none}[type=button],[type=submit],button{-webkit-appearance:button}[type=search]{-webkit-appearance:textfield;outline-offset:-2px}summary{display:list-item}[hidden]{display:none}button{cursor:pointer}svg{height:1rem;width:1rem} </style>
<style>@media only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark) {  body{background:#fff;color:#222;font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;line-height:1.8;min-height:100%}a{color:#025e8d;text-decoration:underline;text-decoration-skip-ink:auto}button{cursor:pointer}img{border:0;height:auto;max-width:100%;vertical-align:middle}html{box-sizing:border-box;font-size:100%;height:100%;overflow-y:scroll}h1{font-size:2.25rem}h2{font-size:1.75rem}h1,h2,h4{font-weight:700;line-height:1.2}h4{font-size:1.25rem}body{font-size:1.125rem}*{box-sizing:inherit}p{margin-bottom:2rem;margin-top:0}p:last-of-type{margin-bottom:0}.c-ad{text-align:center}@media only screen and (min-width:480px){.c-ad{padding:8px}}.c-ad--728x90{display:none}.c-ad--728x90 .c-ad__inner{min-height:calc(1.5em + 94px)}@media only screen and (min-width:876px){.js .c-ad--728x90{display:none}}.c-ad__label{color:#333;font-size:.875rem;font-weight:400;line-height:1.5;margin-bottom:4px}.c-ad__label,.c-status-message{font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif}.c-status-message{align-items:center;box-sizing:border-box;display:flex;position:relative;width:100%}.c-status-message :last-child{margin-bottom:0}.c-status-message--boxed{background-color:#fff;border:1px solid #ccc;line-height:1.4;padding:16px}.c-status-message__heading{font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;font-size:.875rem;font-weight:700}.c-status-message__icon{fill:currentcolor;display:inline-block;flex:0 0 auto;height:1.5em;margin-right:8px;transform:translate(0);vertical-align:text-top;width:1.5em}.c-status-message__icon--top{align-self:flex-start}.c-status-message--info .c-status-message__icon{color:#003f8d}.c-status-message--boxed.c-status-message--info{border-bottom:4px solid #003f8d}.c-status-message--error .c-status-message__icon{color:#c40606}.c-status-message--boxed.c-status-message--error{border-bottom:4px solid #c40606}.c-status-message--success .c-status-message__icon{color:#00b8b0}.c-status-message--boxed.c-status-message--success{border-bottom:4px solid #00b8b0}.c-status-message--warning .c-status-message__icon{color:#edbc53}.c-status-message--boxed.c-status-message--warning{border-bottom:4px solid #edbc53}.eds-c-header{background-color:#fff;border-bottom:2px solid #01324b;font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;font-size:1rem;line-height:1.5;padding:8px 0 0}.eds-c-header__container{align-items:center;display:flex;flex-wrap:nowrap;gap:8px 16px;justify-content:space-between;margin:0 auto 8px;max-width:1280px;padding:0 8px;position:relative}.eds-c-header__nav{border-top:2px solid #c5e0f4;padding-top:4px;position:relative}.eds-c-header__nav-container{align-items:center;display:flex;flex-wrap:wrap;margin:0 auto 4px;max-width:1280px;padding:0 8px;position:relative}.eds-c-header__nav-container>:not(:last-child){margin-right:32px}.eds-c-header__link-container{align-items:center;display:flex;flex:1 0 auto;gap:8px 16px;justify-content:space-between}.eds-c-header__list{list-style:none;margin:0;padding:0}.eds-c-header__list-item{font-weight:700;margin:0 auto;max-width:1280px;padding:8px}.eds-c-header__list-item:not(:last-child){border-bottom:2px solid #c5e0f4}.eds-c-header__item{color:inherit}@media only screen and (min-width:768px){.eds-c-header__item--menu{display:none;visibility:hidden}.eds-c-header__item--menu:first-child+*{margin-block-start:0}}.eds-c-header__item--inline-links{display:none;visibility:hidden}@media only screen and (min-width:768px){.eds-c-header__item--inline-links{display:flex;gap:16px 16px;visibility:visible}}.eds-c-header__item--divider:before{border-left:2px solid #c5e0f4;content:"";height:calc(100% - 16px);margin-left:-15px;position:absolute;top:8px}.eds-c-header__brand{padding:16px 8px}.eds-c-header__brand a{display:block;line-height:1;text-decoration:none}.eds-c-header__brand img{height:1.5rem;width:auto}.eds-c-header__link{color:inherit;display:inline-block;font-weight:700;padding:16px 8px;position:relative;text-decoration-color:transparent;white-space:nowrap;word-break:normal}.eds-c-header__icon{fill:currentcolor;display:inline-block;font-size:1.5rem;height:1em;transform:translate(0);vertical-align:bottom;width:1em}.eds-c-header__icon+*{margin-left:8px}.eds-c-header__expander{background-color:#f0f7fc}.eds-c-header__search{display:block;padding:24px 0}@media only screen and (min-width:768px){.eds-c-header__search{max-width:70%}}.eds-c-header__search-container{position:relative}.eds-c-header__search-label{color:inherit;display:inline-block;font-weight:700;margin-bottom:8px}.eds-c-header__search-input{background-color:#fff;border:1px solid #000;padding:8px 48px 8px 8px;width:100%}.eds-c-header__search-button{background-color:transparent;border:0;color:inherit;height:100%;padding:0 8px;position:absolute;right:0}.has-tethered.eds-c-header__expander{border-bottom:2px solid #01324b;left:0;margin-top:-2px;top:100%;width:100%;z-index:10}@media only screen and (min-width:768px){.has-tethered.eds-c-header__expander--menu{display:none;visibility:hidden}}.has-tethered .eds-c-header__heading{display:none;visibility:hidden}.has-tethered .eds-c-header__heading:first-child+*{margin-block-start:0}.has-tethered .eds-c-header__search{margin:auto}.eds-c-header__heading{margin:0 auto;max-width:1280px;padding:16px 16px 0}.eds-c-pagination{align-items:center;display:flex;flex-wrap:wrap;font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;font-size:.875rem;gap:16px 0;justify-content:center;line-height:1.4;list-style:none;margin:0;padding:32px 0}@media only screen and (min-width:480px){.eds-c-pagination{padding:32px 16px}}.eds-c-pagination__item{margin-right:8px}.eds-c-pagination__item--prev{margin-right:16px}.eds-c-pagination__item--next .eds-c-pagination__link,.eds-c-pagination__item--prev .eds-c-pagination__link{padding:16px 8px}.eds-c-pagination__item--next{margin-left:8px}.eds-c-pagination__item:last-child{margin-right:0}.eds-c-pagination__link{align-items:center;color:#222;cursor:pointer;display:inline-block;font-size:1rem;margin:0;padding:16px 24px;position:relative;text-align:center;transition:all .2s ease 0s}.eds-c-pagination__link:visited{color:#222}.eds-c-pagination__link--disabled{border-color:#555;color:#555;cursor:default}.eds-c-pagination__link--active{background-color:#01324b;background-image:none;border-radius:8px;color:#fff}.eds-c-pagination__link--active:focus,.eds-c-pagination__link--active:hover,.eds-c-pagination__link--active:visited{color:#fff}.eds-c-pagination__link-container{align-items:center;display:flex}.eds-c-pagination__icon{fill:#222;height:1.5rem;width:1.5rem}.eds-c-pagination__icon--disabled{fill:#555}.eds-c-pagination__visually-hidden{clip:rect(0,0,0,0);border:0;clip-path:inset(50%);height:1px;overflow:hidden;padding:0;position:absolute!important;white-space:nowrap;width:1px}.c-breadcrumbs{color:#333;font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;font-size:1rem;list-style:none;margin:0;padding:0}.c-breadcrumbs>li{display:inline}svg.c-breadcrumbs__chevron{fill:#333;height:10px;margin:0 .25rem;width:10px}.c-breadcrumbs--contrast,.c-breadcrumbs--contrast .c-breadcrumbs__link{color:#fff}.c-breadcrumbs--contrast svg.c-breadcrumbs__chevron{fill:#fff}@media only screen and (max-width:479px){.c-breadcrumbs .c-breadcrumbs__item{display:none}.c-breadcrumbs .c-breadcrumbs__item:last-child,.c-breadcrumbs .c-breadcrumbs__item:nth-last-child(2){display:inline}}.c-skip-link{background:#01324b;bottom:auto;color:#fff;font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;font-size:1rem;padding:8px;position:absolute;text-align:center;transform:translateY(-100%);width:100%;z-index:9999}@media (prefers-reduced-motion:reduce){.c-skip-link{transition:top .3s ease-in-out 0s}}@media print{.c-skip-link{display:none}}.c-skip-link:active,.c-skip-link:hover,.c-skip-link:link,.c-skip-link:visited{color:#fff}.c-skip-link:focus{transform:translateY(0)}.l-with-sidebar{display:flex;flex-wrap:wrap}.l-with-sidebar>*{margin:0}.l-with-sidebar__sidebar{flex-basis:var(--with-sidebar--basis,400px);flex-grow:1}.l-with-sidebar>:not(.l-with-sidebar__sidebar){flex-basis:0px;flex-grow:999;min-width:var(--with-sidebar--min,53%)}.l-with-sidebar>:first-child{padding-right:4rem}@supports (gap:1em){.l-with-sidebar>:first-child{padding-right:0}.l-with-sidebar{gap:var(--with-sidebar--gap,4rem)}}.c-header__link{color:inherit;display:inline-block;font-weight:700;padding:16px 8px;position:relative;text-decoration-color:transparent;white-space:nowrap;word-break:normal}.app-masthead__colour-4{--background-color:#ff9500;--gradient-light:rgba(0,0,0,.5);--gradient-dark:rgba(0,0,0,.8)}.app-masthead{background:var(--background-color,#0070a8);position:relative}.app-masthead:after{background:radial-gradient(circle at top right,var(--gradient-light,rgba(0,0,0,.4)),var(--gradient-dark,rgba(0,0,0,.7)));bottom:0;content:"";left:0;position:absolute;right:0;top:0}@media only screen and (max-width:479px){.app-masthead:after{background:linear-gradient(225deg,var(--gradient-light,rgba(0,0,0,.4)),var(--gradient-dark,rgba(0,0,0,.7)))}}.app-masthead__container{color:var(--masthead-color,#fff);margin:0 auto;max-width:1280px;padding:0 16px;position:relative;z-index:1}.u-button{align-items:center;background-color:#01324b;background-image:none;border:4px solid transparent;border-radius:32px;cursor:pointer;display:inline-flex;font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;font-size:.875rem;font-weight:700;justify-content:center;line-height:1.3;margin:0;padding:16px 32px;position:relative;transition:all .2s ease 0s;width:auto}.u-button svg,.u-button--contrast svg,.u-button--primary svg,.u-button--secondary svg,.u-button--tertiary svg{fill:currentcolor}.u-button,.u-button:visited{color:#fff}.u-button,.u-button:hover{box-shadow:0 0 0 1px #01324b;text-decoration:none}.u-button:hover{border:4px solid #fff}.u-button:focus{border:4px solid #fc0;box-shadow:none;outline:0;text-decoration:none}.u-button:focus,.u-button:hover{background-color:#fff;background-image:none;color:#01324b}.app-masthead--pastel .c-pdf-download .u-button--primary:focus svg path,.app-masthead--pastel .c-pdf-download .u-button--primary:hover svg path,.c-context-bar--sticky .c-context-bar__container .c-pdf-download .u-button--primary:focus svg path,.c-context-bar--sticky .c-context-bar__container .c-pdf-download .u-button--primary:hover svg path,.u-button--primary:focus svg path,.u-button--primary:hover svg path,.u-button:focus svg path,.u-button:hover svg path{fill:#01324b}.u-button--primary{background-color:#01324b;background-image:none;border:4px solid transparent;box-shadow:0 0 0 1px #01324b;color:#fff;font-weight:700}.u-button--primary:visited{color:#fff}.u-button--primary:hover{border:4px solid #fff;box-shadow:0 0 0 1px #01324b;text-decoration:none}.u-button--primary:focus{border:4px solid #fc0;box-shadow:none;outline:0;text-decoration:none}.u-button--primary:focus,.u-button--primary:hover{background-color:#fff;background-image:none;color:#01324b}.u-button--secondary{background-color:#fff;border:4px solid #fff;color:#01324b;font-weight:700}.u-button--secondary:visited{color:#01324b}.u-button--secondary:hover{border:4px solid #01324b;box-shadow:none}.u-button--secondary:focus,.u-button--secondary:hover{background-color:#01324b;color:#fff}.app-masthead--pastel .c-pdf-download .u-button--secondary:focus svg path,.app-masthead--pastel .c-pdf-download .u-button--secondary:hover svg path,.c-context-bar--sticky .c-context-bar__container .c-pdf-download .u-button--secondary:focus svg path,.c-context-bar--sticky .c-context-bar__container .c-pdf-download .u-button--secondary:hover svg path,.u-button--secondary:focus svg path,.u-button--secondary:hover svg path,.u-button--tertiary:focus svg path,.u-button--tertiary:hover svg path{fill:#fff}.u-button--tertiary{background-color:#ebf1f5;border:4px solid transparent;box-shadow:none;color:#666;font-weight:700}.u-button--tertiary:visited{color:#666}.u-button--tertiary:hover{border:4px solid #01324b;box-shadow:none}.u-button--tertiary:focus,.u-button--tertiary:hover{background-color:#01324b;color:#fff}.u-button--contrast{background-color:transparent;background-image:none;color:#fff;font-weight:400}.u-button--contrast:visited{color:#fff}.u-button--contrast,.u-button--contrast:focus,.u-button--contrast:hover{border:4px solid #fff}.u-button--contrast:focus,.u-button--contrast:hover{background-color:#fff;background-image:none;color:#000}.u-button--contrast:focus svg path,.u-button--contrast:hover svg path{fill:#000}.u-button--disabled,.u-button:disabled{background-color:transparent;background-image:none;border:4px solid #ccc;color:#000;cursor:default;font-weight:400;opacity:.7}.u-button--disabled svg,.u-button:disabled svg{fill:currentcolor}.u-button--disabled:visited,.u-button:disabled:visited{color:#000}.u-button--disabled:focus,.u-button--disabled:hover,.u-button:disabled:focus,.u-button:disabled:hover{border:4px solid #ccc;text-decoration:none}.u-button--disabled:focus,.u-button--disabled:hover,.u-button:disabled:focus,.u-button:disabled:hover{background-color:transparent;background-image:none;color:#000}.u-button--disabled:focus svg path,.u-button--disabled:hover svg path,.u-button:disabled:focus svg path,.u-button:disabled:hover svg path{fill:#000}.u-button--small,.u-button--xsmall{font-size:.875rem;padding:2px 8px}.u-button--small{padding:8px 16px}.u-button--large{font-size:1.125rem;padding:10px 35px}.u-button--full-width{display:flex;width:100%}.u-button--icon-left svg{margin-right:8px}.u-button--icon-right svg{margin-left:8px}.u-clear-both{clear:both}.u-container{margin:0 auto;max-width:1280px;padding:0 16px}.u-justify-content-space-between{justify-content:space-between}.u-display-none{display:none}.js .u-js-hide,.u-hide{display:none;visibility:hidden}.u-visually-hidden{clip:rect(0,0,0,0);border:0;clip-path:inset(50%);height:1px;overflow:hidden;padding:0;position:absolute!important;white-space:nowrap;width:1px}.u-icon{fill:currentcolor;display:inline-block;height:1em;transform:translate(0);vertical-align:text-top;width:1em}.u-list-reset{list-style:none;margin:0;padding:0}.u-ma-16{margin:16px}.u-mt-0{margin-top:0}.u-mt-24{margin-top:24px}.u-mt-32{margin-top:32px}.u-mb-8{margin-bottom:8px}.u-mb-32{margin-bottom:32px}.u-button-reset{background-color:transparent;border:0;padding:0}.u-sans-serif{font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif}.u-serif{font-family:Merriweather,serif}h1,h2,h4{-webkit-font-smoothing:antialiased}p{overflow-wrap:break-word;word-break:break-word}.u-h4{font-size:1.25rem;font-weight:700;line-height:1.2}.u-mbs-0{margin-block-start:0!important}.c-article-header{font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif}.c-article-identifiers{color:#6f6f6f;display:flex;flex-wrap:wrap;font-size:1rem;line-height:1.3;list-style:none;margin:0 0 8px;padding:0}.c-article-identifiers__item{border-right:1px solid #6f6f6f;list-style:none;margin-right:8px;padding-right:8px}.c-article-identifiers__item:last-child{border-right:0;margin-right:0;padding-right:0}@media only screen and (min-width:876px){.c-article-title{font-size:1.875rem;line-height:1.2}}.c-article-author-list{display:inline;font-size:1rem;list-style:none;margin:0 8px 0 0;padding:0;width:100%}.c-article-author-list__item{display:inline;padding-right:0}.c-article-author-list__show-more{display:none;margin-right:4px}.c-article-author-list__button,.js .c-article-author-list__item--hide,.js .c-article-author-list__show-more{display:none}.js .c-article-author-list--long .c-article-author-list__show-more,.js .c-article-author-list--long+.c-article-author-list__button{display:inline}@media only screen and (max-width:767px){.js .c-article-author-list__item--hide-small-screen{display:none}.js .c-article-author-list--short .c-article-author-list__show-more,.js .c-article-author-list--short+.c-article-author-list__button{display:inline}}#uptodate-client,.js .c-article-author-list--expanded .c-article-author-list__show-more{display:none!important}.js .c-article-author-list--expanded .c-article-author-list__item--hide-small-screen{display:inline!important}.c-article-author-list__button,.c-button-author-list{background:#ebf1f5;border:4px solid #ebf1f5;border-radius:20px;color:#666;font-size:.875rem;line-height:1.4;padding:2px 11px 2px 8px;text-decoration:none}.c-article-author-list__button svg,.c-button-author-list svg{margin:1px 4px 0 0}.c-article-author-list__button:hover,.c-button-author-list:hover{background:#025e8d;border-color:transparent;color:#fff}.c-article-body .c-article-access-provider{padding:8px 16px}.c-article-body .c-article-access-provider,.c-notes{border:1px solid #d5d5d5;border-image:initial;border-left:none;border-right:none;margin:24px 0}.c-article-body .c-article-access-provider__text{color:#555}.c-article-body .c-article-access-provider__text,.c-notes__text{font-size:1rem;margin-bottom:0;padding-bottom:2px;padding-top:2px;text-align:center}.c-article-body .c-article-author-affiliation__address{color:inherit;font-weight:700;margin:0}.c-article-body .c-article-author-affiliation__authors-list{list-style:none;margin:0;padding:0}.c-article-body .c-article-author-affiliation__authors-item{display:inline;margin-left:0}.c-article-authors-search{margin-bottom:24px;margin-top:0}.c-article-authors-search__item,.c-article-authors-search__title{font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif}.c-article-authors-search__title{color:#626262;font-size:1.05rem;font-weight:700;margin:0;padding:0}.c-article-authors-search__item{font-size:1rem}.c-article-authors-search__text{margin:0}.c-code-block{border:1px solid #fff;font-family:monospace;margin:0 0 24px;padding:20px}.c-code-block__heading{font-weight:400;margin-bottom:16px}.c-code-block__line{display:block;overflow-wrap:break-word;white-space:pre-wrap}.c-article-share-box{font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;margin-bottom:24px}.c-article-share-box__description{font-size:1rem;margin-bottom:8px}.c-article-share-box__no-sharelink-info{font-size:.813rem;font-weight:700;margin-bottom:24px;padding-top:4px}.c-article-share-box__only-read-input{border:1px solid #d5d5d5;box-sizing:content-box;display:inline-block;font-size:.875rem;font-weight:700;height:24px;margin-bottom:8px;padding:8px 10px}.c-article-share-box__additional-info{color:#626262;font-size:.813rem}.c-article-share-box__button{background:#fff;box-sizing:content-box;text-align:center}.c-article-share-box__button--link-like{background-color:transparent;border:0;color:#025e8d;cursor:pointer;font-size:.875rem;margin-bottom:8px;margin-left:10px}.c-article-associated-content__container .c-article-associated-content__collection-label{font-size:.875rem;line-height:1.4}.c-article-associated-content__container .c-article-associated-content__collection-title{line-height:1.3}.c-reading-companion{clear:both;min-height:389px}.c-reading-companion__figures-list,.c-reading-companion__references-list{list-style:none;min-height:389px;padding:0}.c-reading-companion__references-list--numeric{list-style:decimal inside}.c-reading-companion__figure-item{border-top:1px solid #d5d5d5;font-size:1rem;padding:16px 8px 16px 0}.c-reading-companion__figure-item:first-child{border-top:none;padding-top:8px}.c-reading-companion__reference-item{font-size:1rem}.c-reading-companion__reference-item:first-child{border-top:none}.c-reading-companion__reference-item a{word-break:break-word}.c-reading-companion__reference-citation{display:inline}.c-reading-companion__reference-links{font-size:.813rem;font-weight:700;list-style:none;margin:8px 0 0;padding:0;text-align:right}.c-reading-companion__reference-links>a{display:inline-block;padding-left:8px}.c-reading-companion__reference-links>a:first-child{display:inline-block;padding-left:0}.c-reading-companion__figure-title{display:block;font-size:1.25rem;font-weight:700;line-height:1.2;margin:0 0 8px}.c-reading-companion__figure-links{display:flex;justify-content:space-between;margin:8px 0 0}.c-reading-companion__figure-links>a{align-items:center;display:flex}.c-article-section__figure-caption{display:block;margin-bottom:8px;word-break:break-word}.c-article-section__figure .video,p.app-article-masthead__access--above-download{margin:0 0 16px}.c-article-section__figure-description{font-size:1rem}.c-article-section__figure-description>*{margin-bottom:0}.c-cod{display:block;font-size:1rem;width:100%}.c-cod__form{background:#ebf0f3}.c-cod__prompt{font-size:1.125rem;line-height:1.3;margin:0 0 24px}.c-cod__label{display:block;margin:0 0 4px}.c-cod__row{display:flex;margin:0 0 16px}.c-cod__row:last-child{margin:0}.c-cod__input{border:1px solid #d5d5d5;border-radius:2px;flex-shrink:0;margin:0;padding:13px}.c-cod__input--submit{background-color:#025e8d;border:1px solid #025e8d;color:#fff;flex-shrink:1;margin-left:8px;transition:background-color .2s ease-out 0s,color .2s ease-out 0s}.c-cod__input--submit-single{flex-basis:100%;flex-shrink:0;margin:0}.c-cod__input--submit:focus,.c-cod__input--submit:hover{background-color:#fff;color:#025e8d}.save-data .c-article-author-institutional-author__sub-division,.save-data .c-article-equation__number,.save-data .c-article-figure-description,.save-data .c-article-fullwidth-content,.save-data .c-article-main-column,.save-data .c-article-satellite-article-link,.save-data .c-article-satellite-subtitle,.save-data .c-article-table-container,.save-data .c-blockquote__body,.save-data .c-code-block__heading,.save-data .c-reading-companion__figure-title,.save-data .c-reading-companion__reference-citation,.save-data .c-site-messages--nature-briefing-email-variant .serif,.save-data .c-site-messages--nature-briefing-email-variant.serif,.save-data .serif,.save-data .u-serif,.save-data h1,.save-data h2,.save-data h3{font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif}.c-pdf-download__link{display:flex;flex:1 1 0%;padding:13px 24px}.c-pdf-download__link:hover{text-decoration:none}@media only screen and (min-width:768px){.c-context-bar--sticky .c-pdf-download__link{align-items:center;flex:1 1 183px}}@media only screen and (max-width:320px){.c-context-bar--sticky .c-pdf-download__link{padding:16px}}.c-article-body .c-article-recommendations-list,.c-book-body .c-article-recommendations-list{display:flex;flex-direction:row;gap:16px 16px;margin:0;max-width:100%;padding:16px 0 0}.c-article-body .c-article-recommendations-list__item,.c-book-body .c-article-recommendations-list__item{flex:1 1 0%}@media only screen and (max-width:767px){.c-article-body .c-article-recommendations-list,.c-book-body .c-article-recommendations-list{flex-direction:column}}.c-article-body .c-article-recommendations-card__authors{display:none;font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;font-size:.875rem;line-height:1.5;margin:0 0 8px}@media only screen and (max-width:767px){.c-article-body .c-article-recommendations-card__authors{display:block;margin:0}}.c-article-body .c-article-history{margin-top:24px}.app-article-metrics-bar p{margin:0}.app-article-masthead{display:flex;flex-direction:column;gap:16px 16px;padding:16px 0 24px}.app-article-masthead__info{display:flex;flex-direction:column;flex-grow:1}.app-article-masthead__brand{border-top:1px solid hsla(0,0%,100%,.8);display:flex;flex-direction:column;flex-shrink:0;gap:8px 8px;min-height:96px;padding:16px 0 0}.app-article-masthead__brand img{border:1px solid #fff;border-radius:8px;box-shadow:0 4px 15px 0 hsla(0,0%,50%,.25);height:auto;left:0;position:absolute;width:72px}.app-article-masthead__journal-link{display:block;font-size:1.125rem;font-weight:700;margin:0 0 8px;max-width:400px;padding:0 0 0 88px;position:relative}.app-article-masthead__journal-title{-webkit-box-orient:vertical;-webkit-line-clamp:3;display:-webkit-box;overflow:hidden}.app-article-masthead__submission-link{align-items:center;display:flex;font-size:1rem;gap:4px 4px;margin:0 0 0 88px}.app-article-masthead__access{align-items:center;display:flex;flex-wrap:wrap;font-size:.875rem;font-weight:300;gap:4px 4px;margin:0}.app-article-masthead__buttons{display:flex;flex-flow:column wrap;gap:16px 16px}.app-article-masthead__access svg,.app-masthead--pastel .c-pdf-download .u-button--primary svg,.app-masthead--pastel .c-pdf-download .u-button--secondary svg,.c-context-bar--sticky .c-context-bar__container .c-pdf-download .u-button--primary svg,.c-context-bar--sticky .c-context-bar__container .c-pdf-download .u-button--secondary svg{fill:currentcolor}.app-article-masthead a{color:#fff}.app-masthead--pastel .c-pdf-download .u-button--primary,.c-context-bar--sticky .c-context-bar__container .c-pdf-download .u-button--primary{background-color:#025e8d;background-image:none;border:2px solid transparent;box-shadow:none;color:#fff;font-weight:700}.app-masthead--pastel .c-pdf-download .u-button--primary:visited,.c-context-bar--sticky .c-context-bar__container .c-pdf-download .u-button--primary:visited{color:#fff}.app-masthead--pastel .c-pdf-download .u-button--primary:hover,.c-context-bar--sticky .c-context-bar__container .c-pdf-download .u-button--primary:hover{text-decoration:none}.app-masthead--pastel .c-pdf-download .u-button--primary:focus,.c-context-bar--sticky .c-context-bar__container .c-pdf-download .u-button--primary:focus{border:4px solid #fc0;box-shadow:none;outline:0;text-decoration:none}.app-masthead--pastel .c-pdf-download .u-button--primary:focus,.app-masthead--pastel .c-pdf-download .u-button--primary:hover,.c-context-bar--sticky .c-context-bar__container .c-pdf-download .u-button--primary:focus,.c-context-bar--sticky .c-context-bar__container .c-pdf-download .u-button--primary:hover{background-color:#fff;background-image:none;color:#01324b}.app-masthead--pastel .c-pdf-download .u-button--primary:hover,.c-context-bar--sticky .c-context-bar__container .c-pdf-download .u-button--primary:hover{background:0 0;border:2px solid #025e8d;box-shadow:none;color:#025e8d}.app-masthead--pastel .c-pdf-download .u-button--secondary,.c-context-bar--sticky .c-context-bar__container .c-pdf-download .u-button--secondary{background:0 0;border:2px solid #025e8d;color:#025e8d;font-weight:700}.app-masthead--pastel .c-pdf-download .u-button--secondary:visited,.c-context-bar--sticky .c-context-bar__container .c-pdf-download .u-button--secondary:visited{color:#01324b}.app-masthead--pastel .c-pdf-download .u-button--secondary:hover,.c-context-bar--sticky .c-context-bar__container .c-pdf-download .u-button--secondary:hover{background-color:#01324b;background-color:#025e8d;border:2px solid transparent;box-shadow:none;color:#fff}.app-masthead--pastel .c-pdf-download .u-button--secondary:focus,.c-context-bar--sticky .c-context-bar__container .c-pdf-download .u-button--secondary:focus{background-color:#fff;background-image:none;border:4px solid #fc0;color:#01324b}@media only screen and (min-width:768px){.app-article-masthead{flex-direction:row;gap:64px 64px;padding:24px 0}.app-article-masthead__brand{border:0;padding:0}.app-article-masthead__brand img{height:auto;position:static;width:auto}.app-article-masthead__buttons{align-items:center;flex-direction:row;margin-top:auto}.app-article-masthead__journal-link{display:flex;flex-direction:column;gap:24px 24px;margin:0 0 8px;padding:0}.app-article-masthead__submission-link{margin:0}}@media only screen and (min-width:1024px){.app-article-masthead__brand{flex-basis:400px}}.app-article-masthead .c-article-identifiers{font-size:.875rem;font-weight:300;line-height:1;margin:0 0 8px;overflow:hidden;padding:0}.app-article-masthead .c-article-identifiers--cite-list{margin:0 0 16px}.app-article-masthead .c-article-identifiers *{color:#fff}.app-article-masthead .c-cod{display:none}.app-article-masthead .c-article-identifiers__item{border-left:1px solid #fff;border-right:0;margin:0 17px 8px -9px;padding:0 0 0 8px}.app-article-masthead .c-article-identifiers__item--cite{border-left:0}.app-article-metrics-bar{display:flex;flex-wrap:wrap;font-size:1rem;padding:16px 0 0;row-gap:24px}.app-article-metrics-bar__item{padding:0 16px 0 0}.app-article-metrics-bar__count{font-weight:700}.app-article-metrics-bar__label{font-weight:400;padding-left:4px}.app-article-metrics-bar__icon{height:auto;margin-right:4px;margin-top:-4px;width:auto}.app-article-metrics-bar__arrow-icon{margin:4px 0 0 4px}.app-article-metrics-bar a{color:#000}.app-article-metrics-bar .app-article-metrics-bar__item--metrics{padding-right:0}.app-overview-section .c-article-author-list,.app-overview-section__authors{line-height:2}.app-article-metrics-bar{margin-top:8px}.c-book-toc-pagination+.c-book-section__back-to-top{margin-top:0}.c-article-body .c-article-access-provider__text--chapter{color:#222;font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;padding:20px 0}.c-article-body .c-article-access-provider__text--chapter svg.c-status-message__icon{fill:#003f8d;vertical-align:middle}.c-article-body-section__content--separator{padding-top:40px}.c-pdf-download__link{max-height:44px}.app-article-access .u-button--primary,.app-article-access .u-button--primary:visited{color:#fff}.c-article-sidebar{display:none}@media only screen and (min-width:1024px){.c-article-sidebar{display:block}}.c-cod__form{border-radius:12px}.c-cod__label{font-size:.875rem}.c-cod .c-status-message{align-items:center;justify-content:center;margin-bottom:16px;padding-bottom:16px}@media only screen and (min-width:1024px){.c-cod .c-status-message{align-items:inherit}}.c-cod .c-status-message__icon{margin-top:4px}.c-cod .c-cod__prompt{font-size:1rem;margin-bottom:16px}.c-article-body .app-article-access,.c-book-body .app-article-access{display:block}@media only screen and (min-width:1024px){.c-article-body .app-article-access,.c-book-body .app-article-access{display:none}}.c-article-body .app-card-service{margin-bottom:32px}@media only screen and (min-width:1024px){.c-article-body .app-card-service{display:none}}.app-article-access .buybox__buy .u-button--secondary,.app-article-access .u-button--primary,.c-cod__row .u-button--primary{background-color:#025e8d;border:2px solid #025e8d;box-shadow:none;font-size:1rem;font-weight:700;gap:8px 8px;justify-content:center;line-height:1.5;padding:8px 24px}.app-article-access .buybox__buy .u-button--secondary,.app-article-access .u-button--primary:hover,.c-cod__row .u-button--primary:hover{background-color:#fff;color:#025e8d}.app-article-access .buybox__buy .u-button--secondary:hover{background-color:#025e8d;color:#fff}.buybox__buy .c-notes__text{color:#666;font-size:.875rem;padding:0 16px 8px}.c-cod__input{flex-basis:auto;width:100%}.c-article-title{font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;font-size:2.25rem;font-weight:700;line-height:1.2;margin:12px 0}.c-reading-companion__figure-item figure{margin:0}@media only screen and (min-width:768px){.c-article-title{margin:16px 0}}.app-article-access{border:1px solid #c5e0f4;border-radius:12px}.app-article-access__heading{border-bottom:1px solid #c5e0f4;font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;font-size:1.125rem;font-weight:700;margin:0;padding:16px;text-align:center}.app-article-access .buybox__info svg{vertical-align:middle}.c-article-body .app-article-access p{margin-bottom:0}.app-article-access .buybox__info{font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;font-size:1rem;margin:0}.app-article-access{margin:0 0 32px}@media only screen and (min-width:1024px){.app-article-access{margin:0 0 24px}}.c-status-message{font-size:1rem}.c-article-body{font-size:1.125rem}.c-article-body dl,.c-article-body ol,.c-article-body p,.c-article-body ul{margin-bottom:32px;margin-top:0}.c-article-access-provider__text:last-of-type,.c-article-body .c-notes__text:last-of-type{margin-bottom:0}.c-article-body ol p,.c-article-body ul p{margin-bottom:16px}.c-article-section__figure-caption{font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif}.c-reading-companion__figure-item{border-top-color:#c5e0f4}.c-reading-companion__sticky{max-width:400px}.c-article-section .c-article-section__figure-description>*{font-size:1rem;margin-bottom:16px}.c-reading-companion__reference-item{border-top:1px solid #d5d5d5;padding:16px 0}.c-reading-companion__reference-item:first-child{padding-top:0}.c-article-share-box__button,.js .c-article-authors-search__item .c-article-button{background:0 0;border:2px solid #025e8d;border-radius:32px;box-shadow:none;color:#025e8d;font-size:1rem;font-weight:700;line-height:1.5;margin:0;padding:8px 24px;transition:all .2s ease 0s}.c-article-authors-search__item .c-article-button{width:100%}.c-pdf-download .u-button{background-color:#fff;border:2px solid #fff;color:#01324b;justify-content:center}.c-context-bar__container .c-pdf-download .u-button svg,.c-pdf-download .u-button svg{fill:currentcolor}.c-pdf-download .u-button:visited{color:#01324b}.c-pdf-download .u-button:hover{border:4px solid #01324b;box-shadow:none}.c-pdf-download .u-button:focus,.c-pdf-download .u-button:hover{background-color:#01324b}.c-pdf-download .u-button:focus svg path,.c-pdf-download .u-button:hover svg path{fill:#fff}.c-context-bar__container .c-pdf-download .u-button{background-image:none;border:2px solid;color:#fff}.c-context-bar__container .c-pdf-download .u-button:visited{color:#fff}.c-context-bar__container .c-pdf-download .u-button:hover{text-decoration:none}.c-context-bar__container .c-pdf-download .u-button:focus{box-shadow:none;outline:0;text-decoration:none}.c-context-bar__container .c-pdf-download .u-button:focus,.c-context-bar__container .c-pdf-download .u-button:hover{background-color:#fff;background-image:none;color:#01324b}.c-context-bar__container .c-pdf-download .u-button:focus svg path,.c-context-bar__container .c-pdf-download .u-button:hover svg path{fill:#01324b}.c-context-bar__container .c-pdf-download .u-button,.c-pdf-download .u-button{box-shadow:none;font-size:1rem;font-weight:700;line-height:1.5;padding:8px 24px}.c-context-bar__container .c-pdf-download .u-button{background-color:#025e8d}.c-pdf-download .u-button:hover{border:2px solid #fff}.c-pdf-download .u-button:focus,.c-pdf-download .u-button:hover{background:0 0;box-shadow:none;color:#fff}.c-context-bar__container .c-pdf-download .u-button:hover{border:2px solid #025e8d;box-shadow:none;color:#025e8d}.c-context-bar__container .c-pdf-download .u-button:focus,.c-pdf-download .u-button:focus{border:2px solid #025e8d}.c-article-share-box__button:focus:focus,.c-article__pill-button:focus:focus,.c-context-bar__container .c-pdf-download .u-button:focus:focus,.c-pdf-download .u-button:focus:focus{outline:3px solid #08c;will-change:transform}.c-pdf-download__link .u-icon{padding-top:0}.c-bibliographic-information__column button{margin-bottom:16px}.c-article-body .c-article-author-affiliation__list p,.c-article-body .c-article-author-information__list p,figure{margin:0}.c-article-share-box__button{margin-right:16px}.c-status-message--boxed{border-radius:12px}.c-article-associated-content__collection-title{font-size:1rem}.app-card-service__description,.c-article-body .app-card-service__description{color:#222;margin-bottom:0;margin-top:8px}.app-article-access__subscriptions a,.app-article-access__subscriptions a:visited,.app-book-series-listing__item a,.app-book-series-listing__item a:hover,.app-book-series-listing__item a:visited,.c-article-author-list a,.c-article-author-list a:visited,.c-article-buy-box a,.c-article-buy-box a:visited,.c-article-peer-review a,.c-article-peer-review a:visited,.c-article-satellite-subtitle a,.c-article-satellite-subtitle a:visited,.c-breadcrumbs__link,.c-breadcrumbs__link:hover,.c-breadcrumbs__link:visited{color:#000}.c-article-author-list svg{height:24px;margin:0 0 0 6px;width:24px}.c-article-header{margin-bottom:32px}@media only screen and (min-width:876px){.js .c-ad--conditional{display:block}}.u-lazy-ad-wrapper{background-color:#fff;display:none;min-height:149px}@media only screen and (min-width:876px){.u-lazy-ad-wrapper{display:block}}p.c-ad__label{margin-bottom:4px}.c-ad--728x90{background-color:#fff;border-bottom:2px solid #cedbe0} } </style>
<style>@media only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark) {  .eds-c-header__brand img{height:24px;width:203px}.app-article-masthead__journal-link img{height:93px;width:72px}@media only screen and (min-width:769px){.app-article-masthead__journal-link img{height:161px;width:122px}} } </style>
<link data-inline-css-source="critical-css" data-test="critical-css-handler" href="/oscar-static/app-springerlink/css/core-darwin-3c86549cfc.css" media="all" onload="this.media='all';this.onload=null" rel="stylesheet"/>
<link data-inline-css-source="critical-css" data-test="critical-css-handler" href="/oscar-static/app-springerlink/css/enhanced-darwin-article-6bccf6e9b8.css" media="only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark)" onload="this.media='only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark)';this.onload=null" rel="stylesheet"/>
<script async="" src="https://pagead2.googlesyndication.com/tag/js/gpt.js" type="text/javascript"></script><script async="" src="//cdn.jsdelivr.net/npm/mathjax@2.7.5/MathJax.js?config=TeX-AMS-MML_SVG.js"></script><script async="" src="https://www.googletagmanager.com/gtm.js?id=GTM-MRVXSHQ"></script><script type="text/javascript">
        config = {
            env: 'live',
            site: '10994.springer.com',
            siteWithPath: '10994.springer.com' + window.location.pathname,
            twitterHashtag: '10994',
            cmsPrefix: 'https://studio-cms.springernature.com/studio/',
            
            
            
            
            publisherBrand: 'Springer',
            mustardcut: false
        };
    </script><script onload="initGTM(window,document,'script','dataLayer','GTM-MRVXSHQ')" src="https://cmp.springer.com/production_live/en/consent-bundle-17-52.js"></script>
<script>
        window.dataLayer = [{"GA Key":"UA-26408784-1","DOI":"10.1007/s10994-023-06487-7","Page":"article","springerJournal":true,"Publishing Model":"Hybrid Access","page":{"attributes":{"environment":"live"}},"Country":"KR","japan":false,"doi":"10.1007-s10994-023-06487-7","Journal Id":10994,"Journal Title":"Machine Learning","imprint":"Springer","Keywords":"Continual learning, Regularization/prior-based approach, Variational dropout, Local and global variables","kwrd":["Continual_learning","Regularization/prior-based_approach","Variational_dropout","Local_and_global_variables"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["cobranding"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["3001948335","3000120396","3000256740","3000522537","3000171421","3000699960","3991460179","3000251005","3000175460"],"businessPartnerIDString":"3001948335|3000120396|3000256740|3000522537|3000171421|3000699960|3991460179|3000251005|3000175460"}},"Access Type":"subscription","Bpids":"3001948335, 3000120396, 3000256740, 3000522537, 3000171421, 3000699960, 3991460179, 3000251005, 3000175460","Bpnames":"South Korea Trial Consortium, Chung Ang University, 7029 SpringerLink South Korea KESLI Korean Journal Package, South Korea Trial Consortium, 5539 SpringerLink South Korea KESLI Full OJA Consortium -, 9823  SpringerLink South Korea Shinwon, South Korea Full eJournal Con - Academic 2015-2017, 8064 South Korea KESLI Korean Consortium, 5539 SpringerLink South Korea KESLI Full eJournal Consortium - Academic","BPID":["3001948335","3000120396","3000256740","3000522537","3000171421","3000699960","3991460179","3000251005","3000175460"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10994-023-06487-7","Full HTML":"Y","Subject Codes":["SCI","SCI21010","SCT19000","SCI21000","SCI19000","SCI21040"],"pmc":["I","I21010","T19000","I21000","I19000","I21040"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1573-0565","pissn":"0885-6125"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Machine Learning","2":"Control, Robotics, Mechatronics","3":"Artificial Intelligence","4":"Simulation and Modeling ","5":"Natural Language Processing (NLP)"},"secondarySubjectCodes":{"1":"I21010","2":"T19000","3":"I21000","4":"I19000","5":"I21040"}},"sucode":"SC6","articleType":"Article"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article"}];
    </script>
<script data-test="springer-link-article-datalayer">
        window.dataLayer = window.dataLayer || [];
        window.dataLayer.push({
            ga4MeasurementId: 'G-B3E4QL2TPR',
            ga360TrackingId: 'UA-26408784-1',
            twitterId: 'o47a7',
            baiduId: 'aef3043f025ccf2305af8a194652d70b',
            ga4ServerUrl: 'https://collect.springer.com',
            imprint: 'springerlink',
                page: {
                    attributes:{
                        featureFlags: [{ name: 'darwin-orion', active: true }, { name: 'chapter-books-recs', active: true } ],
                        darwinAvailable: true
                    }
                }
            
        });
    </script>
<script>
    (function(w, d) {
        w.config = w.config || {};
        w.config.mustardcut = false;

        
        if (w.matchMedia && w.matchMedia('only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark)').matches) {
            w.config.mustardcut = true;
            d.classList.add('js');
            d.classList.remove('grade-c');
            d.classList.remove('no-js');
        }
    })(window, document.documentElement);
</script>
<script class="js-entry">
    if (window.config.mustardcut) {
        (function(w, d) {
            
            
            
                window.Component = {};
                window.suppressShareButton = false;
                window.onArticlePage = true;
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                {'src': '/oscar-static/js/polyfill-es5-bundle-572d4fec60.js', 'async': false}
            ];

            var bodyScripts = [
                
                    {'src': '/oscar-static/js/global-article-es5-bundle-dad1690b0d.js', 'async': false, 'module': false},
                    {'src': '/oscar-static/js/global-article-es6-bundle-e7d03c4cb3.js', 'async': false, 'module': true}
                
                
                    
                
                
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i = 0; i < headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i = 0; i < bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        })(window, document);
    }
</script><script src="/oscar-static/js/polyfill-es5-bundle-572d4fec60.js"></script>
<script data-cc-script="C03" data-src="https://cdn.optimizely.com/js/27195530232.js"></script>
<script data-test="gtm-head">
    window.initGTM = function() {
        if (window.config.mustardcut) {
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                    j = d.createElement(s),
                    dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-MRVXSHQ');
        }
    }
</script>
<script>
(function (w, d, t) {
    function cc() {
        var h = w.location.hostname;
        var e = d.createElement(t),
        s = d.getElementsByTagName(t)[0];

        
        if (h.indexOf('springer.com') > -1 && h.indexOf('biomedcentral.com') === -1 && h.indexOf('springeropen.com') === -1) {
            if (h.indexOf('link-qa.springer.com') > -1 || h.indexOf('test-www.springer.com') > -1) {
                e.src = 'https://cmp.springer.com/production_live/en/consent-bundle-17-52.js';
                e.setAttribute('onload', "initGTM(window,document,'script','dataLayer','GTM-MRVXSHQ')");
            } else {
                e.src = 'https://cmp.springer.com/production_live/en/consent-bundle-17-52.js';
                e.setAttribute('onload', "initGTM(window,document,'script','dataLayer','GTM-MRVXSHQ')");
            }
        } else if (h.indexOf('biomedcentral.com') > -1) {
            if (h.indexOf('biomedcentral.com.qa') > -1) {
                e.src = 'https://cmp.biomedcentral.com/production_live/en/consent-bundle-15-36.js';
                e.setAttribute('onload', "initGTM(window,document,'script','dataLayer','GTM-MRVXSHQ')");
            } else {
                e.src = 'https://cmp.biomedcentral.com/production_live/en/consent-bundle-15-36.js';
                e.setAttribute('onload', "initGTM(window,document,'script','dataLayer','GTM-MRVXSHQ')");
            }
        } else if (h.indexOf('springeropen.com') > -1) {
            if (h.indexOf('springeropen.com.qa') > -1) {
                e.src = 'https://cmp.springernature.com/production_live/en/consent-bundle-16-34.js';
                e.setAttribute('onload', "initGTM(window,document,'script','dataLayer','GTM-MRVXSHQ')");
            } else {
                e.src = 'https://cmp.springernature.com/production_live/en/consent-bundle-16-34.js';
                e.setAttribute('onload', "initGTM(window,document,'script','dataLayer','GTM-MRVXSHQ')");
            }
        } else if (h.indexOf('springernature.com') > -1) {
            if (h.indexOf('beta-qa.springernature.com') > -1) {
                e.src = 'https://cmp.springernature.com/production_live/en/consent-bundle-49-43.js';
                e.setAttribute('onload', "initGTM(window,document,'script','dataLayer','GTM-NK22KLS')");
            } else {
                e.src = 'https://cmp.springernature.com/production_live/en/consent-bundle-49-43.js';
                e.setAttribute('onload', "initGTM(window,document,'script','dataLayer','GTM-NK22KLS')");
            }
        } else {
            e.src = '/oscar-static/js/cookie-consent-es5-bundle-cb57c2c98a.js';
            e.setAttribute('data-consent', h);
        }
        s.insertAdjacentElement('afterend', e);
    }

    cc();
})(window, document, 'script');
</script>
<link href="https://link.springer.com/article/10.1007/s10994-023-06487-7" rel="canonical"/>
<script type="application/ld+json">{"mainEntity":{"headline":"Continual variational dropout: a view of auxiliary local variables in continual learning","description":"Regularization/prior-based approach appears to be one of the critical strategies in continual learning, considering its mechanism for preserving and preventing forgetting the learned knowledge. Without any retraining on previous data or extending the network architecture, the mechanism works by setting a constraint on the important weights of previous tasks when learning the current task. Regularization/prior approach, on the other hand, suffers the challenge of weights being moved intensely to the parameter region, in which the model achieves good performance for the latest task but poor ones for earlier tasks. To that end, we suggest a novel solution to this problem by continually applying variational dropout (CVD), thereby generating task-specific local variables that work as modifying factors for the global variables to fit the task. In particular, as we impose a variational distribution on the auxiliary local variables employed as multiplicative noise to the layers’ input, the model enables the global variables to be retained in a good region for all tasks and reduces the forgetting phenomenon. Furthermore, we obtained theoretical properties that are currently unavailable in existing methods: (1) uncorrelated likelihoods between different data instances reduce the high variance of stochastic gradient variational Bayes; (2) correlated pre-activation improves the representation ability for each task; and (3) data-dependent regularization assures the global variables to be preserved in a good region for all tasks. Throughout our extensive results, adding the local variables shows its significant advantage in enhancing the performance of regularization/prior-based methods by considerable magnitudes on numerous datasets. Specifically, it brings several standard baselines closer to state-of-the-art results.","datePublished":"2023-12-07T00:00:00Z","dateModified":"2023-12-07T00:00:00Z","pageStart":"281","pageEnd":"323","sameAs":"https://doi.org/10.1007/s10994-023-06487-7","keywords":["Continual learning","Regularization/prior-based approach","Variational dropout","Local and global variables","Machine Learning","Control","Robotics","Mechatronics","Artificial Intelligence","Simulation and Modeling","Natural Language Processing (NLP)"],"image":["https://media.springernature.com/lw1200/springer-static/image/art%3A10.1007%2Fs10994-023-06487-7/MediaObjects/10994_2023_6487_Fig1_HTML.png","https://media.springernature.com/lw1200/springer-static/image/art%3A10.1007%2Fs10994-023-06487-7/MediaObjects/10994_2023_6487_Fig2_HTML.png","https://media.springernature.com/lw1200/springer-static/image/art%3A10.1007%2Fs10994-023-06487-7/MediaObjects/10994_2023_6487_Fig3_HTML.png","https://media.springernature.com/lw1200/springer-static/image/art%3A10.1007%2Fs10994-023-06487-7/MediaObjects/10994_2023_6487_Fig4_HTML.png","https://media.springernature.com/lw1200/springer-static/image/art%3A10.1007%2Fs10994-023-06487-7/MediaObjects/10994_2023_6487_Fig5_HTML.png","https://media.springernature.com/lw1200/springer-static/image/art%3A10.1007%2Fs10994-023-06487-7/MediaObjects/10994_2023_6487_Fig6_HTML.png","https://media.springernature.com/lw1200/springer-static/image/art%3A10.1007%2Fs10994-023-06487-7/MediaObjects/10994_2023_6487_Fig7_HTML.png","https://media.springernature.com/lw1200/springer-static/image/art%3A10.1007%2Fs10994-023-06487-7/MediaObjects/10994_2023_6487_Fig8_HTML.png","https://media.springernature.com/lw1200/springer-static/image/art%3A10.1007%2Fs10994-023-06487-7/MediaObjects/10994_2023_6487_Fig9_HTML.png","https://media.springernature.com/lw1200/springer-static/image/art%3A10.1007%2Fs10994-023-06487-7/MediaObjects/10994_2023_6487_Fig10_HTML.png","https://media.springernature.com/lw1200/springer-static/image/art%3A10.1007%2Fs10994-023-06487-7/MediaObjects/10994_2023_6487_Fig11_HTML.png","https://media.springernature.com/lw1200/springer-static/image/art%3A10.1007%2Fs10994-023-06487-7/MediaObjects/10994_2023_6487_Fig12_HTML.png","https://media.springernature.com/lw1200/springer-static/image/art%3A10.1007%2Fs10994-023-06487-7/MediaObjects/10994_2023_6487_Fig13_HTML.png","https://media.springernature.com/lw1200/springer-static/image/art%3A10.1007%2Fs10994-023-06487-7/MediaObjects/10994_2023_6487_Fig14_HTML.png"],"isPartOf":{"name":"Machine Learning","issn":["1573-0565","0885-6125"],"volumeNumber":"113","@type":["Periodical","PublicationVolume"]},"publisher":{"name":"Springer US","logo":{"url":"https://www.springernature.com/app-sn/public/images/logo-springernature.png","@type":"ImageObject"},"@type":"Organization"},"author":[{"name":"Nam Le Hai","affiliation":[{"name":"Hanoi University of Science and Technology","address":{"name":"School of Information and Communication Technology, Hanoi University of Science and Technology, Hanoi, Vietnam","@type":"PostalAddress"},"@type":"Organization"},{"name":"FPT Software AI Center","address":{"name":"FPT Software AI Center, Hanoi, Vietnam","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Trang Nguyen","affiliation":[{"name":"Hanoi University of Science and Technology","address":{"name":"School of Information and Communication Technology, Hanoi University of Science and Technology, Hanoi, Vietnam","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Linh Ngo Van","url":"http://orcid.org/0000-0002-0011-5137","affiliation":[{"name":"Hanoi University of Science and Technology","address":{"name":"School of Information and Communication Technology, Hanoi University of Science and Technology, Hanoi, Vietnam","@type":"PostalAddress"},"@type":"Organization"}],"email":"linhnv@soict.hust.edu.vn","@type":"Person"},{"name":"Thien Huu Nguyen","affiliation":[{"name":"University of Oregon","address":{"name":"University of Oregon, Eugene, USA","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Khoat Than","affiliation":[{"name":"Hanoi University of Science and Technology","address":{"name":"School of Information and Communication Technology, Hanoi University of Science and Technology, Hanoi, Vietnam","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"}],"isAccessibleForFree":false,"hasPart":{"isAccessibleForFree":false,"cssSelector":".main-content","@type":"WebPageElement"},"@type":"ScholarlyArticle"},"@context":"https://schema.org","@type":"WebPage"}</script>
<style type="text/css">.cc-banner{background-color:#01324b;border:none!important;bottom:0;box-sizing:border-box;color:#fff!important;left:0;line-height:1.3;margin:auto 0 0;max-width:100%;outline:0;overflow:visible;padding:0;position:fixed;right:0;width:100%;z-index:99999}.cc-banner::backdrop{background-color:#0000004d}.cc-banner *{color:inherit!important}.cc-banner:focus{box-shadow:none;outline:0}.cc-banner a{color:#fff!important;text-decoration:underline}.cc-banner a:active,.cc-banner a:focus,.cc-banner a:hover{color:inherit;text-decoration:none}.cc-banner a:focus{outline:3px solid #08c!important}.cc-banner h2,.cc-banner h3,.cc-banner h4,.cc-banner h5,.cc-banner h6{font-family:sans-serif;font-style:normal;font-weight:700;margin:0 0 .5em}.cc-banner .cc-h2,.cc-banner h2{font-size:18px}.cc-banner .cc-h3,.cc-banner h3{font-size:16px}.cc-banner .cc-h4,.cc-banner .cc-h5,.cc-banner .cc-h6,.cc-banner h4,.cc-banner h5,.cc-banner h6{font-size:14px}.cc-banner .cc-button{font-size:16px}.cc-banner__content{background-color:#01324b;display:flex;flex-direction:column;margin:0 auto;max-height:90vh;max-width:100%;padding:16px;position:relative}.cc-banner__content:focus{outline:0}@media (min-width:680px){.cc-banner__content{padding:12px}}@media (min-width:980px){.cc-banner__content{max-height:60vh;padding-bottom:20px;padding-top:20px}}@media (min-width:1320px){.cc-banner__content{padding-bottom:40px;padding-top:40px}}.cc-banner__container{display:flex;flex-direction:column;margin:auto;max-width:1320px;overflow:auto}.cc-banner__title{background:none!important;flex-shrink:0;font-size:18px!important;font-size:22px!important;font-weight:700!important;letter-spacing:normal;margin:0 0 12px!important}@media (min-width:680px){.cc-banner__title{font-size:24px!important;margin:0 0 16px!important}}@media (min-width:1320px){.cc-banner__title{font-size:26px!important;margin:0 0 24px!important}}.cc-banner__body{display:flex;flex-direction:column;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif!important;overflow-x:hidden;overflow-y:auto;padding:3px 3px 16px}@media (min-width:980px){.cc-banner__body{flex-direction:row}}.cc-banner__policy p{font-size:16px!important;margin:0;max-width:none}.cc-banner__policy p:not(:last-child){margin:0 0 16px}@media (min-width:980px){.cc-banner__policy p:not(:last-child){margin:0 0 24px}}.cc-banner__policy p a{font-size:16px!important;font-weight:700}.cc-banner__footer{box-shadow:none;flex-shrink:0;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif !important!important;margin:0;padding:12px 0 3px 3px;position:relative}@media (min-width:380px){.cc-banner__footer{align-items:stretch;display:flex;flex-wrap:wrap}}@media (min-width:680px){.cc-banner__footer{box-shadow:none;flex-wrap:nowrap;width:40%}}.cc-banner__footer .cc-banner__button{flex:1 1 auto;font-weight:700;overflow:hidden;padding:.5em 1em;width:100%}@media (min-width:680px){.cc-banner__footer .cc-banner__button{max-width:275px}}.cc-banner__footer .cc-banner__button:not(:last-child){margin-bottom:12px}@media (min-width:680px){.cc-banner__footer .cc-banner__button:not(:last-child){margin-bottom:0;margin-right:16px}}@media (min-width:980px){.cc-banner__footer .cc-banner__button:not(:last-child){margin-right:24px}}.cc-banner__button-preferences{padding-left:0;padding-right:0}@media (min-width:380px){.cc-banner__button-preferences{flex:0 0 auto;margin:auto}}@media (min-width:680px){.cc-banner__button-preferences{margin:0}}@media (min-width:380px) and (max-width:680px){.cc-banner__button-break{display:block}}@media (min-width:680px){.cc-banner--is-tcf .cc-banner__footer{width:auto}}@media (min-width:980px){.cc-banner--is-tcf .cc-banner__footer{padding-right:48px;width:66%}}.cc-banner--is-tcf .cc-banner__stacks-intro{font-size:16px!important}.cc-banner--is-tcf .cc-banner__stacks{margin:16px 0 0}@media (min-width:980px){.cc-banner--is-tcf .cc-banner__stacks{display:flex;flex:0 0 33%;flex-direction:column;margin:0 0 0 48px;overflow:auto}}.cc-banner--is-tcf .cc-banner__stacks-intro{font-size:16px;margin:0 0 12px}.cc-banner--is-tcf .cc-banner__stacks-details{padding:0 3px}.cc-banner--is-tcf .cc-banner .cc-details{border-color:#fff3!important}.cc-box{border-radius:10px;padding:12px}.cc-box--info{background-color:#eff6fb}.cc-box--light{background-color:#faf9f6}.cc-button{border:2px solid #0000;border-radius:32px!important;color:inherit;cursor:pointer;font-size:14px;font-weight:700!important;left:auto;line-height:1.2;margin:0;padding:.5em 1em;right:auto;transition:all .2s}@media (min-width:680px){.cc-button{font-size:14px;padding:.75em 1em}}.cc-button--sm{font-size:12px;padding:2px 8px!important}.cc-button--primary{background-color:#025e8d!important;border-color:#025e8d!important;color:#fff!important}.cc-button--primary:focus{outline:3px solid #08c}.cc-button--primary:focus,.cc-button--primary:hover,.cc-button--secondary{background-color:#fff!important;border-color:#025e8d!important;color:#025e8d!important}.cc-button--secondary:focus{background-color:#025e8d!important;border-color:#fff!important;color:#fff!important;outline:3px solid #08c!important}.cc-button--secondary:hover{background-color:#025e8d!important;border-color:#025e8d!important;color:#fff!important}.cc-button--secondary:active{border:2px solid #01324b!important}.cc-button--secondary.cc-button--contrast{border-color:#fff!important}.cc-button--secondary.cc-button--contrast:hover{background-color:initial!important;color:#fff!important}.cc-button--tertiary{background-color:#f0f7fc!important;background-image:none;border:2px solid #f0f7fc;color:#025e8d!important;text-decoration:underline!important;text-underline-offset:.25em}.cc-button--tertiary:focus{outline:3px solid #08c!important}.cc-button--tertiary:hover{text-decoration-thickness:.25em}.cc-button--tertiary:active{color:#013c5b!important}.cc-button--link{background-color:initial!important;border-color:#0000!important;border-radius:0!important;color:inherit!important;padding:0!important;text-decoration:underline!important}.cc-button--link:focus{outline:3px solid #08c}.cc-button--link:hover{background-color:initial!important;box-shadow:none;text-decoration:none}.cc-button--text{border-radius:0;padding:0}.cc-button--details{padding-right:24px!important;position:relative}.cc-button--details:after,.cc-button--details:before{background-color:currentColor;content:"";position:absolute;transition:transform .25s ease-out}.cc-button--details:before{height:10px;margin-top:-5px;right:12px;top:50%;width:2px}.cc-button--details:after{height:2px;margin-top:-1px;right:8px;top:50%;width:10px}.cc-button--details.cc-active:before{transform:rotate(90deg);transform-origin:center}.cc-button--details.cc-active:after{display:none}.cc-details{border-bottom:1px solid #0000001a;padding:12px 0;position:relative;width:100%}.cc-details__summary{align-items:center;display:flex;font-size:14px;font-weight:700;list-style-type:none}.cc-details__summary:focus,.cc-details__summary:focus-visible{outline:3px solid #08c!important;will-change:auto}.cc-details__title{align-items:baseline;display:flex}.cc-details__title h2,.cc-details__title h3,.cc-details__title h4,.cc-details__title h5{line-height:1.4;margin:0!important}.cc-details__title svg{flex-shrink:0;margin-right:8px;position:relative;top:-1px;transition:all .2s}.cc-details[open] .cc-details__title svg{top:2px;transform:rotate(90deg)}.cc-details__switch{margin-left:auto}.cc-details__section{padding:16px 0 0 18px}.cc-details__section p{margin:0}.cc-details__section p:not(:last-child){margin-bottom:12px}details summary::-webkit-details-marker{display:none}.cc-radio{align-items:center;display:flex;position:relative}.cc-radio *{cursor:pointer}.cc-radio__input{height:22px;left:0;position:absolute;top:0;width:22px}.cc-radio__input:focus{outline:none}.cc-radio__label{color:inherit;font-size:14px;font-weight:700;line-height:23px;margin:0;padding-left:28px}.cc-radio__label:after,.cc-radio__label:before{background-color:#fff;content:"";display:block;position:absolute;transition:transform .25s ease-out}.cc-radio__label:before{border:1px solid #777;border-radius:50%;height:22px;left:0;top:0;width:22px}.cc-radio__label:after{border:7px solid #025e8d;border-radius:50%;height:0;left:4px;opacity:0;top:4px;width:0}.cc-radio__input:focus+.cc-radio__label:before{box-shadow:0 0 0 2px #08c;outline:none}.cc-radio__label--hidden{display:none}.cc-radio__input:checked+.cc-radio__label:after{opacity:1}.cc-radio__input:disabled{cursor:default}.cc-radio__input:disabled+.cc-radio__label{cursor:default;opacity:.5}.cc-switch *{cursor:pointer}.cc-switch{align-items:center;display:flex;line-height:1}.cc-switch__label{cursor:pointer;display:inline-block;font-size:14px;font-weight:700;margin:0;-webkit-user-select:none;user-select:none}.cc-switch__input{-webkit-appearance:none;appearance:none;background-color:#fff;background-image:url("data:image/svg+xml;charset=utf-8,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='-4 -4 8 8'%3E%3Ccircle r='3' fill='rgba(0, 0, 0, 0.25)'/%3E%3C/svg%3E");background-position:0;background-repeat:no-repeat;border:1px solid #777;border-radius:16px;height:18px!important;margin:0 8px 0 0;transition:background-position .2s ease-in-out;width:32px!important}.cc-switch__input:focus{background-image:url("data:image/svg+xml;charset=utf-8,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='-4 -4 8 8'%3E%3Ccircle r='3' fill='%2380b3cc'/%3E%3C/svg%3E")}.cc-switch__input:checked{background-color:#025e8d;background-image:url("data:image/svg+xml;charset=utf-8,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='-4 -4 8 8'%3E%3Ccircle r='3' fill='%23fff'/%3E%3C/svg%3E");background-position:100%}.cc-switch__input:checked~.cc-switch__label>.cc-switch__text-off,.cc-switch__input:not(:checked)~.cc-switch__label>.cc-switch__text-on{display:none}.cc-switch__input:hover{cursor:pointer;outline:none}.cc-switch__input:focus-visible{cursor:pointer;outline:3px solid #fc0}.cc-list>ul,ul.cc-list{list-style-type:disc;margin-left:0;padding-left:0}.cc-list>ul>li,ul.cc-list>li{margin-left:1em}.cc-list>ul>li:not(:last-child),ul.cc-list>li:not(:last-child){margin-bottom:4px}.cc-list__title{margin-bottom:8px!important}.cc-list--inline>ul,ul.cc-list--inline{display:flex;flex-wrap:wrap;list-style-type:none}.cc-list--inline>ul>li,ul.cc-list--inline>li{margin:0 .5em .5em 0}.cc-overlay{background-color:#0000004d;z-index:99998}.cc-overlay,.cc-preferences{bottom:0;left:0;position:fixed;right:0;top:0}.cc-preferences{background-color:#050a14f2!important;border:0;box-sizing:border-box;color:#111;font-family:sans-serif!important;line-height:1.4;margin:auto;max-height:100vh;overflow:auto;padding:0;z-index:100000}.cc-preferences:focus{outline:none}.cc-preferences *,.cc-preferences :after,.cc-preferences :before{box-sizing:inherit!important}.cc-preferences h2,.cc-preferences h3,.cc-preferences h4,.cc-preferences h5,.cc-preferences h6{font-family:sans-serif;font-style:normal;font-weight:700;margin:0 0 .5em}.cc-preferences .cc-h2,.cc-preferences h2{font-size:18px}.cc-preferences .cc-h3,.cc-preferences h3{font-size:16px}.cc-preferences .cc-h4,.cc-preferences .cc-h5,.cc-preferences .cc-h6,.cc-preferences h4,.cc-preferences h5,.cc-preferences h6{font-size:14px}.cc-preferences a{color:#025e8d;text-decoration:underline}.cc-preferences a:hover{color:inherit;text-decoration:none}.cc-preferences h3{background:none;color:#111;text-transform:none}dialog.cc-preferences{background-color:initial}dialog.cc-preferences::backdrop{background-color:#000000e6}.cc-preferences__dialog{display:flex;flex-direction:column;margin:auto;max-height:100vh;max-width:860px;padding:12px;position:relative}.cc-preferences__dialog>:last-child{border-bottom-left-radius:10px;border-bottom-right-radius:10px}@media (min-width:980px){.cc-preferences__dialog{padding:16px}}.cc-preferences__close{background:#0000!important;border:1px solid #ececec;border-radius:50%;color:#111!important;cursor:pointer;font-family:Times New Roman,serif;font-size:40px;height:40px;left:auto;line-height:1;margin-top:-20px;padding:0!important;position:absolute;right:20px;top:50%;width:40px}.cc-preferences__close:focus{outline:3px solid #08c}.cc-preferences__close-label{border:0;clip:rect(0,0,0,0);height:1px;margin:-100%;overflow:hidden;padding:0;position:absolute!important;width:1px}.cc-preferences__header{background:#fff!important;border-bottom:1px solid #0000001a;border-top-left-radius:10px;border-top-right-radius:10px;padding:16px;position:relative;text-align:center}.cc-preferences__title{background:none!important;color:#111!important;font-family:sans-serif!important;font-size:18px!important;font-weight:700!important;margin:0!important;padding-right:16px!important}@media (min-width:480px){.cc-preferences__title{padding-right:0!important}}@media (min-width:980px){.cc-preferences__title{font-size:22px!important}}.cc-preferences__body{background:#fff!important;flex:1 1 auto;min-height:200px;overflow-x:hidden;overflow-y:auto;padding:16px}.cc-preferences__footer{background:#fff!important;border-top:1px solid #d0d0d0;box-shadow:0 0 5px 0 #0003;margin-bottom:0;padding:12px;position:relative}@media (min-width:480px){.cc-preferences__footer{align-items:stretch;display:flex}}.cc-preferences__footer>.cc-button{display:block;width:100%}@media (min-width:480px){.cc-preferences__footer>.cc-button{flex:1 1 auto}}@media (min-width:980px){.cc-preferences__footer>.cc-button{flex-basis:auto}}@media (min-width:480px){.cc-preferences__footer>.cc-button:not(:first-child){margin-left:12px}}.cc-preferences__footer>.cc-button:not(:last-child){margin-bottom:8px}@media (min-width:480px){.cc-preferences__footer>.cc-button:not(:last-child){margin-bottom:0}}.cc-preferences__categories{list-style:none;margin:0;padding:0}.cc-preferences__category:not(:last-child){border-bottom:1px solid #0000001a;margin-bottom:12px;padding-bottom:12px}.cc-preferences__category-description{font-size:14px;margin:0 0 8px}.cc-preferences__category-footer{align-items:center;display:flex;justify-content:space-between}.cc-preferences__status{border:0;clip:rect(0,0,0,0);height:1px;margin:-100%;overflow:hidden;padding:0;position:absolute!important;width:1px}.cc-preferences__controls{display:flex;margin:0}.cc-preferences__controls>:not(:last-child){margin-right:12px}.cc-preferences__always-on{font-size:14px;font-weight:700;padding-left:26px;position:relative}.cc-preferences__always-on:before{background:#0000!important;border:solid;border-top-color:#0000;border-width:0 0 4px 4px;content:"";display:block;height:10px;left:0;position:absolute;top:2px;transform:rotate(-45deg);width:18px}.cc-preferences__details{background-color:#eff6fb!important;border-radius:10px;font-size:14px;margin:12px 0 0;padding:12px}.cc-preferences__cookie-list,.cc-preferences__provider-list{list-style:none;margin:0;padding:0}.cc-preferences__provider-list{columns:170px}.cc-preferences__cookie-title{font-size:1em;margin:0}.cc-preferences__cookie-description{font-size:1em;margin:0 0 8px}.cc-preferences__cookie-domain,.cc-preferences__cookie-lifespan{border-left:1px solid #999;color:#666;margin-left:8px;padding-left:4px}body.cc-has-preferences-open{overflow:hidden;position:relative}.cc-table{border-collapse:collapse;width:100%}.cc-table tbody tr{border-top:1px solid #0000001a}.cc-table td,.cc-table th{font-size:14px;padding:4px 8px;vertical-align:top}.cc-table+.cc-table{margin-top:8px}.cc-table--purposes td:first-child,.cc-table--purposes th:first-child{width:70%}@media (min-width:680px){.cc-table--purposes td:first-child,.cc-table--purposes th:first-child{width:80%}}.cc-table--purposes td:last-child,.cc-table--purposes th:last-child{padding-left:8px;width:40%}@media (min-width:680px){.cc-table--purposes td:last-child,.cc-table--purposes th:last-child{width:30%}}.cc-tabs,.cc-tabs>ul{background-color:#faf9f6;display:flex;list-style:none;margin:0;padding:0}.cc-tabs__panel{border-top:1px solid #ececec;margin-top:-1px}.cc-tabs__button{background-color:initial;border:0;border-left:1px solid #0000;border-right:1px solid #0000;font-size:16px;font-weight:700;padding:15px 16px 12px;position:relative}.cc-tabs__button:before{background-color:initial;content:"";display:block;height:3px;left:0;position:absolute;top:0;width:100%}.cc-tabs__button.cc-active{background-color:#fff;border-color:#025e8d #ececec #ececec}.cc-tabs__button.cc-active:before{background-color:#025e8d}.cc-tabs__content{background-color:#fff;display:none;font-size:14px;padding:16px}.cc-tabs__section:not(:first-child){padding-top:24px}.cc-tcf{font-size:14px;margin-top:16px}.cc-tcf__list{list-style:none;margin:0;padding:0}.cc-vendor-count{color:#666;font-size:16px;margin:0 0 8px}.cc-hide{display:none!important}.cc-show{display:block!important}.cc-external-link{background-color:#ececec;background-image:url("data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyMCIgaGVpZ2h0PSIyMCI+PHBhdGggZD0iTTcgMGExIDEgMCAxIDEgMCAySDIuNmMtLjM3MSAwLS42LjIwOS0uNi41djE1YzAgLjI5MS4yMjkuNS42LjVoMTQuOGMuMzcxIDAgLjYtLjIwOS42LS41VjEzYTEgMSAwIDAgMSAyIDB2NC41YzAgMS40MzgtMS4xNjIgMi41LTIuNiAyLjVIMi42QzEuMTYyIDIwIDAgMTguOTM4IDAgMTcuNXYtMTVDMCAxLjA2MiAxLjE2MiAwIDIuNiAwem02IDBoNmwuMDc1LjAwMy4xMjYuMDE3LjExMS4wMy4xMTEuMDQ0LjA5OC4wNTIuMDk2LjA2Ny4wOS4wOGExIDEgMCAwIDEgLjA5Ny4xMTJsLjA3MS4xMS4wNTQuMTE0LjAzNS4xMDUuMDMuMTQ4TDIwIDF2NmExIDEgMCAwIDEtMiAwVjMuNDE0bC02LjY5MyA2LjY5M2ExIDEgMCAwIDEtMS40MTQtMS40MTRMMTYuNTg0IDJIMTNhMSAxIDAgMCAxLS45OTMtLjg4M0wxMiAxYTEgMSAwIDAgMSAxLTEiLz48L3N2Zz4=");background-position:right 8px center;background-repeat:no-repeat;background-size:10px auto;border-radius:3px;box-shadow:0 1px 0 #0003;color:#111!important;display:inline-block;font-size:12px;padding:4px 26px 4px 8px;text-decoration:none!important}.cc-external-link:hover{text-decoration:underline!important}.cc-m-0{margin:0!important}.cc-grey{color:#666}</style><style type="text/css">.MathJax_Hover_Frame {border-radius: .25em; -webkit-border-radius: .25em; -moz-border-radius: .25em; -khtml-border-radius: .25em; box-shadow: 0px 0px 15px #83A; -webkit-box-shadow: 0px 0px 15px #83A; -moz-box-shadow: 0px 0px 15px #83A; -khtml-box-shadow: 0px 0px 15px #83A; border: 1px solid #A6D ! important; display: inline-block; position: absolute}
.MathJax_Menu_Button .MathJax_Hover_Arrow {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 4px; -webkit-border-radius: 4px; -moz-border-radius: 4px; -khtml-border-radius: 4px; font-family: 'Courier New',Courier; font-size: 9px; color: #F0F0F0}
.MathJax_Menu_Button .MathJax_Hover_Arrow span {display: block; background-color: #AAA; border: 1px solid; border-radius: 3px; line-height: 0; padding: 4px}
.MathJax_Hover_Arrow:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_Hover_Arrow:hover span {background-color: #CCC!important}
</style><style type="text/css">#MathJax_About {position: fixed; left: 50%; width: auto; text-align: center; border: 3px outset; padding: 1em 2em; background-color: #DDDDDD; color: black; cursor: default; font-family: message-box; font-size: 120%; font-style: normal; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 15px; -webkit-border-radius: 15px; -moz-border-radius: 15px; -khtml-border-radius: 15px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_About.MathJax_MousePost {outline: none}
.MathJax_Menu {position: absolute; background-color: white; color: black; width: auto; padding: 5px 0px; border: 1px solid #CCCCCC; margin: 0; cursor: default; font: menu; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 5px; -webkit-border-radius: 5px; -moz-border-radius: 5px; -khtml-border-radius: 5px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
.MathJax_MenuItem {padding: 1px 2em; background: transparent}
.MathJax_MenuArrow {position: absolute; right: .5em; padding-top: .25em; color: #666666; font-size: .75em}
.MathJax_MenuActive .MathJax_MenuArrow {color: white}
.MathJax_MenuArrow.RTL {left: .5em; right: auto}
.MathJax_MenuCheck {position: absolute; left: .7em}
.MathJax_MenuCheck.RTL {right: .7em; left: auto}
.MathJax_MenuRadioCheck {position: absolute; left: .7em}
.MathJax_MenuRadioCheck.RTL {right: .7em; left: auto}
.MathJax_MenuLabel {padding: 1px 2em 3px 1.33em; font-style: italic}
.MathJax_MenuRule {border-top: 1px solid #DDDDDD; margin: 4px 3px}
.MathJax_MenuDisabled {color: GrayText}
.MathJax_MenuActive {background-color: #606872; color: white}
.MathJax_MenuDisabled:focus, .MathJax_MenuLabel:focus {background-color: #E8E8E8}
.MathJax_ContextMenu:focus {outline: none}
.MathJax_ContextMenu .MathJax_MenuItem:focus {outline: none}
#MathJax_AboutClose {top: .2em; right: .2em}
.MathJax_Menu .MathJax_MenuClose {top: -10px; left: -10px}
.MathJax_MenuClose {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; font-family: 'Courier New',Courier; font-size: 24px; color: #F0F0F0}
.MathJax_MenuClose span {display: block; background-color: #AAA; border: 1.5px solid; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; line-height: 0; padding: 8px 0 6px}
.MathJax_MenuClose:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_MenuClose:hover span {background-color: #CCC!important}
.MathJax_MenuClose:hover:focus {outline: none}
</style><style type="text/css">.MJX_Assistive_MathML {position: absolute!important; top: 0; left: 0; clip: rect(1px, 1px, 1px, 1px); padding: 1px 0 0 0!important; border: 0!important; height: 1px!important; width: 1px!important; overflow: hidden!important; display: block!important; -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none}
.MJX_Assistive_MathML.MJX_Assistive_MathML_Block {width: 100%!important}
</style><style type="text/css">#MathJax_Zoom {position: absolute; background-color: #F0F0F0; overflow: auto; display: block; z-index: 301; padding: .5em; border: 1px solid black; margin: 0; font-weight: normal; font-style: normal; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; -webkit-box-sizing: content-box; -moz-box-sizing: content-box; box-sizing: content-box; box-shadow: 5px 5px 15px #AAAAAA; -webkit-box-shadow: 5px 5px 15px #AAAAAA; -moz-box-shadow: 5px 5px 15px #AAAAAA; -khtml-box-shadow: 5px 5px 15px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_ZoomOverlay {position: absolute; left: 0; top: 0; z-index: 300; display: inline-block; width: 100%; height: 100%; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
#MathJax_ZoomFrame {position: relative; display: inline-block; height: 0; width: 0}
#MathJax_ZoomEventTrap {position: absolute; left: 0; top: 0; z-index: 302; display: inline-block; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
</style><style type="text/css">.MathJax_Preview {color: #888}
#MathJax_Message {position: fixed; left: 1em; bottom: 1.5em; background-color: #E6E6E6; border: 1px solid #959595; margin: 0px; padding: 2px 8px; z-index: 102; color: black; font-size: 80%; width: auto; white-space: nowrap}
#MathJax_MSIE_Frame {position: absolute; top: 0; left: 0; width: 0px; z-index: 101; border: 0px; margin: 0px; padding: 0px}
.MathJax_Error {color: #CC0000; font-style: italic}
</style><style type="text/css">.MJXp-script {font-size: .8em}
.MJXp-right {-webkit-transform-origin: right; -moz-transform-origin: right; -ms-transform-origin: right; -o-transform-origin: right; transform-origin: right}
.MJXp-bold {font-weight: bold}
.MJXp-italic {font-style: italic}
.MJXp-scr {font-family: MathJax_Script,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-frak {font-family: MathJax_Fraktur,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-sf {font-family: MathJax_SansSerif,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-cal {font-family: MathJax_Caligraphic,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-mono {font-family: MathJax_Typewriter,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-largeop {font-size: 150%}
.MJXp-largeop.MJXp-int {vertical-align: -.2em}
.MJXp-math {display: inline-block; line-height: 1.2; text-indent: 0; font-family: 'Times New Roman',Times,STIXGeneral,serif; white-space: nowrap; border-collapse: collapse}
.MJXp-display {display: block; text-align: center; margin: 1em 0}
.MJXp-math span {display: inline-block}
.MJXp-box {display: block!important; text-align: center}
.MJXp-box:after {content: " "}
.MJXp-rule {display: block!important; margin-top: .1em}
.MJXp-char {display: block!important}
.MJXp-mo {margin: 0 .15em}
.MJXp-mfrac {margin: 0 .125em; vertical-align: .25em}
.MJXp-denom {display: inline-table!important; width: 100%}
.MJXp-denom > * {display: table-row!important}
.MJXp-surd {vertical-align: top}
.MJXp-surd > * {display: block!important}
.MJXp-script-box > *  {display: table!important; height: 50%}
.MJXp-script-box > * > * {display: table-cell!important; vertical-align: top}
.MJXp-script-box > *:last-child > * {vertical-align: bottom}
.MJXp-script-box > * > * > * {display: block!important}
.MJXp-mphantom {visibility: hidden}
.MJXp-munderover, .MJXp-munder {display: inline-table!important}
.MJXp-over {display: inline-block!important; text-align: center}
.MJXp-over > * {display: block!important}
.MJXp-munderover > *, .MJXp-munder > * {display: table-row!important}
.MJXp-mtable {vertical-align: .25em; margin: 0 .125em}
.MJXp-mtable > * {display: inline-table!important; vertical-align: middle}
.MJXp-mtr {display: table-row!important}
.MJXp-mtd {display: table-cell!important; text-align: center; padding: .5em 0 0 .5em}
.MJXp-mtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-mlabeledtr {display: table-row!important}
.MJXp-mlabeledtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mlabeledtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MJXp-scale0 {-webkit-transform: scaleX(.0); -moz-transform: scaleX(.0); -ms-transform: scaleX(.0); -o-transform: scaleX(.0); transform: scaleX(.0)}
.MJXp-scale1 {-webkit-transform: scaleX(.1); -moz-transform: scaleX(.1); -ms-transform: scaleX(.1); -o-transform: scaleX(.1); transform: scaleX(.1)}
.MJXp-scale2 {-webkit-transform: scaleX(.2); -moz-transform: scaleX(.2); -ms-transform: scaleX(.2); -o-transform: scaleX(.2); transform: scaleX(.2)}
.MJXp-scale3 {-webkit-transform: scaleX(.3); -moz-transform: scaleX(.3); -ms-transform: scaleX(.3); -o-transform: scaleX(.3); transform: scaleX(.3)}
.MJXp-scale4 {-webkit-transform: scaleX(.4); -moz-transform: scaleX(.4); -ms-transform: scaleX(.4); -o-transform: scaleX(.4); transform: scaleX(.4)}
.MJXp-scale5 {-webkit-transform: scaleX(.5); -moz-transform: scaleX(.5); -ms-transform: scaleX(.5); -o-transform: scaleX(.5); transform: scaleX(.5)}
.MJXp-scale6 {-webkit-transform: scaleX(.6); -moz-transform: scaleX(.6); -ms-transform: scaleX(.6); -o-transform: scaleX(.6); transform: scaleX(.6)}
.MJXp-scale7 {-webkit-transform: scaleX(.7); -moz-transform: scaleX(.7); -ms-transform: scaleX(.7); -o-transform: scaleX(.7); transform: scaleX(.7)}
.MJXp-scale8 {-webkit-transform: scaleX(.8); -moz-transform: scaleX(.8); -ms-transform: scaleX(.8); -o-transform: scaleX(.8); transform: scaleX(.8)}
.MJXp-scale9 {-webkit-transform: scaleX(.9); -moz-transform: scaleX(.9); -ms-transform: scaleX(.9); -o-transform: scaleX(.9); transform: scaleX(.9)}
.MathJax_PHTML .noError {vertical-align: ; font-size: 90%; text-align: left; color: black; padding: 1px 3px; border: 1px solid}
</style><meta content="AlK2UR5SkAlj8jjdEc9p3F3xuFYlF6LYjAML3EOqw1g26eCwWPjdmecULvBH5MVPoqKYrOfPhYVL71xAXI1IBQoAAAB8eyJvcmlnaW4iOiJodHRwczovL2RvdWJsZWNsaWNrLm5ldDo0NDMiLCJmZWF0dXJlIjoiV2ViVmlld1hSZXF1ZXN0ZWRXaXRoRGVwcmVjYXRpb24iLCJleHBpcnkiOjE3NTgwNjcxOTksImlzU3ViZG9tYWluIjp0cnVlfQ==" http-equiv="origin-trial"/><meta content="Amm8/NmvvQfhwCib6I7ZsmUxiSCfOxWxHayJwyU1r3gRIItzr7bNQid6O8ZYaE1GSQTa69WwhPC9flq/oYkRBwsAAACCeyJvcmlnaW4iOiJodHRwczovL2dvb2dsZXN5bmRpY2F0aW9uLmNvbTo0NDMiLCJmZWF0dXJlIjoiV2ViVmlld1hSZXF1ZXN0ZWRXaXRoRGVwcmVjYXRpb24iLCJleHBpcnkiOjE3NTgwNjcxOTksImlzU3ViZG9tYWluIjp0cnVlfQ==" http-equiv="origin-trial"/><meta content="A9wSqI5i0iwGdf6L1CERNdmsTPgVu44ewj8QxTBYgsv1LCPUVF7YmWOvTappqB1139jAymxUW/RO8zmMqo4zlAAAAACNeyJvcmlnaW4iOiJodHRwczovL2RvdWJsZWNsaWNrLm5ldDo0NDMiLCJmZWF0dXJlIjoiRmxlZGdlQmlkZGluZ0FuZEF1Y3Rpb25TZXJ2ZXIiLCJleHBpcnkiOjE3MzY4MTI4MDAsImlzU3ViZG9tYWluIjp0cnVlLCJpc1RoaXJkUGFydHkiOnRydWV9" http-equiv="origin-trial"/><meta content="A+d7vJfYtay4OUbdtRPZA3y7bKQLsxaMEPmxgfhBGqKXNrdkCQeJlUwqa6EBbSfjwFtJWTrWIioXeMW+y8bWAgQAAACTeyJvcmlnaW4iOiJodHRwczovL2dvb2dsZXN5bmRpY2F0aW9uLmNvbTo0NDMiLCJmZWF0dXJlIjoiRmxlZGdlQmlkZGluZ0FuZEF1Y3Rpb25TZXJ2ZXIiLCJleHBpcnkiOjE3MzY4MTI4MDAsImlzU3ViZG9tYWluIjp0cnVlLCJpc1RoaXJkUGFydHkiOnRydWV9" http-equiv="origin-trial"/><script async="" src="https://pagead2.googlesyndication.com/pagead/managed/js/gpt/m202410100101/pubads_impl.js"></script><script async="" src="https://cdn.pbgrd.com/core-spl.js"></script></head>
<body class=""><div id="MathJax_Message" style="display: none;"></div><dialog aria-labelledby="cc-banner-label" class="cc-banner" data-cc-banner="" data-nosnippet="" open="">
<div autofocus="" class="cc-banner__content" tabindex="-1">
<div class="cc-banner__container">
<div class="cc-banner__header">
<h2 class="cc-banner__title" id="cc-banner-label">Your privacy, your choice</h2>
</div>
<div class="cc-banner__body">
<div class="cc-banner__policy">
<p>We use essential cookies to make sure the site can function. We also use optional cookies for advertising, personalisation of content, usage analysis, and social media.</p><p>By accepting optional cookies, you consent to the processing of your personal data - including transfers to third parties. Some third parties are outside of the European Economic Area, with varying standards of data protection.</p><p>See our <a data-cc-action="privacy" href="https://link.springer.com/privacystatement">privacy policy</a> for more information on the use of your personal data.</p><p><button class="cc-button cc-button--link cc-button--text" data-cc-action="preferences" type="button">Manage preferences</button> for further information and to change your choices.</p>
</div>
</div>
<div class="cc-banner__footer">
<button class="cc-button cc-button--secondary cc-button--contrast cc-banner__button cc-banner__button-accept" data-cc-action="accept">Accept all cookies</button>
</div>
</div>
</div>
</dialog>
<!-- Google Tag Manager (noscript) -->
<noscript>
<iframe height="0" src="https://www.googletagmanager.com/ns.html?id=GTM-MRVXSHQ" style="display:none;visibility:hidden" width="0"></iframe>
</noscript>
<!-- End Google Tag Manager (noscript) -->
<!-- Google Tag Manager (noscript) -->
<noscript data-test="gtm-body">
<iframe height="0" src="https://www.googletagmanager.com/ns.html?id=GTM-MRVXSHQ" style="display:none;visibility:hidden" width="0"></iframe>
</noscript>
<!-- End Google Tag Manager (noscript) -->
<div aria-hidden="true" class="u-visually-hidden" data-test="darwin-icons">
<!--?xml version="1.0" encoding="UTF-8"?--><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><symbol id="icon-eds-i-accesses-medium" viewbox="0 0 24 24"><path d="M15.59 1a1 1 0 0 1 .706.291l5.41 5.385a1 1 0 0 1 .294.709v13.077c0 .674-.269 1.32-.747 1.796a2.549 2.549 0 0 1-1.798.742H15a1 1 0 0 1 0-2h4.455a.549.549 0 0 0 .387-.16.535.535 0 0 0 .158-.378V7.8L15.178 3H5.545a.543.543 0 0 0-.538.451L5 3.538v8.607a1 1 0 0 1-2 0V3.538A2.542 2.542 0 0 1 5.545 1h10.046ZM8 13c2.052 0 4.66 1.61 6.36 3.4l.124.141c.333.41.516.925.516 1.459 0 .6-.232 1.178-.64 1.599C12.666 21.388 10.054 23 8 23c-2.052 0-4.66-1.61-6.353-3.393A2.31 2.31 0 0 1 1 18c0-.6.232-1.178.64-1.6C3.34 14.61 5.948 13 8 13Zm0 2c-1.369 0-3.552 1.348-4.917 2.785A.31.31 0 0 0 3 18c0 .083.031.161.09.222C4.447 19.652 6.631 21 8 21c1.37 0 3.556-1.35 4.917-2.785A.31.31 0 0 0 13 18a.32.32 0 0 0-.048-.17l-.042-.052C11.553 16.348 9.369 15 8 15Zm0 1a2 2 0 1 1 0 4 2 2 0 0 1 0-4Z"></path></symbol><symbol id="icon-eds-i-altmetric-medium" viewbox="0 0 24 24"><path d="M12 1c5.978 0 10.843 4.77 10.996 10.712l.004.306-.002.022-.002.248C22.843 18.23 17.978 23 12 23 5.925 23 1 18.075 1 12S5.925 1 12 1Zm-1.726 9.246L8.848 12.53a1 1 0 0 1-.718.461L8.003 13l-4.947.014a9.001 9.001 0 0 0 17.887-.001L16.553 13l-2.205 3.53a1 1 0 0 1-1.735-.068l-.05-.11-2.289-6.106ZM12 3a9.001 9.001 0 0 0-8.947 8.013l4.391-.012L9.652 7.47a1 1 0 0 1 1.784.179l2.288 6.104 1.428-2.283a1 1 0 0 1 .722-.462l.129-.008 4.943.012A9.001 9.001 0 0 0 12 3Z"></path></symbol><symbol id="icon-eds-i-arrow-bend-down-medium" viewbox="0 0 24 24"><path d="m11.852 20.989.058.007L12 21l.075-.003.126-.017.111-.03.111-.044.098-.052.104-.074.082-.073 6-6a1 1 0 0 0-1.414-1.414L13 17.585v-12.2C13 4.075 11.964 3 10.667 3H4a1 1 0 1 0 0 2h6.667c.175 0 .333.164.333.385v12.2l-4.293-4.292a1 1 0 0 0-1.32-.083l-.094.083a1 1 0 0 0 0 1.414l6 6c.035.036.073.068.112.097l.11.071.114.054.105.035.118.025Z"></path></symbol><symbol id="icon-eds-i-arrow-bend-down-small" viewbox="0 0 16 16"><path d="M1 2a1 1 0 0 0 1 1h5v8.585L3.707 8.293a1 1 0 0 0-1.32-.083l-.094.083a1 1 0 0 0 0 1.414l5 5 .063.059.093.069.081.048.105.048.104.035.105.022.096.01h.136l.122-.018.113-.03.103-.04.1-.053.102-.07.052-.043 5.04-5.037a1 1 0 1 0-1.415-1.414L9 11.583V3a2 2 0 0 0-2-2H2a1 1 0 0 0-1 1Z"></path></symbol><symbol id="icon-eds-i-arrow-bend-up-medium" viewbox="0 0 24 24"><path d="m11.852 3.011.058-.007L12 3l.075.003.126.017.111.03.111.044.098.052.104.074.082.073 6 6a1 1 0 1 1-1.414 1.414L13 6.415v12.2C13 19.925 11.964 21 10.667 21H4a1 1 0 0 1 0-2h6.667c.175 0 .333-.164.333-.385v-12.2l-4.293 4.292a1 1 0 0 1-1.32.083l-.094-.083a1 1 0 0 1 0-1.414l6-6c.035-.036.073-.068.112-.097l.11-.071.114-.054.105-.035.118-.025Z"></path></symbol><symbol id="icon-eds-i-arrow-bend-up-small" viewbox="0 0 16 16"><path d="M1 13.998a1 1 0 0 1 1-1h5V4.413L3.707 7.705a1 1 0 0 1-1.32.084l-.094-.084a1 1 0 0 1 0-1.414l5-5 .063-.059.093-.068.081-.05.105-.047.104-.035.105-.022L7.94 1l.136.001.122.017.113.03.103.04.1.053.102.07.052.043 5.04 5.037a1 1 0 1 1-1.415 1.414L9 4.415v8.583a2 2 0 0 1-2 2H2a1 1 0 0 1-1-1Z"></path></symbol><symbol id="icon-eds-i-arrow-diagonal-medium" viewbox="0 0 24 24"><path d="M14 3h6l.075.003.126.017.111.03.111.044.098.052.096.067.09.08c.036.035.068.073.097.112l.071.11.054.114.035.105.03.148L21 4v6a1 1 0 0 1-2 0V6.414l-4.293 4.293a1 1 0 0 1-1.414-1.414L17.584 5H14a1 1 0 0 1-.993-.883L13 4a1 1 0 0 1 1-1ZM4 13a1 1 0 0 1 1 1v3.584l4.293-4.291a1 1 0 1 1 1.414 1.414L6.414 19H10a1 1 0 0 1 .993.883L11 20a1 1 0 0 1-1 1l-6.075-.003-.126-.017-.111-.03-.111-.044-.098-.052-.096-.067-.09-.08a1.01 1.01 0 0 1-.097-.112l-.071-.11-.054-.114-.035-.105-.025-.118-.007-.058L3 20v-6a1 1 0 0 1 1-1Z"></path></symbol><symbol id="icon-eds-i-arrow-diagonal-small" viewbox="0 0 16 16"><path d="m2 15-.082-.004-.119-.016-.111-.03-.111-.044-.098-.052-.096-.067-.09-.08a1.008 1.008 0 0 1-.097-.112l-.071-.11-.031-.062-.034-.081-.024-.076-.025-.118-.007-.058L1 14.02V9a1 1 0 1 1 2 0v2.584l2.793-2.791a1 1 0 1 1 1.414 1.414L4.414 13H7a1 1 0 0 1 .993.883L8 14a1 1 0 0 1-1 1H2ZM14 1l.081.003.12.017.111.03.111.044.098.052.096.067.09.08c.036.035.068.073.097.112l.071.11.031.062.034.081.024.076.03.148L15 2v5a1 1 0 0 1-2 0V4.414l-2.96 2.96A1 1 0 1 1 8.626 5.96L11.584 3H9a1 1 0 0 1-.993-.883L8 2a1 1 0 0 1 1-1h5Z"></path></symbol><symbol id="icon-eds-i-arrow-down-medium" viewbox="0 0 24 24"><path d="m20.707 12.728-7.99 7.98a.996.996 0 0 1-.561.281l-.157.011a.998.998 0 0 1-.788-.384l-7.918-7.908a1 1 0 0 1 1.414-1.416L11 17.576V4a1 1 0 0 1 2 0v13.598l6.293-6.285a1 1 0 0 1 1.32-.082l.095.083a1 1 0 0 1-.001 1.414Z"></path></symbol><symbol id="icon-eds-i-arrow-down-small" viewbox="0 0 16 16"><path d="m1.293 8.707 6 6 .063.059.093.069.081.048.105.049.104.034.056.013.118.017L8 15l.076-.003.122-.017.113-.03.085-.032.063-.03.098-.058.06-.043.05-.043 6.04-6.037a1 1 0 0 0-1.414-1.414L9 11.583V2a1 1 0 1 0-2 0v9.585L2.707 7.293a1 1 0 0 0-1.32-.083l-.094.083a1 1 0 0 0 0 1.414Z"></path></symbol><symbol id="icon-eds-i-arrow-left-medium" viewbox="0 0 24 24"><path d="m11.272 3.293-7.98 7.99a.996.996 0 0 0-.281.561L3 12.001c0 .32.15.605.384.788l7.908 7.918a1 1 0 0 0 1.416-1.414L6.424 13H20a1 1 0 0 0 0-2H6.402l6.285-6.293a1 1 0 0 0 .082-1.32l-.083-.095a1 1 0 0 0-1.414.001Z"></path></symbol><symbol id="icon-eds-i-arrow-left-small" viewbox="0 0 16 16"><path d="m7.293 1.293-6 6-.059.063-.069.093-.048.081-.049.105-.034.104-.013.056-.017.118L1 8l.003.076.017.122.03.113.032.085.03.063.058.098.043.06.043.05 6.037 6.04a1 1 0 0 0 1.414-1.414L4.417 9H14a1 1 0 0 0 0-2H4.415l4.292-4.293a1 1 0 0 0 .083-1.32l-.083-.094a1 1 0 0 0-1.414 0Z"></path></symbol><symbol id="icon-eds-i-arrow-right-medium" viewbox="0 0 24 24"><path d="m12.728 3.293 7.98 7.99a.996.996 0 0 1 .281.561l.011.157c0 .32-.15.605-.384.788l-7.908 7.918a1 1 0 0 1-1.416-1.414L17.576 13H4a1 1 0 0 1 0-2h13.598l-6.285-6.293a1 1 0 0 1-.082-1.32l.083-.095a1 1 0 0 1 1.414.001Z"></path></symbol><symbol id="icon-eds-i-arrow-right-small" viewbox="0 0 16 16"><path d="m8.707 1.293 6 6 .059.063.069.093.048.081.049.105.034.104.013.056.017.118L15 8l-.003.076-.017.122-.03.113-.032.085-.03.063-.058.098-.043.06-.043.05-6.037 6.04a1 1 0 0 1-1.414-1.414L11.583 9H2a1 1 0 1 1 0-2h9.585L7.293 2.707a1 1 0 0 1-.083-1.32l.083-.094a1 1 0 0 1 1.414 0Z"></path></symbol><symbol id="icon-eds-i-arrow-up-medium" viewbox="0 0 24 24"><path d="m3.293 11.272 7.99-7.98a.996.996 0 0 1 .561-.281L12.001 3c.32 0 .605.15.788.384l7.918 7.908a1 1 0 0 1-1.414 1.416L13 6.424V20a1 1 0 0 1-2 0V6.402l-6.293 6.285a1 1 0 0 1-1.32.082l-.095-.083a1 1 0 0 1 .001-1.414Z"></path></symbol><symbol id="icon-eds-i-arrow-up-small" viewbox="0 0 16 16"><path d="m1.293 7.293 6-6 .063-.059.093-.069.081-.048.105-.049.104-.034.056-.013.118-.017L8 1l.076.003.122.017.113.03.085.032.063.03.098.058.06.043.05.043 6.04 6.037a1 1 0 0 1-1.414 1.414L9 4.417V14a1 1 0 0 1-2 0V4.415L2.707 8.707a1 1 0 0 1-1.32.083l-.094-.083a1 1 0 0 1 0-1.414Z"></path></symbol><symbol id="icon-eds-i-article-medium" viewbox="0 0 24 24"><path d="M8 7a1 1 0 0 0 0 2h4a1 1 0 1 0 0-2H8ZM8 11a1 1 0 1 0 0 2h8a1 1 0 1 0 0-2H8ZM7 16a1 1 0 0 1 1-1h8a1 1 0 1 1 0 2H8a1 1 0 0 1-1-1Z"></path><path clip-rule="evenodd" d="M5.545 1A2.542 2.542 0 0 0 3 3.538v16.924A2.542 2.542 0 0 0 5.545 23h12.91A2.542 2.542 0 0 0 21 20.462V3.5A2.5 2.5 0 0 0 18.5 1H5.545ZM5 3.538C5 3.245 5.24 3 5.545 3H18.5a.5.5 0 0 1 .5.5v16.962c0 .293-.24.538-.546.538H5.545A.542.542 0 0 1 5 20.462V3.538Z"></path></symbol><symbol id="icon-eds-i-book-medium" viewbox="0 0 24 24"><path d="M18.5 1A2.5 2.5 0 0 1 21 3.5v12c0 1.16-.79 2.135-1.86 2.418l-.14.031V21h1a1 1 0 0 1 .993.883L21 22a1 1 0 0 1-1 1H6.5A3.5 3.5 0 0 1 3 19.5v-15A3.5 3.5 0 0 1 6.5 1h12ZM17 18H6.5a1.5 1.5 0 0 0-1.493 1.356L5 19.5A1.5 1.5 0 0 0 6.5 21H17v-3Zm1.5-15h-12A1.5 1.5 0 0 0 5 4.5v11.837l.054-.025a3.481 3.481 0 0 1 1.254-.307L6.5 16h12a.5.5 0 0 0 .492-.41L19 15.5v-12a.5.5 0 0 0-.5-.5ZM15 6a1 1 0 0 1 0 2H9a1 1 0 1 1 0-2h6Z"></path></symbol><symbol id="icon-eds-i-book-series-medium" viewbox="0 0 24 24"><path clip-rule="evenodd" d="M1 3.786C1 2.759 1.857 2 2.82 2H6.18c.964 0 1.82.759 1.82 1.786V4h3.168c.668 0 1.298.364 1.616.938.158-.109.333-.195.523-.252l3.216-.965c.923-.277 1.962.204 2.257 1.187l4.146 13.82c.296.984-.307 1.957-1.23 2.234l-3.217.965c-.923.277-1.962-.203-2.257-1.187L13 10.005v10.21c0 1.04-.878 1.785-1.834 1.785H7.833c-.291 0-.575-.07-.83-.195A1.849 1.849 0 0 1 6.18 22H2.821C1.857 22 1 21.241 1 20.214V3.786ZM3 4v11h3V4H3Zm0 16v-3h3v3H3Zm15.075-.04-.814-2.712 2.874-.862.813 2.712-2.873.862Zm1.485-5.49-2.874.862-2.634-8.782 2.873-.862 2.635 8.782ZM8 20V6h3v14H8Z" fill-rule="evenodd"></path></symbol><symbol id="icon-eds-i-calendar-acceptance-medium" viewbox="0 0 24 24"><path d="M17 2a1 1 0 0 1 1 1v1h1.5C20.817 4 22 5.183 22 6.5v13c0 1.317-1.183 2.5-2.5 2.5h-15C3.183 22 2 20.817 2 19.5v-13C2 5.183 3.183 4 4.5 4a1 1 0 1 1 0 2c-.212 0-.5.288-.5.5v13c0 .212.288.5.5.5h15c.212 0 .5-.288.5-.5v-13c0-.212-.288-.5-.5-.5H18v1a1 1 0 0 1-2 0V3a1 1 0 0 1 1-1Zm-.534 7.747a1 1 0 0 1 .094 1.412l-4.846 5.538a1 1 0 0 1-1.352.141l-2.77-2.076a1 1 0 0 1 1.2-1.6l2.027 1.519 4.236-4.84a1 1 0 0 1 1.411-.094ZM7.5 2a1 1 0 0 1 1 1v1H14a1 1 0 0 1 0 2H8.5v1a1 1 0 1 1-2 0V3a1 1 0 0 1 1-1Z"></path></symbol><symbol id="icon-eds-i-calendar-date-medium" viewbox="0 0 24 24"><path d="M17 2a1 1 0 0 1 1 1v1h1.5C20.817 4 22 5.183 22 6.5v13c0 1.317-1.183 2.5-2.5 2.5h-15C3.183 22 2 20.817 2 19.5v-13C2 5.183 3.183 4 4.5 4a1 1 0 1 1 0 2c-.212 0-.5.288-.5.5v13c0 .212.288.5.5.5h15c.212 0 .5-.288.5-.5v-13c0-.212-.288-.5-.5-.5H18v1a1 1 0 0 1-2 0V3a1 1 0 0 1 1-1ZM8 15a1 1 0 1 1 0 2 1 1 0 0 1 0-2Zm4 0a1 1 0 1 1 0 2 1 1 0 0 1 0-2Zm-4-4a1 1 0 1 1 0 2 1 1 0 0 1 0-2Zm4 0a1 1 0 1 1 0 2 1 1 0 0 1 0-2Zm4 0a1 1 0 1 1 0 2 1 1 0 0 1 0-2ZM7.5 2a1 1 0 0 1 1 1v1H14a1 1 0 0 1 0 2H8.5v1a1 1 0 1 1-2 0V3a1 1 0 0 1 1-1Z"></path></symbol><symbol id="icon-eds-i-calendar-decision-medium" viewbox="0 0 24 24"><path d="M17 2a1 1 0 0 1 1 1v1h1.5C20.817 4 22 5.183 22 6.5v13c0 1.317-1.183 2.5-2.5 2.5h-15C3.183 22 2 20.817 2 19.5v-13C2 5.183 3.183 4 4.5 4a1 1 0 1 1 0 2c-.212 0-.5.288-.5.5v13c0 .212.288.5.5.5h15c.212 0 .5-.288.5-.5v-13c0-.212-.288-.5-.5-.5H18v1a1 1 0 0 1-2 0V3a1 1 0 0 1 1-1Zm-2.935 8.246 2.686 2.645c.34.335.34.883 0 1.218l-2.686 2.645a.858.858 0 0 1-1.213-.009.854.854 0 0 1 .009-1.21l1.05-1.035H7.984a.992.992 0 0 1-.984-1c0-.552.44-1 .984-1h5.928l-1.051-1.036a.854.854 0 0 1-.085-1.121l.076-.088a.858.858 0 0 1 1.213-.009ZM7.5 2a1 1 0 0 1 1 1v1H14a1 1 0 0 1 0 2H8.5v1a1 1 0 1 1-2 0V3a1 1 0 0 1 1-1Z"></path></symbol><symbol id="icon-eds-i-calendar-impact-factor-medium" viewbox="0 0 24 24"><path d="M17 2a1 1 0 0 1 1 1v1h1.5C20.817 4 22 5.183 22 6.5v13c0 1.317-1.183 2.5-2.5 2.5h-15C3.183 22 2 20.817 2 19.5v-13C2 5.183 3.183 4 4.5 4a1 1 0 1 1 0 2c-.212 0-.5.288-.5.5v13c0 .212.288.5.5.5h15c.212 0 .5-.288.5-.5v-13c0-.212-.288-.5-.5-.5H18v1a1 1 0 0 1-2 0V3a1 1 0 0 1 1-1Zm-3.2 6.924a.48.48 0 0 1 .125.544l-1.52 3.283h2.304c.27 0 .491.215.491.483a.477.477 0 0 1-.13.327l-4.18 4.484a.498.498 0 0 1-.69.031.48.48 0 0 1-.125-.544l1.52-3.284H9.291a.487.487 0 0 1-.491-.482c0-.121.047-.238.13-.327l4.18-4.484a.498.498 0 0 1 .69-.031ZM7.5 2a1 1 0 0 1 1 1v1H14a1 1 0 0 1 0 2H8.5v1a1 1 0 1 1-2 0V3a1 1 0 0 1 1-1Z"></path></symbol><symbol id="icon-eds-i-call-papers-medium" viewbox="0 0 24 24"><g><path d="m20.707 2.883-1.414 1.414a1 1 0 0 0 1.414 1.414l1.414-1.414a1 1 0 0 0-1.414-1.414Z"></path><path clip-rule="evenodd" d="M6 16.054c0 2.026 1.052 2.943 3 2.943a1 1 0 1 1 0 2c-2.996 0-5-1.746-5-4.943v-1.227a4.068 4.068 0 0 1-1.83-1.189 4.553 4.553 0 0 1-.87-1.455 4.868 4.868 0 0 1-.3-1.686c0-1.17.417-2.298 1.17-3.14.38-.426.834-.767 1.338-1 .51-.237 1.06-.36 1.617-.36L6.632 6H7l7.932-2.895A2.363 2.363 0 0 1 18 5.36v9.28a2.36 2.36 0 0 1-3.069 2.25l.084.03L7 14.997H6v1.057Zm9.637-11.057a.415.415 0 0 0-.083.008L8 7.638v5.536l7.424 1.786.104.02c.035.01.072.02.109.02.2 0 .363-.16.363-.36V5.36c0-.2-.163-.363-.363-.363Zm-9.638 3h-.874a1.82 1.82 0 0 0-.625.111l-.15.063a2.128 2.128 0 0 0-.689.517c-.42.47-.661 1.123-.661 1.81 0 .34.06.678.176.992.114.308.28.585.485.816.4.447.925.691 1.464.691h.874v-5Z"></path><path d="M20 8.997h2a1 1 0 1 1 0 2h-2a1 1 0 1 1 0-2ZM20.707 14.293l1.414 1.414a1 1 0 0 1-1.414 1.414l-1.414-1.414a1 1 0 0 1 1.414-1.414Z"></path></g></symbol><symbol id="icon-eds-i-card-medium" viewbox="0 0 24 24"><path d="M19.615 2c.315 0 .716.067 1.14.279.76.38 1.245 1.107 1.245 2.106v15.23c0 .315-.067.716-.279 1.14-.38.76-1.107 1.245-2.106 1.245H4.385a2.56 2.56 0 0 1-1.14-.279C2.485 21.341 2 20.614 2 19.615V4.385c0-.315.067-.716.279-1.14C2.659 2.485 3.386 2 4.385 2h15.23Zm0 2H4.385c-.213 0-.265.034-.317.14A.71.71 0 0 0 4 4.385v15.23c0 .213.034.265.14.317a.71.71 0 0 0 .245.068h15.23c.213 0 .265-.034.317-.14a.71.71 0 0 0 .068-.245V4.385c0-.213-.034-.265-.14-.317A.71.71 0 0 0 19.615 4ZM17 16a1 1 0 0 1 0 2H7a1 1 0 0 1 0-2h10Zm0-3a1 1 0 0 1 0 2H7a1 1 0 0 1 0-2h10Zm-.5-7A1.5 1.5 0 0 1 18 7.5v3a1.5 1.5 0 0 1-1.5 1.5h-9A1.5 1.5 0 0 1 6 10.5v-3A1.5 1.5 0 0 1 7.5 6h9ZM16 8H8v2h8V8Z"></path></symbol><symbol id="icon-eds-i-cart-medium" viewbox="0 0 24 24"><path d="M5.76 1a1 1 0 0 1 .994.902L7.155 6h13.34c.18 0 .358.02.532.057l.174.045a2.5 2.5 0 0 1 1.693 3.103l-2.069 7.03c-.36 1.099-1.398 1.823-2.49 1.763H8.65c-1.272.015-2.352-.927-2.546-2.244L4.852 3H2a1 1 0 0 1-.993-.883L1 2a1 1 0 0 1 1-1h3.76Zm2.328 14.51a.555.555 0 0 0 .55.488l9.751.001a.533.533 0 0 0 .527-.357l2.059-7a.5.5 0 0 0-.48-.642H7.351l.737 7.51ZM18 19a2 2 0 1 1 0 4 2 2 0 0 1 0-4ZM8 19a2 2 0 1 1 0 4 2 2 0 0 1 0-4Z"></path></symbol><symbol id="icon-eds-i-check-circle-medium" viewbox="0 0 24 24"><path d="M12 1c6.075 0 11 4.925 11 11s-4.925 11-11 11S1 18.075 1 12 5.925 1 12 1Zm0 2a9 9 0 1 0 0 18 9 9 0 0 0 0-18Zm5.125 4.72a1 1 0 0 1 .156 1.405l-6 7.5a1 1 0 0 1-1.421.143l-3-2.5a1 1 0 0 1 1.28-1.536l2.217 1.846 5.362-6.703a1 1 0 0 1 1.406-.156Z"></path></symbol><symbol id="icon-eds-i-check-filled-medium" viewbox="0 0 24 24"><path d="M12 1c6.075 0 11 4.925 11 11s-4.925 11-11 11S1 18.075 1 12 5.925 1 12 1Zm5.125 6.72a1 1 0 0 0-1.406.155l-5.362 6.703-2.217-1.846a1 1 0 1 0-1.28 1.536l3 2.5a1 1 0 0 0 1.42-.143l6-7.5a1 1 0 0 0-.155-1.406Z"></path></symbol><symbol id="icon-eds-i-chevron-down-medium" viewbox="0 0 24 24"><path d="M3.305 8.28a1 1 0 0 0-.024 1.415l7.495 7.762c.314.345.757.543 1.224.543.467 0 .91-.198 1.204-.522l7.515-7.783a1 1 0 1 0-1.438-1.39L12 15.845l-7.28-7.54A1 1 0 0 0 3.4 8.2l-.096.082Z"></path></symbol><symbol id="icon-eds-i-chevron-down-small" viewbox="0 0 16 16"><path d="M13.692 5.278a1 1 0 0 1 .03 1.414L9.103 11.51a1.491 1.491 0 0 1-2.188.019L2.278 6.692a1 1 0 0 1 1.444-1.384L8 9.771l4.278-4.463a1 1 0 0 1 1.318-.111l.096.081Z"></path></symbol><symbol id="icon-eds-i-chevron-left-medium" viewbox="0 0 24 24"><path d="M15.72 3.305a1 1 0 0 0-1.415-.024l-7.762 7.495A1.655 1.655 0 0 0 6 12c0 .467.198.91.522 1.204l7.783 7.515a1 1 0 1 0 1.39-1.438L8.155 12l7.54-7.28A1 1 0 0 0 15.8 3.4l-.082-.096Z"></path></symbol><symbol id="icon-eds-i-chevron-left-small" viewbox="0 0 16 16"><path d="M10.722 2.308a1 1 0 0 0-1.414-.03L4.49 6.897a1.491 1.491 0 0 0-.019 2.188l4.838 4.637a1 1 0 1 0 1.384-1.444L6.229 8l4.463-4.278a1 1 0 0 0 .111-1.318l-.081-.096Z"></path></symbol><symbol id="icon-eds-i-chevron-right-medium" viewbox="0 0 24 24"><path d="M8.28 3.305a1 1 0 0 1 1.415-.024l7.762 7.495c.345.314.543.757.543 1.224 0 .467-.198.91-.522 1.204l-7.783 7.515a1 1 0 1 1-1.39-1.438L15.845 12l-7.54-7.28A1 1 0 0 1 8.2 3.4l.082-.096Z"></path></symbol><symbol id="icon-eds-i-chevron-right-small" viewbox="0 0 16 16"><path d="M5.278 2.308a1 1 0 0 1 1.414-.03l4.819 4.619a1.491 1.491 0 0 1 .019 2.188l-4.838 4.637a1 1 0 1 1-1.384-1.444L9.771 8 5.308 3.722a1 1 0 0 1-.111-1.318l.081-.096Z"></path></symbol><symbol id="icon-eds-i-chevron-up-medium" viewbox="0 0 24 24"><path d="M20.695 15.72a1 1 0 0 0 .024-1.415l-7.495-7.762A1.655 1.655 0 0 0 12 6c-.467 0-.91.198-1.204.522l-7.515 7.783a1 1 0 1 0 1.438 1.39L12 8.155l7.28 7.54a1 1 0 0 0 1.319.106l.096-.082Z"></path></symbol><symbol id="icon-eds-i-chevron-up-small" viewbox="0 0 16 16"><path d="M13.692 10.722a1 1 0 0 0 .03-1.414L9.103 4.49a1.491 1.491 0 0 0-2.188-.019L2.278 9.308a1 1 0 0 0 1.444 1.384L8 6.229l4.278 4.463a1 1 0 0 0 1.318.111l.096-.081Z"></path></symbol><symbol id="icon-eds-i-citations-medium" viewbox="0 0 24 24"><path d="M15.59 1a1 1 0 0 1 .706.291l5.41 5.385a1 1 0 0 1 .294.709v13.077c0 .674-.269 1.32-.747 1.796a2.549 2.549 0 0 1-1.798.742h-5.843a1 1 0 1 1 0-2h5.843a.549.549 0 0 0 .387-.16.535.535 0 0 0 .158-.378V7.8L15.178 3H5.545a.543.543 0 0 0-.538.451L5 3.538v8.607a1 1 0 0 1-2 0V3.538A2.542 2.542 0 0 1 5.545 1h10.046ZM5.483 14.35c.197.26.17.62-.049.848l-.095.083-.016.011c-.36.24-.628.45-.804.634-.393.409-.59.93-.59 1.562.077-.019.192-.028.345-.028.442 0 .84.158 1.195.474.355.316.532.716.532 1.2 0 .501-.173.9-.518 1.198-.345.298-.767.446-1.266.446-.672 0-1.209-.195-1.612-.585-.403-.39-.604-.976-.604-1.757 0-.744.11-1.39.33-1.938.222-.549.49-1.009.807-1.38a4.28 4.28 0 0 1 .992-.88c.07-.043.148-.087.232-.133a.881.881 0 0 1 1.121.245Zm5 0c.197.26.17.62-.049.848l-.095.083-.016.011c-.36.24-.628.45-.804.634-.393.409-.59.93-.59 1.562.077-.019.192-.028.345-.028.442 0 .84.158 1.195.474.355.316.532.716.532 1.2 0 .501-.173.9-.518 1.198-.345.298-.767.446-1.266.446-.672 0-1.209-.195-1.612-.585-.403-.39-.604-.976-.604-1.757 0-.744.11-1.39.33-1.938.222-.549.49-1.009.807-1.38a4.28 4.28 0 0 1 .992-.88c.07-.043.148-.087.232-.133a.881.881 0 0 1 1.121.245Z"></path></symbol><symbol id="icon-eds-i-clipboard-check-medium" viewbox="0 0 24 24"><path d="M14.4 1c1.238 0 2.274.865 2.536 2.024L18.5 3C19.886 3 21 4.14 21 5.535v14.93C21 21.86 19.886 23 18.5 23h-13C4.114 23 3 21.86 3 20.465V5.535C3 4.14 4.114 3 5.5 3h1.57c.27-1.147 1.3-2 2.53-2h4.8Zm4.115 4-1.59.024A2.601 2.601 0 0 1 14.4 7H9.6c-1.23 0-2.26-.853-2.53-2H5.5c-.27 0-.5.234-.5.535v14.93c0 .3.23.535.5.535h13c.27 0 .5-.234.5-.535V5.535c0-.3-.23-.535-.485-.535Zm-1.909 4.205a1 1 0 0 1 .19 1.401l-5.334 7a1 1 0 0 1-1.344.23l-2.667-1.75a1 1 0 1 1 1.098-1.672l1.887 1.238 4.769-6.258a1 1 0 0 1 1.401-.19ZM14.4 3H9.6a.6.6 0 0 0-.6.6v.8a.6.6 0 0 0 .6.6h4.8a.6.6 0 0 0 .6-.6v-.8a.6.6 0 0 0-.6-.6Z"></path></symbol><symbol id="icon-eds-i-clipboard-report-medium" viewbox="0 0 24 24"><path d="M14.4 1c1.238 0 2.274.865 2.536 2.024L18.5 3C19.886 3 21 4.14 21 5.535v14.93C21 21.86 19.886 23 18.5 23h-13C4.114 23 3 21.86 3 20.465V5.535C3 4.14 4.114 3 5.5 3h1.57c.27-1.147 1.3-2 2.53-2h4.8Zm4.115 4-1.59.024A2.601 2.601 0 0 1 14.4 7H9.6c-1.23 0-2.26-.853-2.53-2H5.5c-.27 0-.5.234-.5.535v14.93c0 .3.23.535.5.535h13c.27 0 .5-.234.5-.535V5.535c0-.3-.23-.535-.485-.535Zm-2.658 10.929a1 1 0 0 1 0 2H8a1 1 0 0 1 0-2h7.857Zm0-3.929a1 1 0 0 1 0 2H8a1 1 0 0 1 0-2h7.857ZM14.4 3H9.6a.6.6 0 0 0-.6.6v.8a.6.6 0 0 0 .6.6h4.8a.6.6 0 0 0 .6-.6v-.8a.6.6 0 0 0-.6-.6Z"></path></symbol><symbol id="icon-eds-i-close-medium" viewbox="0 0 24 24"><path d="M12 1c6.075 0 11 4.925 11 11s-4.925 11-11 11S1 18.075 1 12 5.925 1 12 1Zm0 2a9 9 0 1 0 0 18 9 9 0 0 0 0-18ZM8.707 7.293 12 10.585l3.293-3.292a1 1 0 0 1 1.414 1.414L13.415 12l3.292 3.293a1 1 0 0 1-1.414 1.414L12 13.415l-3.293 3.292a1 1 0 1 1-1.414-1.414L10.585 12 7.293 8.707a1 1 0 0 1 1.414-1.414Z"></path></symbol><symbol id="icon-eds-i-cloud-upload-medium" viewbox="0 0 24 24"><path d="m12.852 10.011.028-.004L13 10l.075.003.126.017.086.022.136.052.098.052.104.074.082.073 3 3a1 1 0 0 1 0 1.414l-.094.083a1 1 0 0 1-1.32-.083L14 13.416V20a1 1 0 0 1-2 0v-6.586l-1.293 1.293a1 1 0 0 1-1.32.083l-.094-.083a1 1 0 0 1 0-1.414l3-3 .112-.097.11-.071.114-.054.105-.035.118-.025Zm.587-7.962c3.065.362 5.497 2.662 5.992 5.562l.013.085.207.073c2.117.782 3.496 2.845 3.337 5.097l-.022.226c-.297 2.561-2.503 4.491-5.124 4.502a1 1 0 1 1-.009-2c1.619-.007 2.967-1.186 3.147-2.733.179-1.542-.86-2.979-2.487-3.353-.512-.149-.894-.579-.981-1.165-.21-2.237-2-4.035-4.308-4.308-2.31-.273-4.497 1.06-5.25 3.19l-.049.113c-.234.468-.718.756-1.176.743-1.418.057-2.689.857-3.32 2.084a3.668 3.668 0 0 0 .262 3.798c.796 1.136 2.169 1.764 3.583 1.635a1 1 0 1 1 .182 1.992c-2.125.194-4.193-.753-5.403-2.48a5.668 5.668 0 0 1-.403-5.86c.85-1.652 2.449-2.79 4.323-3.092l.287-.039.013-.028c1.207-2.741 4.125-4.404 7.186-4.042Z"></path></symbol><symbol id="icon-eds-i-collection-medium" viewbox="0 0 24 24"><path d="M21 7a1 1 0 0 1 1 1v12.5a2.5 2.5 0 0 1-2.5 2.5H8a1 1 0 0 1 0-2h11.5a.5.5 0 0 0 .5-.5V8a1 1 0 0 1 1-1Zm-5.5-5A2.5 2.5 0 0 1 18 4.5v12a2.5 2.5 0 0 1-2.5 2.5h-11A2.5 2.5 0 0 1 2 16.5v-12A2.5 2.5 0 0 1 4.5 2h11Zm0 2h-11a.5.5 0 0 0-.5.5v12a.5.5 0 0 0 .5.5h11a.5.5 0 0 0 .5-.5v-12a.5.5 0 0 0-.5-.5ZM13 13a1 1 0 0 1 0 2H7a1 1 0 0 1 0-2h6Zm0-3.5a1 1 0 0 1 0 2H7a1 1 0 0 1 0-2h6ZM13 6a1 1 0 0 1 0 2H7a1 1 0 1 1 0-2h6Z"></path></symbol><symbol id="icon-eds-i-conference-series-medium" viewbox="0 0 24 24"><path clip-rule="evenodd" d="M4.5 2A2.5 2.5 0 0 0 2 4.5v11A2.5 2.5 0 0 0 4.5 18h2.37l-2.534 2.253a1 1 0 0 0 1.328 1.494L9.88 18H11v3a1 1 0 1 0 2 0v-3h1.12l4.216 3.747a1 1 0 0 0 1.328-1.494L17.13 18h2.37a2.5 2.5 0 0 0 2.5-2.5v-11A2.5 2.5 0 0 0 19.5 2h-15ZM20 6V4.5a.5.5 0 0 0-.5-.5h-15a.5.5 0 0 0-.5.5V6h16ZM4 8v7.5a.5.5 0 0 0 .5.5h15a.5.5 0 0 0 .5-.5V8H4Z" fill-rule="evenodd"></path></symbol><symbol id="icon-eds-i-delivery-medium" viewbox="0 0 24 24"><path d="M8.51 20.598a3.037 3.037 0 0 1-3.02 0A2.968 2.968 0 0 1 4.161 19L3.5 19A2.5 2.5 0 0 1 1 16.5v-11A2.5 2.5 0 0 1 3.5 3h10a2.5 2.5 0 0 1 2.45 2.004L16 5h2.527c.976 0 1.855.585 2.27 1.49l2.112 4.62a1 1 0 0 1 .091.416v4.856C23 17.814 21.889 19 20.484 19h-.523a1.01 1.01 0 0 1-.121-.007 2.96 2.96 0 0 1-1.33 1.605 3.037 3.037 0 0 1-3.02 0A2.968 2.968 0 0 1 14.161 19H9.838a2.968 2.968 0 0 1-1.327 1.597Zm-2.024-3.462a.955.955 0 0 0-.481.73L5.999 18l.001.022a.944.944 0 0 0 .388.777l.098.065c.316.181.712.181 1.028 0A.97.97 0 0 0 8 17.978a.95.95 0 0 0-.486-.842 1.037 1.037 0 0 0-1.028 0Zm10 0a.955.955 0 0 0-.481.73l-.005.156a.944.944 0 0 0 .388.777l.098.065c.316.181.712.181 1.028 0a.97.97 0 0 0 .486-.886.95.95 0 0 0-.486-.842 1.037 1.037 0 0 0-1.028 0ZM21 12h-5v3.17a3.038 3.038 0 0 1 2.51.232 2.993 2.993 0 0 1 1.277 1.45l.058.155.058-.005.581-.002c.27 0 .516-.263.516-.618V12Zm-7.5-7h-10a.5.5 0 0 0-.5.5v11a.5.5 0 0 0 .5.5h.662a2.964 2.964 0 0 1 1.155-1.491l.172-.107a3.037 3.037 0 0 1 3.022 0A2.987 2.987 0 0 1 9.843 17H13.5a.5.5 0 0 0 .5-.5v-11a.5.5 0 0 0-.5-.5Zm5.027 2H16v3h4.203l-1.224-2.677a.532.532 0 0 0-.375-.316L18.527 7Z"></path></symbol><symbol id="icon-eds-i-download-medium" viewbox="0 0 24 24"><path d="M22 18.5a3.5 3.5 0 0 1-3.5 3.5h-13A3.5 3.5 0 0 1 2 18.5V18a1 1 0 0 1 2 0v.5A1.5 1.5 0 0 0 5.5 20h13a1.5 1.5 0 0 0 1.5-1.5V18a1 1 0 0 1 2 0v.5Zm-3.293-7.793-6 6-.063.059-.093.069-.081.048-.105.049-.104.034-.056.013-.118.017L12 17l-.076-.003-.122-.017-.113-.03-.085-.032-.063-.03-.098-.058-.06-.043-.05-.043-6.04-6.037a1 1 0 0 1 1.414-1.414l4.294 4.29L11 3a1 1 0 0 1 2 0l.001 10.585 4.292-4.292a1 1 0 0 1 1.32-.083l.094.083a1 1 0 0 1 0 1.414Z"></path></symbol><symbol id="icon-eds-i-edit-medium" viewbox="0 0 24 24"><path d="M17.149 2a2.38 2.38 0 0 1 1.699.711l2.446 2.46a2.384 2.384 0 0 1 .005 3.38L10.01 19.906a1 1 0 0 1-.434.257l-6.3 1.8a1 1 0 0 1-1.237-1.237l1.8-6.3a1 1 0 0 1 .257-.434L15.443 2.718A2.385 2.385 0 0 1 17.15 2Zm-3.874 5.689-7.586 7.536-1.234 4.319 4.318-1.234 7.54-7.582-3.038-3.039ZM17.149 4a.395.395 0 0 0-.286.126L14.695 6.28l3.029 3.029 2.162-2.173a.384.384 0 0 0 .106-.197L20 6.864c0-.103-.04-.2-.119-.278l-2.457-2.47A.385.385 0 0 0 17.149 4Z"></path></symbol><symbol id="icon-eds-i-education-medium" viewbox="0 0 24 24"><path clip-rule="evenodd" d="M12.41 2.088a1 1 0 0 0-.82 0l-10 4.5a1 1 0 0 0 0 1.824L3 9.047v7.124A3.001 3.001 0 0 0 4 22a3 3 0 0 0 1-5.83V9.948l1 .45V14.5a1 1 0 0 0 .087.408L7 14.5c-.913.408-.912.41-.912.41l.001.003.003.006.007.015a1.988 1.988 0 0 0 .083.16c.054.097.131.225.236.373.21.297.53.68.993 1.057C8.351 17.292 9.824 18 12 18c2.176 0 3.65-.707 4.589-1.476.463-.378.783-.76.993-1.057a4.162 4.162 0 0 0 .319-.533l.007-.015.003-.006v-.003h.002s0-.002-.913-.41l.913.408A1 1 0 0 0 18 14.5v-4.103l4.41-1.985a1 1 0 0 0 0-1.824l-10-4.5ZM16 11.297l-3.59 1.615a1 1 0 0 1-.82 0L8 11.297v2.94a3.388 3.388 0 0 0 .677.739C9.267 15.457 10.294 16 12 16s2.734-.543 3.323-1.024a3.388 3.388 0 0 0 .677-.739v-2.94ZM4.437 7.5 12 4.097 19.563 7.5 12 10.903 4.437 7.5ZM3 19a1 1 0 1 1 2 0 1 1 0 0 1-2 0Z" fill-rule="evenodd"></path></symbol><symbol id="icon-eds-i-error-diamond-medium" viewbox="0 0 24 24"><path d="M12.002 1c.702 0 1.375.279 1.871.775l8.35 8.353a2.646 2.646 0 0 1 .001 3.744l-8.353 8.353a2.646 2.646 0 0 1-3.742 0l-8.353-8.353a2.646 2.646 0 0 1 0-3.744l8.353-8.353.156-.142c.424-.362.952-.58 1.507-.625l.21-.008Zm0 2a.646.646 0 0 0-.38.123l-.093.08-8.34 8.34a.646.646 0 0 0-.18.355L3 12c0 .171.068.336.19.457l8.353 8.354a.646.646 0 0 0 .914 0l8.354-8.354a.646.646 0 0 0-.001-.914l-8.351-8.354A.646.646 0 0 0 12.002 3ZM12 14.5a1.5 1.5 0 0 1 .144 2.993L12 17.5a1.5 1.5 0 0 1 0-3ZM12 6a1 1 0 0 1 1 1v5a1 1 0 0 1-2 0V7a1 1 0 0 1 1-1Z"></path></symbol><symbol id="icon-eds-i-error-filled-medium" viewbox="0 0 24 24"><path d="M12.002 1c.702 0 1.375.279 1.871.775l8.35 8.353a2.646 2.646 0 0 1 .001 3.744l-8.353 8.353a2.646 2.646 0 0 1-3.742 0l-8.353-8.353a2.646 2.646 0 0 1 0-3.744l8.353-8.353.156-.142c.424-.362.952-.58 1.507-.625l.21-.008ZM12 14.5a1.5 1.5 0 0 0 0 3l.144-.007A1.5 1.5 0 0 0 12 14.5ZM12 6a1 1 0 0 0-1 1v5a1 1 0 0 0 2 0V7a1 1 0 0 0-1-1Z"></path></symbol><symbol id="icon-eds-i-external-link-medium" viewbox="0 0 24 24"><path d="M9 2a1 1 0 1 1 0 2H4.6c-.371 0-.6.209-.6.5v15c0 .291.229.5.6.5h14.8c.371 0 .6-.209.6-.5V15a1 1 0 0 1 2 0v4.5c0 1.438-1.162 2.5-2.6 2.5H4.6C3.162 22 2 20.938 2 19.5v-15C2 3.062 3.162 2 4.6 2H9Zm6 0h6l.075.003.126.017.111.03.111.044.098.052.096.067.09.08c.036.035.068.073.097.112l.071.11.054.114.035.105.03.148L22 3v6a1 1 0 0 1-2 0V5.414l-6.693 6.693a1 1 0 0 1-1.414-1.414L18.584 4H15a1 1 0 0 1-.993-.883L14 3a1 1 0 0 1 1-1Z"></path></symbol><symbol id="icon-eds-i-external-link-small" viewbox="0 0 16 16"><path d="M5 1a1 1 0 1 1 0 2l-2-.001V13L13 13v-2a1 1 0 0 1 2 0v2c0 1.15-.93 2-2.067 2H3.067C1.93 15 1 14.15 1 13V3c0-1.15.93-2 2.067-2H5Zm4 0h5l.075.003.126.017.111.03.111.044.098.052.096.067.09.08.044.047.073.093.051.083.054.113.035.105.03.148L15 2v5a1 1 0 0 1-2 0V4.414L9.107 8.307a1 1 0 0 1-1.414-1.414L11.584 3H9a1 1 0 0 1-.993-.883L8 2a1 1 0 0 1 1-1Z"></path></symbol><symbol id="icon-eds-i-file-download-medium" viewbox="0 0 24 24"><path d="M14.5 1a1 1 0 0 1 .707.293l5.5 5.5A1 1 0 0 1 21 7.5v12.962A2.542 2.542 0 0 1 18.455 23H5.545A2.542 2.542 0 0 1 3 20.462V3.538A2.542 2.542 0 0 1 5.545 1H14.5Zm-.415 2h-8.54A.542.542 0 0 0 5 3.538v16.924c0 .296.243.538.545.538h12.91a.542.542 0 0 0 .545-.538V7.915L14.085 3ZM12 7a1 1 0 0 1 1 1v6.585l2.293-2.292a1 1 0 0 1 1.32-.083l.094.083a1 1 0 0 1 0 1.414l-4 4a1.008 1.008 0 0 1-.112.097l-.11.071-.114.054-.105.035-.149.03L12 18l-.075-.003-.126-.017-.111-.03-.111-.044-.098-.052-.096-.067-.09-.08-4-4a1 1 0 0 1 1.414-1.414L11 14.585V8a1 1 0 0 1 1-1Z"></path></symbol><symbol id="icon-eds-i-file-report-medium" viewbox="0 0 24 24"><path d="M14.5 1a1 1 0 0 1 .707.293l5.5 5.5A1 1 0 0 1 21 7.5v12.962c0 .674-.269 1.32-.747 1.796a2.549 2.549 0 0 1-1.798.742H5.545c-.674 0-1.32-.267-1.798-.742A2.535 2.535 0 0 1 3 20.462V3.538A2.542 2.542 0 0 1 5.545 1H14.5Zm-.415 2h-8.54A.542.542 0 0 0 5 3.538v16.924c0 .142.057.278.158.379.102.102.242.159.387.159h12.91a.549.549 0 0 0 .387-.16.535.535 0 0 0 .158-.378V7.915L14.085 3ZM16 17a1 1 0 0 1 0 2H8a1 1 0 0 1 0-2h8Zm0-3a1 1 0 0 1 0 2H8a1 1 0 0 1 0-2h8Zm-4.793-6.207L13 9.585l1.793-1.792a1 1 0 0 1 1.32-.083l.094.083a1 1 0 0 1 0 1.414l-2.5 2.5a1 1 0 0 1-1.414 0L10.5 9.915l-1.793 1.792a1 1 0 0 1-1.32.083l-.094-.083a1 1 0 0 1 0-1.414l2.5-2.5a1 1 0 0 1 1.414 0Z"></path></symbol><symbol id="icon-eds-i-file-text-medium" viewbox="0 0 24 24"><path d="M14.5 1a1 1 0 0 1 .707.293l5.5 5.5A1 1 0 0 1 21 7.5v12.962A2.542 2.542 0 0 1 18.455 23H5.545A2.542 2.542 0 0 1 3 20.462V3.538A2.542 2.542 0 0 1 5.545 1H14.5Zm-.415 2h-8.54A.542.542 0 0 0 5 3.538v16.924c0 .296.243.538.545.538h12.91a.542.542 0 0 0 .545-.538V7.915L14.085 3ZM16 15a1 1 0 0 1 0 2H8a1 1 0 0 1 0-2h8Zm0-4a1 1 0 0 1 0 2H8a1 1 0 0 1 0-2h8Zm-5-4a1 1 0 0 1 0 2H8a1 1 0 1 1 0-2h3Z"></path></symbol><symbol id="icon-eds-i-file-upload-medium" viewbox="0 0 24 24"><path d="M14.5 1a1 1 0 0 1 .707.293l5.5 5.5A1 1 0 0 1 21 7.5v12.962A2.542 2.542 0 0 1 18.455 23H5.545A2.542 2.542 0 0 1 3 20.462V3.538A2.542 2.542 0 0 1 5.545 1H14.5Zm-.415 2h-8.54A.542.542 0 0 0 5 3.538v16.924c0 .296.243.538.545.538h12.91a.542.542 0 0 0 .545-.538V7.915L14.085 3Zm-2.233 4.011.058-.007L12 7l.075.003.126.017.111.03.111.044.098.052.104.074.082.073 4 4a1 1 0 0 1 0 1.414l-.094.083a1 1 0 0 1-1.32-.083L13 10.415V17a1 1 0 0 1-2 0v-6.585l-2.293 2.292a1 1 0 0 1-1.32.083l-.094-.083a1 1 0 0 1 0-1.414l4-4 .112-.097.11-.071.114-.054.105-.035.118-.025Z"></path></symbol><symbol id="icon-eds-i-filter-medium" viewbox="0 0 24 24"><path d="M21 2a1 1 0 0 1 .82 1.573L15 13.314V18a1 1 0 0 1-.31.724l-.09.076-4 3A1 1 0 0 1 9 21v-7.684L2.18 3.573a1 1 0 0 1 .707-1.567L3 2h18Zm-1.921 2H4.92l5.9 8.427a1 1 0 0 1 .172.45L11 13v6l2-1.5V13a1 1 0 0 1 .117-.469l.064-.104L19.079 4Z"></path></symbol><symbol id="icon-eds-i-funding-medium" viewbox="0 0 24 24"><path clip-rule="evenodd" d="M23 8A7 7 0 1 0 9 8a7 7 0 0 0 14 0ZM9.006 12.225A4.07 4.07 0 0 0 6.12 11.02H2a.979.979 0 1 0 0 1.958h4.12c.558 0 1.094.222 1.489.617l2.207 2.288c.27.27.27.687.012.944a.656.656 0 0 1-.928 0L7.744 15.67a.98.98 0 0 0-1.386 1.384l1.157 1.158c.535.536 1.244.791 1.946.765l.041.002h6.922c.874 0 1.597.748 1.597 1.688 0 .203-.146.354-.309.354H7.755c-.487 0-.96-.178-1.339-.504L2.64 17.259a.979.979 0 0 0-1.28 1.482L5.137 22c.733.631 1.66.979 2.618.979h9.957c1.26 0 2.267-1.043 2.267-2.312 0-2.006-1.584-3.646-3.555-3.646h-4.529a2.617 2.617 0 0 0-.681-2.509l-2.208-2.287ZM16 3a5 5 0 1 0 0 10 5 5 0 0 0 0-10Zm.979 3.5a.979.979 0 1 0-1.958 0v3a.979.979 0 1 0 1.958 0v-3Z" fill-rule="evenodd"></path></symbol><symbol id="icon-eds-i-hashtag-medium" viewbox="0 0 24 24"><path d="M12 1c6.075 0 11 4.925 11 11s-4.925 11-11 11S1 18.075 1 12 5.925 1 12 1Zm0 2a9 9 0 1 0 0 18 9 9 0 0 0 0-18ZM9.52 18.189a1 1 0 1 1-1.964-.378l.437-2.274H6a1 1 0 1 1 0-2h2.378l.592-3.076H6a1 1 0 0 1 0-2h3.354l.51-2.65a1 1 0 1 1 1.964.378l-.437 2.272h3.04l.51-2.65a1 1 0 1 1 1.964.378l-.438 2.272H18a1 1 0 0 1 0 2h-1.917l-.592 3.076H18a1 1 0 0 1 0 2h-2.893l-.51 2.652a1 1 0 1 1-1.964-.378l.437-2.274h-3.04l-.51 2.652Zm.895-4.652h3.04l.591-3.076h-3.04l-.591 3.076Z"></path></symbol><symbol id="icon-eds-i-home-medium" viewbox="0 0 24 24"><path d="M5 22a1 1 0 0 1-1-1v-8.586l-1.293 1.293a1 1 0 0 1-1.32.083l-.094-.083a1 1 0 0 1 0-1.414l10-10a1 1 0 0 1 1.414 0l10 10a1 1 0 0 1-1.414 1.414L20 12.415V21a1 1 0 0 1-1 1H5Zm7-17.585-6 5.999V20h5v-4a1 1 0 0 1 2 0v4h5v-9.585l-6-6Z"></path></symbol><symbol id="icon-eds-i-image-medium" viewbox="0 0 24 24"><path d="M19.615 2A2.385 2.385 0 0 1 22 4.385v15.23A2.385 2.385 0 0 1 19.615 22H4.385A2.385 2.385 0 0 1 2 19.615V4.385A2.385 2.385 0 0 1 4.385 2h15.23Zm0 2H4.385A.385.385 0 0 0 4 4.385v15.23c0 .213.172.385.385.385h1.244l10.228-8.76a1 1 0 0 1 1.254-.037L20 13.392V4.385A.385.385 0 0 0 19.615 4Zm-3.07 9.283L8.703 20h10.912a.385.385 0 0 0 .385-.385v-3.713l-3.455-2.619ZM9.5 6a3.5 3.5 0 1 1 0 7 3.5 3.5 0 0 1 0-7Zm0 2a1.5 1.5 0 1 0 0 3 1.5 1.5 0 0 0 0-3Z"></path></symbol><symbol id="icon-eds-i-impact-factor-medium" viewbox="0 0 24 24"><path d="M16.49 2.672c.74.694.986 1.765.632 2.712l-.04.1-1.549 3.54h1.477a2.496 2.496 0 0 1 2.485 2.34l.005.163c0 .618-.23 1.21-.642 1.675l-7.147 7.961a2.48 2.48 0 0 1-3.554.165 2.512 2.512 0 0 1-.633-2.712l.042-.103L9.108 15H7.46c-1.393 0-2.379-1.11-2.455-2.369L5 12.473c0-.593.142-1.145.628-1.692l7.307-7.944a2.48 2.48 0 0 1 3.555-.165ZM14.43 4.164l-7.33 7.97c-.083.093-.101.214-.101.34 0 .277.19.526.46.526h4.163l.097-.009c.015 0 .03.003.046.009.181.078.264.32.186.5l-2.554 5.817a.512.512 0 0 0 .127.552.48.48 0 0 0 .69-.033l7.155-7.97a.513.513 0 0 0 .13-.34.497.497 0 0 0-.49-.502h-3.988a.355.355 0 0 1-.328-.497l2.555-5.844a.512.512 0 0 0-.127-.552.48.48 0 0 0-.69.033Z"></path></symbol><symbol id="icon-eds-i-info-circle-medium" viewbox="0 0 24 24"><path d="M12 1c6.075 0 11 4.925 11 11s-4.925 11-11 11S1 18.075 1 12 5.925 1 12 1Zm0 2a9 9 0 1 0 0 18 9 9 0 0 0 0-18Zm0 7a1 1 0 0 1 1 1v5h1.5a1 1 0 0 1 0 2h-5a1 1 0 0 1 0-2H11v-4h-.5a1 1 0 0 1-.993-.883L9.5 11a1 1 0 0 1 1-1H12Zm0-4.5a1.5 1.5 0 0 1 .144 2.993L12 8.5a1.5 1.5 0 0 1 0-3Z"></path></symbol><symbol id="icon-eds-i-info-filled-medium" viewbox="0 0 24 24"><path d="M12 1c6.075 0 11 4.925 11 11s-4.925 11-11 11S1 18.075 1 12 5.925 1 12 1Zm0 9h-1.5a1 1 0 0 0-1 1l.007.117A1 1 0 0 0 10.5 12h.5v4H9.5a1 1 0 0 0 0 2h5a1 1 0 0 0 0-2H13v-5a1 1 0 0 0-1-1Zm0-4.5a1.5 1.5 0 0 0 0 3l.144-.007A1.5 1.5 0 0 0 12 5.5Z"></path></symbol><symbol id="icon-eds-i-journal-medium" viewbox="0 0 24 24"><path d="M18.5 1A2.5 2.5 0 0 1 21 3.5v14a2.5 2.5 0 0 1-2.5 2.5h-13a.5.5 0 1 0 0 1H20a1 1 0 0 1 0 2H5.5A2.5 2.5 0 0 1 3 20.5v-17A2.5 2.5 0 0 1 5.5 1h13ZM7 3H5.5a.5.5 0 0 0-.5.5v14.549l.016-.002c.104-.02.211-.035.32-.042L5.5 18H7V3Zm11.5 0H9v15h9.5a.5.5 0 0 0 .5-.5v-14a.5.5 0 0 0-.5-.5ZM16 5a1 1 0 0 1 1 1v4a1 1 0 0 1-1 1h-5a1 1 0 0 1-1-1V6a1 1 0 0 1 1-1h5Zm-1 2h-3v2h3V7Z"></path></symbol><symbol id="icon-eds-i-mail-medium" viewbox="0 0 24 24"><path d="M20.462 3C21.875 3 23 4.184 23 5.619v12.762C23 19.816 21.875 21 20.462 21H3.538C2.125 21 1 19.816 1 18.381V5.619C1 4.184 2.125 3 3.538 3h16.924ZM21 8.158l-7.378 6.258a2.549 2.549 0 0 1-3.253-.008L3 8.16v10.222c0 .353.253.619.538.619h16.924c.285 0 .538-.266.538-.619V8.158ZM20.462 5H3.538c-.264 0-.5.228-.534.542l8.65 7.334c.2.165.492.165.684.007l8.656-7.342-.001-.025c-.044-.3-.274-.516-.531-.516Z"></path></symbol><symbol id="icon-eds-i-mail-send-medium" viewbox="0 0 24 24"><path d="M20.444 5a2.562 2.562 0 0 1 2.548 2.37l.007.078.001.123v7.858A2.564 2.564 0 0 1 20.444 18H9.556A2.564 2.564 0 0 1 7 15.429l.001-7.977.007-.082A2.561 2.561 0 0 1 9.556 5h10.888ZM21 9.331l-5.46 3.51a1 1 0 0 1-1.08 0L9 9.332v6.097c0 .317.251.571.556.571h10.888a.564.564 0 0 0 .556-.571V9.33ZM20.444 7H9.556a.543.543 0 0 0-.32.105l5.763 3.706 5.766-3.706a.543.543 0 0 0-.32-.105ZM4.308 5a1 1 0 1 1 0 2H2a1 1 0 1 1 0-2h2.308Zm0 5.5a1 1 0 0 1 0 2H2a1 1 0 0 1 0-2h2.308Zm0 5.5a1 1 0 0 1 0 2H2a1 1 0 0 1 0-2h2.308Z"></path></symbol><symbol id="icon-eds-i-mentions-medium" viewbox="0 0 24 24"><path d="m9.452 1.293 5.92 5.92 2.92-2.92a1 1 0 0 1 1.415 1.414l-2.92 2.92 5.92 5.92a1 1 0 0 1 0 1.415 10.371 10.371 0 0 1-10.378 2.584l.652 3.258A1 1 0 0 1 12 23H2a1 1 0 0 1-.874-1.486l4.789-8.62C4.194 9.074 4.9 4.43 8.038 1.292a1 1 0 0 1 1.414 0Zm-2.355 13.59L3.699 21h7.081l-.689-3.442a10.392 10.392 0 0 1-2.775-2.396l-.22-.28Zm1.69-11.427-.07.09a8.374 8.374 0 0 0 11.737 11.737l.089-.071L8.787 3.456Z"></path></symbol><symbol id="icon-eds-i-menu-medium" viewbox="0 0 24 24"><path d="M21 4a1 1 0 0 1 0 2H3a1 1 0 1 1 0-2h18Zm-4 7a1 1 0 0 1 0 2H3a1 1 0 0 1 0-2h14Zm4 7a1 1 0 0 1 0 2H3a1 1 0 0 1 0-2h18Z"></path></symbol><symbol id="icon-eds-i-metrics-medium" viewbox="0 0 24 24"><path d="M3 22a1 1 0 0 1-1-1V3a1 1 0 0 1 1-1h6a1 1 0 0 1 1 1v7h4V8a1 1 0 0 1 1-1h6a1 1 0 0 1 1 1v13a1 1 0 0 1-.883.993L21 22H3Zm17-2V9h-4v11h4Zm-6-8h-4v8h4v-8ZM8 4H4v16h4V4Z"></path></symbol><symbol id="icon-eds-i-news-medium" viewbox="0 0 24 24"><path d="M17.384 3c.975 0 1.77.787 1.77 1.762v13.333c0 .462.354.846.815.899l.107.006.109-.006a.915.915 0 0 0 .809-.794l.006-.105V8.19a1 1 0 0 1 2 0v9.905A2.914 2.914 0 0 1 20.077 21H3.538a2.547 2.547 0 0 1-1.644-.601l-.147-.135A2.516 2.516 0 0 1 1 18.476V4.762C1 3.787 1.794 3 2.77 3h14.614Zm-.231 2H3v13.476c0 .11.035.216.1.304l.054.063c.101.1.24.157.384.157l13.761-.001-.026-.078a2.88 2.88 0 0 1-.115-.655l-.004-.17L17.153 5ZM14 15.021a.979.979 0 1 1 0 1.958H6a.979.979 0 1 1 0-1.958h8Zm0-8c.54 0 .979.438.979.979v4c0 .54-.438.979-.979.979H6A.979.979 0 0 1 5.021 12V8c0-.54.438-.979.979-.979h8Zm-.98 1.958H6.979v2.041h6.041V8.979Z"></path></symbol><symbol id="icon-eds-i-newsletter-medium" viewbox="0 0 24 24"><path d="M21 10a1 1 0 0 1 1 1v9.5a2.5 2.5 0 0 1-2.5 2.5h-15A2.5 2.5 0 0 1 2 20.5V11a1 1 0 0 1 2 0v.439l8 4.888 8-4.889V11a1 1 0 0 1 1-1Zm-1 3.783-7.479 4.57a1 1 0 0 1-1.042 0l-7.48-4.57V20.5a.5.5 0 0 0 .501.5h15a.5.5 0 0 0 .5-.5v-6.717ZM15 9a1 1 0 0 1 0 2H9a1 1 0 0 1 0-2h6Zm2.5-8A2.5 2.5 0 0 1 20 3.5V9a1 1 0 0 1-2 0V3.5a.5.5 0 0 0-.5-.5h-11a.5.5 0 0 0-.5.5V9a1 1 0 1 1-2 0V3.5A2.5 2.5 0 0 1 6.5 1h11ZM15 5a1 1 0 0 1 0 2H9a1 1 0 1 1 0-2h6Z"></path></symbol><symbol id="icon-eds-i-notifcation-medium" viewbox="0 0 24 24"><path d="M14 20a1 1 0 0 1 0 2h-4a1 1 0 0 1 0-2h4ZM3 18l-.133-.007c-1.156-.124-1.156-1.862 0-1.986l.3-.012C4.32 15.923 5 15.107 5 14V9.5C5 5.368 8.014 2 12 2s7 3.368 7 7.5V14c0 1.107.68 1.923 1.832 1.995l.301.012c1.156.124 1.156 1.862 0 1.986L21 18H3Zm9-14C9.17 4 7 6.426 7 9.5V14c0 .671-.146 1.303-.416 1.858L6.51 16h10.979l-.073-.142a4.192 4.192 0 0 1-.412-1.658L17 14V9.5C17 6.426 14.83 4 12 4Z"></path></symbol><symbol id="icon-eds-i-publish-medium" viewbox="0 0 24 24"><g><path d="M16.296 1.291A1 1 0 0 0 15.591 1H5.545A2.542 2.542 0 0 0 3 3.538V13a1 1 0 1 0 2 0V3.538l.007-.087A.543.543 0 0 1 5.545 3h9.633L20 7.8v12.662a.534.534 0 0 1-.158.379.548.548 0 0 1-.387.159H11a1 1 0 1 0 0 2h8.455c.674 0 1.32-.267 1.798-.742A2.534 2.534 0 0 0 22 20.462V7.385a1 1 0 0 0-.294-.709l-5.41-5.385Z"></path><path d="M10.762 16.647a1 1 0 0 0-1.525-1.294l-4.472 5.271-2.153-1.665a1 1 0 1 0-1.224 1.582l2.91 2.25a1 1 0 0 0 1.374-.144l5.09-6ZM16 10a1 1 0 1 1 0 2H8a1 1 0 1 1 0-2h8ZM12 7a1 1 0 0 0-1-1H8a1 1 0 1 0 0 2h3a1 1 0 0 0 1-1Z"></path></g></symbol><symbol id="icon-eds-i-refresh-medium" viewbox="0 0 24 24"><g><path d="M7.831 5.636H6.032A8.76 8.76 0 0 1 9 3.631 8.549 8.549 0 0 1 12.232 3c.603 0 1.192.063 1.76.182C17.979 4.017 21 7.632 21 12a1 1 0 1 0 2 0c0-5.296-3.674-9.746-8.591-10.776A10.61 10.61 0 0 0 5 3.851V2.805a1 1 0 0 0-.987-1H4a1 1 0 0 0-1 1v3.831a1 1 0 0 0 1 1h3.831a1 1 0 0 0 .013-2h-.013ZM17.968 18.364c-1.59 1.632-3.784 2.636-6.2 2.636C6.948 21 3 16.993 3 12a1 1 0 1 0-2 0c0 6.053 4.799 11 10.768 11 2.788 0 5.324-1.082 7.232-2.85v1.045a1 1 0 1 0 2 0v-3.831a1 1 0 0 0-1-1h-3.831a1 1 0 0 0 0 2h1.799Z"></path></g></symbol><symbol id="icon-eds-i-search-medium" viewbox="0 0 24 24"><path d="M11 1c5.523 0 10 4.477 10 10 0 2.4-.846 4.604-2.256 6.328l3.963 3.965a1 1 0 0 1-1.414 1.414l-3.965-3.963A9.959 9.959 0 0 1 11 21C5.477 21 1 16.523 1 11S5.477 1 11 1Zm0 2a8 8 0 1 0 0 16 8 8 0 0 0 0-16Z"></path></symbol><symbol id="icon-eds-i-settings-medium" viewbox="0 0 24 24"><path d="M11.382 1h1.24a2.508 2.508 0 0 1 2.334 1.63l.523 1.378 1.59.933 1.444-.224c.954-.132 1.89.3 2.422 1.101l.095.155.598 1.066a2.56 2.56 0 0 1-.195 2.848l-.894 1.161v1.896l.92 1.163c.6.768.707 1.812.295 2.674l-.09.17-.606 1.08a2.504 2.504 0 0 1-2.531 1.25l-1.428-.223-1.589.932-.523 1.378a2.512 2.512 0 0 1-2.155 1.625L12.65 23h-1.27a2.508 2.508 0 0 1-2.334-1.63l-.524-1.379-1.59-.933-1.443.225c-.954.132-1.89-.3-2.422-1.101l-.095-.155-.598-1.066a2.56 2.56 0 0 1 .195-2.847l.891-1.161v-1.898l-.919-1.162a2.562 2.562 0 0 1-.295-2.674l.09-.17.606-1.08a2.504 2.504 0 0 1 2.531-1.25l1.43.223 1.618-.938.524-1.375.07-.167A2.507 2.507 0 0 1 11.382 1Zm.003 2a.509.509 0 0 0-.47.338l-.65 1.71a1 1 0 0 1-.434.51L7.6 6.85a1 1 0 0 1-.655.123l-1.762-.275a.497.497 0 0 0-.498.252l-.61 1.088a.562.562 0 0 0 .04.619l1.13 1.43a1 1 0 0 1 .216.62v2.585a1 1 0 0 1-.207.61L4.15 15.339a.568.568 0 0 0-.036.634l.601 1.072a.494.494 0 0 0 .484.26l1.78-.278a1 1 0 0 1 .66.126l2.2 1.292a1 1 0 0 1 .43.507l.648 1.71a.508.508 0 0 0 .467.338h1.263a.51.51 0 0 0 .47-.34l.65-1.708a1 1 0 0 1 .428-.507l2.201-1.292a1 1 0 0 1 .66-.126l1.763.275a.497.497 0 0 0 .498-.252l.61-1.088a.562.562 0 0 0-.04-.619l-1.13-1.43a1 1 0 0 1-.216-.62v-2.585a1 1 0 0 1 .207-.61l1.105-1.437a.568.568 0 0 0 .037-.634l-.601-1.072a.494.494 0 0 0-.484-.26l-1.78.278a1 1 0 0 1-.66-.126l-2.2-1.292a1 1 0 0 1-.43-.507l-.649-1.71A.508.508 0 0 0 12.62 3h-1.234ZM12 8a4 4 0 1 1 0 8 4 4 0 0 1 0-8Zm0 2a2 2 0 1 0 0 4 2 2 0 0 0 0-4Z"></path></symbol><symbol id="icon-eds-i-shipping-medium" viewbox="0 0 24 24"><path d="M16.515 2c1.406 0 2.706.728 3.352 1.902l2.02 3.635.02.042.036.089.031.105.012.058.01.073.004.075v11.577c0 .64-.244 1.255-.683 1.713a2.356 2.356 0 0 1-1.701.731H4.386a2.356 2.356 0 0 1-1.702-.731 2.476 2.476 0 0 1-.683-1.713V7.948c.01-.217.083-.43.22-.6L4.2 3.905C4.833 2.755 6.089 2.032 7.486 2h9.029ZM20 9H4v10.556a.49.49 0 0 0 .075.26l.053.07a.356.356 0 0 0 .257.114h15.23c.094 0 .186-.04.258-.115a.477.477 0 0 0 .127-.33V9Zm-2 7.5a1 1 0 0 1 0 2h-4a1 1 0 0 1 0-2h4ZM16.514 4H13v3h6.3l-1.183-2.13c-.288-.522-.908-.87-1.603-.87ZM11 3.999H7.51c-.679.017-1.277.36-1.566.887L4.728 7H11V3.999Z"></path></symbol><symbol id="icon-eds-i-step-guide-medium" viewbox="0 0 24 24"><path d="M11.394 9.447a1 1 0 1 0-1.788-.894l-.88 1.759-.019-.02a1 1 0 1 0-1.414 1.415l1 1a1 1 0 0 0 1.601-.26l1.5-3ZM12 11a1 1 0 0 1 1-1h3a1 1 0 1 1 0 2h-3a1 1 0 0 1-1-1ZM12 17a1 1 0 0 1 1-1h3a1 1 0 1 1 0 2h-3a1 1 0 0 1-1-1ZM10.947 14.105a1 1 0 0 1 .447 1.342l-1.5 3a1 1 0 0 1-1.601.26l-1-1a1 1 0 1 1 1.414-1.414l.02.019.879-1.76a1 1 0 0 1 1.341-.447Z"></path><path clip-rule="evenodd" d="M5.545 1A2.542 2.542 0 0 0 3 3.538v16.924A2.542 2.542 0 0 0 5.545 23h12.91A2.542 2.542 0 0 0 21 20.462V7.5a1 1 0 0 0-.293-.707l-5.5-5.5A1 1 0 0 0 14.5 1H5.545ZM5 3.538C5 3.245 5.24 3 5.545 3h8.54L19 7.914v12.547c0 .294-.24.539-.546.539H5.545A.542.542 0 0 1 5 20.462V3.538Z"></path></symbol><symbol id="icon-eds-i-submission-medium" viewbox="0 0 24 24"><g><path d="M5 3.538C5 3.245 5.24 3 5.545 3h9.633L20 7.8v12.662a.535.535 0 0 1-.158.379.549.549 0 0 1-.387.159H6a1 1 0 0 1-1-1v-2.5a1 1 0 1 0-2 0V20a3 3 0 0 0 3 3h13.455c.673 0 1.32-.266 1.798-.742A2.535 2.535 0 0 0 22 20.462V7.385a1 1 0 0 0-.294-.709l-5.41-5.385A1 1 0 0 0 15.591 1H5.545A2.542 2.542 0 0 0 3 3.538V7a1 1 0 0 0 2 0V3.538Z"></path><path d="m13.707 13.707-4 4a1 1 0 0 1-1.414 0l-.083-.094a1 1 0 0 1 .083-1.32L10.585 14 2 14a1 1 0 1 1 0-2l8.583.001-2.29-2.294a1 1 0 0 1 1.414-1.414l4.037 4.04.043.05.043.06.059.098.03.063.031.085.03.113.017.122L14 13l-.004.087-.017.118-.013.056-.034.104-.049.105-.048.081-.07.093-.058.063Z"></path></g></symbol><symbol id="icon-eds-i-table-1-medium" viewbox="0 0 24 24"><path d="M4.385 22a2.56 2.56 0 0 1-1.14-.279C2.485 21.341 2 20.614 2 19.615V4.385c0-.315.067-.716.279-1.14C2.659 2.485 3.386 2 4.385 2h15.23c.315 0 .716.067 1.14.279.76.38 1.245 1.107 1.245 2.106v15.23c0 .315-.067.716-.279 1.14-.38.76-1.107 1.245-2.106 1.245H4.385ZM4 19.615c0 .213.034.265.14.317a.71.71 0 0 0 .245.068H8v-4H4v3.615ZM20 16H10v4h9.615c.213 0 .265-.034.317-.14a.71.71 0 0 0 .068-.245V16Zm0-2v-4H10v4h10ZM4 14h4v-4H4v4ZM19.615 4H10v4h10V4.385c0-.213-.034-.265-.14-.317A.71.71 0 0 0 19.615 4ZM8 4H4.385l-.082.002c-.146.01-.19.047-.235.138A.71.71 0 0 0 4 4.385V8h4V4Z"></path></symbol><symbol id="icon-eds-i-table-2-medium" viewbox="0 0 24 24"><path d="M4.384 22A2.384 2.384 0 0 1 2 19.616V4.384A2.384 2.384 0 0 1 4.384 2h15.232A2.384 2.384 0 0 1 22 4.384v15.232A2.384 2.384 0 0 1 19.616 22H4.384ZM10 15H4v4.616c0 .212.172.384.384.384H10v-5Zm5 0h-3v5h3v-5Zm5 0h-3v5h2.616a.384.384 0 0 0 .384-.384V15ZM10 9H4v4h6V9Zm5 0h-3v4h3V9Zm5 0h-3v4h3V9Zm-.384-5H4.384A.384.384 0 0 0 4 4.384V7h16V4.384A.384.384 0 0 0 19.616 4Z"></path></symbol><symbol id="icon-eds-i-tag-medium" viewbox="0 0 24 24"><path d="m12.621 1.998.127.004L20.496 2a1.5 1.5 0 0 1 1.497 1.355L22 3.5l-.005 7.669c.038.456-.133.905-.447 1.206l-9.02 9.018a2.075 2.075 0 0 1-2.932 0l-6.99-6.99a2.075 2.075 0 0 1 .001-2.933L11.61 2.47c.246-.258.573-.418.881-.46l.131-.011Zm.286 2-8.885 8.886a.075.075 0 0 0 0 .106l6.987 6.988c.03.03.077.03.106 0l8.883-8.883L19.999 4l-7.092-.002ZM16 6.5a1.5 1.5 0 0 1 .144 2.993L16 9.5a1.5 1.5 0 0 1 0-3Z"></path></symbol><symbol id="icon-eds-i-trash-medium" viewbox="0 0 24 24"><path d="M12 1c2.717 0 4.913 2.232 4.997 5H21a1 1 0 0 1 0 2h-1v12.5c0 1.389-1.152 2.5-2.556 2.5H6.556C5.152 23 4 21.889 4 20.5V8H3a1 1 0 1 1 0-2h4.003l.001-.051C7.114 3.205 9.3 1 12 1Zm6 7H6v12.5c0 .238.19.448.454.492l.102.008h10.888c.315 0 .556-.232.556-.5V8Zm-4 3a1 1 0 0 1 1 1v6.005a1 1 0 0 1-2 0V12a1 1 0 0 1 1-1Zm-4 0a1 1 0 0 1 1 1v6a1 1 0 0 1-2 0v-6a1 1 0 0 1 1-1Zm2-8c-1.595 0-2.914 1.32-2.996 3h5.991v-.02C14.903 4.31 13.589 3 12 3Z"></path></symbol><symbol id="icon-eds-i-user-account-medium" viewbox="0 0 24 24"><path d="M12 1c6.075 0 11 4.925 11 11s-4.925 11-11 11S1 18.075 1 12 5.925 1 12 1Zm0 16c-1.806 0-3.52.994-4.664 2.698A8.947 8.947 0 0 0 12 21a8.958 8.958 0 0 0 4.664-1.301C15.52 17.994 13.806 17 12 17Zm0-14a9 9 0 0 0-6.25 15.476C7.253 16.304 9.54 15 12 15s4.747 1.304 6.25 3.475A9 9 0 0 0 12 3Zm0 3a4 4 0 1 1 0 8 4 4 0 0 1 0-8Zm0 2a2 2 0 1 0 0 4 2 2 0 0 0 0-4Z"></path></symbol><symbol id="icon-eds-i-user-add-medium" viewbox="0 0 24 24"><path d="M9 1a5 5 0 1 1 0 10A5 5 0 0 1 9 1Zm0 2a3 3 0 1 0 0 6 3 3 0 0 0 0-6Zm9 10a1 1 0 0 1 1 1v3h3a1 1 0 0 1 0 2h-3v3a1 1 0 0 1-2 0v-3h-3a1 1 0 0 1 0-2h3v-3a1 1 0 0 1 1-1Zm-5.545-.15a1 1 0 1 1-.91 1.78 5.713 5.713 0 0 0-5.705.282c-1.67 1.068-2.728 2.927-2.832 4.956L3.004 20 11.5 20a1 1 0 0 1 .993.883L12.5 21a1 1 0 0 1-1 1H2a1 1 0 0 1-1-1v-.876c.028-2.812 1.446-5.416 3.763-6.897a7.713 7.713 0 0 1 7.692-.378Z"></path></symbol><symbol id="icon-eds-i-user-assign-medium" viewbox="0 0 24 24"><path d="M16.226 13.298a1 1 0 0 1 1.414-.01l.084.093a1 1 0 0 1-.073 1.32L15.39 17H22a1 1 0 0 1 0 2h-6.611l2.262 2.298a1 1 0 0 1-1.425 1.404l-3.939-4a1 1 0 0 1 0-1.404l3.94-4Zm-3.771-.449a1 1 0 1 1-.91 1.781 5.713 5.713 0 0 0-5.705.282c-1.67 1.068-2.728 2.927-2.832 4.956L3.004 20 10.5 20a1 1 0 0 1 .993.883L11.5 21a1 1 0 0 1-1 1H2a1 1 0 0 1-1-1v-.876c.028-2.812 1.446-5.416 3.763-6.897a7.713 7.713 0 0 1 7.692-.378ZM9 1a5 5 0 1 1 0 10A5 5 0 0 1 9 1Zm0 2a3 3 0 1 0 0 6 3 3 0 0 0 0-6Z"></path></symbol><symbol id="icon-eds-i-user-block-medium" viewbox="0 0 24 24"><path d="M9 1a5 5 0 1 1 0 10A5 5 0 0 1 9 1Zm0 2a3 3 0 1 0 0 6 3 3 0 0 0 0-6Zm9 10a5 5 0 1 1 0 10 5 5 0 0 1 0-10Zm-5.545-.15a1 1 0 1 1-.91 1.78 5.713 5.713 0 0 0-5.705.282c-1.67 1.068-2.728 2.927-2.832 4.956L3.004 20 11.5 20a1 1 0 0 1 .993.883L12.5 21a1 1 0 0 1-1 1H2a1 1 0 0 1-1-1v-.876c.028-2.812 1.446-5.416 3.763-6.897a7.713 7.713 0 0 1 7.692-.378ZM15 18a3 3 0 0 0 4.294 2.707l-4.001-4c-.188.391-.293.83-.293 1.293Zm3-3c-.463 0-.902.105-1.294.293l4.001 4A3 3 0 0 0 18 15Z"></path></symbol><symbol id="icon-eds-i-user-check-medium" viewbox="0 0 24 24"><path d="M9 1a5 5 0 1 1 0 10A5 5 0 0 1 9 1Zm0 2a3 3 0 1 0 0 6 3 3 0 0 0 0-6Zm13.647 12.237a1 1 0 0 1 .116 1.41l-5.091 6a1 1 0 0 1-1.375.144l-2.909-2.25a1 1 0 1 1 1.224-1.582l2.153 1.665 4.472-5.271a1 1 0 0 1 1.41-.116Zm-8.139-.977c.22.214.428.44.622.678a1 1 0 1 1-1.548 1.266 6.025 6.025 0 0 0-1.795-1.49.86.86 0 0 1-.163-.048l-.079-.036a5.721 5.721 0 0 0-2.62-.63l-.194.006c-2.76.134-5.022 2.177-5.592 4.864l-.035.175-.035.213c-.03.201-.05.405-.06.61L3.003 20 10 20a1 1 0 0 1 .993.883L11 21a1 1 0 0 1-1 1H2a1 1 0 0 1-1-1v-.876l.005-.223.02-.356.02-.222.03-.248.022-.15c.02-.133.044-.265.071-.397.44-2.178 1.725-4.105 3.595-5.301a7.75 7.75 0 0 1 3.755-1.215l.12-.004a7.908 7.908 0 0 1 5.87 2.252Z"></path></symbol><symbol id="icon-eds-i-user-delete-medium" viewbox="0 0 24 24"><path d="M9 1a5 5 0 1 1 0 10A5 5 0 0 1 9 1Zm0 2a3 3 0 1 0 0 6 3 3 0 0 0 0-6ZM4.763 13.227a7.713 7.713 0 0 1 7.692-.378 1 1 0 1 1-.91 1.781 5.713 5.713 0 0 0-5.705.282c-1.67 1.068-2.728 2.927-2.832 4.956L3.004 20H11.5a1 1 0 0 1 .993.883L12.5 21a1 1 0 0 1-1 1H2a1 1 0 0 1-1-1v-.876c.028-2.812 1.446-5.416 3.763-6.897Zm11.421 1.543 2.554 2.553 2.555-2.553a1 1 0 0 1 1.414 1.414l-2.554 2.554 2.554 2.555a1 1 0 0 1-1.414 1.414l-2.555-2.554-2.554 2.554a1 1 0 0 1-1.414-1.414l2.553-2.555-2.553-2.554a1 1 0 0 1 1.414-1.414Z"></path></symbol><symbol id="icon-eds-i-user-edit-medium" viewbox="0 0 24 24"><path d="m19.876 10.77 2.831 2.83a1 1 0 0 1 0 1.415l-7.246 7.246a1 1 0 0 1-.572.284l-3.277.446a1 1 0 0 1-1.125-1.13l.461-3.277a1 1 0 0 1 .283-.567l7.23-7.246a1 1 0 0 1 1.415-.001Zm-7.421 2.08a1 1 0 1 1-.91 1.78 5.713 5.713 0 0 0-5.705.282c-1.67 1.068-2.728 2.927-2.832 4.956L3.004 20 7.5 20a1 1 0 0 1 .993.883L8.5 21a1 1 0 0 1-1 1H2a1 1 0 0 1-1-1v-.876c.028-2.812 1.446-5.416 3.763-6.897a7.713 7.713 0 0 1 7.692-.378Zm6.715.042-6.29 6.3-.23 1.639 1.633-.222 6.302-6.302-1.415-1.415ZM9 1a5 5 0 1 1 0 10A5 5 0 0 1 9 1Zm0 2a3 3 0 1 0 0 6 3 3 0 0 0 0-6Z"></path></symbol><symbol id="icon-eds-i-user-linked-medium" viewbox="0 0 24 24"><path d="M15.65 6c.31 0 .706.066 1.122.274C17.522 6.65 18 7.366 18 8.35v12.3c0 .31-.066.706-.274 1.122-.375.75-1.092 1.228-2.076 1.228H3.35a2.52 2.52 0 0 1-1.122-.274C1.478 22.35 1 21.634 1 20.65V8.35c0-.31.066-.706.274-1.122C1.65 6.478 2.366 6 3.35 6h12.3Zm0 2-12.376.002c-.134.007-.17.04-.21.12A.672.672 0 0 0 3 8.35v12.3c0 .198.028.24.122.287.09.044.2.063.228.063h.887c.788-2.269 2.814-3.5 5.263-3.5 2.45 0 4.475 1.231 5.263 3.5h.887c.198 0 .24-.028.287-.122.044-.09.063-.2.063-.228V8.35c0-.198-.028-.24-.122-.287A.672.672 0 0 0 15.65 8ZM9.5 19.5c-1.36 0-2.447.51-3.06 1.5h6.12c-.613-.99-1.7-1.5-3.06-1.5ZM20.65 1A2.35 2.35 0 0 1 23 3.348V15.65A2.35 2.35 0 0 1 20.65 18H20a1 1 0 0 1 0-2h.65a.35.35 0 0 0 .35-.35V3.348A.35.35 0 0 0 20.65 3H8.35a.35.35 0 0 0-.35.348V4a1 1 0 1 1-2 0v-.652A2.35 2.35 0 0 1 8.35 1h12.3ZM9.5 10a3.5 3.5 0 1 1 0 7 3.5 3.5 0 0 1 0-7Zm0 2a1.5 1.5 0 1 0 0 3 1.5 1.5 0 0 0 0-3Z"></path></symbol><symbol id="icon-eds-i-user-multiple-medium" viewbox="0 0 24 24"><path d="M9 1a5 5 0 1 1 0 10A5 5 0 0 1 9 1Zm6 0a5 5 0 0 1 0 10 1 1 0 0 1-.117-1.993L15 9a3 3 0 0 0 0-6 1 1 0 0 1 0-2ZM9 3a3 3 0 1 0 0 6 3 3 0 0 0 0-6Zm8.857 9.545a7.99 7.99 0 0 1 2.651 1.715A8.31 8.31 0 0 1 23 20.134V21a1 1 0 0 1-1 1h-3a1 1 0 0 1 0-2h1.995l-.005-.153a6.307 6.307 0 0 0-1.673-3.945l-.204-.209a5.99 5.99 0 0 0-1.988-1.287 1 1 0 1 1 .732-1.861Zm-3.349 1.715A8.31 8.31 0 0 1 17 20.134V21a1 1 0 0 1-1 1H2a1 1 0 0 1-1-1v-.877c.044-4.343 3.387-7.908 7.638-8.115a7.908 7.908 0 0 1 5.87 2.252ZM9.016 14l-.285.006c-3.104.15-5.58 2.718-5.725 5.9L3.004 20h11.991l-.005-.153a6.307 6.307 0 0 0-1.673-3.945l-.204-.209A5.924 5.924 0 0 0 9.3 14.008L9.016 14Z"></path></symbol><symbol id="icon-eds-i-user-notify-medium" viewbox="0 0 24 24"><path d="M9 1a5 5 0 1 1 0 10A5 5 0 0 1 9 1Zm0 2a3 3 0 1 0 0 6 3 3 0 0 0 0-6Zm10 18v1a1 1 0 0 1-2 0v-1h-3a1 1 0 0 1 0-2v-2.818C14 13.885 15.777 12 18 12s4 1.885 4 4.182V19a1 1 0 0 1 0 2h-3Zm-6.545-8.15a1 1 0 1 1-.91 1.78 5.713 5.713 0 0 0-5.705.282c-1.67 1.068-2.728 2.927-2.832 4.956L3.004 20 11.5 20a1 1 0 0 1 .993.883L12.5 21a1 1 0 0 1-1 1H2a1 1 0 0 1-1-1v-.876c.028-2.812 1.446-5.416 3.763-6.897a7.713 7.713 0 0 1 7.692-.378ZM18 14c-1.091 0-2 .964-2 2.182V19h4v-2.818c0-1.165-.832-2.098-1.859-2.177L18 14Z"></path></symbol><symbol id="icon-eds-i-user-remove-medium" viewbox="0 0 24 24"><path d="M9 1a5 5 0 1 1 0 10A5 5 0 0 1 9 1Zm0 2a3 3 0 1 0 0 6 3 3 0 0 0 0-6Zm3.455 9.85a1 1 0 1 1-.91 1.78 5.713 5.713 0 0 0-5.705.282c-1.67 1.068-2.728 2.927-2.832 4.956L3.004 20 11.5 20a1 1 0 0 1 .993.883L12.5 21a1 1 0 0 1-1 1H2a1 1 0 0 1-1-1v-.876c.028-2.812 1.446-5.416 3.763-6.897a7.713 7.713 0 0 1 7.692-.378ZM22 17a1 1 0 0 1 0 2h-8a1 1 0 0 1 0-2h8Z"></path></symbol><symbol id="icon-eds-i-user-single-medium" viewbox="0 0 24 24"><path d="M12 1a5 5 0 1 1 0 10 5 5 0 0 1 0-10Zm0 2a3 3 0 1 0 0 6 3 3 0 0 0 0-6Zm-.406 9.008a8.965 8.965 0 0 1 6.596 2.494A9.161 9.161 0 0 1 21 21.025V22a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1v-.985c.05-4.825 3.815-8.777 8.594-9.007Zm.39 1.992-.299.006c-3.63.175-6.518 3.127-6.678 6.775L5 21h13.998l-.009-.268a7.157 7.157 0 0 0-1.97-4.573l-.214-.213A6.967 6.967 0 0 0 11.984 14Z"></path></symbol><symbol id="icon-eds-i-warning-circle-medium" viewbox="0 0 24 24"><path d="M12 1c6.075 0 11 4.925 11 11s-4.925 11-11 11S1 18.075 1 12 5.925 1 12 1Zm0 2a9 9 0 1 0 0 18 9 9 0 0 0 0-18Zm0 11.5a1.5 1.5 0 0 1 .144 2.993L12 17.5a1.5 1.5 0 0 1 0-3ZM12 6a1 1 0 0 1 1 1v5a1 1 0 0 1-2 0V7a1 1 0 0 1 1-1Z"></path></symbol><symbol id="icon-eds-i-warning-filled-medium" viewbox="0 0 24 24"><path d="M12 1c6.075 0 11 4.925 11 11s-4.925 11-11 11S1 18.075 1 12 5.925 1 12 1Zm0 13.5a1.5 1.5 0 0 0 0 3l.144-.007A1.5 1.5 0 0 0 12 14.5ZM12 6a1 1 0 0 0-1 1v5a1 1 0 0 0 2 0V7a1 1 0 0 0-1-1Z"></path></symbol><symbol id="icon-chevron-left-medium" viewbox="0 0 24 24"><path d="M15.7194 3.3054C15.3358 2.90809 14.7027 2.89699 14.3054 3.28061L6.54342 10.7757C6.19804 11.09 6 11.5335 6 12C6 12.4665 6.19804 12.91 6.5218 13.204L14.3054 20.7194C14.7027 21.103 15.3358 21.0919 15.7194 20.6946C16.103 20.2973 16.0919 19.6642 15.6946 19.2806L8.155 12L15.6946 4.71939C16.0614 4.36528 16.099 3.79863 15.8009 3.40105L15.7194 3.3054Z"></path></symbol><symbol id="icon-chevron-right-medium" viewbox="0 0 24 24"><path d="M8.28061 3.3054C8.66423 2.90809 9.29729 2.89699 9.6946 3.28061L17.4566 10.7757C17.802 11.09 18 11.5335 18 12C18 12.4665 17.802 12.91 17.4782 13.204L9.6946 20.7194C9.29729 21.103 8.66423 21.0919 8.28061 20.6946C7.89699 20.2973 7.90809 19.6642 8.3054 19.2806L15.845 12L8.3054 4.71939C7.93865 4.36528 7.90098 3.79863 8.19908 3.40105L8.28061 3.3054Z"></path></symbol><symbol id="icon-eds-alerts" viewbox="0 0 32 32"><path d="M28 12.667c.736 0 1.333.597 1.333 1.333v13.333A3.333 3.333 0 0 1 26 30.667H6a3.333 3.333 0 0 1-3.333-3.334V14a1.333 1.333 0 1 1 2.666 0v1.252L16 21.769l10.667-6.518V14c0-.736.597-1.333 1.333-1.333Zm-1.333 5.71-9.972 6.094c-.427.26-.963.26-1.39 0l-9.972-6.094v8.956c0 .368.299.667.667.667h20a.667.667 0 0 0 .667-.667v-8.956ZM19.333 12a1.333 1.333 0 1 1 0 2.667h-6.666a1.333 1.333 0 1 1 0-2.667h6.666Zm4-10.667a3.333 3.333 0 0 1 3.334 3.334v6.666a1.333 1.333 0 1 1-2.667 0V4.667A.667.667 0 0 0 23.333 4H8.667A.667.667 0 0 0 8 4.667v6.666a1.333 1.333 0 1 1-2.667 0V4.667a3.333 3.333 0 0 1 3.334-3.334h14.666Zm-4 5.334a1.333 1.333 0 0 1 0 2.666h-6.666a1.333 1.333 0 1 1 0-2.666h6.666Z"></path></symbol><symbol id="icon-eds-arrow-up" viewbox="0 0 24 24"><path class="layer" d="m13.002 7.408 4.88 4.88a.99.99 0 0 0 1.32.08l.09-.08c.39-.39.39-1.03 0-1.42l-6.58-6.58a1.01 1.01 0 0 0-1.42 0l-6.58 6.58a1 1 0 0 0-.09 1.32l.08.1a1 1 0 0 0 1.42-.01l4.88-4.87v11.59a.99.99 0 0 0 .88.99l.12.01c.55 0 1-.45 1-1V7.408z" fill-rule="evenodd"></path></symbol><symbol id="icon-eds-checklist" viewbox="0 0 32 32"><path d="M19.2 1.333a3.468 3.468 0 0 1 3.381 2.699L24.667 4C26.515 4 28 5.52 28 7.38v19.906c0 1.86-1.485 3.38-3.333 3.38H7.333c-1.848 0-3.333-1.52-3.333-3.38V7.38C4 5.52 5.485 4 7.333 4h2.093A3.468 3.468 0 0 1 12.8 1.333h6.4ZM9.426 6.667H7.333c-.36 0-.666.312-.666.713v19.906c0 .401.305.714.666.714h17.334c.36 0 .666-.313.666-.714V7.38c0-.4-.305-.713-.646-.714l-2.121.033A3.468 3.468 0 0 1 19.2 9.333h-6.4a3.468 3.468 0 0 1-3.374-2.666Zm12.715 5.606c.586.446.7 1.283.253 1.868l-7.111 9.334a1.333 1.333 0 0 1-1.792.306l-3.556-2.333a1.333 1.333 0 1 1 1.463-2.23l2.517 1.651 6.358-8.344a1.333 1.333 0 0 1 1.868-.252ZM19.2 4h-6.4a.8.8 0 0 0-.8.8v1.067a.8.8 0 0 0 .8.8h6.4a.8.8 0 0 0 .8-.8V4.8a.8.8 0 0 0-.8-.8Z"></path></symbol><symbol id="icon-eds-citation" viewbox="0 0 36 36"><path d="M23.25 1.5a1.5 1.5 0 0 1 1.06.44l8.25 8.25a1.5 1.5 0 0 1 .44 1.06v19.5c0 2.105-1.645 3.75-3.75 3.75H18a1.5 1.5 0 0 1 0-3h11.25c.448 0 .75-.302.75-.75V11.873L22.628 4.5H8.31a.811.811 0 0 0-.8.68l-.011.13V16.5a1.5 1.5 0 0 1-3 0V5.31A3.81 3.81 0 0 1 8.31 1.5h14.94ZM8.223 20.358a.984.984 0 0 1-.192 1.378l-.048.034c-.54.36-.942.676-1.206.951-.59.614-.885 1.395-.885 2.343.115-.028.288-.042.518-.042.662 0 1.26.237 1.791.711.533.474.799 1.074.799 1.799 0 .753-.259 1.352-.777 1.799-.518.446-1.151.669-1.9.669-1.006 0-1.812-.293-2.417-.878C3.302 28.536 3 27.657 3 26.486c0-1.115.165-2.085.496-2.907.331-.823.734-1.513 1.209-2.071.475-.558.971-.997 1.49-1.318a6.01 6.01 0 0 1 .347-.2 1.321 1.321 0 0 1 1.681.368Zm7.5 0a.984.984 0 0 1-.192 1.378l-.048.034c-.54.36-.942.676-1.206.951-.59.614-.885 1.395-.885 2.343.115-.028.288-.042.518-.042.662 0 1.26.237 1.791.711.533.474.799 1.074.799 1.799 0 .753-.259 1.352-.777 1.799-.518.446-1.151.669-1.9.669-1.006 0-1.812-.293-2.417-.878-.604-.586-.906-1.465-.906-2.636 0-1.115.165-2.085.496-2.907.331-.823.734-1.513 1.209-2.071.475-.558.971-.997 1.49-1.318a6.01 6.01 0 0 1 .347-.2 1.321 1.321 0 0 1 1.681.368Z"></path></symbol><symbol id="icon-eds-i-github-medium" viewbox="0 0 24 24"><path d="M 11.964844 0 C 5.347656 0 0 5.269531 0 11.792969 C 0 17.003906 3.425781 21.417969 8.179688 22.976562 C 8.773438 23.09375 8.992188 22.722656 8.992188 22.410156 C 8.992188 22.136719 8.972656 21.203125 8.972656 20.226562 C 5.644531 20.929688 4.953125 18.820312 4.953125 18.820312 C 4.417969 17.453125 3.625 17.101562 3.625 17.101562 C 2.535156 16.378906 3.703125 16.378906 3.703125 16.378906 C 4.914062 16.457031 5.546875 17.589844 5.546875 17.589844 C 6.617188 19.386719 8.339844 18.878906 9.03125 18.566406 C 9.132812 17.804688 9.449219 17.277344 9.785156 16.984375 C 7.132812 16.710938 4.339844 15.695312 4.339844 11.167969 C 4.339844 9.878906 4.8125 8.824219 5.566406 8.003906 C 5.445312 7.710938 5.03125 6.5 5.683594 4.878906 C 5.683594 4.878906 6.695312 4.566406 8.972656 6.089844 C 9.949219 5.832031 10.953125 5.703125 11.964844 5.699219 C 12.972656 5.699219 14.003906 5.835938 14.957031 6.089844 C 17.234375 4.566406 18.242188 4.878906 18.242188 4.878906 C 18.898438 6.5 18.480469 7.710938 18.363281 8.003906 C 19.136719 8.824219 19.589844 9.878906 19.589844 11.167969 C 19.589844 15.695312 16.796875 16.691406 14.125 16.984375 C 14.558594 17.355469 14.933594 18.058594 14.933594 19.171875 C 14.933594 20.753906 14.914062 22.019531 14.914062 22.410156 C 14.914062 22.722656 15.132812 23.09375 15.726562 22.976562 C 20.480469 21.414062 23.910156 17.003906 23.910156 11.792969 C 23.929688 5.269531 18.558594 0 11.964844 0 Z M 11.964844 0 "></path></symbol><symbol id="icon-eds-i-subjects-medium" viewbox="0 0 24 24"><g fill-rule="evenodd" id="icon-subjects-copy" stroke="none" stroke-width="1"><path d="M13.3846154,2 C14.7015971,2 15.7692308,3.06762994 15.7692308,4.38461538 L15.7692308,7.15384615 C15.7692308,8.47082629 14.7015955,9.53846154 13.3846154,9.53846154 L13.1038388,9.53925278 C13.2061091,9.85347965 13.3815528,10.1423885 13.6195822,10.3804178 C13.9722182,10.7330539 14.436524,10.9483278 14.9293854,10.9918129 L15.1153846,11 C16.2068332,11 17.2535347,11.433562 18.0254647,12.2054189 C18.6411944,12.8212361 19.0416785,13.6120766 19.1784166,14.4609738 L19.6153846,14.4615385 C20.932386,14.4615385 22,15.5291672 22,16.8461538 L22,19.6153846 C22,20.9323924 20.9323924,22 19.6153846,22 L16.8461538,22 C15.5291672,22 14.4615385,20.932386 14.4615385,19.6153846 L14.4615385,16.8461538 C14.4615385,15.5291737 15.5291737,14.4615385 16.8461538,14.4615385 L17.126925,14.460779 C17.0246537,14.1465537 16.8492179,13.857633 16.6112344,13.6196157 C16.2144418,13.2228606 15.6764136,13 15.1153846,13 C14.0239122,13 12.9771569,12.5664197 12.2053686,11.7946314 C12.1335167,11.7227795 12.0645962,11.6485444 11.9986839,11.5721119 C11.9354038,11.6485444 11.8664833,11.7227795 11.7946314,11.7946314 C11.0228431,12.5664197 9.97608778,13 8.88461538,13 C8.323576,13 7.78552852,13.2228666 7.38881294,13.6195822 C7.15078359,13.8576115 6.97533988,14.1465203 6.8730696,14.4607472 L7.15384615,14.4615385 C8.47082629,14.4615385 9.53846154,15.5291737 9.53846154,16.8461538 L9.53846154,19.6153846 C9.53846154,20.932386 8.47083276,22 7.15384615,22 L4.38461538,22 C3.06762347,22 2,20.9323876 2,19.6153846 L2,16.8461538 C2,15.5291721 3.06762994,14.4615385 4.38461538,14.4615385 L4.8215823,14.4609378 C4.95831893,13.6120029 5.3588057,12.8211623 5.97459937,12.2053686 C6.69125996,11.488708 7.64500941,11.0636656 8.6514968,11.0066017 L8.88461538,11 C9.44565477,11 9.98370225,10.7771334 10.3804178,10.3804178 C10.6184472,10.1423885 10.7938909,9.85347965 10.8961612,9.53925278 L10.6153846,9.53846154 C9.29840448,9.53846154 8.23076923,8.47082629 8.23076923,7.15384615 L8.23076923,4.38461538 C8.23076923,3.06762994 9.29840286,2 10.6153846,2 L13.3846154,2 Z M7.15384615,16.4615385 L4.38461538,16.4615385 C4.17220099,16.4615385 4,16.63374 4,16.8461538 L4,19.6153846 C4,19.8278134 4.17218833,20 4.38461538,20 L7.15384615,20 C7.36626945,20 7.53846154,19.8278103 7.53846154,19.6153846 L7.53846154,16.8461538 C7.53846154,16.6337432 7.36625679,16.4615385 7.15384615,16.4615385 Z M19.6153846,16.4615385 L16.8461538,16.4615385 C16.6337432,16.4615385 16.4615385,16.6337432 16.4615385,16.8461538 L16.4615385,19.6153846 C16.4615385,19.8278103 16.6337306,20 16.8461538,20 L19.6153846,20 C19.8278229,20 20,19.8278229 20,19.6153846 L20,16.8461538 C20,16.6337306 19.8278103,16.4615385 19.6153846,16.4615385 Z M13.3846154,4 L10.6153846,4 C10.4029708,4 10.2307692,4.17220099 10.2307692,4.38461538 L10.2307692,7.15384615 C10.2307692,7.36625679 10.402974,7.53846154 10.6153846,7.53846154 L13.3846154,7.53846154 C13.597026,7.53846154 13.7692308,7.36625679 13.7692308,7.15384615 L13.7692308,4.38461538 C13.7692308,4.17220099 13.5970292,4 13.3846154,4 Z" fill-rule="nonzero" id="Shape"></path></g></symbol><symbol id="icon-eds-small-arrow-left" viewbox="0 0 16 17"><path d="M14 8.092H2m0 0L8 2M2 8.092l6 6.035" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2"></path></symbol><symbol id="icon-eds-small-arrow-right" viewbox="0 0 16 16"><g fill-rule="evenodd" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2"><path d="M2 8.092h12M8 2l6 6.092M8 14.127l6-6.035"></path></g></symbol></svg>
</div>
<a class="c-skip-link" href="#main">Skip to main content</a>
<header class="eds-c-header" data-eds-c-header="">
<div class="eds-c-header__container" data-eds-c-header-expander-anchor="">
<div class="eds-c-header__brand">
<a data-test="springerlink-logo" data-track="click_imprint_logo" data-track-action="click logo link" data-track-category="unified header" data-track-context="unified header" data-track-label="link" href="https://link.springer.com">
<img alt="SpringerLink" src="/oscar-static/images/darwin/header/img/logo-springerlink-39ee2a28d8.svg"/>
</a>
</div>
<a class="c-header__link eds-c-header__link" href="https://idp.springer.com/auth/personal/springernature?redirect_uri=https://link.springer.com/article/10.1007/s10994-023-06487-7" id="identity-account-widget"><span class="eds-c-header__widget-fragment-title">Log in</span></a>
</div>
<nav aria-label="header navigation" class="eds-c-header__nav">
<div class="eds-c-header__nav-container">
<div class="eds-c-header__item eds-c-header__item--menu">
<a aria-controls="eds-c-header-nav" aria-expanded="false" aria-haspopup="true" class="eds-c-header__link" data-eds-c-header-expander="" href="javascript:;" role="button">
<svg aria-hidden="true" class="eds-c-header__icon" focusable="false" height="24" width="24">
<use xlink:href="#icon-eds-i-menu-medium"></use>
</svg><span>Menu</span>
</a>
</div>
<div class="eds-c-header__item eds-c-header__item--inline-links">
<a class="eds-c-header__link" data-track="nav_find_a_journal" data-track-action="click find a journal" data-track-category="unified header" data-track-context="unified header" data-track-label="link" href="https://link.springer.com/journals/">
                            Find a journal
                        </a>
<a class="eds-c-header__link" data-track="nav_how_to_publish" data-track-action="click publish with us link" data-track-category="unified header" data-track-context="unified header" data-track-label="link" href="https://www.springernature.com/gp/authors">
                            Publish with us
                        </a>
<a class="eds-c-header__link" data-track="nav_track_your_research" data-track-action="click track your research" data-track-category="unified header" data-track-context="unified header" data-track-label="link" href="https://link.springernature.com/home/">
                            Track your research
                        </a>
</div>
<div class="eds-c-header__link-container">
<div class="eds-c-header__item eds-c-header__item--divider">
<a aria-controls="eds-c-header-popup-search" aria-expanded="false" aria-haspopup="true" class="eds-c-header__link" data-eds-c-header-expander="" data-eds-c-header-test-search-btn="" href="javascript:;" role="button">
<svg aria-hidden="true" class="eds-c-header__icon" focusable="false" height="24" width="24">
<use xlink:href="#icon-eds-i-search-medium"></use>
</svg><span>Search</span>
</a>
</div>
<div class="eds-c-header__item ecommerce-cart" id="ecommerce-header-cart-icon-link" style="display:inline-block">
<a class="eds-c-header__link" href="https://order.springer.com/public/cart" style="appearance:none;border:none;background:none;color:inherit;position:relative">
<svg aria-hidden="true" class="eds-c-header__icon" focusable="false" height="24" id="eds-i-cart" viewbox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
<path d="M2 1a1 1 0 0 0 0 2l1.659.001 2.257 12.808a2.599 2.599 0 0 0 2.435 2.185l.167.004 9.976-.001a2.613 2.613 0 0 0 2.61-1.748l.03-.106 1.755-7.82.032-.107a2.546 2.546 0 0 0-.311-1.986l-.108-.157a2.604 2.604 0 0 0-2.197-1.076L6.042 5l-.56-3.17a1 1 0 0 0-.864-.82l-.12-.007L2.001 1ZM20.35 6.996a.63.63 0 0 1 .54.26.55.55 0 0 1 .082.505l-.028.1L19.2 15.63l-.022.05c-.094.177-.282.299-.526.317l-10.145.002a.61.61 0 0 1-.618-.515L6.394 6.999l13.955-.003ZM18 19a2 2 0 1 0 0 4 2 2 0 0 0 0-4ZM8 19a2 2 0 1 0 0 4 2 2 0 0 0 0-4Z" fill="currentColor" fill-rule="nonzero"></path>
</svg><span>Cart</span><span class="cart-info" style="display:none;position:absolute;top:10px;right:45px;background-color:#C65301;color:#fff;width:18px;height:18px;font-size:11px;border-radius:50%;line-height:17.5px;text-align:center"></span></a>
<script>(function () { var exports = {}; if (window.fetch) {
            
            "use strict";
Object.defineProperty(exports, "__esModule", { value: true });
exports.headerWidgetClientInit = void 0;
var headerWidgetClientInit = function (getCartInfo) {
    document.body.addEventListener("updatedCart", function () {
        updateCartIcon();
    }, false);
    return updateCartIcon();
    function updateCartIcon() {
        return getCartInfo()
            .then(function (res) { return res.json(); })
            .then(refreshCartState)
            .catch(function (_) { });
    }
    function refreshCartState(json) {
        var indicator = document.querySelector("#ecommerce-header-cart-icon-link .cart-info");
        /* istanbul ignore else */
        if (indicator && json.itemCount) {
            indicator.style.display = 'block';
            indicator.textContent = json.itemCount > 9 ? '9+' : json.itemCount.toString();
            var moreThanOneItem = json.itemCount > 1;
            indicator.setAttribute('title', "there ".concat(moreThanOneItem ? "are" : "is", " ").concat(json.itemCount, " item").concat(moreThanOneItem ? "s" : "", " in your cart"));
        }
        return json;
    }
};
exports.headerWidgetClientInit = headerWidgetClientInit;

            
            headerWidgetClientInit(
              function () {
                return window.fetch("https://cart.springer.com/cart-info", {
                  credentials: "include",
                  headers: { Accept: "application/json" }
                })
              }
            )
        }})()</script>
</div>
</div>
</div>
</nav>
</header><div class="eds-c-header__expander eds-c-header__expander--search has-tethered u-js-hide" hidden="" id="eds-c-header-popup-search">
<h2 class="eds-c-header__heading">Search</h2>
<div class="u-container">
<search aria-label="Search from the header" class="eds-c-header__search" role="search">
<form action="//link.springer.com/search" data-test="header-search" data-track="search" data-track-action="submit search form" data-track-category="unified header" data-track-context="search from header" data-track-label="form" method="GET">
<label class="eds-c-header__search-label" for="eds-c-header-search">Search by keyword or author</label>
<div class="eds-c-header__search-container">
<input autocomplete="off" class="eds-c-header__search-input" id="eds-c-header-search" name="query" required="" type="search" value=""/>
<button class="eds-c-header__search-button" type="submit">
<svg aria-hidden="true" class="eds-c-header__icon" focusable="false">
<use xlink:href="#icon-eds-i-search-medium"></use>
</svg>
<span class="u-visually-hidden">Search</span>
</button>
</div>
</form>
</search>
</div>
</div><div class="eds-c-header__expander eds-c-header__expander--menu has-tethered u-js-hide" hidden="" id="eds-c-header-nav">
<h2 class="eds-c-header__heading">Navigation</h2>
<ul class="eds-c-header__list">
<li class="eds-c-header__list-item">
<a class="eds-c-header__link" data-track="nav_find_a_journal" data-track-action="click find a journal" data-track-category="unified header" data-track-context="unified header" data-track-label="link" href="https://link.springer.com/journals/">
                        Find a journal
									 </a>
</li>
<li class="eds-c-header__list-item">
<a class="eds-c-header__link" data-track="nav_how_to_publish" data-track-action="click publish with us link" data-track-category="unified header" data-track-context="unified header" data-track-label="link" href="https://www.springernature.com/gp/authors">
                        Publish with us
									 </a>
</li>
<li class="eds-c-header__list-item">
<a class="eds-c-header__link" data-track="nav_track_your_research" data-track-action="click track your research" data-track-category="unified header" data-track-context="unified header" data-track-label="link" href="https://link.springernature.com/home/">
                        Track your research
									 </a>
</li>
</ul>
</div>
<article class="app-masthead__colour-5" id="main" lang="en">
<section aria-label="article masthead" class="app-masthead">
<div class="app-masthead__container">
<div class="app-article-masthead u-sans-serif js-context-bar-sticky-point-masthead" data-test="masthead-component" data-track-component="article">
<div class="app-article-masthead__info">
<nav aria-label="breadcrumbs" data-test="breadcrumbs">
<ol class="c-breadcrumbs c-breadcrumbs--contrast" itemscope="" itemtype="https://schema.org/BreadcrumbList">
<li class="c-breadcrumbs__item" id="breadcrumb0" itemprop="itemListElement" itemscope="" itemtype="https://schema.org/ListItem">
<a class="c-breadcrumbs__link" data-track="click_breadcrumb" data-track-action="breadcrumbs" data-track-category="article" data-track-context="article page" data-track-label="breadcrumb1" href="/" itemprop="item"><span itemprop="name">Home</span></a><meta content="1" itemprop="position"/>
<svg aria-hidden="true" class="c-breadcrumbs__chevron" focusable="false" height="10" role="img" viewbox="0 0 10 10" width="10">
<path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)"></path>
</svg>
</li>
<li class="c-breadcrumbs__item" id="breadcrumb1" itemprop="itemListElement" itemscope="" itemtype="https://schema.org/ListItem">
<a class="c-breadcrumbs__link" data-track="click_breadcrumb" data-track-action="breadcrumbs" data-track-category="article" data-track-context="article page" data-track-label="breadcrumb2" href="/journal/10994" itemprop="item"><span itemprop="name">Machine Learning</span></a><meta content="2" itemprop="position"/>
<svg aria-hidden="true" class="c-breadcrumbs__chevron" focusable="false" height="10" role="img" viewbox="0 0 10 10" width="10">
<path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)"></path>
</svg>
</li>
<li class="c-breadcrumbs__item" id="breadcrumb2" itemprop="itemListElement" itemscope="" itemtype="https://schema.org/ListItem">
<span itemprop="name">Article</span><meta content="3" itemprop="position"/>
</li>
</ol>
</nav>
<h1 class="c-article-title" data-article-title="" data-test="article-title">Continual variational dropout: a view of auxiliary local variables in continual learning</h1>
<ul class="c-article-identifiers">
<li class="c-article-identifiers__item">
                            Published: <time datetime="2023-12-07">07 December 2023</time>
</li>
</ul>
<ul class="c-article-identifiers c-article-identifiers--cite-list">
<li class="c-article-identifiers__item">
<span data-test="journal-volume">Volume 113</span>, pages 281–323, (<span data-test="article-publication-year">2024</span>)
                        </li>
<li class="c-article-identifiers__item c-article-identifiers__item--cite">
<a data-track="click" data-track-action="cite this article" data-track-category="article body" data-track-label="link" href="#citeas">Cite this article</a>
</li>
</ul>
<div class="app-article-masthead__buttons" data-test="download-article-link-wrapper" data-track-context="masthead">
<div class="c-pdf-container">
<div class="c-pdf-download u-clear-both u-mb-16">
<a class="u-button u-button--full-width u-button--primary u-justify-content-space-between c-pdf-download__link" data-article-pdf="true" data-draft-ignore="true" data-readcube-pdf-url="true" data-test="pdf-link" data-track="content_download" data-track-action="download pdf" data-track-external="" data-track-label="button" data-track-type="article pdf download" download="" href="/content/pdf/10.1007/s10994-023-06487-7.pdf">
<span class="c-pdf-download__text">Download PDF</span>
<svg aria-hidden="true" class="u-icon" focusable="false" height="16" width="16"><use xlink:href="#icon-eds-i-download-medium"></use></svg>
</a>
</div>
</div>
<p class="app-article-masthead__access">
<svg aria-hidden="true" focusable="false" height="16" role="img" width="16"><use xlink:href="#icon-eds-i-check-filled-medium" xmlns:xlink="http://www.w3.org/1999/xlink"></use></svg>
                                Access provided by Chung Ang University</p>
</div>
</div>
<div class="app-article-masthead__brand">
<a class="app-article-masthead__journal-link" data-track="click_journal_home" data-track-action="journal homepage" data-track-context="article page" data-track-label="link" href="/journal/10994">
<picture>
<source height="159" media="(min-width: 768px)" srcset="https://media.springernature.com/w120/springer-static/cover-hires/journal/10994?as=webp,
                                                    https://media.springernature.com/w316/springer-static/cover-hires/journal/10994?as=webp 2x" type="image/webp" width="120"/>
<img alt="" height="95" src="https://media.springernature.com/w72/springer-static/cover-hires/journal/10994?as=webp" srcset="https://media.springernature.com/w144/springer-static/cover-hires/journal/10994?as=webp 2x" width="72"/>
</picture>
<span class="app-article-masthead__journal-title">Machine Learning</span>
</a>
<a class="app-article-masthead__submission-link" data-track="click_aims_and_scope" data-track-action="aims and scope" data-track-context="article page" data-track-label="link" href="https://link.springer.com/journal/10994/aims-and-scope">
                                Aims and scope
                                <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16"><use xlink:href="#icon-eds-i-arrow-right-medium" xmlns:xlink="http://www.w3.org/1999/xlink"></use></svg>
</a>
<a class="app-article-masthead__submission-link" data-track="click_submit_manuscript" data-track-action="submit manuscript" data-track-context="article masthead on springerlink article page" data-track-label="link" href="https://submission.springernature.com/new-submission/10994/3">
                                Submit manuscript
                                <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16"><use xlink:href="#icon-eds-i-arrow-right-medium" xmlns:xlink="http://www.w3.org/1999/xlink"></use></svg>
</a>
</div>
</div>
</div>
</section>
<div class="c-article-main u-container u-mt-24 u-mb-32 l-with-sidebar" data-component="article-container" id="main-content">
<main class="u-serif js-main-column" data-track-component="article body">
<div aria-hidden="true" class="c-context-bar u-hide" data-context-bar="" data-test="context-bar">
<div class="c-context-bar__container u-container">
<div class="c-context-bar__title">
                                Continual variational dropout: a view of auxiliary local variables in continual learning
                            </div>
<div data-test="inCoD" data-track-context="sticky banner">
<div class="c-pdf-container">
<div class="c-pdf-download u-clear-both u-mb-16">
<a class="u-button u-button--full-width u-button--primary u-justify-content-space-between c-pdf-download__link" data-article-pdf="true" data-draft-ignore="true" data-readcube-pdf-url="true" data-test="pdf-link" data-track="content_download" data-track-action="download pdf" data-track-external="" data-track-label="button" data-track-type="article pdf download" download="" href="/content/pdf/10.1007/s10994-023-06487-7.pdf">
<span class="c-pdf-download__text">Download PDF</span>
<svg aria-hidden="true" class="u-icon" focusable="false" height="16" width="16"><use xlink:href="#icon-eds-i-download-medium"></use></svg>
</a>
</div>
</div>
</div>
</div>
</div>
<div class="c-article-header">
<header>
<ul class="c-article-author-list c-article-author-list--short js-no-scroll" data-component-authors-activator="authors-list" data-test="authors-list"><li class="c-article-author-list__item"><a data-author-popup="auth-Nam_Le-Hai-Aff1-Aff3" data-author-search="Hai, Nam Le" data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Nam_Le-Hai-Aff1-Aff3">Nam Le Hai</a><sup class="u-js-hide"><a href="#Aff1" tabindex="-1">1</a>,<a href="#Aff3" tabindex="-1">3</a></sup><sup class="u-js-hide"> <a href="#na1" tabindex="-1">na1</a></sup>, </li><li class="c-article-author-list__item"><a data-author-popup="auth-Trang-Nguyen-Aff1" data-author-search="Nguyen, Trang" data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Trang-Nguyen-Aff1">Trang Nguyen</a><sup class="u-js-hide"><a href="#Aff1" tabindex="-1">1</a></sup><sup class="u-js-hide"> <a href="#na1" tabindex="-1">na1</a></sup>, </li><li class="c-article-author-list__item c-article-author-list__item--hide-small-screen"><a data-author-popup="auth-Linh_Ngo-Van-Aff1" data-author-search="Van, Linh Ngo" data-corresp-id="c1" data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Linh_Ngo-Van-Aff1">Linh Ngo Van<svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16"><use xlink:href="#icon-eds-i-mail-medium" xmlns:xlink="http://www.w3.org/1999/xlink"></use></svg></a><span class="u-js-hide"> 
            <a class="js-orcid" href="http://orcid.org/0000-0002-0011-5137"><span class="u-visually-hidden">ORCID: </span>orcid.org/0000-0002-0011-5137</a></span><sup class="u-js-hide"><a href="#Aff1" tabindex="-1">1</a></sup>, </li><li class="c-article-author-list__item c-article-author-list__item--hide-small-screen"><a data-author-popup="auth-Thien_Huu-Nguyen-Aff2" data-author-search="Nguyen, Thien Huu" data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Thien_Huu-Nguyen-Aff2">Thien Huu Nguyen</a><sup class="u-js-hide"><a href="#Aff2" tabindex="-1">2</a></sup> &amp; </li><li aria-label="Show all 5 authors for this article" class="c-article-author-list__show-more" title="Show all 5 authors for this article">…</li><li class="c-article-author-list__item"><a data-author-popup="auth-Khoat-Than-Aff1" data-author-search="Than, Khoat" data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Khoat-Than-Aff1">Khoat Than</a><sup class="u-js-hide"><a href="#Aff1" tabindex="-1">1</a></sup> </li></ul><button aria-expanded="false" class="c-article-author-list__button"><svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16"><use xlink:href="#icon-eds-i-chevron-down-medium" xmlns:xlink="http://www.w3.org/1999/xlink"></use></svg><span>Show authors</span></button>
<div data-test="article-metrics">
<ul class="app-article-metrics-bar u-list-reset">
<li class="app-article-metrics-bar__item">
<p class="app-article-metrics-bar__count"><svg aria-hidden="true" class="u-icon app-article-metrics-bar__icon" focusable="false" height="24" width="24">
<use xlink:href="#icon-eds-i-accesses-medium"></use>
</svg>367 <span class="app-article-metrics-bar__label">Accesses</span></p>
</li>
<li class="app-article-metrics-bar__item">
<p class="app-article-metrics-bar__count"><svg aria-hidden="true" class="u-icon app-article-metrics-bar__icon" focusable="false" height="24" width="24">
<use xlink:href="#icon-eds-i-citations-medium"></use>
</svg>2 <span class="app-article-metrics-bar__label">Citations</span></p>
</li>
<li class="app-article-metrics-bar__item">
<p class="app-article-metrics-bar__count"><svg aria-hidden="true" class="u-icon app-article-metrics-bar__icon" focusable="false" height="24" width="24">
<use xlink:href="#icon-eds-i-altmetric-medium"></use>
</svg>1 <span class="app-article-metrics-bar__label">Altmetric</span></p>
</li>
<li class="app-article-metrics-bar__item app-article-metrics-bar__item--metrics">
<p class="app-article-metrics-bar__details"><a data-track="click" data-track-action="view metrics" data-track-label="link" href="/article/10.1007/s10994-023-06487-7/metrics" rel="nofollow">Explore all metrics <svg aria-hidden="true" class="u-icon app-article-metrics-bar__arrow-icon" focusable="false" height="24" width="24">
<use xlink:href="#icon-eds-i-arrow-right-medium"></use>
</svg></a></p>
</li>
</ul>
</div>
<div class="u-mt-32">
</div>
</header>
</div>
<div class="c-article-body" data-article-body="true" data-track-component="article body">
<section aria-labelledby="Abs1" data-title="Abstract" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>Regularization/prior-based approach appears to be one of the critical strategies in continual learning, considering its mechanism for preserving and preventing forgetting the learned knowledge. Without any retraining on previous data or extending the network architecture, the mechanism works by setting a constraint on the important weights of previous tasks when learning the current task. Regularization/prior approach, on the other hand, suffers the challenge of weights being moved intensely to the parameter region, in which the model achieves good performance for the latest task but poor ones for earlier tasks. To that end, we suggest a novel solution to this problem by <i>continually</i> applying <i>variational dropout</i> (CVD), thereby generating task-specific local variables that work as modifying factors for the global variables to fit the task. In particular, as we impose a variational distribution on the auxiliary local variables employed as multiplicative noise to the layers’ input, the model enables the global variables to be retained in a good region for all tasks and reduces the forgetting phenomenon. Furthermore, we obtained theoretical properties that are currently unavailable in existing methods: (1) uncorrelated likelihoods between different data instances reduce the high variance of stochastic gradient variational Bayes; (2) correlated pre-activation improves the representation ability for each task; and (3) data-dependent regularization assures the global variables to be preserved in a good region for all tasks. Throughout our extensive results, adding the local variables shows its significant advantage in enhancing the performance of regularization/prior-based methods by considerable magnitudes on numerous datasets. Specifically, it brings several standard baselines closer to state-of-the-art results.</p></div></div></section>
<div data-test="cobranding-download">
<div class="note test-pdf-link" id="cobranding-and-download-availability-text">
</div>
</div>
<section aria-labelledby="inline-recommendations" class="c-article-recommendations" data-title="Inline Recommendations" data-track-component="inline-recommendations">
<h3 class="c-article-recommendations-title" id="inline-recommendations">Similar content being viewed by others</h3>
<div class="c-article-recommendations-list">
<div class="c-article-recommendations-list__item">
<article class="c-article-recommendations-card" itemscope="" itemtype="http://schema.org/ScholarlyArticle">
<div class="c-article-recommendations-card__img"><img alt="" loading="lazy" src="https://media.springernature.com/w92h120/springer-static/cover-hires/book/978-3-031-05933-9?as=webp"/></div>
<div class="c-article-recommendations-card__main">
<h3 class="c-article-recommendations-card__heading" itemprop="name headline">
<a class="c-article-recommendations-card__link" data-track="select_recommendations_1" data-track-action="click recommendations inline - 1" data-track-context="inline recommendations" data-track-label="10.1007/978-3-031-05933-9_2" href="https://link.springer.com/10.1007/978-3-031-05933-9_2?fromPaywallRec=false" itemprop="url">Auxiliary Local Variables for Improving Regularization/Prior Approach in Continual Learning
                                        </a>
</h3>
<div class="c-article-meta-recommendations" data-test="recommendation-info">
<span class="c-article-meta-recommendations__item-type">Chapter</span>
<span class="c-article-meta-recommendations__date">© 2022</span>
</div>
</div>
</article>
</div>
<div class="c-article-recommendations-list__item">
<article class="c-article-recommendations-card" itemscope="" itemtype="http://schema.org/ScholarlyArticle">
<div class="c-article-recommendations-card__img"><img alt="" loading="lazy" src="https://media.springernature.com/w92h120/springer-static/cover-hires/book/978-3-030-69538-5?as=webp"/></div>
<div class="c-article-recommendations-card__main">
<h3 class="c-article-recommendations-card__heading" itemprop="name headline">
<a class="c-article-recommendations-card__link" data-track="select_recommendations_2" data-track-action="click recommendations inline - 2" data-track-context="inline recommendations" data-track-label="10.1007/978-3-030-69538-5_14" href="https://link.springer.com/10.1007/978-3-030-69538-5_14?fromPaywallRec=false" itemprop="url">Regularizing Meta-learning via Gradient Dropout
                                        </a>
</h3>
<div class="c-article-meta-recommendations" data-test="recommendation-info">
<span class="c-article-meta-recommendations__item-type">Chapter</span>
<span class="c-article-meta-recommendations__date">© 2021</span>
</div>
</div>
</article>
</div>
<div class="c-article-recommendations-list__item">
<article class="c-article-recommendations-card" itemscope="" itemtype="http://schema.org/ScholarlyArticle">
<div class="c-article-recommendations-card__img"><img alt="" loading="lazy" src="https://media.springernature.com/w215h120/springer-static/image/art%3A10.1007%2Fs42519-020-00158-9/MediaObjects/42519_2020_158_Fig1_HTML.png"/></div>
<div class="c-article-recommendations-card__main">
<h3 class="c-article-recommendations-card__heading" itemprop="name headline">
<a class="c-article-recommendations-card__link" data-track="select_recommendations_3" data-track-action="click recommendations inline - 3" data-track-context="inline recommendations" data-track-label="10.1007/s42519-020-00158-9" href="https://link.springer.com/10.1007/s42519-020-00158-9?fromPaywallRec=false" itemprop="url">Dropout Fails to Regularize Nonparametric Learners
                                        </a>
</h3>
<div class="c-article-meta-recommendations" data-test="recommendation-info">
<span class="c-article-meta-recommendations__item-type">Article</span>
<span class="c-article-meta-recommendations__date">20 January 2021</span>
</div>
</div>
</article>
</div>
</div>
</section>
<script>
                window.dataLayer = window.dataLayer || [];
                window.dataLayer.push({
                    recommendations: {
                        recommender: 'semantic',
                        model: 'specter',
                        policy_id: 'NA',
                        timestamp: 1729087707,
                        embedded_user: 'null'
                    }
                });
            </script>
<section aria-labelledby="content-related-subjects" data-test="subject-content">
<h3 class="c-article__sub-heading" id="content-related-subjects">Explore related subjects</h3>
<span class="u-sans-serif u-text-s u-display-block u-mb-24">Discover the latest articles, news and stories from top researchers in related subjects.</span>
<ul class="c-article-subject-list" role="list">
<li class="c-article-subject-list__subject">
<a data-track="select_related_subject_1" data-track-context="related subjects from content page" data-track-label="Artificial Intelligence" href="/subject/artificial-intelligence">Artificial Intelligence</a>
</li>
</ul>
</section>
<div class="app-card-service" data-test="article-checklist-banner">
<div>
<a class="app-card-service__link" data-test="article-checklist-banner-link" data-track="click_presubmission_checklist" data-track-action="clicked article page checklist banner test 2 old version" data-track-category="pre-submission-checklist" data-track-context="article page top of reading companion" data-track-label="link" href="https://beta.springernature.com/pre-submission?journalId=10994">
<span class="app-card-service__link-text">Use our pre-submission checklist</span>
<svg aria-hidden="true" class="app-card-service__link-icon" focusable="false"><use xlink:href="#icon-eds-i-arrow-right-small"></use></svg>
</a>
<p class="app-card-service__description">Avoid common mistakes on your manuscript.</p>
</div>
<div class="app-card-service__icon-container">
<svg aria-hidden="true" class="app-card-service__icon" focusable="false">
<use xlink:href="#icon-eds-i-clipboard-check-medium"></use>
</svg>
</div>
</div>
<div class="main-content">
<section data-title="Introduction"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1"><span class="c-article-section__title-number">1 </span>Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>Continual learning, which involves learning a series of consecutive tasks, has become a hot subject among researchers. The trend of continual learning is due to the significant gap in learning ability between artificial neural networks (ANN) and humans. When working on a sequence of tasks, ANN is prone to losing knowledge gained from previous tasks if it cannot review earlier data, while humans can not only preserve acquired knowledge but also actively adjust it to the new tasks. As a result, researchers have focused on bringing the learning ability of methods closer to that of humans.</p><p>In this paper, we concentrate on task incremental learning scenario (Van de Ven &amp; Tolias, <a aria-label="Reference 2019" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR48" id="ref-link-section-d212779005e436" title="Van de Ven, G. M., &amp; Tolias, A. S. (2019). Three scenarios for continual learning. In NeurIPS—Continual learning workshop">2019</a>) where task identity is provided at both training and testing phases. Regularization/prior approach (Kirkpatrick et al., <a aria-label="Reference 2017" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR22" id="ref-link-section-d212779005e439" title="Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Desjardins, G., Rusu, A. A., Milan, K., Quan, J., Ramalho, T., Grabska-Barwinska, A., et al. (2017). Overcoming catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences, 114(13), 3521–3526.">2017</a>; Nguyen et al., <a aria-label="Reference 2018" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR37" id="ref-link-section-d212779005e442" title="Nguyen, C. V., Li, Y., Bui, T. D., &amp; Turner, R. E. (2018). Variational continual learning. In International conference on learning representation.">2018</a>; Farquhar &amp; Gal, <a aria-label="Reference 2018" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR11" id="ref-link-section-d212779005e445" title="Farquhar, S., &amp; Gal, Y. (2018). A unifying bayesian view of continual learning. In The Bayesian deep learning workshop at neural information processing systems">2018</a>; Loo et al., <a aria-label="Reference 2021" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR27" id="ref-link-section-d212779005e448" title="Loo, N., Swaroop, S., &amp; Turner, R. E. (2021). Generalized variational continual learning. In International conference on learning representation">2021</a>; Jung et al., <a aria-label="Reference 2020" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR19" id="ref-link-section-d212779005e452" title="Jung, S., Ahn, H., Cha, S., &amp; Moon, T. (2020). Continual learning with node-importance based adaptive group sparse regularization. In Advances in neural information processing systems">2020</a>; Yin et al., <a aria-label="Reference 2020" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR51" id="ref-link-section-d212779005e455" title="Yin, D., Farajtabar, M., &amp; Li, A. (2020). Sola: Continual learning with second-order loss approximation. In Workshop of advances in neural information processing systems">2020</a>; Phan et al., <a aria-label="Reference 2022" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR41" id="ref-link-section-d212779005e458" title="Phan, H., Tuan, A. P., Nguyen, S., Linh, N. V., &amp; Than, K. (2022). Reducing catastrophic forgetting in neural networks via Gaussian mixture approximation. In Pacific-Asia Conference on Knowledge Discovery and Data Mining (pp. 106–117). Springer: Berlin">2022</a>) emerges as an effective solution that can preserve and prevent forgetting the learned knowledge without any retraining on previous data or extending the network architecture. This approach provides a mechanism in which the learning of the model’ parameters for the current task is controlled by a regularization term added to the objective function. This additional constraint penalizes the deviation of parameters based on their importance. Therefore, it effectively solves the problem of forgetting knowledge and allows the model to work well on all preceding tasks. Consequently, the key factor in this approach is to find an appropriate evaluation to identify the weights’ importance. Nevertheless, this approach might find the optimal solution in regions that deliver good performance for the current task but degrade performance on preceding ones. Though strict constraints are already put on important weights, unimportant ones might be updated intensively in the current task, negatively affecting the performance of earlier tasks. This work aims to tackle this issue to improve the regularization/prior approach.</p><p>Dropout (Srivastava et al., <a aria-label="Reference 2014" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR44" id="ref-link-section-d212779005e464" title="Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., &amp; Salakhutdinov, R. (2014). Dropout: A simple way to prevent neural networks from overfitting. The Journal of Machine Learning Research, 15(1), 1929–1958.">2014</a>), on the other hand, is a well-known regularization strategy for a deterministic neural network (DNN) with deterministic weights. It creates auxiliary random variables following Bernoulli or Gaussian distributions and then multiplies them with the deterministic weights to increase stochasticity in the learning process. The dropout rate is originally fixed and chosen manually; later, it is estimated based on variational inference in several mechanisms (Kingma et al., <a aria-label="Reference 2015" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR20" id="ref-link-section-d212779005e467" title="Kingma, D. P., Salimans, T., &amp; Welling, M. (2015). Variational dropout and the local reparameterization trick. Advances in Neural Information Processing Systems, 28, 2575–2583.">2015</a>; Gal et al., <a aria-label="Reference 2017" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR12" id="ref-link-section-d212779005e470" title="Gal, Y., Hron, J., &amp; Kendall, A. (2017). Concrete dropout. In Advances in Neural Information Processing Systems (pp. 3581–3590).">2017</a>; Liu et al., <a aria-label="Reference 2019" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR26" id="ref-link-section-d212779005e473" title="Liu, Y., Dong, W., Zhang, L., Gong, D., &amp; Shi, Q. (2019). Variational bayesian dropout with a hierarchical prior. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 7124–7133).">2019</a>; Nguyen et al., <a aria-label="Reference 2022a" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR36" id="ref-link-section-d212779005e476" title="Nguyen, H., Pham, H., Nguyen, S., Van Linh, N., &amp; Than, K. (2022a). Adaptive infinite dropout for noisy and sparse data streams. Machine Learning, 111(8), 3025–3060.">2022a</a>; Boluki et al., <a aria-label="Reference 2020" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR6" id="ref-link-section-d212779005e480" title="Boluki, S., Ardywibowo, R., Dadaneh, S. Z., Zhou, M., &amp; Qian, X. (2020). Learnable Bernoulli dropout for bayesian deep learning. In The International Conference on Artificial Intelligence and Statistics, AISTATS (pp. 3905–3916).">2020</a>; Nguyen et al., <a aria-label="Reference 2021" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR35" id="ref-link-section-d212779005e483" title="Nguyen, S., Nguyen, D., Nguyen, K., Than, K., Bui, H., &amp; Ho, N. (2021). Structured dropout variational inference for Bayesian neural networks. Advances in Neural Information Processing Systems, 34, 15188–15202.">2021</a>). Recent research (Goodfellow et al., <a aria-label="Reference 2013" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR14" id="ref-link-section-d212779005e486" title="Goodfellow, I. J., Mirza, M., Xiao, D., Courville, A., &amp; Bengio, Y. (2013). An empirical investigation of catastrophic forgetting in gradient-based neural networks. arXiv preprint 
                  arXiv:1312.6211
                  
                ">2013</a>; De Lange et al., <a aria-label="Reference 2021" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR9" id="ref-link-section-d212779005e489" title="De Lange, M., Aljundi, R., Masana, M., Parisot, S., Jia, X., Leonardis, A., Slabaugh, G., &amp; Tuytelaars, T. (2021). A continual learning survey: Defying forgetting in classification tasks. IEEE Transactions on Pattern Analysis and Machine Intelligence">2021</a>; Mirzadeh et al., <a aria-label="Reference 2020" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR29" id="ref-link-section-d212779005e492" title="Mirzadeh, S., Farajtabar, M., Pascanu, R., &amp; Ghasemzadeh, H. (2020). Understanding the role of training regimes in continual learning. In Advances in neural information processing systems">2020</a>) has focused on the effectiveness of dropout in the context of continual learning. It has been discovered that dropout can significantly help prevent the catastrophic forgetting problem. However, there are still several concerns that should be resolved. Firstly, no findings show that the concept of dropout has been considered in a Bayesian neural network (BNN) (Graves, <a aria-label="Reference 2011" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR15" id="ref-link-section-d212779005e495" title="Graves, A. (2011). Practical variational inference for neural networks. In Advances in Neural Information Processing Systems (pp. 2348–2356). Citeseer.">2011</a>; Blundell et al., <a aria-label="Reference 2015" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR5" id="ref-link-section-d212779005e499" title="Blundell, C., Cornebise, J., Kavukcuoglu, K., &amp; Wierstra, D. (2015). Weight uncertainty in neural network. In International conference on machine learning (pp. 1613–1622). PMLR.">2015</a>; Oh et al., <a aria-label="Reference 2020" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR39" id="ref-link-section-d212779005e502" title="Oh, C., Adamczewski, K., &amp; Park, M. (2020). Radial and directional posteriors for Bayesian deep learning. In The thirty-fourth conference on artificial intelligence, AAAI (pp. 5298–5305)">2020</a>; Swiatkowski et al., <a aria-label="Reference 2020" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR46" id="ref-link-section-d212779005e505" title="Swiatkowski, J., Roth, K., Veeling, B., Tran, L., Dillon, J., Snoek, J., Mandt, S., Salimans, T., Jenatton, R., &amp; Nowozin, S. (2020). The k-tied normal distribution: A compact parameterization of Gaussian mean field posteriors in Bayesian neural networks. In International conference on machine learning (pp. 9289–9299). PMLR.">2020</a>) whose weights are represented by a distribution. As a result, it has not been compatible with current prior-based methods. Secondly, there are few theoretical explanations for why dropout works in continuous learning, as previous research was limited to basic DNN experimental scenarios. Finally, instead of manually adjusting the dropout rate in continual learning contexts that seem impractical, the dropout rate should be individually specified for each task to adapt efficiently to the sequence of tasks. We emphasize that existing adaptive dropout methods (Kingma et al., <a aria-label="Reference 2015" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR20" id="ref-link-section-d212779005e508" title="Kingma, D. P., Salimans, T., &amp; Welling, M. (2015). Variational dropout and the local reparameterization trick. Advances in Neural Information Processing Systems, 28, 2575–2583.">2015</a>; Liu et al., <a aria-label="Reference 2019" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR26" id="ref-link-section-d212779005e511" title="Liu, Y., Dong, W., Zhang, L., Gong, D., &amp; Shi, Q. (2019). Variational bayesian dropout with a hierarchical prior. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 7124–7133).">2019</a>) are inapplicable to regularization/prior methods since they only focus on connecting DNN with dropout and BNN. To achieve this goal, they must consider a restriction of BNNs with fixed priors, such as the log uniform distribution (Kingma et al., <a aria-label="Reference 2015" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR20" id="ref-link-section-d212779005e514" title="Kingma, D. P., Salimans, T., &amp; Welling, M. (2015). Variational dropout and the local reparameterization trick. Advances in Neural Information Processing Systems, 28, 2575–2583.">2015</a>; Molchanov et al., <a aria-label="Reference 2017" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR31" id="ref-link-section-d212779005e518" title="Molchanov, D., Ashukha, A., &amp; Vetrov, D. (2017). Variational dropout sparsifies deep neural networks. In International conference on machine learning (pp. 2498–2507).">2017</a>) or the discrete quantized Gaussian prior (Gal et al., <a aria-label="Reference 2017" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR12" id="ref-link-section-d212779005e521" title="Gal, Y., Hron, J., &amp; Kendall, A. (2017). Concrete dropout. In Advances in Neural Information Processing Systems (pp. 3581–3590).">2017</a>). Then, the dropout rate is also obtained by applying variational inference on the corresponding BNN. In the meantime, regularization/prior-based approach maintains learned knowledge using the learned model from the previous task as the prior in the current task. As a result, adaptive dropout is unavailable for regularization/prior-based methods.</p><br/><p>Our contributions are listed as follows:</p><p>First, this paper introduces a novel method that <i>continually</i> applies <i>variational dropout (CVD)</i> to create auxiliary local variables, thus improving regularization/prior-based methods in continual learning. CVD provides two types of variables: task-specific and shared variables (the original weights of neural networks). The global variables are designed to preserve the acquired knowledge from previous tasks, allowing our methods to be combined with existing regularization/prior-based methods. In the meantime, the local variables characterize each specific task and adjust the global variables to perform well on the corresponding tasks. Intuitively, adding local variables, which are learned and saved for each specific task, can help the global variables escape the trap of a local optimum that is merely good for this task.</p><p>Second, we show the application of CVD in both BNN and DNN. We jointly learn the posteriors of auxiliary local variables for each task and global variables for all tasks. In particular, considering the global variables, the estimated posterior distribution learned from the previous task can be flexibly used as the prior distribution for the current task. Meanwhile, the mechanism for the auxiliary variables is to treat them as Gaussian multiplicative noise in dropout when putting a constraint on approximations of their posteriors. Hence, it gives a mechanism to task-specifically learn dropout rate in continual learning.</p><p>Third, we highlight three critical features of CVD that are not present in the previous regularization/prior-based methods for both DNNs and BNNs: (i) The uncorrelated likelihoods between different data instances reduce the high variance of stochastic gradient variational Bayes. By having the covariance between the likelihoods equals zero, the variance scales down by the minibatch size, resulting in a more stable learning process. (ii) The correlated pre-activation enhances the representation ability for each task. Intuitively, CVD creates hierarchical distributions between dimensions of the pre-activations, allowing better representing ability. (iii) The data-dependent regularization guarantees that the global variables are preserved in a good region for all tasks. Remarkably, our regularization is able to widen the minima and help the model avoid catastrophic forgetting (Mirzadeh et al., <a aria-label="Reference 2020" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR29" id="ref-link-section-d212779005e543" title="Mirzadeh, S., Farajtabar, M., Pascanu, R., &amp; Ghasemzadeh, H. (2020). Understanding the role of training regimes in continual learning. In Advances in neural information processing systems">2020</a>; Shi et al., <a aria-label="Reference 2021" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR43" id="ref-link-section-d212779005e546" title="Shi, G., Chen, J., Zhang, W., Zhan, L.-M., &amp; Wu, X.-M. (2021). Overcoming catastrophic forgetting in incremental few-shot learning by finding flat minima. Advances in Neural Information Processing Systems, 34, 6747–6761.">2021</a>; Cha et al., <a aria-label="Reference 2021" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR7" id="ref-link-section-d212779005e549" title="Cha, S., Hsu, H., Hwang, T., Calmon, F. P., &amp; Moon, T. (2021). CPR: Classifier-projection regularization for continual learning. In 9th International Conference on Learning Representations, ICLR.">2021</a>). Note that our theoretical properties are distinct from the existing local variables approach. Consequently, our CVD can be integrated with other auxiliary variables methods and takes advantage of variational dropout to enhance the performance.</p><p>Finally, we evaluate the practical effectiveness of CVD using five regularization/prior-based methods, including Elastic Weight Consolidation (EWC) (Kirkpatrick et al., <a aria-label="Reference 2017" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR22" id="ref-link-section-d212779005e555" title="Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Desjardins, G., Rusu, A. A., Milan, K., Quan, J., Ramalho, T., Grabska-Barwinska, A., et al. (2017). Overcoming catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences, 114(13), 3521–3526.">2017</a>), Adaptive Group Sparsity based Continual Learning (AGS-CL) (Jung et al., <a aria-label="Reference 2020" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR19" id="ref-link-section-d212779005e558" title="Jung, S., Ahn, H., Cha, S., &amp; Moon, T. (2020). Continual learning with node-importance based adaptive group sparse regularization. In Advances in neural information processing systems">2020</a>), Variational Continual Learning (VCL) (Nguyen et al., <a aria-label="Reference 2018" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR37" id="ref-link-section-d212779005e561" title="Nguyen, C. V., Li, Y., Bui, T. D., &amp; Turner, R. E. (2018). Variational continual learning. In International conference on learning representation.">2018</a>), Generalized Variational Continual Learning (GVCL) (Loo et al., <a aria-label="Reference 2021" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR27" id="ref-link-section-d212779005e564" title="Loo, N., Swaroop, S., &amp; Turner, R. E. (2021). Generalized variational continual learning. In International conference on learning representation">2021</a>), and Uncertainty-based Continual Learning (UCL) (Ahn et al., <a aria-label="Reference 2019" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR1" id="ref-link-section-d212779005e567" title="Ahn, H., Cha, S., Lee, D., &amp; Moon, T. (2019). Uncertainty-based continual learning with adaptive regularization. In Advances in Neural Information Processing Systems (pp. 4392–4402).">2019</a>). The experimental results on several benchmark datasets suggest that CVD can significantly increase the baseline performance. In particular, CVD has the ability to bring standard methods closer to state-of-the-art results in several experiments.</p><p>In the rest of the paper, the related work and background are briefly summarized in Sect. <a data-track="click" data-track-action="section anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#Sec2">2</a>. Sections <a data-track="click" data-track-action="section anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#Sec7">3</a> and <a data-track="click" data-track-action="section anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#Sec12">4</a> present our proposal and experiments respectively. The conclusion is drawn in Sect. <a data-track="click" data-track-action="section anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#Sec20">5</a>.</p></div></div></section><section data-title="Related work and background"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2"><span class="c-article-section__title-number">2 </span>Related work and background</h2><div class="c-article-section__content" id="Sec2-content"><p>This section introduces related work and then presents some backgrounds.</p><h3 class="c-article__sub-heading" id="Sec3"><span class="c-article-section__title-number">2.1 </span>Related work</h3><p>An extensive number of investigations have recently discussed the problem of continual learning (CL), and they may be classified into three groups: memory-based, architecture-based, and regularization/prior-based approaches. This paper focuses on a regularization/prior-based strategy, which does not include storing and re-training previous data or constructing a dynamic architecture. The fundamental concept behind this method is to set a limit on the extent to which the current task’s model parameters differ from those of prior tasks. The constraint is frequently built in one of two ways: adding a regularization term or substituting the current task’s prior with the previous task’s posterior. We will go through some specific strategies from both directions in detail.</p><p>When it comes to regularization (Benzing, <a aria-label="Reference 2020" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR4" id="ref-link-section-d212779005e603" title="Benzing, F. (2020). Understanding regularisation methods for continual learning. In Workshop of Advances in Neural Information Processing Systems.">2020</a>; Li &amp; Hoiem, <a aria-label="Reference 2017" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR24" id="ref-link-section-d212779005e606" title="Li, Z., &amp; Hoiem, D. (2017). Learning without forgetting. IEEE Transactions on Pattern Analysis and Machine Intelligence, 40(12), 2935–2947.">2017</a>; Zenke et al., <a aria-label="Reference 2017" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR52" id="ref-link-section-d212779005e609" title="Zenke, F., Poole, B., &amp; Ganguli, S. (2017). Continual learning through synaptic intelligence. Proceedings of Machine Learning Research, 70, 3987.">2017</a>; Kirkpatrick et al., <a aria-label="Reference 2017" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR22" id="ref-link-section-d212779005e612" title="Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Desjardins, G., Rusu, A. A., Milan, K., Quan, J., Ramalho, T., Grabska-Barwinska, A., et al. (2017). Overcoming catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences, 114(13), 3521–3526.">2017</a>; Van Linh et al., <a aria-label="Reference 2022" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR25" id="ref-link-section-d212779005e615" title="Van Linh, N., Bach, T. X., &amp; Than, K. (2022). A graph convolutional topic model for short and noisy text streams. Neurocomputing, 468, 345–359.">2022</a>; Jung et al., <a aria-label="Reference 2020" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR19" id="ref-link-section-d212779005e619" title="Jung, S., Ahn, H., Cha, S., &amp; Moon, T. (2020). Continual learning with node-importance based adaptive group sparse regularization. In Advances in neural information processing systems">2020</a>; Ahn et al., <a aria-label="Reference 2019" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR1" id="ref-link-section-d212779005e622" title="Ahn, H., Cha, S., Lee, D., &amp; Moon, T. (2019). Uncertainty-based continual learning with adaptive regularization. In Advances in Neural Information Processing Systems (pp. 4392–4402).">2019</a>; Bach et al., <a aria-label="Reference 2023" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR3" id="ref-link-section-d212779005e625" title="Bach, T. X., Anh, N. D., Linh, N. V., &amp; Than, K. (2023). Dynamic transformation of prior knowledge into Bayesian models for data streams. IEEE Transactions on Knowledge and Data Engineering, 35(4), 3742–3750.">2023</a>), an additional regularization term is added to the original loss, which constrains the learning weights of the current task to lie near to the solutions attained from earlier tasks. The regularization term is meant to avoid the catastrophic forgetting phenomenon, whereas the original loss attempts to perform well on the present task. Several strategies are proposed to assess the weight importance that determines the strict level of the constraints in regularization terms. Synaptic Intelligence (SI) (Zenke et al., <a aria-label="Reference 2017" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR52" id="ref-link-section-d212779005e628" title="Zenke, F., Poole, B., &amp; Ganguli, S. (2017). Continual learning through synaptic intelligence. Proceedings of Machine Learning Research, 70, 3987.">2017</a>) estimates the weight importance based on the loss function changes with respect to each weight. Memory-aware Synapses (MAS) (Aljundi et al., <a aria-label="Reference 2018" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR2" id="ref-link-section-d212779005e631" title="Aljundi, R., Babiloni, F., Elhoseiny, M., Rohrbach, M., &amp; Tuytelaars, T. (2018). Memory aware synapses: Learning what (not) to forget. In Proceedings of the European Conference on Computer Vision (ECCV) (pp. 139–154).">2018</a>) uses the change of outputs instead of the loss function to evaluate the weight importance. Elastic Weight Consolidation (EWC) (Kirkpatrick et al., <a aria-label="Reference 2017" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR22" id="ref-link-section-d212779005e634" title="Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Desjardins, G., Rusu, A. A., Milan, K., Quan, J., Ramalho, T., Grabska-Barwinska, A., et al. (2017). Overcoming catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences, 114(13), 3521–3526.">2017</a>) is derived from Bayesian learning, and the diagonal Fisher information matrix measures weight importance. Adaptive Group Sparsity-based Continual Learning (AGS-CL) (Jung et al., <a aria-label="Reference 2020" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR19" id="ref-link-section-d212779005e638" title="Jung, S., Ahn, H., Cha, S., &amp; Moon, T. (2020). Continual learning with node-importance based adaptive group sparse regularization. In Advances in neural information processing systems">2020</a>) focuses on node importance instead of weight importance and uses the average activation of each node to measure.</p><p>Meanwhile, prior-based methods (Farquhar &amp; Gal, <a aria-label="Reference 2018" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR11" id="ref-link-section-d212779005e644" title="Farquhar, S., &amp; Gal, Y. (2018). A unifying bayesian view of continual learning. In The Bayesian deep learning workshop at neural information processing systems">2018</a>; Swaroop et al., <a aria-label="Reference 2018" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR45" id="ref-link-section-d212779005e647" title="Swaroop, S., Nguyen, C.V., Bui, T. D., &amp; Turner, R. E. (2018). Improving and understanding variational continual learning. In NeurIPS Continual Learning Workshop.">2018</a>; Nguyen et al., <a aria-label="Reference 2018" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR37" id="ref-link-section-d212779005e650" title="Nguyen, C. V., Li, Y., Bui, T. D., &amp; Turner, R. E. (2018). Variational continual learning. In International conference on learning representation.">2018</a>; Ahn et al., <a aria-label="Reference 2019" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR1" id="ref-link-section-d212779005e653" title="Ahn, H., Cha, S., Lee, D., &amp; Moon, T. (2019). Uncertainty-based continual learning with adaptive regularization. In Advances in Neural Information Processing Systems (pp. 4392–4402).">2019</a>; Phan et al., <a aria-label="Reference 2022" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR41" id="ref-link-section-d212779005e656" title="Phan, H., Tuan, A. P., Nguyen, S., Linh, N. V., &amp; Than, K. (2022). Reducing catastrophic forgetting in neural networks via Gaussian mixture approximation. In Pacific-Asia Conference on Knowledge Discovery and Data Mining (pp. 106–117). Springer: Berlin">2022</a>; Henning et al., <a aria-label="Reference 2021" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR18" id="ref-link-section-d212779005e660" title="Henning, C., Cervera, M., D’Angelo, F., Von Oswald, J., Traber, R., Ehret, B., Kobayashi, S., Grewe, B. F., &amp; Sacramento, J. (2021). Posterior meta-replay for continual learning. In Advances in neural information processing systems (Vol. 34).">2021</a>) take advantage of Bayesian Neural Networks (BNNs) (MacKay, <a aria-label="Reference 1992" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR28" id="ref-link-section-d212779005e663" title="MacKay, D. J. C. (1992). A practical Bayesian framework for backpropagation networks. Neural Computation, 4(3), 448–472.">1992</a>; Neal, <a aria-label="Reference 1996" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR33" id="ref-link-section-d212779005e666" title="Neal, R. M. (1996). Bayesian learning for neural networks. Berlin: Springer.">1996</a>), in which the model parameters follow an inferred posterior distribution rather than merely point estimates. Regarding this approach, online variational inference (Ghahramani &amp; Attias, <a aria-label="Reference 2000" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR13" id="ref-link-section-d212779005e669" title="Ghahramani, Z., &amp; Attias, H. (2000). Online variational Bayesian learning. In Slides from talk presented at NIPS workshop on online learning.">2000</a>; Sato, <a aria-label="Reference 2001" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR42" id="ref-link-section-d212779005e672" title="Sato, M.-A. (2001). Online model selection based on the variational bayes. Neural Computation, 13(7), 1649–1681.">2001</a>) is used to construct a constraint between consecutive tasks. In more detail, the estimated posterior obtained from the previous task is used as a prior to learn the current task. Variational continual learning (VCL) (Nguyen et al., <a aria-label="Reference 2018" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR37" id="ref-link-section-d212779005e675" title="Nguyen, C. V., Li, Y., Bui, T. D., &amp; Turner, R. E. (2018). Variational continual learning. In International conference on learning representation.">2018</a>) is the first work that applies it to continual learning. The objective function of VCL consists of two terms: Likelihood term and KL-divergence term. When the prior is the approximate posterior learned from previous tasks, KL-divergence between the current approximate posterior and the prior makes VCL more stable. Generalized Variational Continual Learning (GVCL) (Loo et al., <a aria-label="Reference 2021" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR27" id="ref-link-section-d212779005e679" title="Loo, N., Swaroop, S., &amp; Turner, R. E. (2021). Generalized variational continual learning. In International conference on learning representation">2021</a>) is introduced as a tempered version of VCL, which down-weights the KL-divergence in the objective function and mitigates the common over-pruning effect of variational inference by using task-specific FiLM layers. Uncertainty regularized Continual Learning (UCL) (Ahn et al., <a aria-label="Reference 2019" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR1" id="ref-link-section-d212779005e682" title="Ahn, H., Cha, S., Lee, D., &amp; Moon, T. (2019). Uncertainty-based continual learning with adaptive regularization. In Advances in Neural Information Processing Systems (pp. 4392–4402).">2019</a>) improves VCL by defining node importance and then adding two regularization terms. Based on the node importance, the first term limits the change of weights related to important nodes, and the second term makes weights more active in learning new tasks.</p><p>Experiments (Goodfellow et al., <a aria-label="Reference 2013" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR14" id="ref-link-section-d212779005e688" title="Goodfellow, I. J., Mirza, M., Xiao, D., Courville, A., &amp; Bengio, Y. (2013). An empirical investigation of catastrophic forgetting in gradient-based neural networks. arXiv preprint 
                  arXiv:1312.6211
                  
                ">2013</a>; Mirzadeh et al., <a aria-label="Reference 2020" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR30" id="ref-link-section-d212779005e691" title="Mirzadeh, S. I., Farajtabar, M., &amp; Ghasemzadeh, H. (2020). Dropout as an implicit gating mechanism for continual learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops (pp. 232–233).">2020</a>) show that using dropout and stochastic gradient descent without combining continual learning strategies like regularization and additional memory can prevent catastrophic forgetting. This mark of dropout, when integrated with continual learning-based methods, further enhances the original methods significantly in performing well on previous tasks (De Lange et al., <a aria-label="Reference 2021" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR9" id="ref-link-section-d212779005e694" title="De Lange, M., Aljundi, R., Masana, M., Parisot, S., Jia, X., Leonardis, A., Slabaugh, G., &amp; Tuytelaars, T. (2021). A continual learning survey: Defying forgetting in classification tasks. IEEE Transactions on Pattern Analysis and Machine Intelligence">2021</a>; Mirzadeh et al., <a aria-label="Reference 2020" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR29" id="ref-link-section-d212779005e697" title="Mirzadeh, S., Farajtabar, M., Pascanu, R., &amp; Ghasemzadeh, H. (2020). Understanding the role of training regimes in continual learning. In Advances in neural information processing systems">2020</a>; Nguyen et al., <a aria-label="Reference 2019" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR38" id="ref-link-section-d212779005e700" title="Nguyen, V.-S., Nguyen, D.-T., Van, L.N., &amp; Than, K. (2019). Infinite dropout for training bayesian models from data streams. In IEEE international conference on big data (Big Data) (pp. 125–134). IEEE">2019</a>; Ha et al., <a aria-label="Reference 2019" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR16" id="ref-link-section-d212779005e704" title="Ha, C., Tran, V.-D., Van, L. N., &amp; Than, K. (2019). Eliminating overfitting of probabilistic topic models on short and noisy text: The role of dropout. International Journal of Approximate Reasoning, 112, 85–104.">2019</a>; Nguyen et al., <a aria-label="Reference 2022a" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR36" id="ref-link-section-d212779005e707" title="Nguyen, H., Pham, H., Nguyen, S., Van Linh, N., &amp; Than, K. (2022a). Adaptive infinite dropout for noisy and sparse data streams. Machine Learning, 111(8), 3025–3060.">2022a</a>, <a aria-label="Reference b" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR34" id="ref-link-section-d212779005e710" title="Nguyen, T., Mai, T., Nguyen, N., Van, L. N., &amp; Than, K. (2022b). Balancing stability and plasticity when learning topic models from short and noisy text streams. Neurocomputing, 505, 30–43.">b</a>). However, the impact of dropouts has not been discussed adequately, and the dropout rate is merely selected manually and fixed even when the learning tasks are changed. There is a lack of a mechanism to control this parameter when working on multiple tasks. Furthermore, dropout has not been taken into account in BNN, possibly due to the fact that BNN already possesses some of the key qualities of dropout, such as uncertainty and regularization. In continual learning with several tasks, however, we discover that the concept of dropout can be appropriately applied to both DNNs and BNNs. Adding auxiliary variables brings remarkable properties: correlated pre-activation and data-dependent regularization.</p><p>In the meantime, variational dropout (VD) (Kingma et al., <a aria-label="Reference 2015" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR20" id="ref-link-section-d212779005e717" title="Kingma, D. P., Salimans, T., &amp; Welling, M. (2015). Variational dropout and the local reparameterization trick. Advances in Neural Information Processing Systems, 28, 2575–2583.">2015</a>) provides a mechanism for learning the dropout rate in DNNs. We emphasize that VD is not used for Bayesian neural networks (BNNs), whose weights are random variables. It merely aims to connect DNNs with dropout and BNNs. In detail, a noise <span class="mathjax-tex">\(s \sim {\mathcal {N}}(1,\alpha )\)</span> multiplied by a deterministic weight <span class="mathjax-tex">\(\theta _{ij}\)</span> results in a random variable <span class="mathjax-tex">\(w_{ij} = s\theta _{ij}\)</span> following <span class="mathjax-tex">\({\mathcal {N}}(\theta _{ij},\alpha \theta _{ij}^2)\)</span>. Notably, this property is only correct if <span class="mathjax-tex">\(\theta\)</span> is a deterministic value. Therefore, VD is only used for a DNN. Furthermore, VD must make the KL-term (<span class="mathjax-tex">\(KL(q(W) \Vert p(W))\)</span>) independent of <span class="mathjax-tex">\(\theta\)</span> in order to ensure that the objective function (the evidence lower bound) of the related BNN is the same as the objective function of the original DNN with dropout w.r.t parameters <span class="mathjax-tex">\(\theta\)</span>. As a result, VD must set the prior distribution to a fixed log-uniform distribution. This limitation prevents VD from retaining the learned knowledge from the previous task, considering the context of prior-based approach, where posterior acquired from previous tasks is used as the prior distribution to learn the current task. In our work, thanks to separating the posteriors of local and global variables, CVD can function not only on DNNs but also on BNNs in continual learning.</p><h3 class="c-article__sub-heading" id="Sec4"><span class="c-article-section__title-number">2.2 </span>Background</h3><p>Consider a neural network as a probabilistic model <span class="mathjax-tex">\(p({\textbf{y}} \vert {\textbf{x}},\varvec{\theta })\)</span> over output <span class="mathjax-tex">\({\textbf{y}}\)</span> conditioned on input <span class="mathjax-tex">\({\textbf{x}}\)</span> and parameters <span class="mathjax-tex">\(\varvec{\theta }\)</span> which represent the weight matrices. In the Bayesian approach, <span class="mathjax-tex">\(\varvec{\theta }\)</span> are random variables and follow a prior <span class="mathjax-tex">\(p(\varvec{\theta })\)</span>. In continual learning, data streams arrive at and belong to consecutive tasks. Let <span class="mathjax-tex">\(({\textbf{X}}_t, {\textbf{Y}}_t) = \{({\textbf{x}}_t^{(i)}, {\textbf{y}}_t^{(i)})\}_{i=1}^{N_t}\)</span> be data of task <i>t</i> where <span class="mathjax-tex">\(N_t\)</span> is the number of data instances in this task. We focus on two directions: Regularization and online variational inference (OVI). The first direction poses an additional regularization term between the parameters of current and the previous tasks and uses point estimations. The second direction, OVI, is based on variational Bayesian inference and regularizes the parameters by a KL term. In particular, it considers the posterior distribution of the previous task as the prior distribution of the current task. We describe EWC, AGS-CL for regularization and VCL, GVCL for OVI.</p><h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec5"><span class="c-article-section__title-number">2.2.1 </span>Regularization approach</h4><p>Regularization approach creates a connection between parameters learned in past and current task by adding a regularization term <i>R</i>:</p><div class="c-article-equation" id="Equ5"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} L_{t}(\theta ) = L(D_{t}, \theta ) + R(\theta , \theta _{t-1}^{*}) \end{aligned}$$</span></div></div><p>where <span class="mathjax-tex">\(L(D_{t}, \theta )\)</span> is the objective function of the current task <i>t</i> and <span class="mathjax-tex">\(\theta _{t-1}^{*}\)</span> are the parameters learned from the previous task <span class="mathjax-tex">\(t-1\)</span>.</p>
<h3 class="c-article__sub-heading">
<b>EWC</b>
</h3>
<p>(Kirkpatrick et al., <a aria-label="Reference 2017" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR22" id="ref-link-section-d212779005e1535" title="Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Desjardins, G., Rusu, A. A., Milan, K., Quan, J., Ramalho, T., Grabska-Barwinska, A., et al. (2017). Overcoming catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences, 114(13), 3521–3526.">2017</a>) uses maximum a posterior to obtain a point estimation of parameters and then applies Laplace approximation to estimate the true posterior by a Gaussian distribution. Due to the large number of parameters, the full covariance matrix is reduced to be diagonal and is obtained via the diagonal of the Fisher information matrix. This approximated posterior is considered as a prior to create a regularization term when learning new tasks. At task <i>t</i>, the objective function of EWC is described as follows:</p><div class="c-article-equation" id="Equ6"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} \begin{aligned} {\mathcal {L}}(\varvec{\theta })&amp;= \sum _{i=1}^{N_t} \log p({\textbf{y}}_t^{(i)} \vert \varvec{\theta }, {\textbf{x}}_t^{(i)}) + \log p( \varvec{\theta }) \\&amp;=\sum _{i=1}^{N_t} \log p({\textbf{y}}_t^{(i)} \vert \varvec{\theta }, {\textbf{x}}_t^{(i)}) - \frac{\lambda }{2} \sum _{j=1}^{J} \left( \sum _{k=1}^{t-1} F_{k,j} \right) (\theta _{j} - \theta _{t-1,j}^{*})^2 \end{aligned} \end{aligned}$$</span></div></div><p>where <span class="mathjax-tex">\(\lambda\)</span> is a hyperparameter, <i>J</i> is the cardinality of <span class="mathjax-tex">\(\varvec{\theta }\)</span>, <span class="mathjax-tex">\(F_{k,j}\)</span> is Fisher information with respect to <span class="mathjax-tex">\(\theta _j\)</span> after learning task <i>k</i>.</p>
<h3 class="c-article__sub-heading">
<b>AGS-CL</b>
</h3>
<p>(Jung et al., <a aria-label="Reference 2020" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR19" id="ref-link-section-d212779005e2034" title="Jung, S., Ahn, H., Cha, S., &amp; Moon, T. (2020). Continual learning with node-importance based adaptive group sparse regularization. In Advances in neural information processing systems">2020</a>) introduces a dual penalty in the loss function, which considers node importance. The application of the proximal gradient descent technique ensures model sparsity and immobilization throughout the training procedure. Consequently, this approach allows the learner to actively manage the model’s capacity, enabling it to either retain knowledge of previous tasks or adapt to new ones. The objective function for learning task <i>t</i> is defined as</p><div class="c-article-equation" id="Equ7"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} \begin{aligned} {\mathcal {L}}(\varvec{\theta })&amp;= \sum _{i=1}^{N_t} \log p({\textbf{y}}_t^{(i)} \vert \varvec{\theta }, {\textbf{x}}_t^{(i)}) \\ {}&amp;\quad - \mu \sum _{n_\ell \in {\mathcal {G}}_0^{t-1}} \Vert \varvec{\theta }_{n_\ell } \Vert _2 - \lambda \sum _{n_\ell \in {\mathcal {G}} \setminus {\mathcal {G}}_0^{t-1}} \Omega _{n_\ell }^{t-1} \Vert \varvec{\theta }_{n_\ell } - \varvec{\theta }_{n_\ell }^{(t-1)} \Vert _2 \end{aligned} \end{aligned}$$</span></div></div><p>in which <span class="mathjax-tex">\(\varvec{\theta }_{n_\ell }^{(t-1)}\)</span> is the learned parameter vector for node <i>n</i> up to task <span class="mathjax-tex">\(t-1\)</span>, and <span class="mathjax-tex">\(\mu\)</span>, <span class="mathjax-tex">\(\lambda\)</span> are the hyperparameters for the penalty terms. <span class="mathjax-tex">\(\Omega _{n_\ell }^{t-1}\)</span> indicates the importance of node. <span class="mathjax-tex">\({\mathcal {G}}\)</span> and <span class="mathjax-tex">\({\mathcal {G}}_0^{t-1}\)</span> are group of all nodes and group of unimportant nodes, respectively.</p>
<h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec6"><span class="c-article-section__title-number">2.2.2 </span>Prior approach</h4><p>Prior-based methods work based on BNNs. Bayesian inference aims to learn a model with weight parameters <span class="mathjax-tex">\(\theta\)</span> in order to infer a conditional probability <span class="mathjax-tex">\(p(D \vert \theta )\)</span>. Intuitively, BNNs builds a model followed a prior <span class="mathjax-tex">\(p(\theta )\)</span> and obtains a posterior distribution <span class="mathjax-tex">\(p(\theta \vert D)\)</span>. However, the fact that it does not exist any tractable form for <span class="mathjax-tex">\(p(\theta \vert D)\)</span> posed a demand for a simpler approximation.</p>
<h3 class="c-article__sub-heading">
<b>Variational inference (VI)</b>
</h3>
<p>Tackles the intractability in BNNs by using a simpler distribution <span class="mathjax-tex">\(q_{\phi }(\theta )\)</span> with <span class="mathjax-tex">\(\phi\)</span> be the parameter of the distribution. In particular, VI finds <span class="mathjax-tex">\(\phi\)</span> that aligns the variational distribution <span class="mathjax-tex">\(q_{\phi }(\theta )\)</span> closest to the intractable true posterior <span class="mathjax-tex">\(p(\theta \vert D)\)</span>. This can be achieved by minimizing the Kullback-Leiber (KL) between the two considering distribution. The optimization is then transformed to maximize the Evidence Lower Bound:</p><div class="c-article-equation" id="Equ8"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} ELBO = E_{q_{\phi }(\theta )} \log p (D \vert \theta ) - KL (q_{\phi }(\theta ) \Vert p(\theta )) \end{aligned}$$</span></div></div><p>For practical computation, we use mean-field approximation that assumes the approximate posterior <span class="mathjax-tex">\(q_{\phi }(\theta )\)</span> to be fully factorized and <span class="mathjax-tex">\(q_{\phi }(\theta ) = \prod _{j} q_{\phi }(\theta _{j})\)</span>.</p>
<h3 class="c-article__sub-heading">
<b>VCL</b>
</h3>
<p>(Nguyen et al., <a aria-label="Reference 2018" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR37" id="ref-link-section-d212779005e3182" title="Nguyen, C. V., Li, Y., Bui, T. D., &amp; Turner, R. E. (2018). Variational continual learning. In International conference on learning representation.">2018</a>) leverages OVI to learn tasks continuously. At task <i>t</i>, the true posterior is approximated by a Gaussian variational distribution based on the mean-field approximation: <span class="mathjax-tex">\(q_t(\varvec{\theta }) = \prod _j^J {\mathcal {N}}(\mu _{t,j}, \sigma _{t,j}^2)\)</span> where <i>J</i> is the cardinality of <span class="mathjax-tex">\(\theta\)</span>, and <span class="mathjax-tex">\(\mu _{t,j}, \sigma _{t,j}\)</span> are the mean and standard deviation of <span class="mathjax-tex">\(\theta _j\)</span> respectively. Inspired by OVI, <span class="mathjax-tex">\(q_t(\varvec{\theta })\)</span> is exploited as the prior in the next task <span class="mathjax-tex">\(t+1\)</span>. The variational objective of VCL at task <i>t</i> is as follows:</p><div class="c-article-equation" id="Equ9"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} \sum _{i=1}^{N_t} E_{q_t(\varvec{\theta })} \left[ \log p({\textbf{y}}_t^{(i)} \vert \varvec{\theta }, {\textbf{x}}_t^{(i)}) \right] - KL(q_t(\varvec{\theta }) \Vert q_{t-1}(\varvec{\theta })) \end{aligned}$$</span></div></div><p>This objective consists of a Likelihood term and a KL-divergence term. While the former helps the learned model adapt to the current task, the latter prevents it from forgetting the previous tasks. The KL-term has a closed-form:</p><div class="c-article-equation" id="Equ10"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} \frac{1}{2} \left[ \left\| \frac{\varvec{\mu }_t - \varvec{\mu }_{t-1}}{\varvec{\sigma }_{t-1}}\right\| _2^2 + {\textbf{1}}^\intercal \left\{ \left( \frac{\varvec{\sigma }_{t}}{\varvec{\sigma }_{t-1}} \right) ^2 - \log \left( \frac{\varvec{\sigma }_{t}}{\varvec{\sigma }_{t-1}} \right) ^2 \right\} \right] \end{aligned}$$</span></div></div><p>Meanwhile, due to intractability, VCL uses reparameterization trick and Monte-Carlo sampling to calculate the likelihood term:</p><div class="c-article-equation" id="Equ11"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} E_{q_t(\varvec{\theta })} \left[ \log p({\textbf{y}}_t^{(i)} \vert \varvec{\theta }, {\textbf{x}}_t^{(i)}) \right] \approx \frac{1}{K} \sum _{k=1}^K \log p({\textbf{y}}_t^{(i)} \vert \varvec{\theta }^{(k)}, {\textbf{x}}_t^{(i)}) \end{aligned}$$</span></div></div><p>where <span class="mathjax-tex">\(\varvec{\theta }^{(k)} = \varvec{\mu }_t + \varvec{\sigma }_t \odot \varvec{\epsilon }_k\)</span> and <span class="mathjax-tex">\(\varvec{\epsilon }_k\)</span> (<span class="mathjax-tex">\(k\in \{1,2,..,K\}\)</span>) is sampled from a unit Gaussian, and <span class="mathjax-tex">\(\odot\)</span> is the element-wise multiplication.</p>
<h3 class="c-article__sub-heading">
<b>Generalized Variational Continual Learning (GVCL)</b>
</h3>
<p>(Loo et al., <a aria-label="Reference 2021" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR27" id="ref-link-section-d212779005e4269" title="Loo, N., Swaroop, S., &amp; Turner, R. E. (2021). Generalized variational continual learning. In International conference on learning representation">2021</a>) uses a tempered version for the likelihood term of VCL. By adding a factor <span class="mathjax-tex">\(\beta\)</span>, with <span class="mathjax-tex">\(0&lt; \beta &lt; 1\)</span>, the KL-divergence regularization term is down-weighted and the ELBO is rewritten as follow:</p><div class="c-article-equation" id="Equ12"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} \beta -ELBO = E_{q_t({\varvec{\theta }}), q_t({{\textbf{s}}})} \left[ \log p({\textbf{y}}_t^{(i)} \vert {\textbf{s}}, \varvec{\theta }, {\textbf{x}}_t^{(i)}) \right] - \beta {KL(q_t(\varvec{\theta }) \Vert q_{t-1}(\varvec{\theta }))} \end{aligned}$$</span></div></div><p>Besides from <span class="mathjax-tex">\(\beta\)</span>, another hyperparameter <span class="mathjax-tex">\(\lambda\)</span> is added to temper the previous task posterior based on changing its variance, which has the form as:</p><div class="c-article-equation" id="Equ13"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} \begin{aligned}&amp;{\tilde{\sigma }}_{t}^{-2} = \lambda ({\sigma }_{t}^{-2} - {\sigma }_{0}^{-2}) + {\sigma }_{0}^{-2} \end{aligned} \end{aligned}$$</span></div></div><p>where <span class="mathjax-tex">\(\sigma _0^{2}\)</span> is the variance of <span class="mathjax-tex">\(p(\theta )\)</span> at the first task. We then have a modified KL-divergence term:</p><div class="c-article-equation" id="Equ14"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} \frac{1}{2} \left[ \left\| \frac{\varvec{\mu }_t - \varvec{\mu }_{t-1}}{\varvec{{\tilde{\sigma }}}_{t-1}}\right\| _2^2 + {\textbf{1}}^\intercal \left\{ \left( \frac{\varvec{\sigma }_{t}}{\varvec{\sigma }_{t-1}} \right) ^2 - \log \left( \frac{\varvec{\sigma }_{t}}{\varvec{\sigma }_{t-1}} \right) ^2 \right\} \right] \end{aligned}$$</span></div></div><p>GVCL also adds task-specific FiLM layers which contain shift and scale parameters (<span class="mathjax-tex">\(\tau\)</span> and <span class="mathjax-tex">\(\rho\)</span>). They act as local parameters that are saved after training each task. For a linear layer having width <i>H</i>, with <span class="mathjax-tex">\(1 \le i \le H\)</span>, the pre-activation is modified as <span class="mathjax-tex">\(h_{i}^{'} = \rho _{i} h_{i} + \tau _{i}\)</span>. For a convolutional layer having <i>W</i> and <i>H</i> as the dimensions of the output feature, and <i>N</i> filters of size <span class="mathjax-tex">\(K \times K\)</span>, the transformation is <span class="mathjax-tex">\(h_{i,j,k}^{'} = \rho _{i} *h_{i,j,k} + \tau _{i}\)</span> where <span class="mathjax-tex">\(1 \le i \le N\)</span>, <span class="mathjax-tex">\(1 \le j \le W\)</span>, and <span class="mathjax-tex">\(1 \le k \le H\)</span>.</p>
</div></div></section><section data-title="Improving regularization/prior-based methods with continual variational dropout in continual learning"><div class="c-article-section" id="Sec7-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec7"><span class="c-article-section__title-number">3 </span>Improving regularization/prior-based methods with continual variational dropout in continual learning</h2><div class="c-article-section__content" id="Sec7-content"><p>When training the model for a new task, <span class="mathjax-tex">\(\varvec{\theta }\)</span> can be moved intensively to the parameter region, leading to good performance for the current task. In the event that the constraints on previous tasks are not good and strict enough, the learned local optimum can be outside the region that works well on all tasks. To alleviate this problem, we introduce auxiliary local variables for each task which help to create a mechanism for adaptation. The goal is to make the global variables <span class="mathjax-tex">\(\varvec{\theta }\)</span> capture the characteristics of all tasks while the local variables adapt to the corresponding task. As mentioned above, existing methods based on OVI or regularization usually keep the likelihood term unchanged but modify the KL-term or regularization term to deal with the stability-plasticity dilemma. In contrast, our work directly changes the likelihood term and can be employed in a wide range of regularization/prior-based methods.</p><p>We will present CVD for both fully-connected and convolutional neural networks. Specifically, we will describe CVD for a particular layer of these networks and then how it can be applied to VCL, GVCL, and EWC. Similar derivation applied to UCL is demonstrated in Appendix <a data-track="click" data-track-action="section anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#Sec21">A</a>.</p><h3 class="c-article__sub-heading" id="Sec8"><span class="c-article-section__title-number">3.1 </span>Continual variational dropout in each neural network layer</h3><p>Consider a hidden layer <i>l</i> of a fully-connected neural network, let <span class="mathjax-tex">\({\textbf{A}}^{(l)}\)</span> be a <span class="mathjax-tex">\(M \times D\)</span> matrix of input features, <span class="mathjax-tex">\(\varvec{\theta }^{(l)}\)</span> be a <span class="mathjax-tex">\(D \times H\)</span> weight matrix where <i>M</i> is the number of data instances in a minibatch, <i>D</i> and <i>H</i> are the number of input and output dimensions respectively. <span class="mathjax-tex">\(\varvec{\theta }^{(l)}\)</span> follows a prior distribution <span class="mathjax-tex">\(p(\varvec{\theta }^{(l)})\)</span>. Denote the pre-activation <span class="mathjax-tex">\({\textbf{B}}^{(l)} = {\textbf{A}}^{(l)} \varvec{\theta }^{(l)}\)</span>. For each task, we add auxiliary variables <span class="mathjax-tex">\(s^{(l)}\)</span> to the layer <i>l</i> and use a multiplicative combination: <span class="mathjax-tex">\({\textbf{B}}^{(l)} = ({\textbf{A}}^{(l)} \odot {\textbf{s}}^{(l)}) \varvec{\theta }^{(l)}\)</span> where <span class="mathjax-tex">\(\odot\)</span> is the element-wise multiplication. <span class="mathjax-tex">\({\textbf{s}}^{(l)}\)</span> is a <span class="mathjax-tex">\(M \times D\)</span> matrix where <span class="mathjax-tex">\(s_{md}^{(l)}\)</span> is a random variable. It is flexible in choosing a prior for <span class="mathjax-tex">\(s_{md}^{(l)}\)</span>. In this section, we still select log-uniform prior as in Kingma et al. (<a aria-label="Reference 2015" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR20" id="ref-link-section-d212779005e5973" title="Kingma, D. P., Salimans, T., &amp; Welling, M. (2015). Variational dropout and the local reparameterization trick. Advances in Neural Information Processing Systems, 28, 2575–2583.">2015</a>) and will present the variants of CVD with different prior and posterior in Sect. <a data-track="click" data-track-action="section anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#Sec11">3.4</a>. In particular, we can reinterpret multiplicative combination for each data instance <i>m</i> as follows:</p><div class="c-article-equation" id="Equ15"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} {\textbf{B}}_m^{(l)} = ({\textbf{A}}_m^{(l)} \odot {\textbf{s}}_m^{(l)}) \varvec{\theta }^{(l)} = {\textbf{A}}_m^{(l)} \tilde{\varvec{\theta }}^{(l)} \end{aligned}$$</span></div></div><p>where <span class="mathjax-tex">\(\tilde{\varvec{\theta }}^{(l)} = [diag({\textbf{s}}_m^{(l)})] \varvec{\theta }^{(l)}\)</span> and is considered as the model’s parameters for each task. It means that the local variables <span class="mathjax-tex">\({\textbf{s}}_m^{(l)}\)</span> can change the global variable <span class="mathjax-tex">\(\varvec{\theta }^{(l)}\)</span> to adapt to each data instance in a new task. As a result, it can help the global variable <span class="mathjax-tex">\(\varvec{\theta }^{(l)}\)</span> move intensively to the good region for all tasks.</p><p>We use variational inference to learn the local variables. Specifically, we put a constraint on the form of the approximate posterior in order to consider the local variable as a Gaussian multiplicative noise. We approximate the true posterior of <span class="mathjax-tex">\({\textbf{s}}^{(l)}_m\)</span> (<span class="mathjax-tex">\(m\in \{1,..,M\}\)</span> and <span class="mathjax-tex">\({\textbf{s}}^{(l)}_m \in {\mathbb {R}}^{1 \text {x} D}\)</span>) by a Gaussian distribution: <span class="mathjax-tex">\(q({\textbf{s}}^{(l)}_m)= \prod _{d=1}^D {\mathcal {N}}(1,\alpha ^{(l)}_{t,d})\)</span> where the mean is set to 1 and the variance <span class="mathjax-tex">\(\alpha ^{(l)}_{t,d}\)</span> is learned to capture the change of <span class="mathjax-tex">\(\tilde{\varvec{\theta }}^{(l)}\)</span> around <span class="mathjax-tex">\(\varvec{\theta }^{(l)}\)</span>. In addition, <span class="mathjax-tex">\(\alpha _t^{(l)}\)</span> is shared across the inputs of data instances. It is worth noting that although we add a huge number of the auxiliary variables <span class="mathjax-tex">\({\textbf{s}}^{(l)}\)</span> (a <span class="mathjax-tex">\(M \times D\)</span> matrix), the number of parameters <span class="mathjax-tex">\(\varvec{\alpha }^{(l)}_t\)</span> (a <i>D</i>-dimensional vector), which have to be learned, is considerably smaller than the cardinality of the global parameters. We can further reduce <span class="mathjax-tex">\(\varvec{\alpha }^{(l)}_t\)</span> to a scalar. However, using a vector of variational parameters <span class="mathjax-tex">\(\varvec{\alpha }^{(l)}_t\)</span> instead of a scalar makes the approximate posterior richer to characterize a new task <i>t</i>.</p><p>As for convolutional layers, consider an <span class="mathjax-tex">\(M \times C \times W \times H\)</span> matrix of input features <span class="mathjax-tex">\({\textbf{A}}^{(l)}\)</span>, a weight matrix <span class="mathjax-tex">\(\varvec{\theta }^{(l)}\)</span> of shape <span class="mathjax-tex">\(C \times C^{'} \times K \times K\)</span>, where <i>M</i> is the number of data instances in a minibatch, <i>C</i> and <span class="mathjax-tex">\(C^{'}\)</span> are the numbers of input and output channels respectively, <i>W</i> and <i>H</i> are the sizes of the input features, and <i>K</i> is the kernel size. For each task, an auxiliary matrix <span class="mathjax-tex">\({\varvec{s}}^{(l)}\)</span> of shape <span class="mathjax-tex">\(M \times C \times W \times H\)</span> is added to the layer <i>l</i> and modulates the global variable <span class="mathjax-tex">\(\varvec{\theta }^{(l)}\)</span> by using a multiplicative combination in the same way as for the fully-connected layer:</p><div class="c-article-equation" id="Equ16"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} {\textbf{B}}_m^{(l)} = ({\textbf{A}}_m^{(l)} \odot {\textbf{s}}_m^{(l)}) *\varvec{\theta }^{(l)} = {\textbf{A}}_m^{(l)} *\tilde{\varvec{\theta }}^{(l)} \end{aligned}$$</span></div></div><p>where <span class="mathjax-tex">\(*\)</span> is the convolution operation.</p><h3 class="c-article__sub-heading" id="Sec9"><span class="c-article-section__title-number">3.2 </span>CVD for regularization/prior-based methods</h3><p>Next, we will present how to apply CVD in continual learning settings. We use VCL and GVCL as case studies for OVI-based approach; EWC for regularization-based approach.</p>
<h3 class="c-article__sub-heading">
<b>CVD for VCL</b>
</h3>
<p>In terms of online variational inference, the approximate posterior of global variables <span class="mathjax-tex">\(\varvec{\theta }\)</span> at a task <span class="mathjax-tex">\(t-1\)</span> is utilized as the prior in the next task <i>t</i>: <span class="mathjax-tex">\(p(\theta ) = q_{t-1}(\theta )\)</span>. We maximize the log likelihood:</p><div class="c-article-equation" id="Equ17"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} \log p({\textbf{Y}}_t\vert {\textbf{X}}_t) = \sum _{i=1}^{N_t} \log p({\textbf{y}}_t^{(i)} \vert {\textbf{x}}_t^{(i)}) \end{aligned}$$</span></div></div><p>We use mean-field variational inference with variational distributions <span class="mathjax-tex">\(q_t({\varvec{\theta }}), q_t({{\textbf{s}}})\)</span> and obtain ELBO:</p><div class="c-article-equation" id="Equ18"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} \begin{aligned} \sum _{i=1}^{N_t} E_{q_t({\varvec{\theta }}), q_t({{\textbf{s}}})} \left[ \log p({\textbf{y}}_t^{(i)} \vert {\textbf{s}}, \varvec{\theta }, {\textbf{x}}_t^{(i)}) \right] - \kappa KL(q_t({\textbf{s}}) \Vert p({\textbf{s}})) - KL(q_t(\varvec{\theta }) \Vert q_{t-1}(\varvec{\theta })) \\ \end{aligned} \end{aligned}$$</span></div></div><p>Note that the <span class="mathjax-tex">\(KL(q_t(\varvec{\theta }) \Vert q_{t-1}(\varvec{\theta }))\)</span> term helps the learned model prevent forgetting the previous tasks and we keep this term as in VCL. We use log-uniform prior for <span class="mathjax-tex">\({\textbf{s}}\)</span> and the approximation of <span class="mathjax-tex">\(KL(q_t({\textbf{s}}) \Vert p({\textbf{s}}))\)</span> as in VD (Kingma et al., <a aria-label="Reference 2015" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR20" id="ref-link-section-d212779005e8255" title="Kingma, D. P., Salimans, T., &amp; Welling, M. (2015). Variational dropout and the local reparameterization trick. Advances in Neural Information Processing Systems, 28, 2575–2583.">2015</a>). Moreover, we tune a coefficient <span class="mathjax-tex">\(\kappa\)</span> to adjust the effect of this term in experiments. For the likelihood term, we propose a variant of local reparameterization trick to calculate this term. In detail, we consider at each fully-connected layer <i>l</i>: <span class="mathjax-tex">\({\textbf{B}}^{(l)} = ({\textbf{A}}^{(l)} \odot {\textbf{s}}^{(l)}) \varvec{\theta }^{(l)}\)</span> where <span class="mathjax-tex">\(b^{(l)}_{mh} = \sum _{d=1}^D (a^{(l)}_{md}. s^{(l)}_{md})\theta ^{(l)}_{dh}\)</span>. Reparameterization trick is sequentially adopted for variables <span class="mathjax-tex">\({\textbf{s}}^{(l)}\)</span> and <span class="mathjax-tex">\(\varvec{\theta }^{(l)}\)</span>. Because <span class="mathjax-tex">\(q({\textbf{s}}^{(l)}_m)= \prod _{d=1}^D {\mathcal {N}}(1,\alpha ^{(l)}_{t,d})\)</span>, we can calculate the auxiliary local variables <span class="mathjax-tex">\({\textbf{s}}^{(l)}\)</span>:</p><div class="c-article-equation" id="Equ19"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} s^{(l)}_{md} = 1 + \sqrt{\alpha ^{(l)}_{t,d}} \gamma ^{(l)}_{md} \end{aligned}$$</span></div></div><p>where <span class="mathjax-tex">\(\gamma ^{(l)}_{md}\)</span> is sampled from <span class="mathjax-tex">\({\mathcal {N}} (0,1)\)</span>. It is plugged in the pre-activation:</p><div class="c-article-equation" id="Equ20"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} b^{(l)}_{mh} = \sum _{d=1}^D a^{(l)}_{md} (1 + \sqrt{\alpha ^{(l)}_{t,d}} \gamma ^{(l)}_{t,md})\theta ^{(l)}_{dh} \end{aligned}$$</span></div></div><p>Since, <span class="mathjax-tex">\(\theta _{dh}^{(l)} \sim {\mathcal {N}}(\mu _{t,dh}^{(l)}, (\sigma _{t,dh}^{(l)})^{2})\)</span>, we can rewrite: <span class="mathjax-tex">\(b_{mh}^{(l)} \sim {\mathcal {N}}(\omega _{mh}^{(l)}, \delta _{mh}^{(l)})\)</span> where</p><div class="c-article-equation" id="Equ21"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} \omega _{mh}^{(l)}&amp;= \sum _{d=1}^{D} a_{md}^{(l)}(1 + \sqrt{\alpha ^{(l)}_{t,d}} \gamma ^{(l)}_{md}) \mu _{t,dh}^{(l)} \\ \delta _{mh}^{(l)}&amp;= \sum _{d=1}^{D} \left( a_{md}^{(l)}(1 + \sqrt{\alpha ^{(l)}_{t,d}} \gamma ^{(l)}_{md}) \right) ^{2} (\sigma _{t,dh}^{(l)})^{2} \end{aligned}$$</span></div></div><p>By using reparameterization trick, we have <span class="mathjax-tex">\(b_{mh}^{(l)} = \omega _{mh}^{(l)} + \sqrt{\delta _{mh}^{(l)}} \varepsilon _{mh}^{(l)}\)</span> where <span class="mathjax-tex">\(\varepsilon _{mh}^{(l)}\)</span> is sampled from <span class="mathjax-tex">\({\mathcal {N}}(0,1)\)</span>.</p>
<p>For convolutional layers, we express the matrices in term of vectors as in Molchanov et al. (<a aria-label="Reference 2017" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR31" id="ref-link-section-d212779005e9898" title="Molchanov, D., Ashukha, A., &amp; Vetrov, D. (2017). Variational dropout sparsifies deep neural networks. In International conference on machine learning (pp. 2498–2507).">2017</a>). Considering each layer <i>l</i>, we have <span class="mathjax-tex">\(b_{mc'}^{(l)}\)</span> as the corresponding <span class="mathjax-tex">\(W' \times H'\)</span> output matrix of a <span class="mathjax-tex">\(C \times H \times W\)</span> input <span class="mathjax-tex">\(a_{m}^{(l)}\)</span> and a specific <span class="mathjax-tex">\(C \times K \times K\)</span> filter <span class="mathjax-tex">\(\theta _{t,c'}\)</span>. We can rewrite: <span class="mathjax-tex">\(vec(b_{mc'}^{(l)}) \sim {\mathcal {N}}(\omega _{mc'}^{(l)}, \delta _{mc'}^{(l)})\)</span> with:</p><div class="c-article-equation" id="Equ22"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} \omega _{mc'}^{(l)}&amp;= vec \bigg ( a_{m}^{(l)}(1 + \sqrt{\alpha ^{(l)}_{t}} \gamma ^{(l)}_{m}) *\mu _{t,c'}^{(l)} \bigg ) \\ \delta _{mc'}^{(l)}&amp;= diag \Bigg (vec\bigg ( \left( a_{m}^{(l)}(1 + \sqrt{\alpha ^{(l)}_{t}} \gamma ^{(l)}_{m}) \right) ^{2} *(\sigma _{t,c'}^{(l)})^{2}\bigg )\Bigg ) \end{aligned}$$</span></div></div><p>where <span class="mathjax-tex">\(vec(\cdot )\)</span> denotes the vectorization for a matrix.</p><p>Let <span class="mathjax-tex">\(L^{(m)} = E_{q_t({\varvec{\theta }}), q_t({{\textbf{s}}})} \log p({\textbf{y}}_t^{(m)} \vert {\textbf{s}}, \varvec{\theta }, {\textbf{x}}_t^{(m)})\)</span> be the likelihood of data instance <i>m</i> and is expressed as:</p><div class="c-article-equation" id="Equ23"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} L^{(m)} = E_{\varvec{\gamma }_m,\varvec{\varepsilon }_m} \log p({\textbf{y}}_t^{(m)} \vert \varvec{\alpha }_t, \varvec{\mu }_t, \varvec{\sigma }_t, \varvec{\gamma }_m, \varvec{\varepsilon }_m, {\textbf{x}}_t^{(m)}) \end{aligned}$$</span></div></div><p>After sampling <span class="mathjax-tex">\(\varvec{\gamma }\)</span> and <span class="mathjax-tex">\(\varvec{\varepsilon }\)</span>, we can calculate the likelihood term and optimize the objective function with respect to variational parameters <span class="mathjax-tex">\(\varvec{\alpha }_t\)</span>, <span class="mathjax-tex">\(\varvec{\mu }_t\)</span> and <span class="mathjax-tex">\(\varvec{\sigma }_t\)</span> for task <i>t</i>.</p>
<h3 class="c-article__sub-heading">
<b>CVD for GVCL</b>
</h3>
<p>We also add auxiliary variables and maximize the log likelihood: <span class="mathjax-tex">\(\log p({\textbf{Y}}_t\vert {\textbf{X}}_t) = \sum _{i=1}^{N_t} \log p({\textbf{y}}_t^{(i)} \vert {\textbf{x}}_t^{(i)})\)</span>. We obtain the ELBO using mean-field variational inference with variational distributions <span class="mathjax-tex">\(q_t({\varvec{\theta }}), q_t({{\textbf{s}}})\)</span> in the same way as in VCL but with tempered coefficients:</p><div class="c-article-equation" id="Equ24"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} \begin{aligned} \sum _{i=1}^{N_t} E_{q_t({\varvec{\theta }}), q_t({{\textbf{s}}})} \left[ \log p({\textbf{y}}_t^{(i)} \vert {\textbf{s}}, \varvec{\theta }, {\textbf{x}}_t^{(i)}) \right] - \kappa KL(q_t({\textbf{s}}) \Vert p({\textbf{s}})) - \beta KL(q_t(\varvec{\theta }) \Vert q_{t-1}(\varvec{\theta })) \\ \end{aligned} \end{aligned}$$</span></div></div><p>Note that GVCL also introduces FiLM layers as auxiliary variables. The two local variables can be combined as follow:</p><div class="c-article-equation" id="Equ25"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} \begin{aligned} b^{(l)}_{mh} = \rho ^{(l)}_{h}(\sum _{d=1}^D a^{(l)}_{md} (1 + \sqrt{\alpha ^{(l)}_{t,d}} \gamma ^{(l)}_{t,md})\theta ^{(l)}_{dh}) + \tau ^{(l)}_{h} \end{aligned} \end{aligned}$$</span></div></div><p>where <span class="mathjax-tex">\(\rho ^{(l)}\)</span> and <span class="mathjax-tex">\(\tau ^{(l)}\)</span> are the scale and shift parameters of layer <i>l</i>.</p>
<h3 class="c-article__sub-heading">
<b>CVD for EWC</b>
</h3>
<p>Due to adding auxiliary variables, we use type-II maximum a posterior (MAP-II) (Murphy, <a aria-label="Reference 2012" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR32" id="ref-link-section-d212779005e12011" title="Murphy, K. P. (2012). Machine learning: A probabilistic perspective. Cambridge: MIT Press.">2012</a>) to learn the model instead of utilizing MAP as in the original EWC. The objective function is described as follows:</p><div class="c-article-equation" id="Equ26"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} {\mathcal {L}}(\varvec{\theta }) = \sum _{i=1}^{N_t} \log p({\textbf{y}}_t^{(i)} \vert \varvec{\theta }, {\textbf{x}}_t^{(i)}) + \log p( \varvec{\theta }) \end{aligned}$$</span></div></div><p>After adding the auxiliary local variable <span class="mathjax-tex">\({\textbf{s}}\)</span>, the log likelihood <span class="mathjax-tex">\(\log p({\textbf{y}}_t^{(i)} \vert \varvec{\theta }, {\textbf{x}}_t^{(i)})\)</span> over <span class="mathjax-tex">\({\textbf{s}}\)</span> is intractable. To solve this problem, we consider the evidence lower bound:</p><div class="c-article-equation" id="Equ27"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} \begin{aligned}&amp;\sum _{i=1}^{N_t} \log p({\textbf{y}}_t^{(i)} \vert \varvec{\theta }, {\textbf{x}}_t^{(i)}) = \log \int _{{\textbf{s}}} p({\textbf{y}}_t^{(i)} \vert {\textbf{s}}, \varvec{\theta }, {\textbf{x}}_t^{(i)}) p({\textbf{s}}) d{\textbf{s}} \\&amp;\ge \sum _{i=1}^{N_t} E_{q_t({{\textbf{s}}})} \left[ \log p({\textbf{y}}_t^{(i)} \vert {\textbf{s}}, \varvec{\theta }, {\textbf{x}}_t^{(i)}) \right] - KL(q_t({\textbf{s}}) \Vert p({\textbf{s}})) \\ \end{aligned} \end{aligned}$$</span></div></div><p>Therefore, we have the objective function:</p><div class="c-article-equation" id="Equ28"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} \begin{aligned}&amp;\sum _{i=1}^{N_t} E_{q_t({{\textbf{s}}})} \left[ \log p({\textbf{y}}_t^{(i)} \vert {\textbf{s}}, \varvec{\theta }, {\textbf{x}}_t^{(i)}) \right] - KL(q_t({\textbf{s}}) \Vert p({\textbf{s}})) \\&amp;- \frac{\lambda }{2} \sum _{j=1}^{J} \left( \sum _{k=1}^{t-1} F_{k,j} \right) (\theta _{j} - \theta _{t-1,j}^{*})^2 \\ \end{aligned} \end{aligned}$$</span></div></div><p>Due to learning a point estimate for the global variables <span class="mathjax-tex">\(\varvec{\theta }\)</span>, we only adopt the reparameterization trick for the local variables <span class="mathjax-tex">\({\textbf{s}}\)</span>. Furthermore, <span class="mathjax-tex">\(KL(q_t({\textbf{s}}) \Vert p({\textbf{s}}))\)</span> is also calculated as in the VCL case study. We optimize the objective function with respect to <span class="mathjax-tex">\(\varvec{\theta }\)</span> and <span class="mathjax-tex">\(\varvec{\alpha }_t\)</span>. After optimizing, the Fisher information is computed for the global variables <span class="mathjax-tex">\(\varvec{\theta }\)</span> to measure the weight’s importance as in the original EWC.</p>
<h3 class="c-article__sub-heading" id="Sec10"><span class="c-article-section__title-number">3.3 </span>Theoretical analyses</h3><p>In this subsection, we theoretically analyze CVD’s properties such as uncorrelated likelihoods, correlated pre-activation, and data-dependent regularization based on adding auxiliary local variables for a particular task <i>t</i>. While the regularization property assures that the global variables are located in a good region for all tasks, the other properties help to learn each task well.</p>
<h3 class="c-article__sub-heading">
<b>Uncorrelated likelihoods and correlated pre-activation</b>
</h3>
<p>In terms of learning BNNs, most existing methods (Graves, <a aria-label="Reference 2011" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR15" id="ref-link-section-d212779005e13250" title="Graves, A. (2011). Practical variational inference for neural networks. In Advances in Neural Information Processing Systems (pp. 2348–2356). Citeseer.">2011</a>; Blundell et al., <a aria-label="Reference 2015" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR5" id="ref-link-section-d212779005e13253" title="Blundell, C., Cornebise, J., Kavukcuoglu, K., &amp; Wierstra, D. (2015). Weight uncertainty in neural network. In International conference on machine learning (pp. 1613–1622). PMLR.">2015</a>; Oh et al., <a aria-label="Reference 2020" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR39" id="ref-link-section-d212779005e13256" title="Oh, C., Adamczewski, K., &amp; Park, M. (2020). Radial and directional posteriors for Bayesian deep learning. In The thirty-fourth conference on artificial intelligence, AAAI (pp. 5298–5305)">2020</a>; Swiatkowski et al., <a aria-label="Reference 2020" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR46" id="ref-link-section-d212779005e13259" title="Swiatkowski, J., Roth, K., Veeling, B., Tran, L., Dillon, J., Snoek, J., Mandt, S., Salimans, T., Jenatton, R., &amp; Nowozin, S. (2020). The k-tied normal distribution: A compact parameterization of Gaussian mean field posteriors in Bayesian neural networks. In International conference on machine learning (pp. 9289–9299). PMLR.">2020</a>), that use the usual Monte Carlo gradient estimator (Paisley et al., <a aria-label="Reference 2012" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR40" id="ref-link-section-d212779005e13262" title="Paisley, J. W., Blei, D. M., &amp; Jordan, M. I. (2012). Variational bayesian inference with stochastic search. In Proceedings of the 29th international conference on machine learning, ICML">2012</a>) or reparameterization trick (Kingma et al., <a aria-label="Reference 2015" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR20" id="ref-link-section-d212779005e13266" title="Kingma, D. P., Salimans, T., &amp; Welling, M. (2015). Variational dropout and the local reparameterization trick. Advances in Neural Information Processing Systems, 28, 2575–2583.">2015</a>), suffer from high variance. The local reparameterization trick (Kingma et al., <a aria-label="Reference 2015" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR20" id="ref-link-section-d212779005e13269" title="Kingma, D. P., Salimans, T., &amp; Welling, M. (2015). Variational dropout and the local reparameterization trick. Advances in Neural Information Processing Systems, 28, 2575–2583.">2015</a>) is proposed to address this problem. However, even though the local reparameterization trick has the property of uncorrelated likelihoods between different data instances to reduce high variance, it does not have correlated pre-activation to improve representation learning for each task. Therefore, we present these two properties to show the advantages of CVD in comparison with the existing methods in learning BNNs.</p>
<h3 class="c-article__sub-heading">
<b>Uncorrelated likelihoods between data instances</b>
</h3>
<p>Regarding variational inference, the objective function requires computations for two terms: the log-likelihood and the KL divergence. Assuming that the KL divergence can be integrated analytically under an approximated close form, the optimization now comes to estimate the likelihood term. An effective Stochastic gradient variational Bayes (SGVB) (Kingma &amp; Welling, <a aria-label="Reference 2014" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR21" id="ref-link-section-d212779005e13284" title="Kingma, D. P., &amp; Welling, M. (2014). Auto-encoding variational bayes. In: Bengio, Y., LeCun, Y. (eds.) 2nd international conference on learning representations, ICLR.">2014</a>) is introduced as an efficient optimization method for this objective function. Using the reparameterization trick and Monte Carlo technique (Paisley et al., <a aria-label="Reference 2012" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR40" id="ref-link-section-d212779005e13287" title="Paisley, J. W., Blei, D. M., &amp; Jordan, M. I. (2012). Variational bayesian inference with stochastic search. In Proceedings of the 29th international conference on machine learning, ICML">2012</a>), we applied a variant of SGVB to estimate the likelihood term in the learning process with M data points over dataset <span class="mathjax-tex">\(N_t\)</span> as follows:</p><div class="c-article-equation" id="Equ29"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} L^{(m)} = E{q_{t}(\theta ), q_{t}(s)} \log p(y_{t}^{(m)}\vert \theta ) \simeq L_{SGVB}^{(m)} = \frac{\vert N_t \vert }{M}\sum _{i=1}^{M} \log p(y_{t}^{i}\vert x_{t}^{i}, \theta , s) \end{aligned}$$</span></div></div><p>Particularly, based on the reparameterization trick, the global variables <span class="mathjax-tex">\(\theta\)</span> and the local variable <i>s</i> are considered as functions on random noises <span class="mathjax-tex">\(\gamma\)</span> and <span class="mathjax-tex">\(\varepsilon\)</span> that are sampled from noise distributions <span class="mathjax-tex">\(p(\gamma )\)</span> and <span class="mathjax-tex">\(p(\varepsilon )\)</span> respectively. As we use the local reparameterization trick and the preactivations <span class="mathjax-tex">\(b_{mh}\)</span> is sampled instead of <span class="mathjax-tex">\(\theta _{dh}\)</span>, the likelihood of the <span class="mathjax-tex">\(m^{th}\)</span> data instance can then be expressed as:</p><div class="c-article-equation" id="Equ30"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} L^{(m)} = E_{\varvec{\gamma }_m, \varvec{\varepsilon }_m} \log p({\textbf{y}}_t^{(m)} \vert \varvec{\alpha }_t, \varvec{\mu }_t, \varvec{\sigma }_t, \varvec{\gamma }_m, \varvec{\varepsilon }_m, {\textbf{x}}_t^{(m)}) \end{aligned}$$</span></div></div><p>For two data instances <i>m</i> and <span class="mathjax-tex">\({m}'\)</span>, it is straightforward to see that <span class="mathjax-tex">\(\{\varvec{\gamma }_{m}, \varvec{\varepsilon }_{m}\}\)</span> and <span class="mathjax-tex">\(\{\varvec{\gamma }_{{m}'}, \varvec{\varepsilon }_{{m}'}\}\)</span> are independent, therefore, <span class="mathjax-tex">\(Cov [L^{(m)},L^{({m}')}]=0\)</span>. This property helps to reduce the high variance of stochastic gradient variational Bayes (Kingma et al., <a aria-label="Reference 2015" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR20" id="ref-link-section-d212779005e14172" title="Kingma, D. P., Salimans, T., &amp; Welling, M. (2015). Variational dropout and the local reparameterization trick. Advances in Neural Information Processing Systems, 28, 2575–2583.">2015</a>). Indeed, the likelihood of all data instances is approximated by minibatch-based Monte Carlo estimator and is written: <span class="mathjax-tex">\(L = \frac{N_t}{M} \sum _{m}^M L^{(m)}\)</span> where <i>M</i> is the size of minibatch and <span class="mathjax-tex">\(N_t\)</span> is the number of all data instances. The variance of <i>L</i> is expressed as follows:</p><div class="c-article-equation" id="Equ31"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} \begin{aligned} \frac{N_t^2}{M^2} \bigg ( \sum _{m=1}^{M} Var[L^{(m)}] + 2 \sum _{m=1}^{M}\sum _{{m}'=m+1}^{M} Cov[L^{(m)},L^{({m}')}] \bigg ) \end{aligned} \end{aligned}$$</span></div></div><p>We emphasize that because data instances are drawn from empirical distribution, the variance <span class="mathjax-tex">\(Var [L^{(m)}]\)</span> and covariance <span class="mathjax-tex">\(Cov [L^{(m)},L^{({m}')}]\)</span> are computed with respect to the unit Gaussian distributions <span class="mathjax-tex">\(\varvec{\gamma }\)</span> and <span class="mathjax-tex">\(\varvec{\varepsilon }\)</span>. Since <span class="mathjax-tex">\(Cov[L^{(m)},L^{({m}')}] =0\)</span>, CVD can achieve a lower variance estimation.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-container-section="figure" data-test="figure" data-title="Fig. 1" id="figure-1"><figure><figcaption><b class="c-article-section__figure-caption" data-test="figure-caption-text" id="Fig1">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-action="view figure" data-track-label="image" href="/article/10.1007/s10994-023-06487-7/figures/1" rel="nofollow"><picture><source srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10994-023-06487-7/MediaObjects/10994_2023_6487_Fig1_HTML.png?as=webp" type="image/webp"/><img alt="figure 1" aria-describedby="Fig1" height="354" loading="lazy" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10994-023-06487-7/MediaObjects/10994_2023_6487_Fig1_HTML.png" width="685"/></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>T-SNE of last layer’s pre-activation. Features from the same classes are closer when adding variational dropout</p></div></div><div class="u-text-right u-hide-print"><a aria-label="Full size image figure 1" class="c-article__pill-button" data-test="article-link" data-track="click" data-track-action="view figure" data-track-dest="link:Figure1 Full size image" data-track-label="button" href="/article/10.1007/s10994-023-06487-7/figures/1" rel="nofollow"><span>Full size image</span><svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16"><use xlink:href="#icon-eds-i-chevron-right-small" xmlns:xlink="http://www.w3.org/1999/xlink"></use></svg></a></div></figure></div>
<h3 class="c-article__sub-heading">
<b>Correlated pre-activation</b>
</h3>
<p>Figure <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#Fig1">1</a> visualizes the last layer’s pre-activations. As can be seen in the figure, by adding the variational dropout, the features of the same class are drawn closer, therefore forming clearer clusters. We indicate that the appearance of auxiliary local variables has constructed a hierarchical distribution on the pre-activations, resulting in a better representation capability.</p>
<p>We emphasize that the original local reparameterization trick (Kingma et al., <a aria-label="Reference 2015" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR20" id="ref-link-section-d212779005e14770" title="Kingma, D. P., Salimans, T., &amp; Welling, M. (2015). Variational dropout and the local reparameterization trick. Advances in Neural Information Processing Systems, 28, 2575–2583.">2015</a>) fully factorizes <span class="mathjax-tex">\(b_m\)</span>. Therefore, it only obtains the property of uncorrelated likelihoods but does not capture correlated representation. Clearly, employing the local reparameterization trick to the original pre-activation <span class="mathjax-tex">\({\bar{b}}^{(l)}_{mh}\)</span> without any auxiliary local variables makes it follow a distribution <span class="mathjax-tex">\({\mathcal {N}}({\bar{\omega }}_{mh}^{(l)}, {\bar{\delta }}_{mh}^{(l)})\)</span> where:</p><div class="c-article-equation" id="Equ32"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} {\bar{\omega }}_{mh}^{(l)}&amp;= \sum _{d=1}^{D} a_{md}^{(l)} \mu _{t,dh}^{(l)} \\ {\bar{\delta }}_{mh}^{(l)}&amp;= \sum _{d=1}^{D} \left( a_{md}^{(l)}\right) ^{2} (\sigma _{t,dh}^{(l)})^{2} \end{aligned}$$</span></div></div><p>with <span class="mathjax-tex">\({\mathcal {N}}(\mu _{t,dh}^{(l)}, (\sigma _{t,dh}^{(l)})^{2})\)</span> is the distribution of the global weight <span class="mathjax-tex">\(\theta _{dh}^{(l)}\)</span>. Since the true posterior is approximated based on mean-field approximation, the pre-activation is fully factorized, leading to no correlation between the dimensions.</p><p>We show that, for each data instance <i>m</i>, adding the local variables for each layer <i>l</i> results in correlated pre-activation. Recall that considering an element <i>h</i>, we have <span class="mathjax-tex">\(b_{mh} \sim {\mathcal {N}}(\omega _{mh}, \delta _{mh})\)</span> where <span class="mathjax-tex">\(\omega _{mh}, \delta _{mh}\)</span> are functions over random variable <span class="mathjax-tex">\(\varvec{\gamma }_m\)</span> which is derived from reparameterization trick on the local variables. As a consequence, CVD creates a hierarchical distribution on the pre-activation <span class="mathjax-tex">\({\textbf{b}}_m\)</span> and can achieve better representation on each layer.</p>
<h3 class="c-article__sub-heading">
<b>An effective regularization for continual learning</b>
</h3>
<p>Recently, Wei et al. (<a aria-label="Reference 2020" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR50" id="ref-link-section-d212779005e15537" title="Wei, C., Kakade, S., &amp; Ma, T. (2020). The implicit and explicit regularization effects of dropout. In International conference on machine learning (pp. 10181–10192). PMLR.">2020</a>) proved that dropout with Bernoulli distribution leads to the approximate regularization term:</p><div class="c-article-equation" id="Equ33"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} Reg_{dropout} = \frac{q}{1-q} \left\langle H_{h^{(l)}({\textbf{x}})}({\bar{L}}); \, diag \left( (h^{(l)}({\textbf{x}}))^{2} \right) \right\rangle \end{aligned}$$</span></div></div><p>where <i>q</i> is the dropout probability. <span class="mathjax-tex">\({\bar{L}}\)</span> is the original loss function without adding noises, <span class="mathjax-tex">\((h^{(l)}({\textbf{x}}))\)</span> is the <span class="mathjax-tex">\(l^{th}\)</span> hidden layer, <span class="mathjax-tex">\(H_{h^{(l)}({\textbf{x}})}({\bar{L}})\)</span> is the Hessian matrix of <span class="mathjax-tex">\({\bar{L}}\)</span> w.r.t <span class="mathjax-tex">\((h^{(l)}({\textbf{x}}))\)</span> and <span class="mathjax-tex">\(\left\langle ; \, \right\rangle\)</span> is the inner product of vectorizations of matrices.</p>
<p>In this work, we extend the analyses to Gaussian multiplicative noise instead of only Bernoulli drop connect as in Wei et al. (<a aria-label="Reference 2020" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR50" id="ref-link-section-d212779005e16017" title="Wei, C., Kakade, S., &amp; Ma, T. (2020). The implicit and explicit regularization effects of dropout. In International conference on machine learning (pp. 10181–10192). PMLR.">2020</a>). More specifically, we demonstrate that CVD achieves similar regularization as it induces a data-dependent regularization.</p>
<h3 class="c-article__sub-heading" id="FPar1">
<b>Proposition 1</b>
</h3>
<p><i>Let </i> <i>F</i> <i>be a function which is derived from the neural network</i>, <span class="mathjax-tex">\(h^{(l)}\)</span> <i>be the</i> <i>l</i>-<i>th hidden layer of the network</i>, <span class="mathjax-tex">\(F^{(l)}\)</span> <i>be the function based on the part of architecture from layer</i> <i>l</i> <i>to output layer</i>: <span class="mathjax-tex">\(F({\textbf{x}}) = F^{(l)}(h^{(l)}({\textbf{x}}))\)</span> <i>and</i> <span class="mathjax-tex">\({\bar{L}}(F({\textbf{x}})\)</span> <i>be a loss function. Dropout with multiplicative Gaussian noises leads to an approximate regularization term</i> <span class="mathjax-tex">\(Reg_{mult}\)</span> <i>as follows</i>:</p><div class="c-article-equation" id="Equ1"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned}&amp;Reg_{mult} = \frac{1}{2} \left\langle H_{h^{(l)}({\textbf{x}})}({\bar{L}}); \, diag \left( \varvec{\alpha }^{(l)} \odot (h^{(l)}({\textbf{x}}))^2 \right) \right\rangle \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (1)
                </div></div><p><i>where</i> <span class="mathjax-tex">\(H_{{\textbf{x}}}(F)\)</span> <i>is the Hessian of</i> <i>F</i> <i>w.r.t.</i> <span class="mathjax-tex">\({\textbf{x}}\)</span> <i>and</i> <span class="mathjax-tex">\(\alpha\)</span> <i>is the variance of Gaussian dropout</i>, <span class="mathjax-tex">\(\left\langle ; \, \right\rangle\)</span> <i>is the inner product of vectorizations of matrices</i>.</p>
<h3 class="c-article__sub-heading" id="FPar2">
<i>Proof</i>
</h3>
<p>We consider the loss function at each task <i>t</i>, which is negative of the likelihood, when adding auxiliary variables <span class="mathjax-tex">\({\textbf{s}}\)</span> to a layer <i>l</i> of the network. For simplicity, we remove the task index <i>t</i> in the loss function. The loss function is presented as follows:</p><div class="c-article-equation" id="Equ34"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} {\bar{L}}(F({\textbf{x}}, {\textbf{s}})) = {\bar{L}}( F^{(l)}( h^{(l)}({\textbf{x}}) \odot {\textbf{s}}^{(l)} ) ) \end{aligned}$$</span></div></div><p>where <span class="mathjax-tex">\({\textbf{s}}^{(l)} \sim {\mathcal {N}}({\textbf{1}}, diag(\varvec{\alpha }^{(l)}))\)</span> is the auxiliary local variables at layer <i>l</i>.</p>
<p>Because <span class="mathjax-tex">\({\textbf{s}}^{(l)} \sim {\mathcal {N}}({\textbf{1}}, diag(\varvec{\alpha }^{(l)}))\)</span>, we use reparameterization trick for <span class="mathjax-tex">\({\textbf{s}}^{(l)}\)</span>: <span class="mathjax-tex">\({\textbf{s}}^{(l)} ={\textbf{1}} + \sqrt{\varvec{\alpha }}^{(l)} \odot \varvec{\gamma }\)</span> where <span class="mathjax-tex">\(\varvec{\gamma } \sim {\mathcal {N}}({\textbf{0}}, I)\)</span> and then plug it to the loss function (2):</p><div class="c-article-equation" id="Equ35"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} {\bar{L}}(F({\textbf{x}}, {\textbf{s}}))&amp;= {\bar{L}}( F^{(l)}( h^{(l)}({\textbf{x}}) \odot ( {\textbf{1}} + \sqrt{\varvec{\alpha }^{(l)}} \odot \varvec{\gamma }) ) ) \\&amp;= {\bar{L}}( F^{(l)}( h^{(l)}({\textbf{x}}) + (h^{(l)}({\textbf{x}}) \odot \sqrt{\varvec{\alpha }^{(l)}})\odot \varvec{\gamma }) ) \end{aligned}$$</span></div></div><p>For simplicity, let <span class="mathjax-tex">\(\varvec{\delta } = ( h^{(l)}({\textbf{x}}) \odot \sqrt{\varvec{\alpha }^{(l)}} ) \odot \varvec{\gamma }\)</span>. Because <span class="mathjax-tex">\(\varvec{\gamma } \sim {\mathcal {N}}({\textbf{0}}, I)\)</span>, we achieve <span class="mathjax-tex">\(\varvec{\delta } \sim {\mathcal {N}} \Big ({\textbf{0}}, diag(\varvec{\alpha }^{(l)} \odot (h^{(l)}({\textbf{x}}))^2) \Big )\)</span> and rewrite the loss function:</p><div class="c-article-equation" id="Equ36"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} {\bar{L}}(F({\textbf{x}}, {\textbf{s}})) = {\bar{L}}( F^{(l)}( h^{(l)}({\textbf{x}}) + \varvec{\delta } ) ) \end{aligned}$$</span></div></div><p>We utilize Taylor expansion to approximate the loss function around <span class="mathjax-tex">\(\varvec{\delta }\)</span>:</p><div class="c-article-equation" id="Equ37"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} {\bar{L}}(F({\textbf{x}}, {\textbf{s}})) - {\bar{L}}(F({\textbf{x}})) \approx \varvec{J}_{h^{(l)}({\textbf{x}})}({\bar{L}}) \varvec{\delta } + \frac{1}{2} \varvec{\delta }^\top \varvec{H}_{h^{(l)}({\textbf{x}})}({\bar{L}}) \varvec{\delta } \end{aligned}$$</span></div></div><p>where <span class="mathjax-tex">\(\varvec{J}_{{\textbf{x}}}(F)\)</span> is the Jacobian of <i>F</i> w.r.t <span class="mathjax-tex">\({\textbf{x}}\)</span> and <span class="mathjax-tex">\(\varvec{H}_{{\textbf{x}}}(F)\)</span> is the Hessian of <i>F</i> w.r.t <span class="mathjax-tex">\({\textbf{x}}\)</span>. Note that the approximation error can approach 0 when <span class="mathjax-tex">\(\varvec{\delta }\)</span> is small enough. This happens when <span class="mathjax-tex">\(\varvec{\alpha }^{(l)}\)</span> is small enough. Because <span class="mathjax-tex">\(E_{\varvec{\delta }} [\varvec{J}_{h^{(l)}({\textbf{x}})}({\bar{L}}) \varvec{\delta } ] =0\)</span>, we can achieve an approximate regularization term <span class="mathjax-tex">\(Reg_{mult}\)</span>:</p><div class="c-article-equation" id="Equ38"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} Reg_{mult}&amp;= E_{{\textbf{s}}} [{\bar{L}}(F({\textbf{x}}, {\textbf{s}}))] - {\bar{L}}(F({\textbf{x}})) \\&amp;\approx \frac{1}{2} E_{\varvec{\delta }} \left[ \varvec{\delta }^\top \varvec{H}_{h^{(l)}({\textbf{x}})}({\bar{L}}) \varvec{\delta } \right] = \frac{1}{2} \left\langle \varvec{H}_{h^{(l)}({\textbf{x}})}({\bar{L}}); \, E[\varvec{\delta } \varvec{\delta }^\top ] \right\rangle \\&amp;= \frac{1}{2} \left\langle \varvec{H}_{h^{(l)}({\textbf{x}})}({\bar{L}}); \, diag \left( \varvec{\alpha }^{(l)} \odot (h^{(l)}({\textbf{x}}))^2 \right) \right\rangle \end{aligned}$$</span></div></div><p>where <span class="mathjax-tex">\(\left\langle ; \, \right\rangle\)</span> is the inner product of the vectorizations of two matrices. <span class="mathjax-tex">\(\square\)</span></p>
<p>Meanwhile, based on this regularization, Mirzadeh et al. (<a aria-label="Reference 2020" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR29" id="ref-link-section-d212779005e19171" title="Mirzadeh, S., Farajtabar, M., Pascanu, R., &amp; Ghasemzadeh, H. (2020). Understanding the role of training regimes in continual learning. In Advances in neural information processing systems">2020</a>) analyzed that minimizing the second derivative of the loss w.r.t the activation can obtain the flatness of the minima where the model can perform well on all tasks. Similarly, CVD also has this property as discussed in Mirzadeh et al. (<a aria-label="Reference 2020" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR29" id="ref-link-section-d212779005e19174" title="Mirzadeh, S., Farajtabar, M., Pascanu, R., &amp; Ghasemzadeh, H. (2020). Understanding the role of training regimes in continual learning. In Advances in neural information processing systems">2020</a>); Shi et al. (<a aria-label="Reference 2021" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR43" id="ref-link-section-d212779005e19177" title="Shi, G., Chen, J., Zhang, W., Zhan, L.-M., &amp; Wu, X.-M. (2021). Overcoming catastrophic forgetting in incremental few-shot learning by finding flat minima. Advances in Neural Information Processing Systems, 34, 6747–6761.">2021</a>); Cha et al. (<a aria-label="Reference 2021" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR7" id="ref-link-section-d212779005e19180" title="Cha, S., Hsu, H., Hwang, T., Calmon, F. P., &amp; Moon, T. (2021). CPR: Classifier-projection regularization for continual learning. In 9th International Conference on Learning Representations, ICLR.">2021</a>), therefore it can guarantee to preserve global variables in good region for all tasks.</p><h3 class="c-article__sub-heading" id="Sec11"><span class="c-article-section__title-number">3.4 </span>Variants of CVD</h3><p>In this section, we justify the flexibility of CVD as it can adapt versatilely in diverse circumstances.</p><p><b>Diversity in priors of CVD:</b> CVD can work effectively with a variety of priors. In this section, we show the case when the prior of CVD follows a Gaussian instead of log uniform, and extend the approximated true posterior to <span class="mathjax-tex">\(q({\textbf{s}}^{(l)}_m)= \prod _{d=1}^D {\mathcal {N}}(\zeta ^{(l)}_{t,d},\alpha ^{(l)}_{t,d})\)</span> where now both the mean <span class="mathjax-tex">\(\zeta ^{(l)}_{t,d}\)</span> and the variance <span class="mathjax-tex">\(\alpha ^{(l)}_{t,d}\)</span> are learnable parameters to adjust the global parameters <span class="mathjax-tex">\(\theta ^{(l)}\)</span> fitting to task-specific data. The reparameterization trick is adopted for <span class="mathjax-tex">\(s^{(l)}\)</span> as follow:</p><div class="c-article-equation" id="Equ39"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} s^{(l)}_{md} = \zeta ^{(l)}_{t,d} + \sqrt{\alpha ^{(l)}_{t,d}} \gamma ^{(l)}_{md} \end{aligned}$$</span></div></div><p>where <span class="mathjax-tex">\(\gamma ^{(l)}_{md}\)</span> is sampled from <i>N</i> (0, 1). It is the plugged in the preactivation:</p><div class="c-article-equation" id="Equ40"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} b^{(l)}_{mh} = \sum _{d=1}^D a^{(l)}_{md} (\zeta ^{(l)}_{t,d} + \sqrt{\alpha ^{(l)}_{t,d}} \gamma ^{(l)}_{t,md})\theta ^{(l)}_{dh} \end{aligned}$$</span></div></div><p>Since, <span class="mathjax-tex">\(\theta _{dh}^{(l)} \sim {\mathcal {N}}(\mu _{t,dh}^{(l)}, (\sigma _{t,dh}^{(l)})^{2})\)</span>, we rewrite: <span class="mathjax-tex">\(b_{mh}^{(l)} \sim {\mathcal {N}}(\omega _{mh}^{(l)}, \delta _{mh}^{(l)})\)</span> where</p><div class="c-article-equation" id="Equ41"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} \omega _{mh}^{(l)}&amp;= \sum _{d=1}^{D} a_{md}^{(l)}(\zeta ^{(l)}_{t,d} + \sqrt{\alpha ^{(l)}_{t,d}} \gamma ^{(l)}_{md}) \mu _{t,dh}^{(l)} \\ \delta _{mh}^{(l)}&amp;= \sum _{d=1}^{D} \left( a_{md}^{(l)}(\zeta ^{(l)}_{t,d} + \sqrt{\alpha ^{(l)}_{t,d}} \gamma ^{(l)}_{md}) \right) ^{2} (\sigma _{t,dh}^{(l)})^{2} \end{aligned}$$</span></div></div><p>Adopting reparameterization trick, we have <span class="mathjax-tex">\(b_{mh}^{(l)} = \omega _{mh}^{(l)} + \sqrt{\delta _{mh}^{(l)}} \varepsilon _{mh}^{(l)}\)</span> where <span class="mathjax-tex">\(\varepsilon _{mh}^{(l)}\)</span> is sampled from <span class="mathjax-tex">\({\mathcal {N}}(0,1)\)</span>. Let <span class="mathjax-tex">\(L^{(m)} = E_{q_t({\varvec{\theta }}), q_t({{\textbf{s}}})} \log p({\textbf{y}}_t^{(m)} \vert {\textbf{s}}, \varvec{\theta }, {\textbf{x}}_t^{(m)})\)</span> be the likelihood of data instance <i>m</i> and is expressed as:</p><div class="c-article-equation" id="Equ42"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} L^{(m)} = E_{\varvec{\gamma }_m,\varvec{\varepsilon }_m} \log p({\textbf{y}}_t^{(m)} \vert \varvec{\zeta }_{t}, \varvec{\alpha }_t, \varvec{\mu }_t, \varvec{\sigma }_t, \varvec{\gamma }_m, \varvec{\varepsilon }_m, {\textbf{x}}_t^{(m)}) \end{aligned}$$</span></div></div><p>After sampling <span class="mathjax-tex">\(\varvec{\gamma }\)</span> and <span class="mathjax-tex">\(\varvec{\varepsilon }\)</span>, we can calculate the likelihood term and optimize the objective function with respect to variational parameters <span class="mathjax-tex">\(\varvec{\zeta }_{t}\)</span>, <span class="mathjax-tex">\(\varvec{\alpha }_t\)</span>, <span class="mathjax-tex">\(\varvec{\mu }_t\)</span> and <span class="mathjax-tex">\(\varvec{\sigma }_t\)</span> for task <i>t</i>. The ELBO is the same as the original CVD for VCL and the <span class="mathjax-tex">\(KL(q_t({\textbf{s}}) \Vert p({\textbf{s}}))\)</span> is approximated between two Gaussians.</p><p>It is noteworthy that this variant of CVD also obtains the first two theoretical properties as in the original one. The uncorrelated likelihoods between data instances are preserved thanks to the local reparameterization trick applied to the pre-activations. The correlated pre-activation is achieved through the random variable <span class="mathjax-tex">\(\varvec{\gamma }_m\)</span> derived from the reparameterization trick on the local variables, resulting in a correlation between dimensions of each row in the pre-activation matrix.</p>
<h3 class="c-article__sub-heading">
<b>Inducing FiLM layers</b>
</h3>
<p>To the extent of auxiliary variables, GVCL (Loo et al., <a aria-label="Reference 2021" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR27" id="ref-link-section-d212779005e21316" title="Loo, N., Swaroop, S., &amp; Turner, R. E. (2021). Generalized variational continual learning. In International conference on learning representation">2021</a>) introduced FiLM layers as local factors to capture the task-specific features. Apparently, both FiLM layers and CVD make use of two variables with different roles; namely, the global ones guarantee good performance for all tasks, and the local ones adjust the global to do well for each task. Nevertheless, the setting of CVD is different from FiLM layers as ours is injected after the activations while FiLM layers are placed following the pre-activations. This section introduces a different setting for CVD, which can then induce FiLM layers.</p>
<p>In particular, we set the local variable <i>s</i> directly modulates the pre-activations instead of adjusting the global variables <span class="mathjax-tex">\(\theta\)</span>. Let <span class="mathjax-tex">\(b_{mh}\)</span> be the original pre-activation and <span class="mathjax-tex">\(b^{'}_{mh}\)</span> be the modulated pre-activations. We applied the above variant of CVD on the pre-activation as follows:</p><div class="c-article-equation" id="Equ43"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned}&amp;b^{'(l)}_{mh} = b_{mh}\left( \zeta ^{(l)}_{t,h} + \sqrt{\alpha ^{(l)}_{t,h}} \gamma ^{(l)}_{t,mh}\right) \\&amp;= b_{mh}\zeta ^{(l)}_{t,h} + b_{mh}\sqrt{\alpha ^{(l)}_{t,h}} \gamma ^{(l)}_{t,mh} \end{aligned}$$</span></div></div><p>Let <span class="mathjax-tex">\(\delta ^{(l)}_{t,mh} = b_{mh}\sqrt{\alpha ^{(l)}_{t,h}} \gamma ^{(l)}_{t,mh}\)</span> we obtain:</p><div class="c-article-equation" id="Equ44"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned}&amp;b^{'(l)}_{mh} = b_{mh}\zeta ^{(l)}_{t,h} + \delta ^{(l)}_{t,mh} \end{aligned}$$</span></div></div><p>It can be seen that by positioning CVD on the pre-activations, we recover both the scale and shift term in the FiLM layers used in GVCL. In particular, <span class="mathjax-tex">\(\zeta ^{(l)}_{t,h}\)</span> plays the role of the scale coefficient and <span class="mathjax-tex">\(\delta ^{(l)}_{t,mh}\)</span> acts as the shift as in FiLM. Consequently, CVD inherits characteristics of FiLM layers, such as manipulating the activations and solving the over-pruning effect in OVI methods.</p><p>Moreover, CVD still preserves the theoretical properties analyzed above in this variant. Indeed, the local reparameterization trick is adopted, mitigating the sampling from the global variables to the pre-activations, hence obtaining the uncorrelated likelihoods property. Regarding the correlated pre-activation, the considering variant of CVD delegates the correlations to the following layers. To be precise, we denote the two consecutive layers as (<i>l</i>) and <span class="mathjax-tex">\((l+1)\)</span>. We then have the input for the next layer:</p><div class="c-article-equation" id="Equ45"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned}&amp;x^{(l+1)}_{mh} = f(b^{'(l)}_{mh}) = f(b_{mh}\zeta ^{(l)}_{t,h} + \delta ^{(l)}_{t,mh}) \end{aligned}$$</span></div></div><p>where <span class="mathjax-tex">\(f(\cdot )\)</span> is the activation function. By forwarding the input, we obtain the pre-activation of <span class="mathjax-tex">\((l+1)^{th}\)</span> layer:</p><div class="c-article-equation" id="Equ46"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} b^{(l+1)}_{mg}&amp;= \sum _{h=1}^{H} x_{mh}^{(l)} \theta ^{(l+1)}_{hg} = \sum _{h=1}^{H} f(b^{'(l)}_{mh}) \theta ^{(l+1)}_{hg} \\&amp;= \sum _{h=1}^{H} f(b_{mh}\zeta ^{(l)}_{t,h} + b_{mh}\sqrt{\alpha ^{(l)}_{t,h}} \gamma ^{(l)}_{t,mh}) \theta ^{(l+1)}_{hg} \end{aligned}$$</span></div></div><p>where <i>g</i> is the number of output features of layer <span class="mathjax-tex">\((l+1)^{th}\)</span>.</p><p>Recall that the local reparameterization trick is applied on the pre-activation <span class="mathjax-tex">\(b^{(l+1)}_{mg}\)</span>, which now follows a distribution <span class="mathjax-tex">\({\mathcal {N}}(\omega _{mg}, \delta _{mg})\)</span> where <span class="mathjax-tex">\(\omega _{mg}, \delta _{mg}\)</span> are functions over random variable <span class="mathjax-tex">\(\varvec{\gamma }_m\)</span> derived from reparameterization trick on the local variables of the previous layer. As a consequence, serving variational dropout as an adjusting factor on the pre-activation remains the correlated pre-activation as it transfers the hierarchical distribution to the next layer. In this way, this setting of CVD acquires the good representation capability of the original version.</p></div></div></section><section data-title="Experiments"><div class="c-article-section" id="Sec12-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec12"><span class="c-article-section__title-number">4 </span>Experiments</h2><div class="c-article-section__content" id="Sec12-content"><p>We use five regularization/prior-based methods: EWC, AGS-CL, VCL, UCL, and GVCL, to evaluate how CVD improves them on six datasets: Split MNIST, Permuted MNIST, Split CIFAR-100, Split CIFAR-10/100, Split Omniglot, Split CUB-200 and Split ImageNet-R. We ignore considering episodic or corset memory to boost the effectiveness of all methods in our experiments. These datasets are generated from six original ones: MNIST (Deng, <a aria-label="Reference 2012" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR8" id="ref-link-section-d212779005e22903" title="Deng, L. (2012). The MNIST database of handwritten digit images for machine learning research [best of the web]. IEEE Signal Processing Magazine, 29(6), 141–142.">2012</a>), CIFAR10, CIFAR100 (Krizhevsky &amp; Hinton, <a aria-label="Reference 2009" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR23" id="ref-link-section-d212779005e22906" title="Krizhevsky, A. (2009). Learning multiple layers of features from tiny images. Technical report, University of Toronto.">2009</a>), Omniglot, CUB-200 (Wah et al., <a aria-label="Reference 2011" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR49" id="ref-link-section-d212779005e22909" title="Wah, C., Branson, S., Welinder, P., Perona, P., &amp; Belongie, S. (2011). The Caltech-UCSD Birds-200-2011 dataset.">2011</a>) and ImageNet-R (Hendrycks et al., <a aria-label="Reference 2021" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR17" id="ref-link-section-d212779005e22912" title="Hendrycks, D., Basart, S., Mu, N., Kadavath, S., Wang, F., Dorundo, E., Desai, R., Zhu, T., Parajuli, S., Guo, M., Song, D., Steinhardt, J., &amp; Gilmer, J. (2021). The many faces of robustness: A critical analysis of out-of-distribution generalization. In Proceedings of the IEEE/CVF international conference on computer vision (pp. 8340–8349).">2021</a>), and can be applied to simulate the process of continuous arriving data. In this work, we inherit AGS-CL setting for Split CUB-200 and UCL’s experiment setup for the remaining four datasets except Split ImageNet-R. We employ Split ImageNet-R to evaluate complicated classification data, which contains 10 sequences of tasks. All scenarios assume that data arrives task by task and all data points of a task come at the same time.</p><p>We carried out experiments on two versions for each method, namely, the original one and the CVD application on the baseline. The superiority of CVD is clearly shown in Tables <a data-track="click" data-track-action="table anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#Tab2">2</a> and <a data-track="click" data-track-action="table anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#Tab3">3</a>, and thoroughly discussed in Sects. <a data-track="click" data-track-action="section anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#Sec13">4.1</a>, <a data-track="click" data-track-action="section anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#Sec14">4.2</a>, <a data-track="click" data-track-action="section anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#Sec15">4.3</a> and <a data-track="click" data-track-action="section anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#Sec16">4.4</a>. Furthermore, we verified the practicality of the correlated pre-activation and uncorrelated likelihood properties in Sect. <a data-track="click" data-track-action="section anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#Sec17">4.5</a>. In Sect. <a data-track="click" data-track-action="section anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#Sec18">4.6</a>, the two extended variants of CVD are proved to attain competitive or superior performance compared to the original CVD and other auxiliary local variables. Finally, we review the effectiveness of dropout with adaptive droprate against fixed droprate, the training time, and sensitivity to the additional hyperparameters <span class="mathjax-tex">\(\kappa\)</span> in Sect. <a data-track="click" data-track-action="section anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#Sec19">4.7</a>.</p>
<h3 class="c-article__sub-heading">
<b>Datasets</b>
</h3>
<p>We use six benchmark datasets to evaluate our method: Split MNIST, Permuted MNIST, Split CIFAR 100, Split CIFAR 10/100, Split Ominiglot, Split CUB-200 and Split ImageNet-R. The first two are variants of MNIST. Split MNIST consists of 5 consecutive tasks, each containing 2 classes divided from the original one. Permuted MNIST comprises 10 tasks; each is formed by a different permutation of MNIST. Split CIFAR 100 divides CIFAR100 into 10 tasks that hold 10 classes each. Split CIFAR 10/100 combined the two datasets, with CIFAR10 as the first task and CIFAR 100 as the following 10 tasks. Split Omniglot is made up of 50 alphabets and 50 tasks. Both Split CUB-200 and Split ImageNet-R contain 10 tasks and 20 classes in each task. We provide the specific number of tasks, number of training samples, number of testing samples, and labels per task in Table <a data-track="click" data-track-action="table anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#Tab1">1</a>.
</p><div class="c-article-table" data-container-section="table" data-test="inline-table" id="table-1"><figure><figcaption class="c-article-table__figcaption"><b data-test="table-caption" id="Tab1">Table 1 Details of benchmark datasets</b></figcaption><div class="u-text-right u-hide-print"><a aria-label="Full size table 1" class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" href="/article/10.1007/s10994-023-06487-7/tables/1" rel="nofollow"><span>Full size table</span><svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16"><use xlink:href="#icon-eds-i-chevron-right-small" xmlns:xlink="http://www.w3.org/1999/xlink"></use></svg></a></div></figure></div>
<h3 class="c-article__sub-heading">
<b>Evaluation Metrics</b>
</h3>
<p>We evaluate our method using average accuracy across all tasks. The average accuracy after learning task t is formulated as follows:</p><div class="c-article-equation" id="Equ47"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} ACC_{t} = \frac{1}{t} \sum ^{t}_{i=1} R_{t,i} \end{aligned}$$</span></div></div><p>where <span class="mathjax-tex">\(R_{j,i}\)</span> is the accuracy on <span class="mathjax-tex">\(i\mathrm{{th}}\)</span> task after learned task <span class="mathjax-tex">\(j\mathrm{{th}}\)</span>.</p>
<h3 class="c-article__sub-heading">
<b>Settings</b>
</h3>
<p>We use again the source codes of EWC and UCL, which are released from the original UCL paper,<sup><a href="#Fn1"><span class="u-visually-hidden">Footnote </span>1</a></sup> GVCL from the GVCL paper<sup><a href="#Fn2"><span class="u-visually-hidden">Footnote </span>2</a></sup> and AGS-CL from their paper.<sup><a href="#Fn3"><span class="u-visually-hidden">Footnote </span>3</a></sup> We implement VCL based on the source code of UCL. Similar to the experiments of UCL paper (Ahn et al., <a aria-label="Reference 2019" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR1" id="ref-link-section-d212779005e23448" title="Ahn, H., Cha, S., Lee, D., &amp; Moon, T. (2019). Uncertainty-based continual learning with adaptive regularization. In Advances in Neural Information Processing Systems (pp. 4392–4402).">2019</a>), we use fully-connected neural networks (FNNs) for Split and Permuted MNIST datasets and convolutional neural networks (CNN) for Split CIFAR-100, CIFAR-10/100 and Omniglot datasets. We omit experiments on MNIST-based datasets for AGS-CL due to the extensive experiment load and the absence of such experiments in the original AGS-CL paper. For Split CUB-200 and ImageNet-R datasets, we experiment with the top two performance models (GVCL and AGS-CL). We follow (Jung et al., <a aria-label="Reference 2020" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR19" id="ref-link-section-d212779005e23451" title="Jung, S., Ahn, H., Cha, S., &amp; Moon, T. (2020). Continual learning with node-importance based adaptive group sparse regularization. In Advances in neural information processing systems">2020</a>) to use pre-trained AlexNet and employ Vision Transformer (ViT) (Dosovitskiy et al., <a aria-label="Reference 2020" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR10" id="ref-link-section-d212779005e23455" title="Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., &amp;  Houlsby, N. (2020). An image is worth 16x16 words: Transformers for image recognition at scale. In International conference on learning representations">2020</a>) for Split CUB-200 and Split ImageNet-R, respectively. CVD is injected into several layers of the networks, we explicitly show their architectures and settings in the Appendix <a data-track="click" data-track-action="section anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#Sec22">B</a>. We conduct all experiments on the datasets with minibatch sizes of 256 and 100 epochs and adopt the Adam optimizer with a learning rate of 0.001 for each task except Split CUB-200. In the Split CUB-200 experiment, we set minibatch sizes to 64 and utilize SGD optimizer with a learning rate of 0.005. For a fair comparison, all results are averaged over 5 different seeds, which also determine the order of sequences of tasks.
</p><div class="c-article-table" data-container-section="table" data-test="inline-table" id="table-2"><figure><figcaption class="c-article-table__figcaption"><b data-test="table-caption" id="Tab2">Table 2 Average accuracy over all tasks on 5 benchmark datasets</b></figcaption><div class="u-text-right u-hide-print"><a aria-label="Full size table 2" class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" href="/article/10.1007/s10994-023-06487-7/tables/2" rel="nofollow"><span>Full size table</span><svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16"><use xlink:href="#icon-eds-i-chevron-right-small" xmlns:xlink="http://www.w3.org/1999/xlink"></use></svg></a></div></figure></div>
<div class="c-article-section__figure js-c-reading-companion-figures-item" data-container-section="figure" data-test="figure" data-title="Fig. 2" id="figure-2"><figure><figcaption><b class="c-article-section__figure-caption" data-test="figure-caption-text" id="Fig2">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-action="view figure" data-track-label="image" href="/article/10.1007/s10994-023-06487-7/figures/2" rel="nofollow"><picture><source srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10994-023-06487-7/MediaObjects/10994_2023_6487_Fig2_HTML.png?as=webp" type="image/webp"/><img alt="figure 2" aria-describedby="Fig2" height="374" loading="lazy" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10994-023-06487-7/MediaObjects/10994_2023_6487_Fig2_HTML.png" width="685"/></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>Average accuracy and task accuracy on Split MNIST</p></div></div><div class="u-text-right u-hide-print"><a aria-label="Full size image figure 2" class="c-article__pill-button" data-test="article-link" data-track="click" data-track-action="view figure" data-track-dest="link:Figure2 Full size image" data-track-label="button" href="/article/10.1007/s10994-023-06487-7/figures/2" rel="nofollow"><span>Full size image</span><svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16"><use xlink:href="#icon-eds-i-chevron-right-small" xmlns:xlink="http://www.w3.org/1999/xlink"></use></svg></a></div></figure></div><div class="c-article-section__figure js-c-reading-companion-figures-item" data-container-section="figure" data-test="figure" data-title="Fig. 3" id="figure-3"><figure><figcaption><b class="c-article-section__figure-caption" data-test="figure-caption-text" id="Fig3">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-action="view figure" data-track-label="image" href="/article/10.1007/s10994-023-06487-7/figures/3" rel="nofollow"><picture><source srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10994-023-06487-7/MediaObjects/10994_2023_6487_Fig3_HTML.png?as=webp" type="image/webp"/><img alt="figure 3" aria-describedby="Fig3" height="374" loading="lazy" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10994-023-06487-7/MediaObjects/10994_2023_6487_Fig3_HTML.png" width="685"/></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>Average accuracy and task accuracy on PMNIST</p></div></div><div class="u-text-right u-hide-print"><a aria-label="Full size image figure 3" class="c-article__pill-button" data-test="article-link" data-track="click" data-track-action="view figure" data-track-dest="link:Figure3 Full size image" data-track-label="button" href="/article/10.1007/s10994-023-06487-7/figures/3" rel="nofollow"><span>Full size image</span><svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16"><use xlink:href="#icon-eds-i-chevron-right-small" xmlns:xlink="http://www.w3.org/1999/xlink"></use></svg></a></div></figure></div><h3 class="c-article__sub-heading" id="Sec13"><span class="c-article-section__title-number">4.1 </span>Effectiveness of CVD on Split MNIST and permuted MNIST datasets</h3><p>Table <a data-track="click" data-track-action="table anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#Tab2">2</a> illustrates the average accuracy over all tasks after finishing training the last task on five datasets (Split MNIST, Permuted MNIST, Split CIFAR 10/100, CIFAR 100 and Omniglot). For Split MNIST, consistently, CVD-based methods outperform the corresponding baselines. In particular, CVD improves EWC significantly from <span class="mathjax-tex">\(97.65\)</span> to <span class="mathjax-tex">\(99.79\%\)</span> average accuracy and can outperform the state-of-the-art method such as UCL. The performances of CVD for VCL, UCL, and GVCL also increase noticeably. However, we found that in VCL, UCL, and GVCL, the original baselines seem to work well in this experiment and are comparable to CVD. This can be explained as the data distribution between tasks may be similar, and there might be excess uncertainty in VCL and UCL when adding noise through dropout.</p><p>For Permuted MNIST, after training 10 tasks sequentially, EWC, VCL, UCL, GVCL, and GVCL-F achieve performances of <span class="mathjax-tex">\(92.82\%, 87.32\%\)</span>, <span class="mathjax-tex">\(96.18\%\)</span>, <span class="mathjax-tex">\(96.01\%\)</span> and <span class="mathjax-tex">\(97.59\%\)</span> respectively. After applying CVD, all the five baselines witnessed slight growth in the average accuracy. In detail, the accuracy of GVCL and GVCL-F grows from <span class="mathjax-tex">\(96.01\)</span> to <span class="mathjax-tex">\(97.00\%\)</span> and <span class="mathjax-tex">\(97.59\)</span> to <span class="mathjax-tex">\(97.94\%\)</span>, respectively. In the meantime, UCL slightly increases by <span class="mathjax-tex">\(0.16\%\)</span>, and VCL raises by <span class="mathjax-tex">\(1.53\%\)</span>. Similarly, EWC with CVD also grows from <span class="mathjax-tex">\(92.82\%\)</span> to <span class="mathjax-tex">\(92.96\%\)</span>. The reason for these slight rises in OVI methods, namely UCL, GVCL, and GVCL, is due to the fact that these methods themselves already have the property of uncertainty. This explanation can also answer for the indistinct results in the case of EWC since the original source code of UCL does include dropout in the architecture for EWC when working with Permuted MNIST.</p><p>Figures <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#Fig2">2</a> and <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#Fig3">3</a> illustrate the average accuracy after learning each task, as well as the final accuracy for each task when completed learning on Split MNIST and Permuted MNIST. Regarding Split MNIST, the gap between the average accuracy of CVD-based and others grows more remarkable as the number of tasks increases. The final accuracy of each task also confirms the effect of adding local variables in preserving tasks’ performance. However, in GVCL, the effect of CVD is not as immense as FiLM due to the excess of uncertainty.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-container-section="figure" data-test="figure" data-title="Fig. 4" id="figure-4"><figure><figcaption><b class="c-article-section__figure-caption" data-test="figure-caption-text" id="Fig4">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-action="view figure" data-track-label="image" href="/article/10.1007/s10994-023-06487-7/figures/4" rel="nofollow"><picture><source srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10994-023-06487-7/MediaObjects/10994_2023_6487_Fig4_HTML.png?as=webp" type="image/webp"/><img alt="figure 4" aria-describedby="Fig4" height="371" loading="lazy" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10994-023-06487-7/MediaObjects/10994_2023_6487_Fig4_HTML.png" width="685"/></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>Average accuracy and task accuracy on Split CIFAR 10/100</p></div></div><div class="u-text-right u-hide-print"><a aria-label="Full size image figure 4" class="c-article__pill-button" data-test="article-link" data-track="click" data-track-action="view figure" data-track-dest="link:Figure4 Full size image" data-track-label="button" href="/article/10.1007/s10994-023-06487-7/figures/4" rel="nofollow"><span>Full size image</span><svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16"><use xlink:href="#icon-eds-i-chevron-right-small" xmlns:xlink="http://www.w3.org/1999/xlink"></use></svg></a></div></figure></div><div class="c-article-section__figure js-c-reading-companion-figures-item" data-container-section="figure" data-test="figure" data-title="Fig. 5" id="figure-5"><figure><figcaption><b class="c-article-section__figure-caption" data-test="figure-caption-text" id="Fig5">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-action="view figure" data-track-label="image" href="/article/10.1007/s10994-023-06487-7/figures/5" rel="nofollow"><picture><source srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10994-023-06487-7/MediaObjects/10994_2023_6487_Fig5_HTML.png?as=webp" type="image/webp"/><img alt="figure 5" aria-describedby="Fig5" height="372" loading="lazy" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10994-023-06487-7/MediaObjects/10994_2023_6487_Fig5_HTML.png" width="685"/></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>Average accuracy and task accuracy on Split CIFAR 100</p></div></div><div class="u-text-right u-hide-print"><a aria-label="Full size image figure 5" class="c-article__pill-button" data-test="article-link" data-track="click" data-track-action="view figure" data-track-dest="link:Figure5 Full size image" data-track-label="button" href="/article/10.1007/s10994-023-06487-7/figures/5" rel="nofollow"><span>Full size image</span><svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16"><use xlink:href="#icon-eds-i-chevron-right-small" xmlns:xlink="http://www.w3.org/1999/xlink"></use></svg></a></div></figure></div><h3 class="c-article__sub-heading" id="Sec14"><span class="c-article-section__title-number">4.2 </span>Effectiveness of CVD on Split CIFAR 100 and Split CIFAR 10/100:</h3><p>Figures <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#Fig4">4</a> and <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#Fig8">8</a>-Left illustrate the effectiveness of CVD in comparison with the baselines on the Split CIFAR 10/100 dataset. Overall, all methods have witnessed a remarkable increase in performance compared to their baseline results when applying CVD. In addition, the figure also depicts the average performances of all the methods after each task. It is clear that the average accuracy of all methods decreases when evaluating on test sets of previous tasks and the current one, which indicates the forgetting of acquired knowledge. However, CVD can help the baselines mitigate this phenomenon with the support of local variables to achieve better average accuracy through tasks. Specifically, EWC with CVD outperforms some of the others, including UCL and VCL, by achieving an average accuracy of <span class="mathjax-tex">\(74.23\%\)</span>, which even overcomes UCL with CVD (<span class="mathjax-tex">\(73.13\%\)</span>). While AGS-CL already incorporates two regularization terms to manage its learning capacity, achieving a high performance at 76.03%, it’s noteworthy that the integration of CVD leading to additional regularization term further enhances accuracy, pushing it to 77.03%. This indicates the effectiveness of CVD to existing regularization-based methods. Furthermore, VCL is claimed to have low performance on CIFAR dataset (Loo et al., <a aria-label="Reference 2021" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR27" id="ref-link-section-d212779005e26026" title="Loo, N., Swaroop, S., &amp; Turner, R. E. (2021). Generalized variational continual learning. In International conference on learning representation">2021</a>), and the improvements of CVD on VCL are also remarkable. Considerable gaps are also observed in the experiments with GVCL and GVCL-F. Notably, CVD boosts the performance of these two by <span class="mathjax-tex">\(7.23\%\)</span> and <span class="mathjax-tex">\(3.36\%\)</span>, respectively.</p><p>In Split CIFAR 100, as can be seen in Figs. <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#Fig5">5</a> and <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#Fig8">8</a>-Middle, all results are under the curves which have been presented in Split CIFAR 10/100 experiments, even with fewer tasks. This can be explained as Split CIFAR 10/100 can prevent overfitting on Split CIFAR 100 using a model pre-trained on CIFAR-10 (the first task is CIFAR-10), which contains much more data than that of a task split from CIFAR-100. Again, all five methods show significant improvement after applying CVD. Specifically, CVD on the two GVCL and EWC obtain the most noticeable improvement (from 55.13 to 60.19% for GVCL, 61.71 to 65.25% for GVCL-F, and 60.56 to 63.20% for EWC).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-container-section="figure" data-test="figure" data-title="Fig. 6" id="figure-6"><figure><figcaption><b class="c-article-section__figure-caption" data-test="figure-caption-text" id="Fig6">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-action="view figure" data-track-label="image" href="/article/10.1007/s10994-023-06487-7/figures/6" rel="nofollow"><picture><source srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10994-023-06487-7/MediaObjects/10994_2023_6487_Fig6_HTML.png?as=webp" type="image/webp"/><img alt="figure 6" aria-describedby="Fig6" height="558" loading="lazy" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10994-023-06487-7/MediaObjects/10994_2023_6487_Fig6_HTML.png" width="685"/></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>The change of accuracy through tasks on EWC</p></div></div><div class="u-text-right u-hide-print"><a aria-label="Full size image figure 6" class="c-article__pill-button" data-test="article-link" data-track="click" data-track-action="view figure" data-track-dest="link:Figure6 Full size image" data-track-label="button" href="/article/10.1007/s10994-023-06487-7/figures/6" rel="nofollow"><span>Full size image</span><svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16"><use xlink:href="#icon-eds-i-chevron-right-small" xmlns:xlink="http://www.w3.org/1999/xlink"></use></svg></a></div></figure></div><p>We also provide the performances w.r.t. each task in the case of EWC on Split CIFAR 10/100 in Fig. <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#Fig6">6</a>. This figure displays information about the test accuracy of a task corresponding to the trained model of the task on the horizontal axis. From this perspective, we are able to identify how forgetting occurs on each task. As can be seen, CVD makes EWC more stable on all tasks, while most tasks are noticeably performed worse for the original. Similar results also appear on VCL, UCL, and GVCL; however, we do not visualize them here to save space. The figures for AGS-CL, VCL, UCL, and GVCL can instead be found in the Appendix <a data-track="click" data-track-action="section anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#Sec23">C</a> supplementary material. From these results, CVD is shown to not only improve performance in most tasks but also reduce the forgetting phenomenon more effectively.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-container-section="figure" data-test="figure" data-title="Fig. 7" id="figure-7"><figure><figcaption><b class="c-article-section__figure-caption" data-test="figure-caption-text" id="Fig7">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-action="view figure" data-track-label="image" href="/article/10.1007/s10994-023-06487-7/figures/7" rel="nofollow"><picture><source srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10994-023-06487-7/MediaObjects/10994_2023_6487_Fig7_HTML.png?as=webp" type="image/webp"/><img alt="figure 7" aria-describedby="Fig7" height="364" loading="lazy" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10994-023-06487-7/MediaObjects/10994_2023_6487_Fig7_HTML.png" width="685"/></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>Average accuracy and task accuracy on Split Omniglot</p></div></div><div class="u-text-right u-hide-print"><a aria-label="Full size image figure 7" class="c-article__pill-button" data-test="article-link" data-track="click" data-track-action="view figure" data-track-dest="link:Figure7 Full size image" data-track-label="button" href="/article/10.1007/s10994-023-06487-7/figures/7" rel="nofollow"><span>Full size image</span><svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16"><use xlink:href="#icon-eds-i-chevron-right-small" xmlns:xlink="http://www.w3.org/1999/xlink"></use></svg></a></div></figure></div><div class="c-article-section__figure js-c-reading-companion-figures-item" data-container-section="figure" data-test="figure" data-title="Fig. 8" id="figure-8"><figure><figcaption><b class="c-article-section__figure-caption" data-test="figure-caption-text" id="Fig8">Fig. 8</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-action="view figure" data-track-label="image" href="/article/10.1007/s10994-023-06487-7/figures/8" rel="nofollow"><picture><source srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10994-023-06487-7/MediaObjects/10994_2023_6487_Fig8_HTML.png?as=webp" type="image/webp"/><img alt="figure 8" aria-describedby="Fig8" height="479" loading="lazy" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10994-023-06487-7/MediaObjects/10994_2023_6487_Fig8_HTML.png" width="685"/></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-8-desc"><p>Average accuracy and task accuracy of AGS-CL</p></div></div><div class="u-text-right u-hide-print"><a aria-label="Full size image figure 8" class="c-article__pill-button" data-test="article-link" data-track="click" data-track-action="view figure" data-track-dest="link:Figure8 Full size image" data-track-label="button" href="/article/10.1007/s10994-023-06487-7/figures/8" rel="nofollow"><span>Full size image</span><svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16"><use xlink:href="#icon-eds-i-chevron-right-small" xmlns:xlink="http://www.w3.org/1999/xlink"></use></svg></a></div></figure></div><h3 class="c-article__sub-heading" id="Sec15"><span class="c-article-section__title-number">4.3 </span>Effectiveness of CVD on Split Omniglot</h3><p>In this experiment, we also use CNN but with a different architecture from those used in the CIFAR experiment. The difference between Split Omniglot and the other four mentioned datasets is that the number of classes is diverse among tasks, thus making it more practical. We rescale, augment, and split the data following the code provided in Ahn et al. (<a aria-label="Reference 2019" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR1" id="ref-link-section-d212779005e26151" title="Ahn, H., Cha, S., Lee, D., &amp; Moon, T. (2019). Uncertainty-based continual learning with adaptive regularization. In Advances in Neural Information Processing Systems (pp. 4392–4402).">2019</a>). The results of this experiment are illustrated in Figs. <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#Fig7">7</a> and <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#Fig8">8</a>-Right.</p><p>We can observe that CVD makes EWC and AGS-CL remarkably more stable. Furthermore, CVD helps EWC achieve a nearly state-of-the-art performance with <span class="mathjax-tex">\(90.14\%\)</span> average accuracy on all 50 tasks, only after the two CVD versions of GVCL (with a result of <span class="mathjax-tex">\(92.27\%\)</span> for the regular CVD version and <span class="mathjax-tex">\(93.63\%\)</span> when using CVD and FiLM layer), thus obtaining a better performance than UCL and VCL in this experiment. Similarly, the results of VCL and UCL with CVD undergo a slighter fluctuation than the original method across 50 tasks. Additionally, the task accuracy demonstrates the ability to preserve learned knowledge, as in most tasks, the final accuracy of CVD is higher than the baselines. This indicates that CVD can also more effectively deal with the stability-plasticity dilemma in the case of long task sequences. Therefore, all other three methods (VCL, UCL, and GVCL), also deliver better performances by around <span class="mathjax-tex">\(6\%\)</span> for the first two and <span class="mathjax-tex">\(3\%\)</span> for the last one in comparison to the original application. Moreover, CVD also shows superiority against the FiLM layer in this experiment by <span class="mathjax-tex">\(1.36\%\)</span>.</p><h3 class="c-article__sub-heading" id="Sec16"><span class="c-article-section__title-number">4.4 </span>Effectiveness of CVD on Split CUB-200 and Split ImageNet-R</h3><p>In this section, we present the results of our experiments designed to assess the effectiveness of CVD on more challenging datasets. Our investigation encompasses two large datasets, which are variants of CUB-200 and ImagetNet-R, using two methods (AGS-CL and GVCL) in this experiment. The results are presented in Table <a data-track="click" data-track-action="table anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#Tab3">3</a> and Fig. <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#Fig9">9</a>.
</p><div class="c-article-table" data-container-section="table" data-test="inline-table" id="table-3"><figure><figcaption class="c-article-table__figcaption"><b data-test="table-caption" id="Tab3">Table 3 Average accuracy over all tasks on Split CUB-200 and Split ImageNet-R</b></figcaption><div class="u-text-right u-hide-print"><a aria-label="Full size table 3" class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" href="/article/10.1007/s10994-023-06487-7/tables/3" rel="nofollow"><span>Full size table</span><svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16"><use xlink:href="#icon-eds-i-chevron-right-small" xmlns:xlink="http://www.w3.org/1999/xlink"></use></svg></a></div></figure></div><p>For the Split CUB-200 dataset, we applied CVD to the AlexNet architecture. The results demonstrated a consistent trend observed in previous experiments, indicating that CVD significantly enhances the stability and performance of the original methods. Noticeably, CVD contributes to a substantial improvement of over 3% in the performance of OVI-based methods, underscoring the alignment of CVD’s theoretical principles with its empirical impact on these methods. Moreover, CVD also shows superiority against the FiLM layer in this experiment by <span class="mathjax-tex">\(3.31\%\)</span>.</p><p>To assess the capabilities of CVD on a more challenging dataset, we employ Imagenet-R. We leverage a pretrained Vision Transformer (ViT) to conduct experiments on Imagenet-R. Due to the limitations of our computational resources, we froze the backbone to reduce computational complexity and added several linear layers on top (see the architecture in Appendix <a data-track="click" data-track-action="section anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#Sec22">B</a>). Figure <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#Fig9">9</a> reveals that all methods achieve low accuracy and experience a noteworthy forgetting phenomenon. However, the application of CVD proves instrumental in mitigating this phenomenon, which improves significantly the performance of AGS-CL from 44.88% to 47.79% and GVCL from 46.97 to 48.53%. Additionally, the combination of FiLM and CVD pushes GVCL to achieve the best performance of 52.34%. These results underscore the robust efficacy of CVD in specific and local variables in general on a wide range of models and dataset scales.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-container-section="figure" data-test="figure" data-title="Fig. 9" id="figure-9"><figure><figcaption><b class="c-article-section__figure-caption" data-test="figure-caption-text" id="Fig9">Fig. 9</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-action="view figure" data-track-label="image" href="/article/10.1007/s10994-023-06487-7/figures/9" rel="nofollow"><picture><source srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10994-023-06487-7/MediaObjects/10994_2023_6487_Fig9_HTML.png?as=webp" type="image/webp"/><img alt="figure 9" aria-describedby="Fig9" height="379" loading="lazy" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10994-023-06487-7/MediaObjects/10994_2023_6487_Fig9_HTML.png" width="685"/></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-9-desc"><p>Average accuracy and task accuracy on Split CUB-200 and Split ImagetNet-R</p></div></div><div class="u-text-right u-hide-print"><a aria-label="Full size image figure 9" class="c-article__pill-button" data-test="article-link" data-track="click" data-track-action="view figure" data-track-dest="link:Figure9 Full size image" data-track-label="button" href="/article/10.1007/s10994-023-06487-7/figures/9" rel="nofollow"><span>Full size image</span><svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16"><use xlink:href="#icon-eds-i-chevron-right-small" xmlns:xlink="http://www.w3.org/1999/xlink"></use></svg></a></div></figure></div><h3 class="c-article__sub-heading" id="Sec17"><span class="c-article-section__title-number">4.5 </span>Analysis on theoretical properties</h3><p>This section shows a detailed analysis of our suggested theoretical properties of CVD. To demonstrate the effectiveness of CVD when learning each task, we design an offline experimental scenario in which there is a single task and all data is available for a training process with multiple passes through the dataset.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-container-section="figure" data-test="figure" data-title="Fig. 10" id="figure-10"><figure><figcaption><b class="c-article-section__figure-caption" data-test="figure-caption-text" id="Fig10">Fig. 10</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-action="view figure" data-track-label="image" href="/article/10.1007/s10994-023-06487-7/figures/10" rel="nofollow"><picture><source srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10994-023-06487-7/MediaObjects/10994_2023_6487_Fig10_HTML.png?as=webp" type="image/webp"/><img alt="figure 10" aria-describedby="Fig10" height="382" loading="lazy" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10994-023-06487-7/MediaObjects/10994_2023_6487_Fig10_HTML.png" width="685"/></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-10-desc"><p>Average accuracy of using reparameterization trick, local reparameterization trick and CVD. The experiments is performed on four datasets, including MNIST, CIFAR 10, CIFAR 10/100 and Omniglot</p></div></div><div class="u-text-right u-hide-print"><a aria-label="Full size image figure 10" class="c-article__pill-button" data-test="article-link" data-track="click" data-track-action="view figure" data-track-dest="link:Figure10 Full size image" data-track-label="button" href="/article/10.1007/s10994-023-06487-7/figures/10" rel="nofollow"><span>Full size image</span><svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16"><use xlink:href="#icon-eds-i-chevron-right-small" xmlns:xlink="http://www.w3.org/1999/xlink"></use></svg></a></div></figure></div><p><b>Analysis of the preactivation properties:</b> Firstly, for the correlated pre-activation properties, we prove the superiority of CVD compared to the baselines by evaluating the accuracy on four datasets, MNIST, CIFAR10, CIFAR10/100, and Omniglot. In particular, we compare three extensions, including using the reparameterization trick, the application of the local reparameterization trick, and our proposed variational dropout. As shown in Fig. <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#Fig10">10</a>, CVD exceeds the two baselines by significant clearance on all datasets, especially on the two datasets CIFAR10 and CIFAR100, which require higher represent ability than the remainings. Namely, while CVD improves slightly by <span class="mathjax-tex">\(0.03\%\)</span> and <span class="mathjax-tex">\(1.37\%\)</span> on MNIST and Omniglot, respectively, the difference grows more significant for more complicated datasets. On CIFAR100, the dominance of CVD against regular the local reparameterization trick is <span class="mathjax-tex">\(8.98\%\)</span> and climbs to <span class="mathjax-tex">\(11.49\%\)</span> on CIFAR10/100. These observations confirm the effectiveness of the hierarchical distribution obtained when adding the variational dropout, resulting in correlated pre-activations and leading to better feature representation.
</p><div class="c-article-table" data-container-section="table" data-test="inline-table" id="table-4"><figure><figcaption class="c-article-table__figcaption"><b data-test="table-caption" id="Tab4">Table 4 Average variance of gradients with respect to the variance of weights</b></figcaption><div class="u-text-right u-hide-print"><a aria-label="Full size table 4" class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" href="/article/10.1007/s10994-023-06487-7/tables/4" rel="nofollow"><span>Full size table</span><svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16"><use xlink:href="#icon-eds-i-chevron-right-small" xmlns:xlink="http://www.w3.org/1999/xlink"></use></svg></a></div></figure></div>
<h3 class="c-article__sub-heading">
<b>Analysis of the uncorrelated likelihoods properties</b>
</h3>
<p>In order to clarify the uncorrelated likelihood property of CVD and the local reparameterization trick, we estimate the average variance of gradients with respect to the variance of weights in Table <a data-track="click" data-track-action="table anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#Tab4">4</a>. In particular, after training the model, we pass the whole training set and calculate the gradients of the top and bottom layers of the corresponding architecture, i.e., for MNIST, the first and last fully connected layers’ gradient are considered. For the other three datasets, CIFAR 10, CIFAR 10/100, and Omniglot, the first and last convolutional layers are taken into account. The comparison is based on three adaptations: original, using local reparameterization trick, and using CVD.</p>
<p>For most cases, the local reparameterization trick gains the lowest variance of gradients owing to the uncorrelated likelihood property. Our proposed CVD adds variational dropout to the architecture, which explicitly adds noise to the gradient, hence yielding a slightly greater variance to the baseline local reparameterization trick. Nevertheless, our variance still accesses a lower estimation compared to the regular reparameterization trick, as CVD also takes advantage of the local reparameterization trick.</p><h3 class="c-article__sub-heading" id="Sec18"><span class="c-article-section__title-number">4.6 </span>Experiments on CVD variants</h3>
<div class="c-article-table" data-container-section="table" data-test="inline-table" id="table-5"><figure><figcaption class="c-article-table__figcaption"><b data-test="table-caption" id="Tab5">Table 5 Average accuracy corresponding to different prior distributions</b></figcaption><div class="u-text-right u-hide-print"><a aria-label="Full size table 5" class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" href="/article/10.1007/s10994-023-06487-7/tables/5" rel="nofollow"><span>Full size table</span><svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16"><use xlink:href="#icon-eds-i-chevron-right-small" xmlns:xlink="http://www.w3.org/1999/xlink"></use></svg></a></div></figure></div>
<h3 class="c-article__sub-heading">
<b>Experiments on diversity of priors</b>
</h3>
<p>We demonstrate the diverse adaptation of CVD by showing that it works with various prior distributions. We report the average accuracy of CVD when using two prior distributions, log uniform and gaussian. The details of the application for log uniform are given in Sect. <a data-track="click" data-track-action="section anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#Sec9">3.2</a>, and the Gaussian is in Sect. <a data-track="click" data-track-action="section anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#Sec11">3.4</a>. We run the experiments on five datasets Split MNIST, Permuted MNIST, Split CIFAR 10/100, Split CIFAR 100, and Omniglot. The original baseline and the two variants of CVD are compared. Two approaches are taken into account, including VCL and GVCL.</p>
<p>Table <a data-track="click" data-track-action="table anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#Tab5">5</a> shows the details results of this experiment. CVD with Gaussian prior achieves competitive results with log uniform in most cases. The most significant gap can be seen in VCL on Permuted MNIST. As stated above, adding variational dropout to VCL makes it prone to excess uncertainty, reducing the effectiveness of CVD and thus, leading to less than expected results. By additional learning the <span class="mathjax-tex">\(\zeta ^{(l)}_{t,d}\)</span> as the mean of the approximated posterior, the adaptive mechanism is enhanced, therefore allowing remarkably more outstanding performance.
</p><div class="c-article-table" data-container-section="table" data-test="inline-table" id="table-6"><figure><figcaption class="c-article-table__figcaption"><b data-test="table-caption" id="Tab6">Table 6 Average accuracy when setting the variational dropout as FiLM layers</b></figcaption><div class="u-text-right u-hide-print"><a aria-label="Full size table 6" class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" href="/article/10.1007/s10994-023-06487-7/tables/6" rel="nofollow"><span>Full size table</span><svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16"><use xlink:href="#icon-eds-i-chevron-right-small" xmlns:xlink="http://www.w3.org/1999/xlink"></use></svg></a></div></figure></div><div class="c-article-section__figure js-c-reading-companion-figures-item" data-container-section="figure" data-test="figure" data-title="Fig. 11" id="figure-11"><figure><figcaption><b class="c-article-section__figure-caption" data-test="figure-caption-text" id="Fig11">Fig. 11</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-action="view figure" data-track-label="image" href="/article/10.1007/s10994-023-06487-7/figures/11" rel="nofollow"><picture><source srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10994-023-06487-7/MediaObjects/10994_2023_6487_Fig11_HTML.png?as=webp" type="image/webp"/><img alt="figure 11" aria-describedby="Fig11" height="475" loading="lazy" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10994-023-06487-7/MediaObjects/10994_2023_6487_Fig11_HTML.png" width="685"/></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-11-desc"><p>Average accuracy and task accuracy on Split MNIST, Split CIFAR, 10/100 and Split CIFAR 100</p></div></div><div class="u-text-right u-hide-print"><a aria-label="Full size image figure 11" class="c-article__pill-button" data-test="article-link" data-track="click" data-track-action="view figure" data-track-dest="link:Figure11 Full size image" data-track-label="button" href="/article/10.1007/s10994-023-06487-7/figures/11" rel="nofollow"><span>Full size image</span><svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16"><use xlink:href="#icon-eds-i-chevron-right-small" xmlns:xlink="http://www.w3.org/1999/xlink"></use></svg></a></div></figure></div>
<h3 class="c-article__sub-heading">
<b>Experiments on inducing FiLM layer variant</b>
</h3>
<p>Table <a data-track="click" data-track-action="table anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#Tab6">6</a> demonstrates the average accuracy when setting the variational dropout in the same way as FiLM layers in GVCL-F. In particular, the variational dropout is plugged into each pre-activation. In this experiment, we use the variant of CVD the same as in Sect. <a data-track="click" data-track-action="section anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#Sec11">3.4</a>, in which the prior of CVD follows a Gaussian instead of log uniform, and the approximated posterior is <span class="mathjax-tex">\(q({\textbf{s}}^{(l)}_m)= \prod _{d=1}^D {\mathcal {N}}(\zeta ^{(l)}_{t,d},\alpha ^{(l)}_{t,d})\)</span>. The experiment is carried out on all five datasets Split MNIST, Permuted MNIST, Split CIFAR 100, Split CIFAR 10/100, and Omniglot.</p>
<p>As shown in Table <a data-track="click" data-track-action="table anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#Tab6">6</a>, the average accuracy of CVD shows slight superiority on all datasets compared to FiLM. The competitive results reflect that the properties of FiLM layers can be inherited by CVD when having the same setting as CVD can recover the shift and scale coefficient of FiLM layers. Furthermore, the noise in variational dropout adds up to the effectiveness of CVD.</p><p>To provide a more comprehensive view of the difference between CVD and FiLM layers, we also visualize the average accuracy through the incremental process and the final accuracy with respect to each task in Fig. <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#Fig11">11</a>. It can be seen that, for earlier tasks, there is no clear dominance between FiLM layer and CVD in terms of final accuracy. Nevertheless, regarding later tasks, CVD stands out with significant gaps over FiLM layers. We state that CVD has superior plasticity compared to the FiLM layer, thanks to the theoretical properties still preserved in this variant of CVD.</p><h3 class="c-article__sub-heading" id="Sec19"><span class="c-article-section__title-number">4.7 </span>Ablation study</h3><p>In this section, we point out the preference of CVD over regular dropout with fixed droprate. We also confirm the effectiveness of CVD in terms of practicality by reporting the training time and sensitivity to the additional hyperparameters.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-container-section="figure" data-test="figure" data-title="Fig. 12" id="figure-12"><figure><figcaption><b class="c-article-section__figure-caption" data-test="figure-caption-text" id="Fig12">Fig. 12</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-action="view figure" data-track-label="image" href="/article/10.1007/s10994-023-06487-7/figures/12" rel="nofollow"><picture><source srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10994-023-06487-7/MediaObjects/10994_2023_6487_Fig12_HTML.png?as=webp" type="image/webp"/><img alt="figure 12" aria-describedby="Fig12" height="395" loading="lazy" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10994-023-06487-7/MediaObjects/10994_2023_6487_Fig12_HTML.png" width="685"/></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-12-desc"><p>Comparison between adaptive droprate and fixed droprate</p></div></div><div class="u-text-right u-hide-print"><a aria-label="Full size image figure 12" class="c-article__pill-button" data-test="article-link" data-track="click" data-track-action="view figure" data-track-dest="link:Figure12 Full size image" data-track-label="button" href="/article/10.1007/s10994-023-06487-7/figures/12" rel="nofollow"><span>Full size image</span><svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16"><use xlink:href="#icon-eds-i-chevron-right-small" xmlns:xlink="http://www.w3.org/1999/xlink"></use></svg></a></div></figure></div><div class="c-article-section__figure js-c-reading-companion-figures-item" data-container-section="figure" data-test="figure" data-title="Fig. 13" id="figure-13"><figure><figcaption><b class="c-article-section__figure-caption" data-test="figure-caption-text" id="Fig13">Fig. 13</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-action="view figure" data-track-label="image" href="/article/10.1007/s10994-023-06487-7/figures/13" rel="nofollow"><picture><source srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10994-023-06487-7/MediaObjects/10994_2023_6487_Fig13_HTML.png?as=webp" type="image/webp"/><img alt="figure 13" aria-describedby="Fig13" height="198" loading="lazy" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10994-023-06487-7/MediaObjects/10994_2023_6487_Fig13_HTML.png" width="685"/></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-13-desc"><p>Average accuracy through tasks of adaptive droprate with different fixed droprate</p></div></div><div class="u-text-right u-hide-print"><a aria-label="Full size image figure 13" class="c-article__pill-button" data-test="article-link" data-track="click" data-track-action="view figure" data-track-dest="link:Figure13 Full size image" data-track-label="button" href="/article/10.1007/s10994-023-06487-7/figures/13" rel="nofollow"><span>Full size image</span><svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16"><use xlink:href="#icon-eds-i-chevron-right-small" xmlns:xlink="http://www.w3.org/1999/xlink"></use></svg></a></div></figure></div>
<h3 class="c-article__sub-heading">
<b>Adaptive droprate vs fixed droprate</b>
</h3>
<p>We perform analyses on the distinction between using dropout with fixed droprate and adaptive droprate. To highlight the effect of adaptive droprate against fixed one, we compare the difference in average accuracy on the two datasets, Split CIFAR 10 and Split CIFAR 100. All methods (VCL, EWC, UCL, GVCL, and GVCL-F) are taken into consideration.</p>
<p>As shown in Fig. <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#Fig12">12</a>, it is straightforward to see the superiority of adaptive droprate over keeping it fixed in continual learning. Dropout, in general, can help prevent the learned model from being stuck at the local optimum that only works well for a task in standard cases. Nevertheless, adding local variables to each task achieves even a more flexible adapting mechanism to function well on a large number of tasks. Therefore, CVD significantly outperforms regular dropouts on all the baselines.</p><p>Figure <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#Fig13">13</a> demonstrates the justification of adaptive dropout over different fixed droprate. For all fixed droprate, the accuracy suffers from serious degradation right after the first task. In the meantime, adaptive droprate can adjust itself to separated tasks, thus, effectively preventing the catastrophic forgetting phenomenon.</p><p>We emphasize that it is impractical to tune the dropout rate in continual learning scenarios manually. Instead, the dropout rate should be separately characterized for each task to adapt well to the sequence of tasks. Moreover, regarding the event where dropout tends to fall into the challenge of excessive uncertainty, using an adaptive drop rate would build more flexible control on the level of uncertainty. Based on these evidences, we claim that adaptive droprate would be much more practical and efficient for continual learning scenarios.</p>
<h3 class="c-article__sub-heading">
<b>Sensitivity of </b>
<span class="mathjax-tex">\(\kappa\)</span>
</h3>
<p>Figure <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#Fig14">14</a> shows that our method is robust against the impact of the additional hyperparameter <span class="mathjax-tex">\(\kappa\)</span>; the performance of our approach does not vary much when adjusting this hyperparameter, allowing us to achieve consistent results even with variations in this parameter values. The stability of our approach makes it easier to tune and can reduce the need for complex hyperparameter tuning processes. Therefore, it is reliable and could be suitable for a wide range of real-world applications where the ability to hyperparameter tuning processes is critical yet expensive.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-container-section="figure" data-test="figure" data-title="Fig. 14" id="figure-14"><figure><figcaption><b class="c-article-section__figure-caption" data-test="figure-caption-text" id="Fig14">Fig. 14</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-action="view figure" data-track-label="image" href="/article/10.1007/s10994-023-06487-7/figures/14" rel="nofollow"><picture><source srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10994-023-06487-7/MediaObjects/10994_2023_6487_Fig14_HTML.png?as=webp" type="image/webp"/><img alt="figure 14" aria-describedby="Fig14" height="463" loading="lazy" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10994-023-06487-7/MediaObjects/10994_2023_6487_Fig14_HTML.png" width="685"/></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-14-desc"><p>Sensitivity of <span class="mathjax-tex">\(\kappa\)</span></p></div></div><div class="u-text-right u-hide-print"><a aria-label="Full size image figure 14" class="c-article__pill-button" data-test="article-link" data-track="click" data-track-action="view figure" data-track-dest="link:Figure14 Full size image" data-track-label="button" href="/article/10.1007/s10994-023-06487-7/figures/14" rel="nofollow"><span>Full size image</span><svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16"><use xlink:href="#icon-eds-i-chevron-right-small" xmlns:xlink="http://www.w3.org/1999/xlink"></use></svg></a></div></figure></div><div class="c-article-table" data-container-section="table" data-test="inline-table" id="table-7"><figure><figcaption class="c-article-table__figcaption"><b data-test="table-caption" id="Tab7">Table 7 Training time with respect to each step</b></figcaption><div class="u-text-right u-hide-print"><a aria-label="Full size table 7" class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" href="/article/10.1007/s10994-023-06487-7/tables/7" rel="nofollow"><span>Full size table</span><svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16"><use xlink:href="#icon-eds-i-chevron-right-small" xmlns:xlink="http://www.w3.org/1999/xlink"></use></svg></a></div></figure></div>
<h3 class="c-article__sub-heading">
<b>Training time</b>
</h3>
<p>We evaluate the training time and report our measures in Table <a data-track="click" data-track-action="table anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#Tab7">7</a>. In particular, we use the NVIDIA A100-SXM4-80GB and record training time with respect to each minibatch, with 256 samples in each. It is clear that our strategy necessitates more training time due to the added overhead of training the added task-specific parameters. Nevertheless, the slight increase in computation costs is a negligible trade-off for the remarkable improvements in the performance of CVD.</p>
</div></div></section><section data-title="Conclusion"><div class="c-article-section" id="Sec20-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec20"><span class="c-article-section__title-number">5 </span>Conclusion</h2><div class="c-article-section__content" id="Sec20-content"><p>In this paper, we introduce a novel method that adds auxiliary local variables to improve regularization/prior-based methods. We consider CVD in both DNNs and BNNs instead of only DNNs as in Gaussian multiplicative noise. We find that CVD creates an adaptive mechanism to learn a new task and prevents forgetting the knowledge acquired from previous tasks. In particular, we theoretically demonstrate the advantages of CVD in terms of uncorrelated likelihoods between different data instances, correlated pre-activation, and an effective data-dependent regularization for continual learning. Our extensive experiments show that CVD improves the performance of regularization/prior-based methods with significant magnitudes.</p></div></div></section>
</div>
<section data-title="Availability of data and materials"><div class="c-article-section" id="availability-of-data-and-materials-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="availability-of-data-and-materials">Availability of data and materials</h2><div class="c-article-section__content" id="availability-of-data-and-materials-content">
<p>Not applicable.</p>
</div></div></section><section data-title="Notes"><div class="c-article-section" id="notes-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="notes">Notes</h2><div class="c-article-section__content" id="notes-content"><ol class="c-article-footnote c-article-footnote--listed"><li class="c-article-footnote--listed__item" data-counter="1." id="Fn1"><div class="c-article-footnote--listed__content"><p><a href="https://github.com/csm9493/UCL">https://github.com/csm9493/UCL</a>.</p></div></li><li class="c-article-footnote--listed__item" data-counter="2." id="Fn2"><div class="c-article-footnote--listed__content"><p><a href="https://github.com/yolky/gvcl">https://github.com/yolky/gvcl</a>.</p></div></li><li class="c-article-footnote--listed__item" data-counter="3." id="Fn3"><div class="c-article-footnote--listed__content"><p><a href="https://github.com/sangwon79/AGS-CL">https://github.com/sangwon79/AGS-CL</a>.</p></div></li></ol></div></div></section><div id="MagazineFulltextArticleBodySuffix"><section aria-labelledby="Bib1" data-title="References"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ul class="c-article-references" data-track-component="outbound reference" data-track-context="references section"><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR1">Ahn, H., Cha, S., Lee, D., &amp; Moon, T. (2019). Uncertainty-based continual learning with adaptive regularization. In <i>Advances in Neural Information Processing Systems</i> (pp. 4392–4402).</p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR2">Aljundi, R., Babiloni, F., Elhoseiny, M., Rohrbach, M., &amp; Tuytelaars, T. (2018). Memory aware synapses: Learning what (not) to forget. In <i>Proceedings of the European Conference on Computer Vision</i> (<i>ECCV</i>) (pp. 139–154).</p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR3">Bach, T. X., Anh, N. D., Linh, N. V., &amp; Than, K. (2023). Dynamic transformation of prior knowledge into Bayesian models for data streams. <i>IEEE Transactions on Knowledge and Data Engineering,</i> <i>35</i>(4), 3742–3750.</p><p class="c-article-references__links u-hide-print"><a aria-label="Article reference 3" data-doi="10.1109/TKDE.2021.3139469" data-track="click_references" data-track-action="article reference" data-track-item_id="10.1109/TKDE.2021.3139469" data-track-label="10.1109/TKDE.2021.3139469" data-track-value="article reference" href="https://doi.org/10.1109%2FTKDE.2021.3139469" rel="nofollow noopener">Article</a> 
    <a aria-label="Google Scholar reference 3" data-track="click_references" data-track-action="google scholar reference" data-track-item_id="link" data-track-label="link" data-track-value="google scholar reference" href="http://scholar.google.com/scholar_lookup?&amp;title=Dynamic%20transformation%20of%20prior%20knowledge%20into%20Bayesian%20models%20for%20data%20streams&amp;journal=IEEE%20Transactions%20on%20Knowledge%20and%20Data%20Engineering&amp;doi=10.1109%2FTKDE.2021.3139469&amp;volume=35&amp;issue=4&amp;pages=3742-3750&amp;publication_year=2023&amp;author=Bach%2CTX&amp;author=Anh%2CND&amp;author=Linh%2CNV&amp;author=Than%2CK" rel="nofollow noopener">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR4">Benzing, F. (2020). Understanding regularisation methods for continual learning. In <i>Workshop of Advances in Neural Information Processing Systems</i>.</p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR5">Blundell, C., Cornebise, J., Kavukcuoglu, K., &amp; Wierstra, D. (2015). Weight uncertainty in neural network. In <i>International conference on machine learning</i> (pp. 1613–1622). PMLR.</p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR6">Boluki, S., Ardywibowo, R., Dadaneh, S. Z., Zhou, M., &amp; Qian, X. (2020). Learnable Bernoulli dropout for bayesian deep learning. In <i>The International Conference on Artificial Intelligence and Statistics, AISTATS</i> (pp. 3905–3916).</p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR7">Cha, S., Hsu, H., Hwang, T., Calmon, F. P., &amp; Moon, T. (2021). CPR: Classifier-projection regularization for continual learning. In <i>9th International Conference on Learning Representations, ICLR</i>.</p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR8">Deng, L. (2012). The MNIST database of handwritten digit images for machine learning research [best of the web]. <i>IEEE Signal Processing Magazine,</i> <i>29</i>(6), 141–142.</p><p class="c-article-references__links u-hide-print"><a aria-label="Article reference 8" data-doi="10.1109/MSP.2012.2211477" data-track="click_references" data-track-action="article reference" data-track-item_id="10.1109/MSP.2012.2211477" data-track-label="10.1109/MSP.2012.2211477" data-track-value="article reference" href="https://doi.org/10.1109%2FMSP.2012.2211477" rel="nofollow noopener">Article</a> 
    <a aria-label="Google Scholar reference 8" data-track="click_references" data-track-action="google scholar reference" data-track-item_id="link" data-track-label="link" data-track-value="google scholar reference" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20MNIST%20database%20of%20handwritten%20digit%20images%20for%20machine%20learning%20research%20%5Bbest%20of%20the%20web%5D&amp;journal=IEEE%20Signal%20Processing%20Magazine&amp;doi=10.1109%2FMSP.2012.2211477&amp;volume=29&amp;issue=6&amp;pages=141-142&amp;publication_year=2012&amp;author=Deng%2CL" rel="nofollow noopener">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR9">De Lange, M., Aljundi, R., Masana, M., Parisot, S., Jia, X., Leonardis, A., Slabaugh, G., &amp; Tuytelaars, T. (2021). A continual learning survey: Defying forgetting in classification tasks. <i>IEEE Transactions on Pattern Analysis and Machine Intelligence</i></p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR10">Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., &amp;  Houlsby, N. (2020). An image is worth 16x16 words: Transformers for image recognition at scale. In <i>International conference on learning representations</i></p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR11">Farquhar, S., &amp; Gal, Y. (2018). A unifying bayesian view of continual learning. In <i>The Bayesian deep learning workshop at neural information processing systems</i></p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR12">Gal, Y., Hron, J., &amp; Kendall, A. (2017). Concrete dropout. In <i>Advances in Neural Information Processing Systems</i> (pp. 3581–3590).</p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR13">Ghahramani, Z., &amp; Attias, H. (2000). Online variational Bayesian learning. In <i>Slides from talk presented at NIPS workshop on online learning</i>.</p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR14">Goodfellow, I. J., Mirza, M., Xiao, D., Courville, A., &amp; Bengio, Y. (2013). An empirical investigation of catastrophic forgetting in gradient-based neural networks. arXiv preprint <a data-track="click_references" data-track-action="external reference" data-track-label="http://arxiv.org/abs/1312.6211" data-track-value="external reference" href="http://arxiv.org/abs/1312.6211">arXiv:1312.6211</a></p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR15">Graves, A. (2011). Practical variational inference for neural networks. In <i>Advances in Neural Information Processing Systems</i> (pp. 2348–2356). Citeseer.</p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR16">Ha, C., Tran, V.-D., Van, L. N., &amp; Than, K. (2019). Eliminating overfitting of probabilistic topic models on short and noisy text: The role of dropout. <i>International Journal of Approximate Reasoning,</i> <i>112</i>, 85–104.</p><p class="c-article-references__links u-hide-print"><a aria-label="Article reference 16" data-doi="10.1016/j.ijar.2019.05.010" data-track="click_references" data-track-action="article reference" data-track-item_id="10.1016/j.ijar.2019.05.010" data-track-label="10.1016/j.ijar.2019.05.010" data-track-value="article reference" href="https://doi.org/10.1016%2Fj.ijar.2019.05.010" rel="nofollow noopener">Article</a> 
    <a aria-label="Google Scholar reference 16" data-track="click_references" data-track-action="google scholar reference" data-track-item_id="link" data-track-label="link" data-track-value="google scholar reference" href="http://scholar.google.com/scholar_lookup?&amp;title=Eliminating%20overfitting%20of%20probabilistic%20topic%20models%20on%20short%20and%20noisy%20text%3A%20The%20role%20of%20dropout&amp;journal=International%20Journal%20of%20Approximate%20Reasoning&amp;doi=10.1016%2Fj.ijar.2019.05.010&amp;volume=112&amp;pages=85-104&amp;publication_year=2019&amp;author=Ha%2CC&amp;author=Tran%2CV-D&amp;author=Van%2CLN&amp;author=Than%2CK" rel="nofollow noopener">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR17">Hendrycks, D., Basart, S., Mu, N., Kadavath, S., Wang, F., Dorundo, E., Desai, R., Zhu, T., Parajuli, S., Guo, M., Song, D., Steinhardt, J., &amp; Gilmer, J. (2021). The many faces of robustness: A critical analysis of out-of-distribution generalization. In <i>Proceedings of the IEEE/CVF international conference on computer vision</i> (pp. 8340–8349).</p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR18">Henning, C., Cervera, M., D’Angelo, F., Von Oswald, J., Traber, R., Ehret, B., Kobayashi, S., Grewe, B. F., &amp; Sacramento, J. (2021). Posterior meta-replay for continual learning. In <i>Advances in neural information processing systems</i> (Vol. 34).</p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR19">Jung, S., Ahn, H., Cha, S., &amp; Moon, T. (2020). Continual learning with node-importance based adaptive group sparse regularization. In <i>Advances in neural information processing systems</i></p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR20">Kingma, D. P., Salimans, T., &amp; Welling, M. (2015). Variational dropout and the local reparameterization trick. <i>Advances in Neural Information Processing Systems,</i> <i>28</i>, 2575–2583.</p><p class="c-article-references__links u-hide-print"><a aria-label="Google Scholar reference 20" data-track="click_references" data-track-action="google scholar reference" data-track-item_id="link" data-track-label="link" data-track-value="google scholar reference" href="http://scholar.google.com/scholar_lookup?&amp;title=Variational%20dropout%20and%20the%20local%20reparameterization%20trick&amp;journal=Advances%20in%20Neural%20Information%20Processing%20Systems&amp;volume=28&amp;pages=2575-2583&amp;publication_year=2015&amp;author=Kingma%2CDP&amp;author=Salimans%2CT&amp;author=Welling%2CM" rel="nofollow noopener">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR21">Kingma, D. P., &amp; Welling, M. (2014). Auto-encoding variational bayes. In: Bengio, Y., LeCun, Y. (eds.) <i>2nd international conference on learning representations, ICLR</i>.</p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR22">Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Desjardins, G., Rusu, A. A., Milan, K., Quan, J., Ramalho, T., Grabska-Barwinska, A., et al. (2017). Overcoming catastrophic forgetting in neural networks. <i>Proceedings of the National Academy of Sciences,</i> <i>114</i>(13), 3521–3526.</p><p class="c-article-references__links u-hide-print"><a aria-label="Article reference 22" data-doi="10.1073/pnas.1611835114" data-track="click_references" data-track-action="article reference" data-track-item_id="10.1073/pnas.1611835114" data-track-label="10.1073/pnas.1611835114" data-track-value="article reference" href="https://doi.org/10.1073%2Fpnas.1611835114" rel="nofollow noopener">Article</a> 
    <a aria-label="MathSciNet reference 22" data-track="click_references" data-track-action="mathscinet reference" data-track-item_id="link" data-track-label="link" data-track-value="mathscinet reference" href="http://www.ams.org/mathscinet-getitem?mr=3635506" rel="nofollow noopener">MathSciNet</a> 
    <a aria-label="Google Scholar reference 22" data-track="click_references" data-track-action="google scholar reference" data-track-item_id="link" data-track-label="link" data-track-value="google scholar reference" href="http://scholar.google.com/scholar_lookup?&amp;title=Overcoming%20catastrophic%20forgetting%20in%20neural%20networks&amp;journal=Proceedings%20of%20the%20National%20Academy%20of%20Sciences&amp;doi=10.1073%2Fpnas.1611835114&amp;volume=114&amp;issue=13&amp;pages=3521-3526&amp;publication_year=2017&amp;author=Kirkpatrick%2CJ&amp;author=Pascanu%2CR&amp;author=Rabinowitz%2CN&amp;author=Veness%2CJ&amp;author=Desjardins%2CG&amp;author=Rusu%2CAA&amp;author=Milan%2CK&amp;author=Quan%2CJ&amp;author=Ramalho%2CT&amp;author=Grabska-Barwinska%2CA" rel="nofollow noopener">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR23">Krizhevsky, A. (2009). <i>Learning multiple layers of features from tiny images</i>. Technical report, University of Toronto.</p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR24">Li, Z., &amp; Hoiem, D. (2017). Learning without forgetting. <i>IEEE Transactions on Pattern Analysis and Machine Intelligence,</i> <i>40</i>(12), 2935–2947.</p><p class="c-article-references__links u-hide-print"><a aria-label="Article reference 24" data-doi="10.1109/TPAMI.2017.2773081" data-track="click_references" data-track-action="article reference" data-track-item_id="10.1109/TPAMI.2017.2773081" data-track-label="10.1109/TPAMI.2017.2773081" data-track-value="article reference" href="https://doi.org/10.1109%2FTPAMI.2017.2773081" rel="nofollow noopener">Article</a> 
    <a aria-label="Google Scholar reference 24" data-track="click_references" data-track-action="google scholar reference" data-track-item_id="link" data-track-label="link" data-track-value="google scholar reference" href="http://scholar.google.com/scholar_lookup?&amp;title=Learning%20without%20forgetting&amp;journal=IEEE%20Transactions%20on%20Pattern%20Analysis%20and%20Machine%20Intelligence&amp;doi=10.1109%2FTPAMI.2017.2773081&amp;volume=40&amp;issue=12&amp;pages=2935-2947&amp;publication_year=2017&amp;author=Li%2CZ&amp;author=Hoiem%2CD" rel="nofollow noopener">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR25">Van Linh, N., Bach, T. X., &amp; Than, K. (2022). A graph convolutional topic model for short and noisy text streams. <i>Neurocomputing,</i> <i>468</i>, 345–359.</p><p class="c-article-references__links u-hide-print"><a aria-label="Article reference 25" data-doi="10.1016/j.neucom.2021.10.047" data-track="click_references" data-track-action="article reference" data-track-item_id="10.1016/j.neucom.2021.10.047" data-track-label="10.1016/j.neucom.2021.10.047" data-track-value="article reference" href="https://doi.org/10.1016%2Fj.neucom.2021.10.047" rel="nofollow noopener">Article</a> 
    <a aria-label="Google Scholar reference 25" data-track="click_references" data-track-action="google scholar reference" data-track-item_id="link" data-track-label="link" data-track-value="google scholar reference" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20graph%20convolutional%20topic%20model%20for%20short%20and%20noisy%20text%20streams&amp;journal=Neurocomputing&amp;doi=10.1016%2Fj.neucom.2021.10.047&amp;volume=468&amp;pages=345-359&amp;publication_year=2022&amp;author=Linh%2CN&amp;author=Bach%2CTX&amp;author=Than%2CK" rel="nofollow noopener">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR26">Liu, Y., Dong, W., Zhang, L., Gong, D., &amp; Shi, Q. (2019). Variational bayesian dropout with a hierarchical prior. In <i>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</i> (pp. 7124–7133).</p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR27">Loo, N., Swaroop, S., &amp; Turner, R. E. (2021). Generalized variational continual learning. In <i>International conference on learning representation</i></p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR28">MacKay, D. J. C. (1992). A practical Bayesian framework for backpropagation networks. <i>Neural Computation,</i> <i>4</i>(3), 448–472.</p><p class="c-article-references__links u-hide-print"><a aria-label="Article reference 28" data-doi="10.1162/neco.1992.4.3.448" data-track="click_references" data-track-action="article reference" data-track-item_id="10.1162/neco.1992.4.3.448" data-track-label="10.1162/neco.1992.4.3.448" data-track-value="article reference" href="https://doi.org/10.1162%2Fneco.1992.4.3.448" rel="nofollow noopener">Article</a> 
    <a aria-label="Google Scholar reference 28" data-track="click_references" data-track-action="google scholar reference" data-track-item_id="link" data-track-label="link" data-track-value="google scholar reference" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20practical%20Bayesian%20framework%20for%20backpropagation%20networks&amp;journal=Neural%20Computation&amp;doi=10.1162%2Fneco.1992.4.3.448&amp;volume=4&amp;issue=3&amp;pages=448-472&amp;publication_year=1992&amp;author=MacKay%2CDJC" rel="nofollow noopener">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR29">Mirzadeh, S., Farajtabar, M., Pascanu, R., &amp; Ghasemzadeh, H. (2020). Understanding the role of training regimes in continual learning. In <i>Advances in neural information processing systems</i></p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR30">Mirzadeh, S. I., Farajtabar, M., &amp; Ghasemzadeh, H. (2020). Dropout as an implicit gating mechanism for continual learning. In <i>Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops</i> (pp. 232–233).</p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR31">Molchanov, D., Ashukha, A., &amp; Vetrov, D. (2017). Variational dropout sparsifies deep neural networks. In <i>International conference on machine learning</i> (pp. 2498–2507).</p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR32">Murphy, K. P. (2012). <i>Machine learning: A probabilistic perspective</i>. Cambridge: MIT Press.</p><p class="c-article-references__links u-hide-print"><a aria-label="Google Scholar reference 32" data-track="click_references" data-track-action="google scholar reference" data-track-item_id="link" data-track-label="link" data-track-value="google scholar reference" href="http://scholar.google.com/scholar_lookup?&amp;title=Machine%20learning%3A%20A%20probabilistic%20perspective&amp;publication_year=2012&amp;author=Murphy%2CKP" rel="nofollow noopener">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR33">Neal, R. M. (1996). <i>Bayesian learning for neural networks</i>. Berlin: Springer.</p><p class="c-article-references__links u-hide-print"><a aria-label="Book reference 33" data-doi="10.1007/978-1-4612-0745-0" data-track="click_references" data-track-action="book reference" data-track-item_id="10.1007/978-1-4612-0745-0" data-track-label="10.1007/978-1-4612-0745-0" data-track-value="book reference" href="https://link.springer.com/doi/10.1007/978-1-4612-0745-0" rel="noopener">Book</a> 
    <a aria-label="Google Scholar reference 33" data-track="click_references" data-track-action="google scholar reference" data-track-item_id="link" data-track-label="link" data-track-value="google scholar reference" href="http://scholar.google.com/scholar_lookup?&amp;title=Bayesian%20learning%20for%20neural%20networks&amp;doi=10.1007%2F978-1-4612-0745-0&amp;publication_year=1996&amp;author=Neal%2CRM" rel="nofollow noopener">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR34">Nguyen, T., Mai, T., Nguyen, N., Van, L. N., &amp; Than, K. (2022b). Balancing stability and plasticity when learning topic models from short and noisy text streams. <i>Neurocomputing,</i> <i>505</i>, 30–43.</p><p class="c-article-references__links u-hide-print"><a aria-label="Article reference 34" data-doi="10.1016/j.neucom.2022.07.019" data-track="click_references" data-track-action="article reference" data-track-item_id="10.1016/j.neucom.2022.07.019" data-track-label="10.1016/j.neucom.2022.07.019" data-track-value="article reference" href="https://doi.org/10.1016%2Fj.neucom.2022.07.019" rel="nofollow noopener">Article</a> 
    <a aria-label="Google Scholar reference 34" data-track="click_references" data-track-action="google scholar reference" data-track-item_id="link" data-track-label="link" data-track-value="google scholar reference" href="http://scholar.google.com/scholar_lookup?&amp;title=Balancing%20stability%20and%20plasticity%20when%20learning%20topic%20models%20from%20short%20and%20noisy%20text%20streams&amp;journal=Neurocomputing&amp;doi=10.1016%2Fj.neucom.2022.07.019&amp;volume=505&amp;pages=30-43&amp;publication_year=2022&amp;author=Nguyen%2CT&amp;author=Mai%2CT&amp;author=Nguyen%2CN&amp;author=Van%2CLN&amp;author=Than%2CK" rel="nofollow noopener">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR35">Nguyen, S., Nguyen, D., Nguyen, K., Than, K., Bui, H., &amp; Ho, N. (2021). Structured dropout variational inference for Bayesian neural networks. <i>Advances in Neural Information Processing Systems,</i> <i>34</i>, 15188–15202.</p><p class="c-article-references__links u-hide-print"><a aria-label="Google Scholar reference 35" data-track="click_references" data-track-action="google scholar reference" data-track-item_id="link" data-track-label="link" data-track-value="google scholar reference" href="http://scholar.google.com/scholar_lookup?&amp;title=Structured%20dropout%20variational%20inference%20for%20Bayesian%20neural%20networks&amp;journal=Advances%20in%20Neural%20Information%20Processing%20Systems&amp;volume=34&amp;pages=15188-15202&amp;publication_year=2021&amp;author=Nguyen%2CS&amp;author=Nguyen%2CD&amp;author=Nguyen%2CK&amp;author=Than%2CK&amp;author=Bui%2CH&amp;author=Ho%2CN" rel="nofollow noopener">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR36">Nguyen, H., Pham, H., Nguyen, S., Van Linh, N., &amp; Than, K. (2022a). Adaptive infinite dropout for noisy and sparse data streams. <i>Machine Learning,</i> <i>111</i>(8), 3025–3060.</p><p class="c-article-references__links u-hide-print"><a aria-label="Article reference 36" data-doi="10.1007/s10994-022-06169-w" data-track="click_references" data-track-action="article reference" data-track-item_id="10.1007/s10994-022-06169-w" data-track-label="10.1007/s10994-022-06169-w" data-track-value="article reference" href="https://link.springer.com/doi/10.1007/s10994-022-06169-w" rel="noopener">Article</a> 
    <a aria-label="MathSciNet reference 36" data-track="click_references" data-track-action="mathscinet reference" data-track-item_id="link" data-track-label="link" data-track-value="mathscinet reference" href="http://www.ams.org/mathscinet-getitem?mr=4460135" rel="nofollow noopener">MathSciNet</a> 
    <a aria-label="Google Scholar reference 36" data-track="click_references" data-track-action="google scholar reference" data-track-item_id="link" data-track-label="link" data-track-value="google scholar reference" href="http://scholar.google.com/scholar_lookup?&amp;title=Adaptive%20infinite%20dropout%20for%20noisy%20and%20sparse%20data%20streams&amp;journal=Machine%20Learning&amp;doi=10.1007%2Fs10994-022-06169-w&amp;volume=111&amp;issue=8&amp;pages=3025-3060&amp;publication_year=2022&amp;author=Nguyen%2CH&amp;author=Pham%2CH&amp;author=Nguyen%2CS&amp;author=Linh%2CN&amp;author=Than%2CK" rel="nofollow noopener">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR37">Nguyen, C. V., Li, Y., Bui, T. D., &amp; Turner, R. E. (2018). Variational continual learning. In <i>International conference on learning representation</i>.</p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR38">Nguyen, V.-S., Nguyen, D.-T., Van, L.N., &amp; Than, K. (2019). Infinite dropout for training bayesian models from data streams. In <i>IEEE international conference on big data</i> (<i>Big Data</i>) (pp. 125–134). IEEE</p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR39">Oh, C., Adamczewski, K., &amp; Park, M. (2020). Radial and directional posteriors for Bayesian deep learning. In <i>The thirty-fourth conference on artificial intelligence, AAAI</i> (pp. 5298–5305)</p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR40">Paisley, J. W., Blei, D. M., &amp; Jordan, M. I. (2012). Variational bayesian inference with stochastic search. In <i>Proceedings of the 29th international conference on machine learning, ICML</i></p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR41">Phan, H., Tuan, A. P., Nguyen, S., Linh, N. V., &amp; Than, K. (2022). Reducing catastrophic forgetting in neural networks via Gaussian mixture approximation. In <i>Pacific-Asia Conference on Knowledge Discovery and Data Mining</i> (pp. 106–117). Springer: Berlin</p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR42">Sato, M.-A. (2001). Online model selection based on the variational bayes. <i>Neural Computation,</i> <i>13</i>(7), 1649–1681.</p><p class="c-article-references__links u-hide-print"><a aria-label="Article reference 42" data-doi="10.1162/089976601750265045" data-track="click_references" data-track-action="article reference" data-track-item_id="10.1162/089976601750265045" data-track-label="10.1162/089976601750265045" data-track-value="article reference" href="https://doi.org/10.1162%2F089976601750265045" rel="nofollow noopener">Article</a> 
    <a aria-label="Google Scholar reference 42" data-track="click_references" data-track-action="google scholar reference" data-track-item_id="link" data-track-label="link" data-track-value="google scholar reference" href="http://scholar.google.com/scholar_lookup?&amp;title=Online%20model%20selection%20based%20on%20the%20variational%20bayes&amp;journal=Neural%20Computation&amp;doi=10.1162%2F089976601750265045&amp;volume=13&amp;issue=7&amp;pages=1649-1681&amp;publication_year=2001&amp;author=Sato%2CM-A" rel="nofollow noopener">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR43">Shi, G., Chen, J., Zhang, W., Zhan, L.-M., &amp; Wu, X.-M. (2021). Overcoming catastrophic forgetting in incremental few-shot learning by finding flat minima. <i>Advances in Neural Information Processing Systems,</i> <i>34</i>, 6747–6761.</p><p class="c-article-references__links u-hide-print"><a aria-label="Google Scholar reference 43" data-track="click_references" data-track-action="google scholar reference" data-track-item_id="link" data-track-label="link" data-track-value="google scholar reference" href="http://scholar.google.com/scholar_lookup?&amp;title=Overcoming%20catastrophic%20forgetting%20in%20incremental%20few-shot%20learning%20by%20finding%20flat%20minima&amp;journal=Advances%20in%20Neural%20Information%20Processing%20Systems&amp;volume=34&amp;pages=6747-6761&amp;publication_year=2021&amp;author=Shi%2CG&amp;author=Chen%2CJ&amp;author=Zhang%2CW&amp;author=Zhan%2CL-M&amp;author=Wu%2CX-M" rel="nofollow noopener">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR44">Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., &amp; Salakhutdinov, R. (2014). Dropout: A simple way to prevent neural networks from overfitting. <i>The Journal of Machine Learning Research,</i> <i>15</i>(1), 1929–1958.</p><p class="c-article-references__links u-hide-print"><a aria-label="MathSciNet reference 44" data-track="click_references" data-track-action="mathscinet reference" data-track-item_id="link" data-track-label="link" data-track-value="mathscinet reference" href="http://www.ams.org/mathscinet-getitem?mr=3231592" rel="nofollow noopener">MathSciNet</a> 
    <a aria-label="Google Scholar reference 44" data-track="click_references" data-track-action="google scholar reference" data-track-item_id="link" data-track-label="link" data-track-value="google scholar reference" href="http://scholar.google.com/scholar_lookup?&amp;title=Dropout%3A%20A%20simple%20way%20to%20prevent%20neural%20networks%20from%20overfitting&amp;journal=The%20Journal%20of%20Machine%20Learning%20Research&amp;volume=15&amp;issue=1&amp;pages=1929-1958&amp;publication_year=2014&amp;author=Srivastava%2CN&amp;author=Hinton%2CG&amp;author=Krizhevsky%2CA&amp;author=Sutskever%2CI&amp;author=Salakhutdinov%2CR" rel="nofollow noopener">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR45">Swaroop, S., Nguyen, C.V., Bui, T. D., &amp; Turner, R. E. (2018). Improving and understanding variational continual learning. In <i>NeurIPS Continual Learning Workshop</i>.</p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR46">Swiatkowski, J., Roth, K., Veeling, B., Tran, L., Dillon, J., Snoek, J., Mandt, S., Salimans, T., Jenatton, R., &amp; Nowozin, S. (2020). The k-tied normal distribution: A compact parameterization of Gaussian mean field posteriors in Bayesian neural networks. In <i>International conference on machine learning</i> (pp. 9289–9299). PMLR.</p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR47">Van, L.N., Hai, N.L., Pham, H., &amp; Than, K. (2022). Auxiliary local variables for improving regularization/prior approach in continual learning. In  <i>Pacific-Asia conference on knowledge discovery and data mining</i> (pp. 16–28). Springer: Berlin</p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR48">Van de Ven, G. M., &amp; Tolias, A. S. (2019). Three scenarios for continual learning. In <i>NeurIPS—Continual learning workshop</i></p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR49">Wah, C., Branson, S., Welinder, P., Perona, P., &amp; Belongie, S. (2011). The Caltech-UCSD Birds-200-2011 dataset.</p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR50">Wei, C., Kakade, S., &amp; Ma, T. (2020). The implicit and explicit regularization effects of dropout. In <i>International conference on machine learning</i> (pp. 10181–10192). PMLR.</p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR51">Yin, D., Farajtabar, M., &amp; Li, A. (2020). Sola: Continual learning with second-order loss approximation. In <i>Workshop of advances in neural information processing systems</i></p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR52">Zenke, F., Poole, B., &amp; Ganguli, S. (2017). Continual learning through synaptic intelligence. <i>Proceedings of Machine Learning Research,</i> <i>70</i>, 3987.</p><p class="c-article-references__links u-hide-print"><a aria-label="Google Scholar reference 52" data-track="click_references" data-track-action="google scholar reference" data-track-item_id="link" data-track-label="link" data-track-value="google scholar reference" href="http://scholar.google.com/scholar_lookup?&amp;title=Continual%20learning%20through%20synaptic%20intelligence&amp;journal=Proceedings%20of%20Machine%20Learning%20Research&amp;volume=70&amp;publication_year=2017&amp;author=Zenke%2CF&amp;author=Poole%2CB&amp;author=Ganguli%2CS" rel="nofollow noopener">
                    Google Scholar</a> 
                </p></li></ul><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="https://citation-needed.springer.com/v2/references/10.1007/s10994-023-06487-7?format=refman&amp;flavour=references" rel="nofollow">Download references<svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16"><use xlink:href="#icon-eds-i-download-medium" xmlns:xlink="http://www.w3.org/1999/xlink"></use></svg></a></p></div></div></div></section></div><section data-title="Funding"><div class="c-article-section" id="Fun-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Fun">Funding</h2><div class="c-article-section__content" id="Fun-content"><p> This research has been supported in part by the NSF grant CNS-1747798 to the IUCRC Center for Big Learning and the NSF grant # 2239570.</p></div></div></section><section aria-labelledby="author-information" data-title="Author information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><span class="c-article-author-information__subtitle u-visually-hidden" id="author-notes">Author notes</span><ol class="c-article-author-information__list"><li class="c-article-author-information__item" id="na1"><p>Nam Le Hai and Trang Nguyen have contributed equally to this work.</p></li></ol><h3 class="c-article__sub-heading" id="affiliations">Authors and Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">School of Information and Communication Technology, Hanoi University of Science and Technology, Hanoi, Vietnam</p><p class="c-article-author-affiliation__authors-list">Nam Le Hai, Trang Nguyen, Linh Ngo Van &amp; Khoat Than</p></li><li id="Aff2"><p class="c-article-author-affiliation__address">University of Oregon, Eugene, USA</p><p class="c-article-author-affiliation__authors-list">Thien Huu Nguyen</p></li><li id="Aff3"><p class="c-article-author-affiliation__address">FPT Software AI Center, Hanoi, Vietnam</p><p class="c-article-author-affiliation__authors-list">Nam Le Hai</p></li></ol><div class="u-js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Nam_Le-Hai-Aff1-Aff3"><span class="c-article-authors-search__title u-h3 js-search-name">Nam Le Hai</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" href="/search?dc.creator=Nam%20Le%20Hai" rel="nofollow">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" data-track="click" data-track-action="author link - pubmed" data-track-label="link" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Nam%20Le%20Hai" rel="nofollow">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" data-track="click" data-track-action="author link - scholar" data-track-label="link" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Nam%20Le%20Hai%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" rel="nofollow">Google Scholar</a></span></p></div></div></li><li id="auth-Trang-Nguyen-Aff1"><span class="c-article-authors-search__title u-h3 js-search-name">Trang Nguyen</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" href="/search?dc.creator=Trang%20Nguyen" rel="nofollow">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" data-track="click" data-track-action="author link - pubmed" data-track-label="link" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Trang%20Nguyen" rel="nofollow">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" data-track="click" data-track-action="author link - scholar" data-track-label="link" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Trang%20Nguyen%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" rel="nofollow">Google Scholar</a></span></p></div></div></li><li id="auth-Linh_Ngo-Van-Aff1"><span class="c-article-authors-search__title u-h3 js-search-name">Linh Ngo Van</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" href="/search?dc.creator=Linh%20Ngo%20Van" rel="nofollow">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" data-track="click" data-track-action="author link - pubmed" data-track-label="link" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Linh%20Ngo%20Van" rel="nofollow">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" data-track="click" data-track-action="author link - scholar" data-track-label="link" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Linh%20Ngo%20Van%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" rel="nofollow">Google Scholar</a></span></p></div></div></li><li id="auth-Thien_Huu-Nguyen-Aff2"><span class="c-article-authors-search__title u-h3 js-search-name">Thien Huu Nguyen</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" href="/search?dc.creator=Thien%20Huu%20Nguyen" rel="nofollow">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" data-track="click" data-track-action="author link - pubmed" data-track-label="link" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Thien%20Huu%20Nguyen" rel="nofollow">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" data-track="click" data-track-action="author link - scholar" data-track-label="link" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Thien%20Huu%20Nguyen%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" rel="nofollow">Google Scholar</a></span></p></div></div></li><li id="auth-Khoat-Than-Aff1"><span class="c-article-authors-search__title u-h3 js-search-name">Khoat Than</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" href="/search?dc.creator=Khoat%20Than" rel="nofollow">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" data-track="click" data-track-action="author link - pubmed" data-track-label="link" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Khoat%20Than" rel="nofollow">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" data-track="click" data-track-action="author link - scholar" data-track-label="link" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Khoat%20Than%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" rel="nofollow">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="contributions">Contributions</h3><p>The contributions of each author are presented as follows: NLH: Methodology, Software, Validation, Formal analysis, Visualization, Writing—original draft, Investigation. TN: Methodology, Software, Validation, Formal analysis, Writing—original draft, Investigation. LNV: Conceptualization, Methodology, Validation, Formal analysis, Writing—original draft, Visualization, Investigation, Project administration. THN: Methodology, Validation, Formal analysis, Writing—review, Visualization, Supervision. KT: Methodology, Validation, Formal analysis, Writing—review, Supervision, Funding acquisition.</p><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a href="mailto:linhnv@soict.hust.edu.vn" id="corresp-c1">Linh Ngo Van</a>.</p></div></div></section><section data-title="Ethics declarations"><div class="c-article-section" id="ethics-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="ethics">Ethics declarations</h2><div class="c-article-section__content" id="ethics-content">
<h3 class="c-article__sub-heading" id="FPar3">Conflicts of interest</h3>
<p>The authors declare that they have no competing interests.</p>
<h3 class="c-article__sub-heading" id="FPar4">Code availability</h3>
<p>The implementation for CVD can be found in <a href="https://github.com/nguyenvuthientrang/CVD">https://github.com/nguyenvuthientrang/CVD</a>.</p>
<h3 class="c-article__sub-heading" id="FPar5">Ethics approval</h3>
<p>Not applicable.</p>
<h3 class="c-article__sub-heading" id="FPar6">Consent to participate</h3>
<p>Not applicable.</p>
<h3 class="c-article__sub-heading" id="FPar7">Consent for publication</h3>
<p>Not applicable.</p>
</div></div></section><section data-title="Additional information"><div class="c-article-section" id="additional-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="additional-information">Additional information</h2><div class="c-article-section__content" id="additional-information-content"><p>Editor: Gustavo Batista.</p><h3 class="c-article__sub-heading">Publisher's Note</h3><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p><p>A part of this work appears in Van et al. (<a aria-label="Reference 2022" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR47" id="ref-link-section-d212779005e419" title="Van, L.N., Hai, N.L., Pham, H., &amp; Than, K. (2022). Auxiliary local variables for improving regularization/prior approach in continual learning. In  Pacific-Asia conference on knowledge discovery and data mining (pp. 16–28). Springer: Berlin">2022</a>).</p></div></div></section><section aria-labelledby="appendices"><div class="c-article-section" id="appendices-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="appendices">Appendices</h2><div class="c-article-section__content" id="appendices-content"><h3 class="c-article__sub-heading" id="App1">Appendix A: Auxiliary local variables for uncertainty regularized continual learning (UCL)</h3><p>In this section, we review UCL (Ahn et al., <a aria-label="Reference 2019" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR1" id="ref-link-section-d212779005e30721" title="Ahn, H., Cha, S., Lee, D., &amp; Moon, T. (2019). Uncertainty-based continual learning with adaptive regularization. In Advances in Neural Information Processing Systems (pp. 4392–4402).">2019</a>) which is one of the state-of-the-art methods for continual learning and how to apply CVD to this method. UCL uses the same the likelihood term of VCL, but reinterprets the KL term of VCL to improve this term. The KL term is rewritten as follows:</p><div class="c-article-equation" id="Equ2"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} \frac{1}{2} \sum _{l=1}^L \left[ \left\| \frac{\varvec{\mu }_t^{(l)} - \varvec{\mu }_{t-1}^{(l)}}{\varvec{\sigma }_{t-1}^{(l)}}\right\| _2^2 + {\textbf{1}}^\intercal \left\{ \left( \frac{\varvec{\sigma }_{t}^{(l)}}{\varvec{\sigma }_{t-1}^{(l)}} \right) ^2 - \log \left( \frac{\varvec{\sigma }_{t}^{(l)}}{\varvec{\sigma }_{t-1}^{(l)}} \right) ^2 \right\} \right] \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (A1)
                </div></div><p>where <i>l</i> is the layer index of the neural network. UCL improves VCL by defining node importance and then adding two regularization terms. Based on the node importance, the first term limits the change of weights related to important nodes and the other makes weights more active to learn new tasks. In detail, UCL constrains the standard deviation of all weights connecting to the same node <i>u</i> in layer <i>l</i> to have the same value <span class="mathjax-tex">\(\varvec{\sigma }_{u}^{(l)}\)</span> and then uses this parameter to measure node importance. Moreover, it modifies this KL term to freeze the weights related to important nodes:</p><div class="c-article-equation" id="Equ3"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} KL&amp;= \sum _{l=1}^{L}\Big [\Big ( \frac{1}{2}\Big \Vert \mathbf {\Lambda }^{(l)}\odot (\varvec{\mu }_{t}^{(l)}-\varvec{\mu }_{t-1}^{(l)})\Big \Vert _2^2 \nonumber \\&amp;+ (\varvec{\sigma }_{\text {init}}^{(l)})^2 \Big \Vert \Big (\frac{\varvec{\mu }_{t-1}^{(l)}}{\varvec{\sigma }_{t-1}^{(l)}}\Big )^{2}\odot (\varvec{\mu }_{t}^{(l)}-\varvec{\mu }_{t-1}^{(l)}) \Big \Vert _1\Big )\nonumber \\&amp;+ \frac{\beta }{2}{\textbf{1}}^\top \Big \{\Big (\frac{\varvec{\sigma }_t^{(l)}}{\varvec{\sigma }_{t-1}^{(l)}}\Big )^2-\log \Big (\frac{\varvec{\sigma }_t^{(l)}}{\varvec{\sigma }_{t-1}^{(l)}}\Big )^2 + (\varvec{\sigma }_t^{(l)})^2-\log (\varvec{\sigma }_t^{(l)})^2\Big \}\Big ] \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (A2)
                </div></div><p>where <span class="mathjax-tex">\(\varvec{\sigma }^{(l)}_{\text {init}}\)</span> is the initial standard deviation hyperparameter for all weights on the <i>l</i>-th layer. The matrix <span class="mathjax-tex">\(\mathbf {\Lambda }^{(l)}_{uv}\triangleq \max \Big \{\frac{\varvec{\sigma }_{\text {init}}^{(l)}}{\varvec{\sigma }_{t-1,u}^{(l)}}, \frac{\varvec{\sigma }_{\text {init}}^{(l-1)}}{\varvec{\sigma }_{t-1,v}^{(l-1)}}\Big \}\)</span> defines the regularization strength for the weight <span class="mathjax-tex">\(\varvec{\mu }_{t,uv}^{(l)}\)</span>.</p>
<h3 class="c-article__sub-heading">
<b>CVD for UCL</b>
</h3>
<p>We also add auxiliary variables to the original model and then maximize the log likelihood: <span class="mathjax-tex">\(\log p({\textbf{Y}}_t \vert {\textbf{X}}_t) = \sum _{i=1}^{N_t} \log p({\textbf{y}}_t^{(i)} \vert {\textbf{x}}_t^{(i)})\)</span>. Due to the intractability of the likelihood, we use mean-field variational inference with variational distributions <span class="mathjax-tex">\(q_t({\varvec{\theta }}), q_t({{\textbf{s}}})\)</span>:</p><div class="c-article-equation" id="Equ4"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned}&amp;\sum _{i=1}^{N_t} \log \int _{\varvec{\theta }} \int _{{\textbf{s}}} p({\textbf{y}}_t^{(i)} \vert {\textbf{s}}, \varvec{\theta }, {\textbf{x}}_t^{(i)}) p({\textbf{s}}) p(\varvec{\theta }) d\varvec{\theta } d{\textbf{s}} \nonumber \\&amp;\quad = \sum _{i=1}^{N^{(t)}} \log \int _{\varvec{\theta }} \int _{s} \frac{p(y_{i}^{(t)} \vert s, \varvec{\theta }, x_{i}^{(t)}) p(s) q_{t-1}(\varvec{\theta })}{q(s) q(\varvec{\theta })} q(\varvec{\theta }) q(s) d\varvec{\theta } ds \nonumber \\&amp;\quad \ge \sum _{i=1}^{N_t} E_{q_t({\varvec{\theta }}), q_t({{\textbf{s}}})} \left[ \log p({\textbf{y}}_t^{(i)} \vert {\textbf{s}}, \varvec{\theta }, {\textbf{x}}_t^{(i)}) \right] \nonumber \\&amp;\quad - KL(q_t({\textbf{s}}) \Vert p({\textbf{s}})) - KL(q_t(\varvec{\theta }) \Vert q_{t-1}(\varvec{\theta })) \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (A3)
                </div></div><p>Note that <span class="mathjax-tex">\(KL(q_t(\varvec{\theta }) \Vert q_{t-1}(\varvec{\theta }))\)</span> is a different point between CVD for VCL and UCL. While CVD for VCL uses the KL term in Eq. (<a data-track="click" data-track-action="equation anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#Equ2">A1</a>), the KL term in CVD for UCL is expressed in Eq. (<a data-track="click" data-track-action="equation anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#Equ3">A2</a>). The remaining terms are built as in CVD for VCL.</p>
<h3 class="c-article__sub-heading" id="App2">Appendix B: Architectures and settings</h3><p><b>Split MNIST and permuted MNIST</b></p><p>For Split MNIST, we use a fully-connected neural network (FCNN) with two hidden layers and multi-head output layer. Table <a data-track="click" data-track-action="table anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#Tab8">8</a> shows the detail of the network for Split MNIST dataset. For Permuted MNIST dataset, we also use FCNN but single-head output layer, and the architecture is shown in Table <a data-track="click" data-track-action="table anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#Tab9">9</a>.
</p><div class="c-article-table" data-container-section="table" data-test="inline-table" id="table-8"><figure><figcaption class="c-article-table__figcaption"><b data-test="table-caption" id="Tab8">Table 8 Network architecture for Split MNIST</b></figcaption><div class="u-text-right u-hide-print"><a aria-label="Full size table 8" class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" href="/article/10.1007/s10994-023-06487-7/tables/8" rel="nofollow"><span>Full size table</span><svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16"><use xlink:href="#icon-eds-i-chevron-right-small" xmlns:xlink="http://www.w3.org/1999/xlink"></use></svg></a></div></figure></div><div class="c-article-table" data-container-section="table" data-test="inline-table" id="table-9"><figure><figcaption class="c-article-table__figcaption"><b data-test="table-caption" id="Tab9">Table 9 Network architecture for Permuted MNIST</b></figcaption><div class="u-text-right u-hide-print"><a aria-label="Full size table 9" class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" href="/article/10.1007/s10994-023-06487-7/tables/9" rel="nofollow"><span>Full size table</span><svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16"><use xlink:href="#icon-eds-i-chevron-right-small" xmlns:xlink="http://www.w3.org/1999/xlink"></use></svg></a></div></figure></div><p>We tune the hyperparameters of both CVD and the combined methods. Moreover, we again use the strategy of parameter initialization as in UCL’s experiments (Ahn et al., <a aria-label="Reference 2019" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#ref-CR1" id="ref-link-section-d212779005e33418" title="Ahn, H., Cha, S., Lee, D., &amp; Moon, T. (2019). Uncertainty-based continual learning with adaptive regularization. In Advances in Neural Information Processing Systems (pp. 4392–4402).">2019</a>) in the beginning step of training process. All the hyperparameters of methods are listed as below:</p><ul class="u-list-style-bullet">
<li>
<p>UCL—<span class="mathjax-tex">\(\beta :\)</span> {0.0001; 0.001; 0.01; 0.02; 0.03}, <span class="mathjax-tex">\(\alpha :\)</span> {0.01; 0.3; 5}</p>
</li>
<li>
<p>EWC—<span class="mathjax-tex">\(\lambda :\)</span> {40; 400; 4000; 10000; 40000}</p>
</li>
<li>
<p>VCL—not needed</p>
</li>
<li>
<p>GVCL—<span class="mathjax-tex">\(\beta :\)</span> {0.05; 0.1; 0.2}, <span class="mathjax-tex">\(\lambda :\)</span> {1; 10; 100; 1000}</p>
</li>
<li>
<p>CVD—KL_weight <span class="mathjax-tex">\(\kappa\)</span>: {0.0001; 0.001; 0.01; 0.1; 1}</p>
</li>
</ul><p><b>Split CIFAR-10/100 and Split CIFAR-100:</b></p><p>The detail of architecture is used in CIFAR experiments is shown in Table <a data-track="click" data-track-action="table anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#Tab10">10</a> and all the hyperparameters are listed as below:</p><ul class="u-list-style-bullet">
<li>
<p>UCL—<span class="mathjax-tex">\(\beta :\)</span> {0.0001; 0.0002; 0.001; 0.002}, <span class="mathjax-tex">\(\alpha :\)</span> {0.01; 0.3; 5}, <i>r</i> :  {0.5; 0.125}, <span class="mathjax-tex">\(lr(\sigma ):\)</span> {0.01; 0.02}</p>
</li>
<li>
<p>EWC—<span class="mathjax-tex">\(\lambda :\)</span> {400; 1000; 4000; 10000; 25000; 40000}</p>
</li>
<li>
<p>VCL—not needed</p>
</li>
<li>
<p>GVCL—<span class="mathjax-tex">\(\beta :\)</span> {0.05; 0.1; 0.2}, <span class="mathjax-tex">\(\lambda :\)</span> {1; 10; 100; 1000}</p>
</li>
<li>
<p>AGS-CL—<span class="mathjax-tex">\(\lambda :\)</span> {1.5; 100; 400; 1000; 7000; 10000}, <span class="mathjax-tex">\(\mu :\)</span> {0.5; 10; 20}, <span class="mathjax-tex">\(\rho :\)</span> {0.1; 0.2; 0.3; 0.4; 0.5}</p>
</li>
<li>
<p>CVD—KL_weight <span class="mathjax-tex">\(\kappa\)</span>: {0.0001; 0.001; 0.01; 0.1; 1}</p>
</li>
</ul><div class="c-article-table" data-container-section="table" data-test="inline-table" id="table-10"><figure><figcaption class="c-article-table__figcaption"><b data-test="table-caption" id="Tab10">Table 10 Network architecture for Split CIFAR-10/100</b></figcaption><div class="u-text-right u-hide-print"><a aria-label="Full size table 10" class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" href="/article/10.1007/s10994-023-06487-7/tables/10" rel="nofollow"><span>Full size table</span><svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16"><use xlink:href="#icon-eds-i-chevron-right-small" xmlns:xlink="http://www.w3.org/1999/xlink"></use></svg></a></div></figure></div><p><b>Split Omniglot:</b></p><p>The detail of architecture for Omniglot dataset is given in Table <a data-track="click" data-track-action="table anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#Tab11">11</a>. Since, for each task, the number of classes is different, we denoted the classes of <span class="mathjax-tex">\(i^{th}\)</span> task as <span class="mathjax-tex">\(C_i\)</span>. All the hyperparameters are listed as below:</p><ul class="u-list-style-bullet">
<li>
<p>UCL—<span class="mathjax-tex">\(\beta :\)</span> {0.0001; 0.0002; 0.001; 0.002}, <span class="mathjax-tex">\(\alpha :\)</span> {0.01; 0.3; 5}, <i>r</i> :  {0.5; 0.125}, <span class="mathjax-tex">\(lr(\sigma ):\)</span> {0.01; 0.02}</p>
</li>
<li>
<p>EWC—<span class="mathjax-tex">\(\lambda :\)</span> {4000; 10000; 25000; 40000; 100000}</p>
</li>
<li>
<p>VCL—not needed</p>
</li>
<li>
<p>GVCL—<span class="mathjax-tex">\(\beta :\)</span> {0.05; 0.1; 0.2}, <span class="mathjax-tex">\(\lambda :\)</span> {1; 10; 100; 1000}</p>
</li>
<li>
<p>AGS-CL—<span class="mathjax-tex">\(\lambda :\)</span> {1.5; 100; 400; 1000; 7000; 10000}, <span class="mathjax-tex">\(\mu :\)</span> {0.5; 10; 20}, <span class="mathjax-tex">\(\rho :\)</span> {0.1; 0.2; 0.3; 0.4; 0.5}</p>
</li>
<li>
<p>CVD—KL_weight <span class="mathjax-tex">\(\kappa\)</span>: {0.0001; 0.001; 0.01; 0.1; 1}</p>
</li>
</ul><div class="c-article-table" data-container-section="table" data-test="inline-table" id="table-11"><figure><figcaption class="c-article-table__figcaption"><b data-test="table-caption" id="Tab11">Table 11 Network architecture for Split Omniglot</b></figcaption><div class="u-text-right u-hide-print"><a aria-label="Full size table 11" class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" href="/article/10.1007/s10994-023-06487-7/tables/11" rel="nofollow"><span>Full size table</span><svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16"><use xlink:href="#icon-eds-i-chevron-right-small" xmlns:xlink="http://www.w3.org/1999/xlink"></use></svg></a></div></figure></div><p><b>Split CUB-200:</b></p><p>We use AlexNet in this experiment, the detail of architecture is given in Table <a data-track="click" data-track-action="table anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#Tab12">12</a>. All the hyperparameters are listed as below:</p><ul class="u-list-style-bullet">
<li>
<p>GVCL—<span class="mathjax-tex">\(\beta :\)</span> {0.05; 0.1; 0.2}, <span class="mathjax-tex">\(\lambda :\)</span> {1; 10; 100; 1000}</p>
</li>
<li>
<p>AGS-CL—<span class="mathjax-tex">\(\lambda :\)</span> {1.5; 100; 400; 1000; 7000; 10000}, <span class="mathjax-tex">\(\mu :\)</span> {0.5; 10; 20}, <span class="mathjax-tex">\(\rho :\)</span> {0.1; 0.2; 0.3; 0.4; 0.5}</p>
</li>
<li>
<p>CVD-KL_weight <span class="mathjax-tex">\(\kappa\)</span>: {0.0001; 0.001; 0.01; 0.1; 1}</p>
</li>
</ul><div class="c-article-table" data-container-section="table" data-test="inline-table" id="table-12"><figure><figcaption class="c-article-table__figcaption"><b data-test="table-caption" id="Tab12">Table 12 Network architecture for Split CUB-200</b></figcaption><div class="u-text-right u-hide-print"><a aria-label="Full size table 12" class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" href="/article/10.1007/s10994-023-06487-7/tables/12" rel="nofollow"><span>Full size table</span><svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16"><use xlink:href="#icon-eds-i-chevron-right-small" xmlns:xlink="http://www.w3.org/1999/xlink"></use></svg></a></div></figure></div><p><b>Split ImangeNet-R:</b></p><p>We freeze the pretrained ViT backbone and add 4 dense layers to build model in this experiment, the detail of the architecture is given in Table <a data-track="click" data-track-action="table anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#Tab13">13</a>. All the hyperparameters are listed as below:</p><ul class="u-list-style-bullet">
<li>
<p>GVCL—<span class="mathjax-tex">\(\beta :\)</span> {0.001; 0.05; 0.1; 0.2}, <span class="mathjax-tex">\(\lambda :\)</span> {1; 10; 50, 100; 1000}</p>
</li>
<li>
<p>AGS-CL—<span class="mathjax-tex">\(\lambda :\)</span> {1.5; 50; 100; 400; 1000; 7000; 10000}, <span class="mathjax-tex">\(\mu :\)</span> {0.5; 10; 20}, <span class="mathjax-tex">\(\rho :\)</span> {0.1; 0.2; 0.3; 0.4; 0.5}</p>
</li>
<li>
<p>CVD-KL_weight <span class="mathjax-tex">\(\kappa\)</span>: {0.0001; 0.001; 0.01; 0.1; 1; 1.5}</p>
</li>
</ul><div class="c-article-table" data-container-section="table" data-test="inline-table" id="table-13"><figure><figcaption class="c-article-table__figcaption"><b data-test="table-caption" id="Tab13">Table 13 Network architecture for Split ImageNet-R</b></figcaption><div class="u-text-right u-hide-print"><a aria-label="Full size table 13" class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" href="/article/10.1007/s10994-023-06487-7/tables/13" rel="nofollow"><span>Full size table</span><svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16"><use xlink:href="#icon-eds-i-chevron-right-small" xmlns:xlink="http://www.w3.org/1999/xlink"></use></svg></a></div></figure></div><h3 class="c-article__sub-heading" id="App3">Appendix C: Supplement visualizations</h3><p>Figures <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#Fig15">15</a>, <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#Fig16">16</a>, <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#Fig17">17</a> and <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#Fig18">18</a> are the supplement illustrations for the analysis in Sect. <a data-track="click" data-track-action="section anchor" data-track-label="link" href="/article/10.1007/s10994-023-06487-7#Sec14">4.2</a>. Similarly, the charts show the test accuracy of a task corresponding to the trained model of the task on the horizontal axis. As can be seen from this extent, CVD allows AGS-CL, VCL, UCL, and GVCL to be more steady across the tasks, whereas the original performs substantially worse. Based on these findings, it can be concluded that CVD not only enhances performance in most tasks but also effectively minimizes the forgetting phenomena.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-container-section="figure" data-test="figure" data-title="Fig. 15" id="figure-15"><figure><figcaption><b class="c-article-section__figure-caption" data-test="figure-caption-text" id="Fig15">Fig. 15</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-action="view figure" data-track-label="image" href="/article/10.1007/s10994-023-06487-7/figures/15" rel="nofollow"><picture><source srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10994-023-06487-7/MediaObjects/10994_2023_6487_Fig15_HTML.png?as=webp" type="image/webp"/><img alt="figure 15" aria-describedby="Fig15" height="562" loading="lazy" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10994-023-06487-7/MediaObjects/10994_2023_6487_Fig15_HTML.png" width="685"/></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-15-desc"><p>The change of accuracy through tasks on VCL</p></div></div><div class="u-text-right u-hide-print"><a aria-label="Full size image figure 15" class="c-article__pill-button" data-test="article-link" data-track="click" data-track-action="view figure" data-track-dest="link:Figure15 Full size image" data-track-label="button" href="/article/10.1007/s10994-023-06487-7/figures/15" rel="nofollow"><span>Full size image</span><svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16"><use xlink:href="#icon-eds-i-chevron-right-small" xmlns:xlink="http://www.w3.org/1999/xlink"></use></svg></a></div></figure></div><div class="c-article-section__figure js-c-reading-companion-figures-item" data-container-section="figure" data-test="figure" data-title="Fig. 16" id="figure-16"><figure><figcaption><b class="c-article-section__figure-caption" data-test="figure-caption-text" id="Fig16">Fig. 16</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-action="view figure" data-track-label="image" href="/article/10.1007/s10994-023-06487-7/figures/16" rel="nofollow"><picture><source srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10994-023-06487-7/MediaObjects/10994_2023_6487_Fig16_HTML.png?as=webp" type="image/webp"/><img alt="figure 16" aria-describedby="Fig16" height="558" loading="lazy" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10994-023-06487-7/MediaObjects/10994_2023_6487_Fig16_HTML.png" width="685"/></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-16-desc"><p>The change of accuracy through tasks on UCL</p></div></div><div class="u-text-right u-hide-print"><a aria-label="Full size image figure 16" class="c-article__pill-button" data-test="article-link" data-track="click" data-track-action="view figure" data-track-dest="link:Figure16 Full size image" data-track-label="button" href="/article/10.1007/s10994-023-06487-7/figures/16" rel="nofollow"><span>Full size image</span><svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16"><use xlink:href="#icon-eds-i-chevron-right-small" xmlns:xlink="http://www.w3.org/1999/xlink"></use></svg></a></div></figure></div><div class="c-article-section__figure js-c-reading-companion-figures-item" data-container-section="figure" data-test="figure" data-title="Fig. 17" id="figure-17"><figure><figcaption><b class="c-article-section__figure-caption" data-test="figure-caption-text" id="Fig17">Fig. 17</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-action="view figure" data-track-label="image" href="/article/10.1007/s10994-023-06487-7/figures/17" rel="nofollow"><picture><source srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10994-023-06487-7/MediaObjects/10994_2023_6487_Fig17_HTML.png?as=webp" type="image/webp"/><img alt="figure 17" aria-describedby="Fig17" height="553" loading="lazy" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10994-023-06487-7/MediaObjects/10994_2023_6487_Fig17_HTML.png" width="685"/></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-17-desc"><p>The change of accuracy through tasks on AGS-CL</p></div></div><div class="u-text-right u-hide-print"><a aria-label="Full size image figure 17" class="c-article__pill-button" data-test="article-link" data-track="click" data-track-action="view figure" data-track-dest="link:Figure17 Full size image" data-track-label="button" href="/article/10.1007/s10994-023-06487-7/figures/17" rel="nofollow"><span>Full size image</span><svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16"><use xlink:href="#icon-eds-i-chevron-right-small" xmlns:xlink="http://www.w3.org/1999/xlink"></use></svg></a></div></figure></div><div class="c-article-section__figure js-c-reading-companion-figures-item" data-container-section="figure" data-test="figure" data-title="Fig. 18" id="figure-18"><figure><figcaption><b class="c-article-section__figure-caption" data-test="figure-caption-text" id="Fig18">Fig. 18</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-action="view figure" data-track-label="image" href="/article/10.1007/s10994-023-06487-7/figures/18" rel="nofollow"><picture><source srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10994-023-06487-7/MediaObjects/10994_2023_6487_Fig18_HTML.png?as=webp" type="image/webp"/><img alt="figure 18" aria-describedby="Fig18" height="562" loading="lazy" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10994-023-06487-7/MediaObjects/10994_2023_6487_Fig18_HTML.png" width="685"/></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-18-desc"><p>The change of accuracy through tasks on GVCL</p></div></div><div class="u-text-right u-hide-print"><a aria-label="Full size image figure 18" class="c-article__pill-button" data-test="article-link" data-track="click" data-track-action="view figure" data-track-dest="link:Figure18 Full size image" data-track-label="button" href="/article/10.1007/s10994-023-06487-7/figures/18" rel="nofollow"><span>Full size image</span><svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16"><use xlink:href="#icon-eds-i-chevron-right-small" xmlns:xlink="http://www.w3.org/1999/xlink"></use></svg></a></div></figure></div></div></div></section><section data-title="Rights and permissions"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p>Springer Nature or its licensor (e.g. a society or other partner) holds exclusive rights to this article under a publishing agreement with the author(s) or other rightsholder(s); author self-archiving of the accepted manuscript version of this article is solely governed by the terms of such publishing agreement and applicable law.</p><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Continual%20variational%20dropout%3A%20a%20view%20of%20auxiliary%20local%20variables%20in%20continual%20learning&amp;author=Nam%20Le%20Hai%20et%20al&amp;contentID=10.1007%2Fs10994-023-06487-7&amp;copyright=The%20Author%28s%29%2C%20under%20exclusive%20licence%20to%20Springer%20Science%2BBusiness%20Media%20LLC%2C%20part%20of%20Springer%20Nature&amp;publication=0885-6125&amp;publicationDate=2023-12-07&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and permissions</a></p></div></div></section><section aria-labelledby="article-info" data-title="About this article"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="u-hide-print c-bibliographic-information__column c-bibliographic-information__column--border"><a data-crossmark="10.1007/s10994-023-06487-7" data-test="crossmark" data-track="click" data-track-action="Click Crossmark" data-track-label="link" href="https://crossmark.crossref.org/dialog/?doi=10.1007/s10994-023-06487-7" rel="noopener" target="_blank"><img alt="Check for updates. Verify currency and authenticity via CrossMark" height="81" loading="lazy" src="data:image/svg+xml;base64,<svg height="81" width="57" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m17.35 35.45 21.3-14.2v-17.03h-21.3" fill="#989898"/><path d="m38.65 35.45-21.3-14.2v-17.03h21.3" fill="#747474"/><path d="m28 .5c-12.98 0-23.5 10.52-23.5 23.5s10.52 23.5 23.5 23.5 23.5-10.52 23.5-23.5c0-6.23-2.48-12.21-6.88-16.62-4.41-4.4-10.39-6.88-16.62-6.88zm0 41.25c-9.8 0-17.75-7.95-17.75-17.75s7.95-17.75 17.75-17.75 17.75 7.95 17.75 17.75c0 4.71-1.87 9.22-5.2 12.55s-7.84 5.2-12.55 5.2z" fill="#535353"/><path d="m41 36c-5.81 6.23-15.23 7.45-22.43 2.9-7.21-4.55-10.16-13.57-7.03-21.5l-4.92-3.11c-4.95 10.7-1.19 23.42 8.78 29.71 9.97 6.3 23.07 4.22 30.6-4.86z" fill="#9c9c9c"/><path d="m.2 58.45c0-.75.11-1.42.33-2.01s.52-1.09.91-1.5c.38-.41.83-.73 1.34-.94.51-.22 1.06-.32 1.65-.32.56 0 1.06.11 1.51.35.44.23.81.5 1.1.81l-.91 1.01c-.24-.24-.49-.42-.75-.56-.27-.13-.58-.2-.93-.2-.39 0-.73.08-1.05.23-.31.16-.58.37-.81.66-.23.28-.41.63-.53 1.04-.13.41-.19.88-.19 1.39 0 1.04.23 1.86.68 2.46.45.59 1.06.88 1.84.88.41 0 .77-.07 1.07-.23s.59-.39.85-.68l.91 1c-.38.43-.8.76-1.28.99-.47.22-1 .34-1.58.34-.59 0-1.13-.1-1.64-.31-.5-.2-.94-.51-1.31-.91-.38-.4-.67-.9-.88-1.48-.22-.59-.33-1.26-.33-2.02zm8.4-5.33h1.61v2.54l-.05 1.33c.29-.27.61-.51.96-.72s.76-.31 1.24-.31c.73 0 1.27.23 1.61.71.33.47.5 1.14.5 2.02v4.31h-1.61v-4.1c0-.57-.08-.97-.25-1.21-.17-.23-.45-.35-.83-.35-.3 0-.56.08-.79.22-.23.15-.49.36-.78.64v4.8h-1.61zm7.37 6.45c0-.56.09-1.06.26-1.51.18-.45.42-.83.71-1.14.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.36c.07.62.29 1.1.65 1.44.36.33.82.5 1.38.5.29 0 .57-.04.83-.13s.51-.21.76-.37l.55 1.01c-.33.21-.69.39-1.09.53-.41.14-.83.21-1.26.21-.48 0-.92-.08-1.34-.25-.41-.16-.76-.4-1.07-.7-.31-.31-.55-.69-.72-1.13-.18-.44-.26-.95-.26-1.52zm4.6-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.07.45-.31.29-.5.73-.58 1.3zm2.5.62c0-.57.09-1.08.28-1.53.18-.44.43-.82.75-1.13s.69-.54 1.1-.71c.42-.16.85-.24 1.31-.24.45 0 .84.08 1.17.23s.61.34.85.57l-.77 1.02c-.19-.16-.38-.28-.56-.37-.19-.09-.39-.14-.61-.14-.56 0-1.01.21-1.35.63-.35.41-.52.97-.52 1.67 0 .69.17 1.24.51 1.66.34.41.78.62 1.32.62.28 0 .54-.06.78-.17.24-.12.45-.26.64-.42l.67 1.03c-.33.29-.69.51-1.08.65-.39.15-.78.23-1.18.23-.46 0-.9-.08-1.31-.24-.4-.16-.75-.39-1.05-.7s-.53-.69-.7-1.13c-.17-.45-.25-.96-.25-1.53zm6.91-6.45h1.58v6.17h.05l2.54-3.16h1.77l-2.35 2.8 2.59 4.07h-1.75l-1.77-2.98-1.08 1.23v1.75h-1.58zm13.69 1.27c-.25-.11-.5-.17-.75-.17-.58 0-.87.39-.87 1.16v.75h1.34v1.27h-1.34v5.6h-1.61v-5.6h-.92v-1.2l.92-.07v-.72c0-.35.04-.68.13-.98.08-.31.21-.57.4-.79s.42-.39.71-.51c.28-.12.63-.18 1.04-.18.24 0 .48.02.69.07.22.05.41.1.57.17zm.48 5.18c0-.57.09-1.08.27-1.53.17-.44.41-.82.72-1.13.3-.31.65-.54 1.04-.71.39-.16.8-.24 1.23-.24s.84.08 1.24.24c.4.17.74.4 1.04.71s.54.69.72 1.13c.19.45.28.96.28 1.53s-.09 1.08-.28 1.53c-.18.44-.42.82-.72 1.13s-.64.54-1.04.7-.81.24-1.24.24-.84-.08-1.23-.24-.74-.39-1.04-.7c-.31-.31-.55-.69-.72-1.13-.18-.45-.27-.96-.27-1.53zm1.65 0c0 .69.14 1.24.43 1.66.28.41.68.62 1.18.62.51 0 .9-.21 1.19-.62.29-.42.44-.97.44-1.66 0-.7-.15-1.26-.44-1.67-.29-.42-.68-.63-1.19-.63-.5 0-.9.21-1.18.63-.29.41-.43.97-.43 1.67zm6.48-3.44h1.33l.12 1.21h.05c.24-.44.54-.79.88-1.02.35-.24.7-.36 1.07-.36.32 0 .59.05.78.14l-.28 1.4-.33-.09c-.11-.01-.23-.02-.38-.02-.27 0-.56.1-.86.31s-.55.58-.77 1.1v4.2h-1.61zm-47.87 15h1.61v4.1c0 .57.08.97.25 1.2.17.24.44.35.81.35.3 0 .57-.07.8-.22.22-.15.47-.39.73-.73v-4.7h1.61v6.87h-1.32l-.12-1.01h-.04c-.3.36-.63.64-.98.86-.35.21-.76.32-1.24.32-.73 0-1.27-.24-1.61-.71-.33-.47-.5-1.14-.5-2.02zm9.46 7.43v2.16h-1.61v-9.59h1.33l.12.72h.05c.29-.24.61-.45.97-.63.35-.17.72-.26 1.1-.26.43 0 .81.08 1.15.24.33.17.61.4.84.71.24.31.41.68.53 1.11.13.42.19.91.19 1.44 0 .59-.09 1.11-.25 1.57-.16.47-.38.85-.65 1.16-.27.32-.58.56-.94.73-.35.16-.72.25-1.1.25-.3 0-.6-.07-.9-.2s-.59-.31-.87-.56zm0-2.3c.26.22.5.37.73.45.24.09.46.13.66.13.46 0 .84-.2 1.15-.6.31-.39.46-.98.46-1.77 0-.69-.12-1.22-.35-1.61-.23-.38-.61-.57-1.13-.57-.49 0-.99.26-1.52.77zm5.87-1.69c0-.56.08-1.06.25-1.51.16-.45.37-.83.65-1.14.27-.3.58-.54.93-.71s.71-.25 1.08-.25c.39 0 .73.07 1 .2.27.14.54.32.81.55l-.06-1.1v-2.49h1.61v9.88h-1.33l-.11-.74h-.06c-.25.25-.54.46-.88.64-.33.18-.69.27-1.06.27-.87 0-1.56-.32-2.07-.95s-.76-1.51-.76-2.65zm1.67-.01c0 .74.13 1.31.4 1.7.26.38.65.58 1.15.58.51 0 .99-.26 1.44-.77v-3.21c-.24-.21-.48-.36-.7-.45-.23-.08-.46-.12-.7-.12-.45 0-.82.19-1.13.59-.31.39-.46.95-.46 1.68zm6.35 1.59c0-.73.32-1.3.97-1.71.64-.4 1.67-.68 3.08-.84 0-.17-.02-.34-.07-.51-.05-.16-.12-.3-.22-.43s-.22-.22-.38-.3c-.15-.06-.34-.1-.58-.1-.34 0-.68.07-1 .2s-.63.29-.93.47l-.59-1.08c.39-.24.81-.45 1.28-.63.47-.17.99-.26 1.54-.26.86 0 1.51.25 1.93.76s.63 1.25.63 2.21v4.07h-1.32l-.12-.76h-.05c-.3.27-.63.48-.98.66s-.73.27-1.14.27c-.61 0-1.1-.19-1.48-.56-.38-.36-.57-.85-.57-1.46zm1.57-.12c0 .3.09.53.27.67.19.14.42.21.71.21.28 0 .54-.07.77-.2s.48-.31.73-.56v-1.54c-.47.06-.86.13-1.18.23-.31.09-.57.19-.76.31s-.33.25-.41.4c-.09.15-.13.31-.13.48zm6.29-3.63h-.98v-1.2l1.06-.07.2-1.88h1.34v1.88h1.75v1.27h-1.75v3.28c0 .8.32 1.2.97 1.2.12 0 .24-.01.37-.04.12-.03.24-.07.34-.11l.28 1.19c-.19.06-.4.12-.64.17-.23.05-.49.08-.76.08-.4 0-.74-.06-1.02-.18-.27-.13-.49-.3-.67-.52-.17-.21-.3-.48-.37-.78-.08-.3-.12-.64-.12-1.01zm4.36 2.17c0-.56.09-1.06.27-1.51s.41-.83.71-1.14c.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.37c.08.62.29 1.1.65 1.44.36.33.82.5 1.38.5.3 0 .58-.04.84-.13.25-.09.51-.21.76-.37l.54 1.01c-.32.21-.69.39-1.09.53s-.82.21-1.26.21c-.47 0-.92-.08-1.33-.25-.41-.16-.77-.4-1.08-.7-.3-.31-.54-.69-.72-1.13-.17-.44-.26-.95-.26-1.52zm4.61-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.08.45-.31.29-.5.73-.57 1.3zm3.01 2.23c.31.24.61.43.92.57.3.13.63.2.98.2.38 0 .65-.08.83-.23s.27-.35.27-.6c0-.14-.05-.26-.13-.37-.08-.1-.2-.2-.34-.28-.14-.09-.29-.16-.47-.23l-.53-.22c-.23-.09-.46-.18-.69-.3-.23-.11-.44-.24-.62-.4s-.33-.35-.45-.55c-.12-.21-.18-.46-.18-.75 0-.61.23-1.1.68-1.49.44-.38 1.06-.57 1.83-.57.48 0 .91.08 1.29.25s.71.36.99.57l-.74.98c-.24-.17-.49-.32-.73-.42-.25-.11-.51-.16-.78-.16-.35 0-.6.07-.76.21-.17.15-.25.33-.25.54 0 .14.04.26.12.36s.18.18.31.26c.14.07.29.14.46.21l.54.19c.23.09.47.18.7.29s.44.24.64.4c.19.16.34.35.46.58.11.23.17.5.17.82 0 .3-.06.58-.17.83-.12.26-.29.48-.51.68-.23.19-.51.34-.84.45-.34.11-.72.17-1.15.17-.48 0-.95-.09-1.41-.27-.46-.19-.86-.41-1.2-.68z" fill="#535353"/></g></svg>" width="57"/></a></div><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Hai, N.L., Nguyen, T., Van, L.N. <i>et al.</i> Continual variational dropout: a view of auxiliary local variables in continual learning.
                    <i>Mach Learn</i> <b>113</b>, 281–323 (2024). https://doi.org/10.1007/s10994-023-06487-7</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-external="" data-track-label="link" href="https://citation-needed.springer.com/v2/references/10.1007/s10994-023-06487-7?format=refman&amp;flavour=citation" rel="nofollow">Download citation<svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16"><use xlink:href="#icon-eds-i-download-medium" xmlns:xlink="http://www.w3.org/1999/xlink"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2022-07-12">12 July 2022</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Revised<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2023-09-19">19 September 2023</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2023-11-04">04 November 2023</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2023-12-07">07 December 2023</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2024-01">January 2024</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--full-width"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value">https://doi.org/10.1007/s10994-023-06487-7</span></p></li></ul><div data-component="share-box"><div class="c-article-share-box u-display-block"><h3 class="c-article__sub-heading">Share this article</h3><p class="c-article-share-box__description">Anyone you share the following link with will be able to read this content:</p><button class="js-get-share-url c-article-share-box__button" data-track="click" data-track-action="get shareable link" data-track-external="" data-track-label="button" id="get-share-url" type="button">Get shareable link</button><div class="js-no-share-url-container u-display-none" hidden=""><p class="js-c-article-share-box__no-sharelink-info c-article-share-box__no-sharelink-info">Sorry, a shareable link is not currently available for this article.</p></div><div class="js-share-url-container u-display-none" hidden=""><p class="js-share-url c-article-share-box__only-read-input" data-track="click" data-track-action="select share url" data-track-label="button" id="share-url"></p><button class="js-copy-share-url c-article-share-box__button--link-like" data-track="click" data-track-action="copy share url" data-track-external="" data-track-label="button" id="copy-share-url" type="button">Copy to clipboard</button></div><p class="js-c-article-share-box__additional-info c-article-share-box__additional-info">
                            Provided by the Springer Nature SharedIt content-sharing initiative
                        </p></div></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span><a data-track="click" data-track-action="view keyword" data-track-label="link" href='/search?query=Continual%20learning&amp;facet-discipline="Computer%20Science"'>Continual learning</a></span></li><li class="c-article-subject-list__subject"><span><a data-track="click" data-track-action="view keyword" data-track-label="link" href='/search?query=Regularization%2Fprior-based%20approach&amp;facet-discipline="Computer%20Science"'>Regularization/prior-based approach</a></span></li><li class="c-article-subject-list__subject"><span><a data-track="click" data-track-action="view keyword" data-track-label="link" href='/search?query=Variational%20dropout&amp;facet-discipline="Computer%20Science"'>Variational dropout</a></span></li><li class="c-article-subject-list__subject"><span><a data-track="click" data-track-action="view keyword" data-track-label="link" href='/search?query=Local%20and%20global%20variables&amp;facet-discipline="Computer%20Science"'>Local and global variables</a></span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
</div>
</main>
<div class="c-article-sidebar u-text-sm u-hide-print l-with-sidebar__sidebar" data-container-type="reading-companion" data-track-component="reading companion" id="sidebar">
<aside>
<div class="app-card-service" data-test="article-checklist-banner">
<div>
<a class="app-card-service__link" data-test="article-checklist-banner-link" data-track="click_presubmission_checklist" data-track-action="clicked article page checklist banner test 2 old version" data-track-category="pre-submission-checklist" data-track-context="article page top of reading companion" data-track-label="link" href="https://beta.springernature.com/pre-submission?journalId=10994">
<span class="app-card-service__link-text">Use our pre-submission checklist</span>
<svg aria-hidden="true" class="app-card-service__link-icon" focusable="false"><use xlink:href="#icon-eds-i-arrow-right-small"></use></svg>
</a>
<p class="app-card-service__description">Avoid common mistakes on your manuscript.</p>
</div>
<div class="app-card-service__icon-container">
<svg aria-hidden="true" class="app-card-service__icon" focusable="false">
<use xlink:href="#icon-eds-i-clipboard-check-medium"></use>
</svg>
</div>
</div>
<div data-test="collections">
</div>
<div data-test="editorial-summary">
</div>
<div class="c-reading-companion">
<div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
<div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
<div class="u-lazy-ad-wrapper u-mt-16 u-show" data-component-mpu=""><div class="c-ad c-ad--300x250">
<div class="c-ad__inner">
<p class="c-ad__label">Advertisement</p>
<div class="div-gpt-ad grade-c-hide" data-gpt="" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=s10994-023-06487-7;" data-gpt-unitpath="/270604982/springerlink/10994/article" data-pa11y-ignore="" data-test="MPU1-ad" id="div-gpt-ad-MPU1">
</div>
</div>
</div>
</div>
</div>
<div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
<div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
</div>
</div>
</aside>
</div>
</div>
</article>
<div class="app-elements">
<footer>
<div class="eds-c-footer">
<div class="eds-c-footer__container">
<div class="eds-c-footer__grid eds-c-footer__group--separator">
<div class="eds-c-footer__group">
<h3 class="eds-c-footer__heading">Discover content</h3>
<ul class="eds-c-footer__list">
<li class="eds-c-footer__item"><a class="eds-c-footer__link" data-track="nav_journals_a_z" data-track-action="journals a-z" data-track-context="unified footer" data-track-label="link" href="https://link.springer.com/journals/a/1">Journals A-Z</a></li>
<li class="eds-c-footer__item"><a class="eds-c-footer__link" data-track="nav_books_a_z" data-track-action="books a-z" data-track-context="unified footer" data-track-label="link" href="https://link.springer.com/books/a/1">Books A-Z</a></li>
</ul>
</div>
<div class="eds-c-footer__group">
<h3 class="eds-c-footer__heading">Publish with us</h3>
<ul class="eds-c-footer__list">
<li class="eds-c-footer__item"><a class="eds-c-footer__link" data-track="nav_journal_finder" data-track-action="journal finder" data-track-context="unified footer" data-track-label="link" href="https://link.springer.com/journals">Journal finder</a></li>
<li class="eds-c-footer__item"><a class="eds-c-footer__link" data-track="nav_publish_your_research" data-track-action="publish your research" data-track-context="unified footer" data-track-label="link" href="https://www.springernature.com/gp/authors">Publish your research</a></li>
<li class="eds-c-footer__item"><a class="eds-c-footer__link" data-track="nav_open_access_publishing" data-track-action="open access publishing" data-track-context="unified footer" data-track-label="link" href="https://www.springernature.com/gp/open-research/about/the-fundamentals-of-open-access-and-open-research">Open access publishing</a></li>
</ul>
</div>
<div class="eds-c-footer__group">
<h3 class="eds-c-footer__heading">Products and services</h3>
<ul class="eds-c-footer__list">
<li class="eds-c-footer__item"><a class="eds-c-footer__link" data-track="nav_our_products" data-track-action="our products" data-track-context="unified footer" data-track-label="link" href="https://www.springernature.com/gp/products">Our products</a></li>
<li class="eds-c-footer__item"><a class="eds-c-footer__link" data-track="nav_librarians" data-track-action="librarians" data-track-context="unified footer" data-track-label="link" href="https://www.springernature.com/gp/librarians">Librarians</a></li>
<li class="eds-c-footer__item"><a class="eds-c-footer__link" data-track="nav_societies" data-track-action="societies" data-track-context="unified footer" data-track-label="link" href="https://www.springernature.com/gp/societies">Societies</a></li>
<li class="eds-c-footer__item"><a class="eds-c-footer__link" data-track="nav_partners_and_advertisers" data-track-action="partners and advertisers" data-track-context="unified footer" data-track-label="link" href="https://www.springernature.com/gp/partners">Partners and advertisers</a></li>
</ul>
</div>
<div class="eds-c-footer__group">
<h3 class="eds-c-footer__heading">Our imprints</h3>
<ul class="eds-c-footer__list">
<li class="eds-c-footer__item"><a class="eds-c-footer__link" data-track="nav_imprint_Springer" data-track-action="Springer" data-track-context="unified footer" data-track-label="link" href="https://www.springer.com/">Springer</a></li>
<li class="eds-c-footer__item"><a class="eds-c-footer__link" data-track="nav_imprint_Nature_Portfolio" data-track-action="Nature Portfolio" data-track-context="unified footer" data-track-label="link" href="https://www.nature.com/">Nature Portfolio</a></li>
<li class="eds-c-footer__item"><a class="eds-c-footer__link" data-track="nav_imprint_BMC" data-track-action="BMC" data-track-context="unified footer" data-track-label="link" href="https://www.biomedcentral.com/">BMC</a></li>
<li class="eds-c-footer__item"><a class="eds-c-footer__link" data-track="nav_imprint_Palgrave_Macmillan" data-track-action="Palgrave Macmillan" data-track-context="unified footer" data-track-label="link" href="https://www.palgrave.com/">Palgrave Macmillan</a></li>
<li class="eds-c-footer__item"><a class="eds-c-footer__link" data-track="nav_imprint_Apress" data-track-action="Apress" data-track-context="unified footer" data-track-label="link" href="https://www.apress.com/">Apress</a></li>
</ul>
</div>
</div>
</div>
<div class="eds-c-footer__container">
<nav aria-label="footer navigation">
<ul class="eds-c-footer__links">
<li class="eds-c-footer__item">
<button class="eds-c-footer__link" data-cc-action="preferences" data-track="dialog_manage_cookies" data-track-action="Manage cookies" data-track-context="unified footer" data-track-label="link"><span class="eds-c-footer__button-text">Your privacy choices/Manage cookies</span></button>
</li>
<li class="eds-c-footer__item">
<a class="eds-c-footer__link" data-track="nav_california_privacy_statement" data-track-action="california privacy statement" data-track-context="unified footer" data-track-label="link" href="https://www.springernature.com/gp/legal/ccpa">Your US state privacy rights</a>
</li>
<li class="eds-c-footer__item">
<a class="eds-c-footer__link" data-track="nav_accessibility_statement" data-track-action="accessibility statement" data-track-context="unified footer" data-track-label="link" href="https://www.springernature.com/gp/info/accessibility">Accessibility statement</a>
</li>
<li class="eds-c-footer__item">
<a class="eds-c-footer__link" data-track="nav_terms_and_conditions" data-track-action="terms and conditions" data-track-context="unified footer" data-track-label="link" href="https://link.springer.com/termsandconditions">Terms and conditions</a>
</li>
<li class="eds-c-footer__item">
<a class="eds-c-footer__link" data-track="nav_privacy_policy" data-track-action="privacy policy" data-track-context="unified footer" data-track-label="link" href="https://link.springer.com/privacystatement">Privacy policy</a>
</li>
<li class="eds-c-footer__item">
<a class="eds-c-footer__link" data-track="nav_help_and_support" data-track-action="help and support" data-track-context="unified footer" data-track-label="link" href="https://support.springernature.com/en/support/home">Help and support</a>
</li>
<li class="eds-c-footer__item">
<a class="eds-c-footer__link" data-track-action="cancel contracts here" href="https://support.springernature.com/en/support/solutions/articles/6000255911-subscription-cancellations">Cancel contracts here</a>
</li>
</ul>
</nav>
<div class="eds-c-footer__user">
<p class="eds-c-footer__user-info">
<span data-test="footer-user-ip">165.194.104.233</span>
</p>
<p class="eds-c-footer__user-info" data-test="footer-business-partners">South Korea Trial Consortium (3001948335)  - Chung Ang University (3000120396)  - 7029 SpringerLink South Korea KESLI Korean Journal Package (3000256740)  - South Korea Trial Consortium (3000522537)  - 5539 SpringerLink South Korea KESLI Full OJA Consortium - (3000171421)  - 9823  SpringerLink South Korea Shinwon (3000699960)  - South Korea Full eJournal Con - Academic 2015-2017 (3991460179)  - 8064 South Korea KESLI Korean Consortium (3000251005)  - 5539 SpringerLink South Korea KESLI Full eJournal Consortium - Academic (3000175460) </p>
</div>
<a class="eds-c-footer__link" href="https://www.springernature.com/">
<img alt="Springer Nature" height="20" loading="lazy" src="/oscar-static/images/logo-springernature-white-19dd4ba190.svg" width="200"/>
</a>
<p class="eds-c-footer__legal" data-test="copyright">© 2024 Springer Nature</p>
</div>
</div>
</footer>
</div>
<script nomodule="true" src="/oscar-static/js/global-article-es5-bundle-dad1690b0d.js"></script><script src="/oscar-static/js/global-article-es6-bundle-e7d03c4cb3.js" type="module"></script><div data-cc-ghost="" style="height: 317px;"></div><script charset="" id="fetch-contextual-ads-campaign-data" type="text/javascript">(function(){var g=function(a){a=new CustomEvent("campaignDataLoaded",{detail:a});document.dispatchEvent(a)},b=google_tag_manager["rm"]["50443292"](66);if(b){var k=1500;b=google_tag_manager["rm"]["50443292"](67);b="link"===b?google_tag_manager["rm"]["50443292"](68):google_tag_manager["rm"]["50443292"](69);var m=function(a,c){var h=!1,l=setTimeout(function(){h=!0;c(null)},k),d=new XMLHttpRequest;d.onload=function(){var e=null,f=[];if(200===d.status){try{e=JSON.parse(d.responseText)}catch(p){e={}}f=e.campaigns||[]}h||(clearTimeout(l),c(f.length?f.join(","):null))};d.open("GET",
"/platform/contextual?doi\x3d"+a);d.send()},n=function(a,c){a&&-1===a.indexOf("not set")?m(a,c):setTimeout(c,1)};n(b,function(a){window.campaignsForContextualAds=a;g(a);window.dataLayer.push({content:{article:{campaignID:a}}})})}else window.campaignsForContextualAds="",g(null)})();</script>
<script charset="" id="gtm-setup-accessdetailsloaded-handler" type="text/javascript">document.addEventListener("accessdetailsloaded",function(a){a=a.detail||{};var b={event:"update-access-details"};a.institutional_business_partner_ids&&a.resolved_by||console.log("BPID data could not be retrieved from /exposed-details");a.logged_in_to_sn_profile||console.log("logged_in_to_sn_profile could not be retrieved from /exposed-details");var c=a.institutional_business_partner_ids&&a.institutional_business_partner_ids.join?a.institutional_business_partner_ids.join(";"):"",d=a.resolved_by&&a.resolved_by.join?
a.resolved_by.join(";"):"",e=a.personal_business_partner_id_found,f=a.logged_in_to_sn_profile,g=a.snid;b.user={};b.user.profile={};b.user.profile.profileInfo={resolvedBy:d||null,bpid:c||null,personal:e||null,logged_in_to_sn_profile:f,snid:g||null};b.session={};b.session.authentication={};b.session.authentication.token=a.token||null;b.session.authentication.legacy={};window.dataLayer.push(b);window.idpUserDataLoaded=!0},!1);window.dataLayer.push({event:"accessdetailsloaded-handler-added"});
window.accessDetailsLoadedHandlerAdded=!0;</script><script charset="" id="polyfill-matches" type="text/javascript">Element.prototype.matches||(Element.prototype.matches=Element.prototype.matchesSelector||Element.prototype.mozMatchesSelector||Element.prototype.msMatchesSelector||Element.prototype.oMatchesSelector||Element.prototype.webkitMatchesSelector||function(a){a=(this.document||this.ownerDocument).querySelectorAll(a);for(var b=a.length;0<=--b&&a.item(b)!==this;);return-1<b});</script><script charset="" id="fetch-idp-user-data" type="text/javascript">(function(a){if("function"===typeof window.CustomEvent)return!1;var c=function(d,b){b=b||{};var e=document.createEvent("CustomEvent");e.initCustomEvent(d,b.bubbles||!1,b.cancelable||!1,b.detail||a);return e};c.prototype=window.Event.prototype;window.CustomEvent=c})();var parse=function(a,c){try{return 200===a?JSON.parse(c):null}catch(d){return null}},dispatch=function(a){a=new CustomEvent("accessdetailsloaded",{detail:a});document.dispatchEvent(a)},site=google_tag_manager["rm"]["50443292"](107),idpUrl;
switch(site){case "nature":idpUrl=-1<window.location.hostname.indexOf("local-www")||-1<window.location.hostname.indexOf("test-www")?"https://staging-idp.nature.com/exposed-details":"https://idp.nature.com/exposed-details";break;case "link":idpUrl=-1<window.location.hostname.indexOf("link-qa")?"https://staging-idp.springer.com/exposed-details":"https://idp.springer.com/exposed-details";break;case "springer":idpUrl=-1<window.location.hostname.indexOf("local-www")||-1<window.location.hostname.indexOf("test-www")?
"https://staging-idp.springer.com/exposed-details":"https://idp.springer.com/exposed-details"}if(void 0!==idpUrl){var transport=new XMLHttpRequest;transport.open("GET",idpUrl,!0);transport.withCredentials=!0;transport.onreadystatechange=function(){4===transport.readyState&&dispatch(parse(transport.status,transport.responseText))};transport.send()}else dispatch(null);</script><script charset="" id="create-ad-slots" type="text/javascript">function createAndLoadAds(){function h(b){"nature"===a&&google_tag_manager["rm"]["50443292"](136)(b);"bmc"!==a&&"springeropen"!==a||google_tag_manager["rm"]["50443292"](148)(b);("link"===a&&"oscar"===google_tag_manager["rm"]["50443292"](149)||"springer"===a||"link"===a&&!0===google_tag_manager["rm"]["50443292"](150))&&google_tag_manager["rm"]["50443292"](175)(b);"link"===a&&"bunsen"===google_tag_manager["rm"]["50443292"](176)&&google_tag_manager["rm"]["50443292"](201)(b);"link"===a&&"Core"===google_tag_manager["rm"]["50443292"](202)&&google_tag_manager["rm"]["50443292"](214)(b)}window.googletag=window.googletag||{cmd:[]};var a=google_tag_manager["rm"]["50443292"](215),e=google_tag_manager["rm"]["50443292"](217);
if(-1===window.location.search.indexOf("hide_ads\x3dtrue"))if(window.adSlots&&0!==Object.keys(window.adSlots).length)console.log("Ads previously loaded. Will not update ad slots until next page load.");else{window.adSlots||(window.adSlots={});window.getAd=function(b,f){for(var c in window.adSlots)if(-1<c.indexOf(b)){if("object"===f)return adSlots[c];if("slot"===f)return adSlots[c].slot}};googletag.cmd.push(function(){googletag.pubads().setPrivacySettings({limitedAds:e});console.log("limitedAds is ",
e);googletag.pubads().setRequestNonPersonalizedAds(google_tag_manager["rm"]["50443292"](219));googletag.pubads().enableSingleRequest();googletag.pubads().disableInitialLoad()});for(var g=document.querySelectorAll("[data-gpt]"),d=0;g[d];++d)h(g[d]);googletag.cmd.push(function(){googletag.enableServices()})}}createAndLoadAds();</script>
</body></html>