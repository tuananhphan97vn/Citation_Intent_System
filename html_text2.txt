







Deep reinforcement learning for the dynamic and uncertain vehicle routing problem | Applied Intelligence



































































































































































Your privacy, your choice



We use essential cookies to make sure the site can function. We also use optional cookies for advertising, personalisation of content, usage analysis, and social media.By accepting optional cookies, you consent to the processing of your personal data - including transfers to third parties. Some third parties are outside of the European Economic Area, with varying standards of data protection.See our href124 for more information on the use of your personal data.Manage preferences for further information and to change your choices.



Accept all cookies

















href200



Advertisement


href340








href7

href251




href233


href249
href336
href63



href109


href9






Search



Search by keyword or author






Search






Navigation


href208


href247


href164











href350





href372





Article



Deep reinforcement learning for the dynamic and uncertain vehicle routing problem


                            Published: 18 April 2022




Volume 53, pages 405–422, (2023)
                        

href13





href95




                                Access provided by Chung Ang University



href150
href176
href357









                                Deep reinforcement learning for the dynamic and uncertain vehicle routing problem
                            




Download PDF









href201href362 & href0 
            href2721 





5723 Accesses




38 Citations


href326








AbstractAccurate and real-time tracking for real-world urban logistics has become a popular research topic in the field of intelligent transportation. While the routing of urban logistic service is usually accomplished via complex mathematical and analytical methods. However, the nature and scope of real-world urban logistics are highly dynamic, and the existing optimization technique cannot precisely formulate the dynamic characteristics of the route. To ensure customers’ demands are met, planners need to respond to these changes quickly (sometimes instantaneously). This paper proposes the formulation of a novel deep reinforcement learning framework to solve a dynamic and uncertain vehicle routing problem (DU-VRP), whose objective is to meet the uncertain servicing needs of customers in a dynamic environment. Considering uncertain information about the demands of customers in this problem, the partial observation Markov decision process is designed to frequently observe the changes in customers’ demands in a real-time decision support system that consists of a deep neural network with a dynamic attention mechanism. Besides, a cutting-edge reinforcement learning algorithm is presented to control the value function of the DU-VRP for better training the routing process’s dynamics and uncertainty. Computational experiments are conducted considering different data sources to obtain satisfactory solutions of the DU-VRP.





Similar content being viewed by others






href19


Chapter
© 2022









href269


Chapter
© 2022









href55


Chapter
© 2023








Explore related subjects
Discover the latest articles, news and stories from top researchers in related subjects.


href143





href257
Avoid common mistakes on your manuscript.








1 IntroductionE-commerce giant enterprises such as Alibaba and JD have recently invested a large number of resources in group-on buying, especially for fresh food. This trend is becoming more popular and receiving more attention as the next-generation research topic [href80]. Meanwhile, to incorporate the dynamic dispatching policy and the real-time routing strategy for fresh food delivery services, vehicle routing is a cornerstone activity that is frequently related to the decision- making processes of several real-world transportation systems [href379]. In particular, meeting customers’ expectations and optimizing delivery requirements are expected to ensure food businesses a competitive advantage because food delivery accounts for a large portion of operational costs [href351]. Although many researchers have been working on deterministic or static vehicle routing problems (VRPs), little attention has been paid to dealing with uncertainty and dynamics in the VRPs simultaneously [href193,href315,href24]. Furthermore, in classical VRP models, the delivery requirements in VRPs are assumed as static, implying that some essential real-world constraints (i.e., dynamic and uncertain demands of customers) are neglected. This assumption may result in the inapplicability of real-world implementation. For example, in the food delivery industry, the failure of meeting the uncertain demands punctually would result in losing the freshness of food.The basic VRP was initially formulated by Dantzig and Ramser as a generalization of the Traveling Salesman Problem (TSP) [href284]. The VRP has been known as among the most difficult problems in combinatorial optimization [href331]. Due to its significance in practice, several VRP variants have been widely studied since the 1950s till now [href216]. In the following, two extended VRPs are described by indicating their main differences from the basic VRP problem. One is the Dynamic VRP (DVRP), which differs from the basic VRP by supposing that some values associated with customers, locations, and vehicles may change dynamically [href129]. Such a proposition is common in today’s vibrant production and transportation environment. Another is the Dynamic and Un certain VRP (DU-VRP), which is an extension of the DVRP by incorporating both uncertainties and dynamics in the vehicle routing procedure simultaneously. In practice, it is difficult to know the exact and fully-confirmed information in advance. For example, dispatchers have to update real-time information regarding the vehicles’ positions and consider the uncertainty of each customer’s demand. In this study, we focus on the DU-VRP as it has a wider range of applicability.More preciously, we consider to solve the DU-VRP by a Deep Reinforcement Learning (DRL) approach. A small-size DU-VRP example shown in Fig. href166 reveals how demand uncertainty affects the routes of vehicles in a consistent fashion and indicates the importance of real-time communication under a DU-VRP environment. Real-time communication needs to collect uncertain information and assign tasks in a dynamic way. In the DU-VRP, new customers arrive dynamically in the service areas. Figure 1 is drawn to illustrate the route execution of a single vehicle in an uncertain and dynamic environment. An initial route intends to visit the currently-known customers (i.e., Nodes A, B, C, D, E) before the vehicle leaves the depot at the time t0. While the vehicle executes this route, some requests from customers (i.e., Nodes L, X, Y, Z, K) may appear at times t1, t2, t3 in order. Each customer has a given demand by themselves. Furthermore, consumer demands may be uncertain and dynamically changing, and the requests that result from their demands are highly uncertain. In this case, this initial route may be adjusted to meet changes of the environment, e.g., the executed route at the time tf may be changed to be A → L→ B → X → C → Y → D → Z → E → K.
Fig. 1href198Illustration of the DU-VRPhref61A real-time communication scheme under a DU-VRP environment is depicted in Fig. href352. The dispatcher can tailor the routing to the dynamic environment at each time step. In a dynamic DU-VRP environment, a dispatcher is treated as an agent to give instructions to the vehicles. The dispatcher makes a sequence decision to fulfill the needs of each customer when the vehicle is ready (e.g., Customer A highlighted by the first double-headed arrow in Fig. 2). When the location of the customer results in environmental changes, the DU-VRP system receives environmental information in real time and makes a potential decision on updating a new route.
Fig. 2href11Timeline of events for the DU-VRPhref184An essential challenge in solving the DU-VRP is enabling the dispatchers to learn from the customer-vehicle data in real time. Furthermore, consumer demands may be uncertain and dynamically changing, and the requests that result from their demands are highly uncertain. Each customer-vehicle-based time is associated with the high-dimensional data to address the dynamic constraints. These realistic constraints may be designed by the Markov decision process (MDP), in which the dispatcher can select an action in a dynamic environment [href141]. However, conventional optimization methods are inapplicable to handle large-scale and complex MDPs, especially for collecting or controlling the components of the so-called value function in MDP, due to the so-called “curse of data dimensionality” that happens when the accuracy of the model decreases as the number of features increases. On the other hand, the dispatcher determines the daily route of vehicles in a given map while the locations of potential customers may differ. One primary concern in this practical setting is that uncertainty data need to be collected repeatedly with different customers’ demands in the map sequentially. Traditional optimization methods (e.g., robust optimization approach, distributionally robust optimization approach and chance-constrained programming approach) may not be effective in dealing with system dynamics for real-time decision-making [href68]. Hence, to reduce the DU-VRP data dimensionally, the development of a cutting-edge DRL approach through investigating a learning-while-optimizing strategy is needed.The recent progress in machine learning (ML) shows that deep reinforcement learning (DRL), which integrates deep learning (DL) and reinforcement learning (RL) techniques, is able to handle such a kind of routing problems with dynamics and uncertainties competently [href328]. While reinforcement learning is a model-free approach in which an agent learns the best rules from its environment [href136]. The policy of the MDP is fully dependent on the current state at each time step. Partially observable Markov decision processes (POMDPs) are the improved MDPs in which the state at each time step is not known with inerrability, that is, the state is not completely observable [href107, href353]. Graph-based POMDPs are an extension of standard POMDPs that add routing and graph information to the state space, action space, and reward structure.Graph-based POMDPs are an appropriate solution for the DU-VRP, because a dispatcher does not fully receive customer information across the service area but just at certain points. As a result, graph-based POMDPs make it easier to connect dynamic routing challenges to the graph-based approaches that are commonly used to address them – design and amend routes as new information is learnt. The end-to-end architecture has been proposed to address the underlying dynamics and uncertainties in real-world transportation systems, owing to its advantages in storing previous data and producing multi-step predictions [href126, href367]. To adapt partially observable decision-making for the DU-VRP situations, we tempt to model this problem as a group-based POMDP and develop a novel DRL method to solve the DU-VRP efficiently.The main contributions of this study are highlighted as follows. 


This study is among the first to investigate how to develop a novel end-to-end DRL framework for the DU-VRP.


We design a novel graph-based POMDP model to reflect the dynamic environment and uncertain customers’ demands in the DU-VRP.


An adaptive policy is designed to train the objective function value and enhance the learning performance, especially for the graph-based POMDP model.


The proposed DRL methodology is validated by baseline comparisons and sensitivity analysis using real-world data.

The remainder of this paper is organized as follows. We systematically integrate the recent applications of DRL and RL methods on the VRPs and related transportation cases in Section href313. Section href298 defines and formulates the DU-VRP by a graph-based POMDP and proposes a novel DRL methodology for the graph-based POMDP. Section href189 conducts computational experiments to validate the proposed DRL methodology’s performance. The conclusions of this study and future research directions are provided in the last section.2 Literature reviewWhile software and hardware computing capabilities have advanced in recent years, academia and business have increasingly focused on using machine learning approaches to address combinatorial optimization issues [href330]. To analyze large-scale data, DL’s perceptual skills and RL’s reasoning cognition may be used to combinatorial optimization issues. It will outperform standard approaches in terms of resilience and generality. The two key objectives for using deep reinforcement learning are to approximate and explore innovative policies [href346].Traditional optimization methods may be utilized to deal with the uncertainties in transportation systems and provide optimum VRP solutions. On the other hand, these technologies may not be effective in dealing with system dynamics for real-time decision-making. Because these methods need longer to create solutions, they cannot respond to system dynamics as quickly as reinforcement learning approaches, particularly when dealing with routing problems [href40]. As a result, RL may be a superior option to classic optimisation approaches since it can be taught with real-world data to interact more effectively with system dynamics and consumer demand uncertainties, resulting in quicker real-time routing and dispatching choices.Reinforcement learning is a popular technique for dealing with dynamic decision-making difficulties in the face of uncertainty. This portion of the review focuses mostly on model-free RL algorithms, which may be divided into three categories. There are three types of RL algorithms: 1) value-based algorithms (e.g., Q-learning, Deep Q-learning (DQN)) [href185, href220], 2) policy-based algorithms (e.g., REINFORCE) [href118], and 3) actor-critic algorithms, which integrate value-based with policy-based algorithms [href344]. Several new techniques for solving routing problems have been proposed in the literature, including: 1) recurrent neural network (RNN) model with an attentional mechanism based on an encoder-decoder architecture [href122,href65,href6], 2) graph embedding network models to express the policy that captures a node’s property in the context of its graph neighbourhood [href301, href211, href85], and 3) the representation of the environment based on the MDP [href76, href67, href2].Deep Reinforcement learning enhances classic RL by employing deep neural networks (DNNs) as function approximators, which has shown to be successful in a variety of control issues. Asynchronous techniques are given as an alternative, and they can execute concurrent and independent computing tasks. Minh et al. [href209] introduced the asynchronous advantage actor-critic (A3C), which allows many actor-critic agents to train in parallel. Each agent has its own set of parameters and interacts with the environment in a unique way. A global network receives updates from each agent and combines them asynchronously to create a global policy. Advantage actor-critic(A2C) is a synchronous and deterministic variation of A3C. Experiments by Minh et al. [href111] suggested that A3C can train quicker than DQN.We systematically integrate the recent applications of DRL and RL methods on the VRPs and related transportation cases, including urban freight deliveries and on-demand ridesharing for passenger transportation. On-demand ridesharing concerns based on DRL are investigated from two perspectives: 1) order dispatching that matches rider demands with available cars; and 2) vehicle repositioning is a proactive method of balancing vehicle supply and demand by moving idle automobiles from one zone to another. Existing DRL papers for urban freight delivery, order dispatching, and vehicle repositioning are reviewed in the next three paragraphs. We shall synthesize the problem characteristics and DRL approaches, highlighting the novelty of our paper.The travelling salesman problem (TSP), which involves the routing of a single vehicle, is a rudimentary variant of VRP problems. Archetti et al. [href15] considered the dynamic TSP with stochastic release dates as an MDP and proposed an RL-based approach for solving this TSP. Only spatial information of nodes is relevant in the TSP difficulties discussed above. DRL actions are used to gradually develop the vehicle path by adding nodes one at a time. VRPs have been created using cutting-edge approaches based on approximate algorithms, but they still have issues with long running time and poor solution quality when dealing with huge problems. The goal is to determine the best vehicle delivery routes and technician service routes in order to reduce the overall distance traveled by the delivery and service routes. Qiu et al. [href314] proposed a DRL method to solve it. Traditional approaches face a contradiction between computational efficiency and solution quality in the face of the present rapid rise in logistical needs. The majority of current optimization methods are taking a long time to compute in urban transportation networks. To address these issues, Zhao et al. [href127] developed a new DRL model that includes an encoder-decoder network with an attentional mechanism. To efficiently tackle the problem, Zhang et al. [href159] designed the Multi-Agent Attention Model, a unique RL method that can solve routing problems instantaneously while benefiting from extensive offline training. Yu et al. [href41] presented a novel DRL-based technique for developing routes in the shortest time possible. Their method combines the heuristic and the DNN to train the vehicle routing data. In the framework of crowdsourced urban delivery, Ahamed et al. [href354] designed a new DRL-based technique for solving the difficulty of distributing shipment orders to ad hoc couriers. Silva et al. [href279] presented a multi-agent RL approach to solve the VRP by using the RL ideas, especially the Q-Learning algorithm to improve the agent’s self-adaptive skills and to adapt quickly to the unique characteristics of the situation to be handled by the architecture. The goal of this framework is for each agent to be able to change their activities based on their interactions with other agents and the environment. Kang et al. [href205] considered a graph-based DRL algorithm that handles the acceptance of order requests and determines the route itineraries of multiple vehicles.In view of the important challenges to the efficient dispatching of orders, Zou et al. [href349] designed a DRL technique based on DQN for solving the online food ordering delivery problem. Kullman et al. [href194] addressed the situation of a ride-hailing service operator managing a fleet of electric vehicles. To maximize the profit, the operator must allocate cars to demands as they come, as well as recharge and reposition vehicles in advance of future requests. Kullman et al. [href312] applied DRL to tackle this challenge, creating policies that make decisions based on Q-value approximations learnt by deep neural networks. On a ride-sharing platform, ride dispatching is a core operation task that matches drivers to trip-requesting clients on a constant basis. Wang et al. [href235] offered an MDP model for the ride dispatching problem and presented learning techniques for improving the existing policy for drivers based on DQN with action search. Liang et al. [href128] provided a scalable framework for promoting real-time taxi operations that incorporates RL with a centralized programming structure. To construct a near-optimal strategy, Turan et al. [href145] developed the DRL method via Proximal Policy Optimization (PPO). Chen et al. [href16] used fleets of drones and vehicles to solve the challenge of dynamic same-day delivery. They describe a DQN strategy that takes advantage of both state and action space data. Liu et al. [href192] enhanced DRL for food route delivery by leveraging Dijkstra’s algorithm to determine the existing policy and compute the vehicle tour in each iteration, allowing the system to learn from past trajectories more effectively.In view of the important challenges to the efficient dispatch of vehicle repositioning orders, Liu et al. [href183] designed a novel RL method to handle the path of driving vehicles in emergency situations. Tang et al. [href31] offered an RL approach to solve an automated electric taxi fleet’s online routing problem with the consideration of the dynamic demands of the taxis. Koh et al. [href223] designed a novel DRL approach to solve a real-time vehicle navigation problem. Mao et al. [href37] designed a novel DRL technique to solve a real-world taxi dispatching service problem. They specifically include the RL framework, which focuses on how to find an optimum policy when the MDP technique is unavailable. As a result, rather than depending on any previous model knowledge, the agent in RL will communicate directly with the transport system and change the control approach. AI-Abbasi et al. [href274] introduced a distributed model-free pool (DeepPool) that learns optimal allocation strategies by observing the environment using DQN approaches. DeepPool also incorporates travel demand information with DL models to control vehicle dispatching for enhanced ride-sharing services. Basso et al. [href342] presented a Safe RL approach to solve the Dynamic Stochastic Electric Vehicle Routing Problem. The main concept is to use Monte Carlo simulations to learn offline about stochastic consumer demands and energy usage in order to design the route predictably and securely online. Guo et al. [href51] focused on establishing a real-time routing system to optimize the rentability of car service providers, given the complicated metropolitan situation and unpredictable social occurrences (e.g., taxi drivers). To deal with this challenge, an RL framework with a dynamic attention mechanism and a DNN is presented by Guo et al. [href104]. Because of the variety of settings, the ambiguity in sensor data, and the complicated interaction with other road users, operational decisions for vehicles are difficult. In the form of DRL with Monte Carlo tree search, Hoel et al. [href132] offered a comprehensive framework for tactical decision making that integrates the notions of planning and learning.We compare our current scenario with a wide variety of existing methods. Relevant papers solving VRPs under various scenarios are summarized in Table href239. Below is a summary of the existing methods from each category. The third column shows the observable characteristics of the state. It can be observed that most of the existing methods are for perfect state observability. The fourth column shows the characteristics of online or offline. With a few exceptions, as indicated in Table 1, the bulk of research in both urban freight delivery and order dispatching operates in a full-state observability environment and online fashion.
Table 1 Summary of DRL applications to VRPshref27These existing works did not optimize over time and in the face of uncertainty by capturing the dynamic aspects of customers’ demands. Because the underlying mathematical model is a non-trivial sequential stochastic optimization model that is impossible to solve, the bulk of the studies is based on full observability techniques without a mathematical model for the suggested dispatch policies. Some studies provided a mathematical model with an assumption of operating in a full state observability environment but ignoring the uncertainty in the future customer’s demand. Few works [href112, href8] applied the RL approach to optimise different forms of routing for partial observability. Moreover, each of these studies focused on a certain set of decisions that must be made in order to operate a fleet of vehicles. Based on the above literature review, we come to some insightful conclusions on the current research gaps and then highlight the main innovative aspects of our study. 


Till now, academic papers on the application of DRL (RL) to handle the dynamic and uncertain issues in the VRP system have rarely been found. To the best of our knowledge, this study is pioneering to explore how to develop a novel DRL methodology for the DU-VRP.


In the literature, most of DRL (DL) methods are based on full state observability. Instead, we adopt a new graph-based partially observable Markov decision process (POMDP) to formulate the partial state observability of the uncertainty and dynamics in the DU-VRP system.


Most of the existing research applies single neural networks (e.g., NN, RNN, etc.) to approximate the value function in their end-to-end DRL framework. In comparison, we propose a cutting-edge graph-embedding network to tackle the dynamic customers’ coordinates in the DU-VRP environment.

3 DRL methodology for DU-VRPThis section explains how the DRL methodology is developed for the DU-VRP. First, the DU-VPR is modelled and described by the POMDP in Section href382. Then, we propose a cutting-edge DRL methodology to solve the DU-VRP, combining a DL approach with a graph embedding network for the DU-VRP and an RL approach to optimize the graph embedding network in Section href261.3.1 Modelling the DU-VRP by POMDPIn contrast to the static VRP, real-world applications are often produced in a dynamic environment. Typically, in the dynamic VRP, the information available to the dispatchers may change during the execution of the routes with new demands. Therefore, DU-VRP situations do not necessarily have the Markov property because of the temporal dependence of customers’ demands. The problem with such an implementation is responding to new demands while optimizing the total traveling cost. Due to the difficulty of uncertainties and dynamics, static mixed integer programming (MIP) models are not directly applicable for modeling the DU-VRP. Graph-based POMDPs are an extension of standard POMDPs that add routing and graph information to the state space, action space, and reward structure. As a contribution to the literature, we present a novel graph-based partially observed Markov decision process (POMDP) to model the DU-VRP. Figure href120 illustrates the sequential process of the POMDP for modeling the DU-VRP by the notations defined as follows.


Time steps
Vehicle routings are considered to be optimized over a finite time horizon for each time step t = 0,1,2,…, T, where T represents the length of a morning or an evening peak period.


States
The graph-based POMDP is associated with a directed graph model, \(G=(\mathbb {C},E)\), where \(\mathbb {C}=\{c_{0},c_{1},\ldots ,\) cn,cn+ 1} represents a set of nodes; c0 and cn+ 1 are the source and sink depots; ci is customer i(i = 1,…,n). It is assumed that each customer is associated with only one node. The element \(E=\{(i,j),c_{i},c_{j}\in \mathbb {C}\} \) represents a set of arcs. A fleet of non-homogeneous vehicles k = 1,…,m is regarded as a set of agents. Each agent (vehicle) starts its route at the dummy source depot c0 and stops at the dummy sink depot cn+ 1. Each customer i has a demand di. A new binary variable uik that equals 1 if an agent k meets the demand of customer i , and 0 otherwise, is defined. A new binary variable xijk that equals 1 if an agent k travels from customer i to customer j; 0, otherwise, is defined.
Fig. 3href57Illustration the sequential process of the POMDP for DU-VRPhref265
The graph is transformed into the set of states \(\mathcal {S} = \{S_{t},t=1,\ldots ,T\}\) at time steps. Each state St consists of three elements: a set of customers in the current state \(\mathbb {C}_{t}\); vehicles’ location \({\mathscr{M}}_{t}\); and customers’ demands \(\mathcal {D}_{t}\) at time step t. Note that \(\mathbb {C}_{t}\) is composed of \(\mathbb {C}_{t}^{next}\) and \(\mathbb {C}_{t}^{new}\), \(\mathbb {C}_{t}\) ∈ \(\mathbb {C}\), where \(\mathbb {C}_{t}^{next}\) represents the unserved customers at time step t, and \(\mathbb {C}_{t}^{new}\) represents the new customers at time step t. Note that the vehicles’ locations are represented by \({\mathscr{M}}_{t}=\{{m_{k}^{t}},k=1,\ldots ,n\}\), where \({m_{k}^{t}}\) defines the location of vehicle k at time step t. Note that the customers’ demand \(\mathcal {D}_{t}\) is obtained dynamically during the servicing of the new customers. With the above formulations, the state space of the DU-VRP is denoted as \(S_{t}=\{(\mathbb {C}_{t},{\mathscr{M}}_{t},\mathcal {D}_{t})\mid \mathbb {C}_{t}\in \mathbb {R}_{+}^{n},{\mathscr{M}}_{t} \in \mathbb {R}_{+}^{n},\mathcal {D}_{t} \in \mathbb {R}_{+}^{n}.\)


Observation states
The DU-VRP decisions are dynamically executed in a real map, in which the demand of customers is inaccessible for the agents indeed. As a result, the agent has to infer a probability distribution over the set of possible states based on a set of observations. The observation states are associated with an observed current customers’ location \(\tilde {{\mathscr{M}}}_{t}\) and observed customers’ demands \(\tilde {\mathcal {D}}_{t}\) at time step t,defined as \(O_{t}=\{(\tilde {{\mathscr{M}}}_{t},\tilde {\mathcal {D}}_{t}) \mid \tilde {{\mathscr{M}}}_{t}=\{\tilde {m}_{ik}^{t},i,k=1,\ldots ,n\},\tilde {\mathcal {D}}_{t}=\{\tilde {d}_{i}^{t},i=1,\ldots ,n\}\) where \(\tilde {m}_{ik}^{t}\) is the observed distance between new customer i and vehicle k at time step t, and \(\tilde {d}_{i}^{t}\) is the observed demand of customer i at time step t. Thus, the whole observation space is defined as \(\mathcal {O}=\{O_{t},t=1,\ldots ,T\}\).


Action
Notation Yt+ 1 is defined as a pointer referring to the action of the available customers at time step t. The action sequence is denoted as \(\mathcal {Y}=\{Y_{1},\ldots ,Y_{t}\}\) recording the past information of actions.


Transition probability
The transition probability \(\mathbb {T}(s'\mid s,y)=\Pr (S_{t+1}=s'\mid S_{t}=s,y_{t}=y)\) represents the probability of a given action yt from the current state s to the next state s′ at time step t, for all \(s',s\in \mathcal {S}\), and \(y\in \mathcal {Y}\).


Observation probability
The observation transition probability \(p(o' \mid s,y)=\Pr (O_{t+1} = o' \mid S_{t} = s,Y_{t} = y)\) represents the probability of a given action Yt from the current state St to the observation state Ot+ 1 at time step t, for all \(o' \in \mathcal {O},s \in \mathcal {S}\), and \(y \in \mathcal {Y}\).


Observation Reward function
The reward function is defined as R(Ot+ 1,Yt+ 1) = V (Ot+ 1,Yt+ 1) − P(Ot+ 1,Yt+ 1), where V (Ot+ 1,Yt+ 1), P(Ot+ 1,Yt+ 1) are the complete observation reward and partial observation penalty functions, respectively. V (Ot+ 1, \(Y_{t+1} )={\sum }_{k=1}^{m}{\sum }_{i=1}^{n}u_{ik}-{\sum }_{k=1}^{m}{\sum }_{i=1}^{n}{\sum }_{j=1}^{n}\) cijxijk,
is calculated as the reward of demand fulfillment and the negative value of the distance between two customers during the action. \(P(O_{t+1},Y_{t+1} )=C_{1}{\sum }_{k=1}^{m}\) \(\max \limits \{{\sum }_{i=1}^{n}d_{i}{\sum }_{j=1}^{n}x_{ijk}-q_{k},0\}\), where the qk and di represent the capacity of vehicle k and demand of customer i, respectively. In this penalty function, the penalty term with coefficient C1 guarantees capacity constraints of each vehicle. The immediate reward Rt is generated after the vehicle moved from Ot to Ot+ 1 taking action yt+ 1.


Policy
The policy is a stochastic rule π, which controls the tour to serve all customer requests at each time step.


State Value function
The state value function is defined as \(V_ \pi (o) = \mathbb {E}_{\pi }\) \([ {\sum }_{k=0}^{\infty } R_{t+k+1} {\mid } O_{t}{=}o ]\), for all \(o \in \mathcal {O}\), which is the expected return when starting in state o executing a policy π.


Action Value function
The action value function \(\mathbb {Q}(o,y) = \mathbb {E} [R_{t+1}+v(O_{t+1} ) \mid O_{t}=o,Y_{t}=y]\), represents the expected return for taking action y in state o, for all \(o\in \mathcal {O}\), v ∈ V, \(y \in \mathcal {Y}\).

3.2 DRL methodologyThis subsection proposes a cutting-edge graph embedding network model (GENM), which is fundamentally an encoder-decoder structure. The modelling specifics of the proposed DRL methodology for the DU-VRP are described as follows.3.2.1 Overview of DRL methodologyThe main structure of the proposed DRL methodology is illustrated in Fig. href277. At the beginning of each time slot, the dispatchers receive data from the information dispatch center (IDC) that collects the current customer demand in real time. The main function of the IDC is to update the overall flow of the information about the locations and demands according to changes in the dynamic environment. The environment information can add a set of observations to the POMDP model through a DL approach, which is designed to obtain the observation values. Then, the agent observes the environment at each state and produces an action through an encoder-decoder network. Finally, the RL approach is performed to implement the optimized tour strategy.
Fig. 4href20The structure of the DRL methodology for the DU-VRPhref34According to the above analysis, the proposed DRL methodology for the DU-VRP is based on the bidirectional interfaces with a combination of DL and RL approaches. The DL approach develops tour strategy through the graph from the tour graph creation when receiving the real-time information of customers’ demands, and then delivers the tour strategy to the RL approach. Subsequently, the RL approach optimizes the tour strategy for each vehicle in such a way that each vehicle implements its tour strategy to drive towards the next node. Besides, the DL approach transmits a request for the current state to the POMDP model with the updated data.3.2.2 A DL approachWe design a DL approach containing the graph embedding network, RNN, attention mechanism with pooling layer. The DL approach is introduced to construct a real-time controller which can estimate the state and action values using the encoder-decoder network. In the input layer of the encoder-decoder network, we design a graph embedding method to analyze the characteristics of the DU-VRP at each state in Fig. href232. The Rectified Linear Unit (ReLU) activation function is the linear unit, as it can accelerate the convergence of the embedding node. This graph-based method has been applied to describe the relationships between each pair of nodes in the graph by compressing an adjacency matrix with the consideration of customer coordinates. This graph embedding network, also called Struc2vec (S2V), combines the environment state with the previously developed tour graph [href273]. This encoder-decoder network maps each customer in a tour graph into a low-dimensional vector in sequence with attention mechanism [href140].
Fig. 5href237The process of the input layer in the deep learning for the DU-VRPhref355To deal with the partially observable decision-making of the DU-VRP, an improved overall process of RNN is developed based on the graph embedding method in this study, are displayed in Fig. href171. The S2V and the RNNs are determined by the encoder network. However, due to conventional RNNs’ vanishing gradients, RNNs are limited in their ability to learn long-term relationships [href22]. Long Short Term Memory (LSTM) is a concept that is used in RNNs cells to solve long-term dependence difficulties and carry information across numerous time steps. The LSTM cells in the RNN ensure the customer information to handle the input context from the S2V network, which is computed as a p-dimensional feature embedding such as \({\mu _{i}^{t}}\), for each node ci at time step t in a graph G. The current nodes are aggregated recursively according to the graph’s topology. After some iterations, a new p-dimensional feature embedding for each node will be produced by the S2V network. The S2V network initializes the embedding \({\mu _{i}^{0}}=0\) for all elements of \(\mathbb {C}\), considering both current graph information and long-range interactions between these nodes. All embeddings update synchronously at each iteration, as shown in Eq. (1):
$$  \mu_{i}^{t+1}\gets f(c_{i}, \{ {\mu_{j}^{t}} \}_{j\in\mathcal{N}(i)} , \{\tilde{m}_{ij}^{t} \}_{j\in\mathcal{N}(i)},\tilde{d}_{i}^{t};{\Theta})  $$
                    (1)
                where \(\mathcal {N}(i)\) expresses the set of neighbors of node ci in G, \(\tilde {m}_{ij}^{t}\) represents the observed distance between customer i and customer j at time step t, \(\tilde {d}_{i}^{t}\) represents the observed demand of customer i at time step t, and f() is a nonlinear function parameterized by Θ. The features of the nodes ci are propagated to other nodes through a nonlinear function.
Fig. 6href383The process of an improved neural network for the DU-VRPhref125After the Tth iteration, the embedding of each node will contain information regarding its neighbors on the graph, including node features and propagation functions. The nonlinear function f(), which is used to update the embedding μi is defined as shown in Eq. (2):
$$ \begin{array}{@{}rcl@{}} \mu_{i}^{t+1} \!&\gets&\! \mathit{ReLU}(\theta_{1} c_{i} + \theta_{2}{\sum}_{j\in\mathcal{N}(i)}{\mu_{j}^{t}}  +  \theta_{3} {\sum}_{j\in\mathcal{N}(i)} \mathit{ReLU}(\theta_{4} \tilde{d}_{i}^{t} ) \\&&\!+\theta_{5} {\sum}_{j\in\mathcal{N}(i)} ReLU(\theta_{6} \tilde{m}_{ij}^{t} )) \end{array} $$
                    (2)
                where 𝜃1 ∈ Rp;𝜃2,𝜃3, 𝜃5 ∈ Rp×p;and 𝜃4,𝜃6 ∈ Rp represent the dimensional model vectors, which are represented by the parameter Θ. The ReLU function \((ReLU\left (z\right )=\max \limits \) {0,z}) is applied to accelerate the convergence of the embedding node. Then the embeddings for each node are input into an RNN, which consists of LSTM that uses the input data to create a sequence of p-dimensional latent memory states \({C_{i}^{e}}\in \mathbb {R}^{p}\) in the encoder network. Equations (3) to (8) are the internal expressions of the LSTM cells:
$$  f_{i}=\sigma(W_{f}\cdot\left[h_{i-1},{\mu_{i}^{T}}\right]+b_{f})  $$
                    (3)
                $$  J_{i}=\sigma(W_{j}\cdot\left[h_{i-1},{\mu_{i}^{T}}\right]+b_{j})  $$
                    (4)
                $$  {\widetilde{C}}_{i}=tanh(W_{C}\cdot\left[h_{i-1},{\mu_{i}^{T}}\right]+b_{C})  $$
                    (5)
                $$  {C_{i}^{e}}=f_{i}\ast C_{i-1}^{e}+J_{i}\ast{\widetilde{C}}_{i}  $$
                    (6)
                $$  o_{i}=\sigma(W_{O}\left[h_{i-1},{\mu_{i}^{T}}\right]+b_{O})  $$
                    (7)
                $$  h_{i}=o_{i}\ast tanh({C_{i}^{e}})  $$
                    (8)
                where \(\sigma \left (z\right )=1/(1+e^{-z})\) is a sigmoid function, and \(\tanh \) is an activation function that converts the input to a real-value between -1 and 1. The symbol ∗ indicates the multiplication of the corresponding elements of the operation; Wf,Wj,WC,WO denote the weight parameters; and bf, bj,bC,bO denote the bias parameters. A set of state vectors S = (S1,S2,…,St) with hidden states H = (h1,h2,…, ht) is produced when the embedding node passes through the encoder.Next, the LSTMs cells are adopted to develop the decoder, which is decoding the encoder context and outputting the predicted sequence to the nodes in tours. The decoder constructs a series of p-dimensional latent memory by Eq. (2). Following that, the decoder is used to decode the encoders’ input and output the tour in an iterative fashion. In each iteration, the decoder applies a series of p-dimensional latent states of the current partial constructed customer \(y\left (<i\right )\) to predict the distribution of the next node.To predict a distribution among the customers from the encoding context, we improve an attention mechanism (AM) method proposed by Nazari et al. [href138]. The encoder part only contains the coordinates and demands information at the current iteration and the decoder part only remembers static information from the past in the Nazari et al. [href381]. In the output layer of the DL approach, the AM is applied to compute the alignment vector in the decoder network. We design an attention mechanism with a pooling layer to modify the input of the decoder network. All the hidden states \({C_{t}^{i}}\) consist of the static (the states information) and dynamic (the observation states information) element \({C_{i}^{e}}\in \mathbb {R}\) from each iteration in the encoder. \({C_{t}^{i}}\) is important as it is a combination of the outputs from the encoder and decoder, containing the previous information about customers’ demands. Before feeding the hidden state into the decoder, we first apply a pooling layer on one of the dimensions of the hidden state and make sure it has the same shape as the decoder input. The encoder and decoder hidden state \({C_{i}^{d}}\in \mathbb {R}\) are combined to determine the alignment vector as follow by Eqs. (9)-(12).
$$  C_{t}^{\prime}=pooling\left( {C_{t}^{i}}\right)  $$
                    (9)
                $$  {a_{t}^{i}}={v_{a}^{T}}\ast tanh\left( W^{e}C_{t}^{\prime}+W^{d}{C_{i}^{d}}\right)  $$
                    (10)
                $$  a_{t}\left( \left\{{C_{i}^{e}}\right\}_{i=1}^{n},{C_{i}^{d}},W^{e},W^{d},v\right)=softmax({a_{t}^{i}})  $$
                    (11)
                $$  c_{t}={\sum}_{i=1}^{n}C_{t}^{\prime} {a_{t}^{i}}  $$
                    (12)
                where va ∈ Rp represents an attention vector and We,Wd ∈ Rp×p denote attention matrices. The function softmax is called an activation function that allocates the probability of visiting the next customer \(y\left (i\right )\) according to Eq. (13):
$$  p\left( y\left( i\right) \mid y\left( <i\right),S\right )=softmax\left( \widetilde{u_{i}}\right)  $$
                    (13)
                where \(\widetilde {u_{i}} = v^{T}tanh\left (W_{c}\left [\left \{{C_{i}^{e}}\right \}_{i=1}^{n},c_{t}\right ]\right ),v\!\in \! R^{p}\) represents an attention vector. This probability distribution indicates that the decoder network is attending to the customer in the given state S in the form of a probability. Consequently, we use the rule of the probability chain to factorize the probability of a tour, as obtained by Eq. (14):
$$  p\left( Y \mid S\right) = {\prod}_{i=1}^{n}{p\left( y\left( i\right) \mid y\left( <i\right),S\right)}  $$
                    (14)
                3.2.3 An RL approachWe present an RL approach to train the model parameters of the proposed DL approach for DU-VRP. The RL approach, which is different from the supervised learning approach, uses a reward mechanism for the adaptive agents to improve their performance, instead of relying on optimal solutions to the DU-VRP. In this RL approach, as shown in Fig. href28, the agents can interact with the environmental changes controlled by a processor that consists of the graph-learning, sequence-learning, and policy-learning networks. The processor can observe the states from a dynamic DU-VRP environment. Subsequent to the observation, the updated state is the input to the graph-learning network. Then, the sequence-learning network reorders the updated state and delivers the outcome to the policy-learning network, which produces the optimal action to the environment.
Fig. 7href88Interaction agent-environment in the reinforcement learning for the DU-VRPhref178To deal with the partially observable state of the DU- VRP, we develop a novel RL approach based on the POMDP model. The asynchronous advantage actor-critic (called “A3C”) algorithm uses asynchronous gradient descent for optimization of deep neural network controllers based on actor-critic methods, introduced by Mnih [href364], where an ‘actor’ refers to the learned policy, and ’critic’ refers to add a value function to the REINFORCE policy gradient algorithm [href244]. The actor develops a sample route throughout the training procedure. The value function, on the other hand, adjusts the intended rewards and improves itself using the created routes before revising the policy. The value function is then used to update the policy’s parameters in the area of improving performance. Because the critic is modelled using a bootstrap approach, the variance is reduced, making the learning more reliable than vanilla policy gradient methods. The A3C algorithm allows many actor-critic agents to train in parallel. Each actor-critic has its own set of parameters and interacts with the environment in a unique way. A global network receives updates from each agent and combines them asynchronously to create a global policy. In this way, parallel actor-learners accumulate updates for improving training stability. A model-free policy-based RL approach by integrating the A3C algorithm is proposed to train the model parameters for the DU-VRP, as displayed in Fig. href58.
Fig. 8href54The framework of the asynchronous advantage actor-critic algorithmhref250There are two sets of parameters in the A3C algorithm. In the DL method, we employ the symbol Γ to denote the collection of model parameters, including the graph embedding 𝜃; LSTM parameters W,b; and attention parameters v,W. The performance measure is defined based on the policy gradient method as follow:
$$  J(\mathrm{\Gamma})=\mathbb{E}_{\pi\sim\mathrm{\Gamma}(.\mid o)}V_{\pi}\left( o\right)  $$
                    (15)
                The ‘critic’ is provided to estimate the action value function associated with parameters φ, which can train the model parameters Γ as ‘actor’. The A3C algorithm follows an approximate policy gradient
$$  \mathrm{\nabla}_{\mathrm{\Gamma}}J(\mathrm{\Gamma}) = \mathbb{E}_{\pi\sim\mathrm{\Gamma} \left (. \mid o \right)}\left[\mathrm{\nabla}_{\mathrm{\Gamma}}log\pi_{\mathrm{\Gamma}}\left( Y \mid O\right) Q_{\varphi}\left( O , y\right)\right]  $$
                    (16)
                Then, we introduce a baseline function B(O), which can reduce variance without changing expectations for the RL approach [href155]. Typically, the baseline function is the state value function \(B\left (O\right )=V_{\pi }\left (O,\varphi \right )\). Thus,
$$ \begin{array}{@{}rcl@{}} \mathrm{\nabla}_{\mathrm{\Gamma}}J(\mathrm{\Gamma})&=&\mathbb{E}_{\pi\sim\mathrm{\Gamma}\left( . \mid O\right)}\left[\mathrm{\nabla}_{\mathrm{\Gamma}}log\pi_{\mathrm{\Gamma}}\left( Y \mid O\right)\left( Q_{\varphi}\left( O,y\right)\right.\right.\\&&\left.\left.-V_{\pi}\left( O,\varphi\right)\right)\right] \end{array} $$
                    (17)
                The proposed A3C algorithm’s pseudo-code is described below, where Γo,φo are the weights of a global network with weights assigned to the actor and critic, respectively. Besides, the agents runn in parallel threads with their own network parameters, are denoted as Γn,φn for agent n.
4 Experiments and resultsThis section conducts three groups of experiments to assess the proposed DRL model’s performance. The initial set of experiments is designed to train the DRL model. The performance of the DRL model is then demonstrated on the basic VRP dataset in the second set of experiments. Finally, the third groups of experiments is implemented in real-world scenarios to ensure that the DRL method is successful. All programs are coded in PyCharm with Python 3.7 and Pytorch 1.8, and the results reported below are obtained on a PC with an Intel Core i7 processor and 32GB of RAM.4.1 Experimental parametersSince the DU-VRP is a relatively new topic with no standard dataset, the initial set of experiments uses randomly generated cases. Meanwhile, customer locations are created at random on a 1 ∗ 1 map, with the depot as the first node and the rest as customer nodes. The customers’ demand is a random variable parameter within [1,9]. The vehicles have a capacity ranging from 20 to 60. Finally, vehicles of various capacities will arrive at the depot and transport items to all customers.The configuration of the parameters in the initial set of experiments is shown in Table href337.
Table 2 The experiment configuration of DU-VRP instanceshref212The initial set of experiments focuses on creating an optimal strategy to control the DRL method. During the procedure, parallel actor-learners in the proposed algorithm are introduced to speed up the proposed technique, and randomly generated issues are employed as training cases. Many parameters in the proposed technique must be adjusted in order to reach the optimal outcome. These parameters are chosen according to the training process. Table href311 shows the values of parameters.
Table 3 Parameters configuration for the proposed DRLhref148We save and test the RL approach after the training process to see how it performs. The experiment instance is presented in Table href21 during the test phase. Table href213 shows the average tour lengths of our proposed DRL.
Table 4 Testing process of the experimental instanceshref359Table 5 Average tour length of proposed DRLhref52The proposed DL approach can handle the DU-VRP efficiently by applying RL for generating better policies, as shown in Table 5.4.2 Performance on benchmarking datasetsIn this second set of the experiment, we adopt the RNNs with 128 hidden units in the encoder network, which corresponds to the number of nodes in the graph embedding. The inputs by small batch B = 128 are generated using the benchmark data set during the training. These small batches directly determine the weights of our model in optimization. Each static (the states information) element consisting of the customer coordinate is embedded into a vector with 128 hidden units sharing among the inputs. The dynamic (the observation states information) element is then embedded in the same way. The dynamic elements including the customers’ demand, the remaining load of vehicles are mapped to a vector of size 128. For the decoder network, we employ a layer of 128 hidden units in the LSTM. In the actor and critical networks, we initialize two types of settings, either by using greedy search for decoding, or by applying the best result from a batch of sampled results as the softmax results. We initialise two types of settings in the actor and critic networks: greedy search for decoding and using the optimal performance from a set of observed outputs as the softmax results.To evaluate the proposed A3C performance, the different numbers of customers with the same vehicle are introduced. To show the overall performance of DRL, an evaluation index called the gap, which is calculated between the DRL and the OPT by (18), where X means the DRL, T(X) denotes the average route time of X and OPT denotes the best solution.
$$  GAP=\frac{T(X)-T(OPT)}{T(OPT)}  $$
                    (18)
                This sort of difficulty is rather common, and a typical benchmark can be from the website of the NEO (cf.href204https:// https://neo.lcc.uma.es/vrp/vrp-instances/neo.lcc.uma.es/vrp/vrp-instances/). We apply the two benchmark data sets (Set-A and Set-B) to verify the proposed DRL methodology. The proposed A3C algorithm is modified in the second set of experiments to fit the DU-VRP situation. Besides, we set the dynamic scenarios to verify the proposed algorithm. Furthermore, the benchmark’s best solution is utilised to illustrate the algorithm’s overall level. All instances are utilised to evaluate the performance of all algorithms during the experiment. The number of entities (vehicles) is limited for each instance to ensure that all algorithms use the same computer capacity and can meet demand changes. First, we apply the data of Set-A to verify the performance of DRL. Besides, the OPT is obtained from the website, the table column ’KX’ represents there are X vehicles, and the instance named ’A-nY’ means the data with Y customers. The table columns refer respectively to: the Entity (Vehicles), the number of instances in the entity (No.), the instances name(Instances), the GAP(%), the value of the total traveling distance (Distance) obtained with OPT and the DRL, and the computational time in seconds (Time). Table href218 shows the performance of the algorithms while solving the data of Set-A.
Table 6 Performance of the algorithms while solving the benchmarking instanceshref276The partial observations are obtained and shown in Table 6. It is observed that DRL can achieve satisfactory results in a very short time. For the instances with number of customers 32, 34 in K5 the performance of DRL methodology is close to OPT. In particular, the proposed A3C RL approach, in particular, outperforms the benchmark, and improves or reaches the adopting a strategic in all cases. The gap between the A3C RL approach and the OPT from 1.58% to 9.75% while the average gap is 5.79% in these cases. To summarise, the proposed A3C RL approach can produce an satisfactory solution in a short amount of time and its usefulness has been demonstrated.Then, using the additional data from Set-B, we evaluate the A3C RL approach performance with different amounts of demands. The performance of the algorithms in solving the data of Set-B instances with the various scales (instance types and number of customers) are analyzed in Table href259, which indicates that the ’KX’ represents the X vehicles and the instance named ’B-nY’ means the instance with Y customers.
Table 7 Performance of the algorithms while solving the benchmarking instanceshref309From Table 7, it is observed that the GAP for these VRP instances is controlled by the 13%. All instances are controlled by the 10% except the number of customer 39 in K5. For the instances with the number of customers 31, 34 and 35 in K5 the performance of A3C RL approach is close to OPT. Specifically, in the case of a certain the number of vehicles, the calculation time increases with the increase of customer demand, but the GAP between DRL and OPT is very small. Then, as the number of vehicles increases, so does the demand of customers, and the GAP between DRL and OPT is within 10%. After that, the GAP value is always within 10%, indicating that DRL can meet the solution quality under the change of demand. As a consequence, the proposed A3C RL approach can produce outstanding outcomes under different customers’ demands. Finally, the outcome is displayed in Table 7, and it appears to be satisfactory and supports the A3C RL approach’s efficacy. To summarise, the proposed A3C RL approach can produce an satisfactory solution in a short amount of time and its usefulness has been demonstrated.4.3 Performance in real-world scenariosThe third group of experiments is shown in a real-world transportation network using data collected from Cologne, Germany (cf. href18https://sumo.dlr.de/docs/Data/Scenarios/TAPAS https://sumo.dlr.de/docs/Data/Scenarios/TAPASCologne.htmlCologne.html). We adopt the same RNNs with 128 hidden units in the encoder network, which is consistent with the number of nodes in the graph embedding. The inputs by small-batch B = 128 are generated using the Cologne data set during the training. These small batches directly determine the weights of our model in optimization.4.3.1 Experimental settingsAll of the experiments in the third group use the data of Cologne, Germany. The real-world data are obtained from the OpenStreetMap file [href243]. The latitude and longitude of each node in this real-world map are applied to construct the actual roadmap. A Python package is used to download administrative boundary shapes and street networks from Osmnx [href23]. The sampling size is divided into five-minute parts from 8:00 am to 10:00 pm and all demands have been satisfied. Averaging all speeds that can be matched to the route in the relevant time segment using GPS records yields the real-time traffic speed of any arbitrary road. In the proposed DRL experiment, we adopt the RNNs with 128 hidden units in the encoder network, which is consistent with the number of nodes in the graph embedding. The inputs by small-batch B = 128 are generated using the small transport network during the training. These small batches directly determine the weights of our model in optimization. We run 3,000 training steps to train the DRL model on the Cologne dataset, and set the buffer size to 5,000 experiences. We apply the Adam optimizer [href231] with the learning rate of 1e-3 to train encoder and decoder network. Over the training stages, the 𝜖-greedy [href366] policy with a decay rate of 0.9995 is utilised. There are N × N vertex generating procedures in each episode. We suppose that the initial node is the depot. The test case obtained by Osmnx is used to construct a tour graph for parameters adjustment in graph-embedding-network by A3C Algorithm. The training process is repeated 10000 episodes for approximately 45 hours. At the start of each episode, the graph is randomly generated as input with corresponding demands.4.3.2 Solution qualityWe test the experiments in different tour sampling situations with various customers’ demands in the same transportation network to show the quality of the A3C RL approach. The customers’ locations are randomly selected from nodes in the Cologne map. We design three types of sampling strategies to generate test cases: (i) greedy, which chooses the node with the maximum probability as another location; (ii) beam search (BS), which takes account of the most likely paths and then chooses the node with the shortest distance length; and (iii) sample search (SS), which selects M candidate tours based on the possible outcomes established by the probability policy \(p\left (y\left (i\right ) \mid y\left (<i\right ),S\right )\). We test the experiments with candidates 100, 300, 500, 1280, or 12800, respectively, using the sample search strategy for the DU-VRP. The performance of the algorithms while solving Cologne data set with the various scales (e.g., strategy choices and instance sizes) are analyzed in Table href151, which the Stategy named ’DRL-X-Y’ means the DRL with X strategies and with Y candidates. In summary, the proposed DRL methodology is applicable to solve the DUVRP in real time.
Table 8 Performance of the algorithms while solving real-world instanceshref455 ConclusionIn this paper, we introduce and define a more generalized problem called the dynamic and uncertain vehicle routing problem (DU-VRP), which covers the classical vehicle routing problem with dynamic environment and uncertain customers’ demands. We design a partially observed Markov decision process (POMDP) to reflect the partial observation states in the DU-VRP environment. The proposed DRL methodology combines the merits of a deep learning approach and a reinforcement learning approach to learn and optimize the routing decisions in a dynamic and uncertain environment. Computational results show that the proposed DRL methodology can tackle the curse of dimensionality because it can handle the partial observation of the state space with an improved attention mechanism method.Regarding future research directions, other machine learning approaches for training deep neural networks will be investigated in depth. The proposed DRL methodology would be extended to tackle more real-world applicable VRPs with incomplete traffic information [href75, href221]; to solve other routing-production-related combinatorial optimization problems such as job shop scheduling with the parallel use of resources and sequence-dependent setup times [href123, href152]; and to deal with industry-oriented mining optimisation problems [href275,href327,href295].

ReferencesSteever Z, Karwan M, Murray C (2019) Dynamic courier routing for a food delivery service. Comput Oper Res 107:173–188. href108href219 
    href26 
    href363 
    href167 
                Drent C, Keizer MO, Houtum GJ van (2020) Dynamic dispatching and repositioning policies for fast-response service networks. Eur J Oper Res 285:583–598. href42href321 
    href322 
    href177 
    href83 
                Hong J, Lee M, Cheong T, Lee HC (2019) Routing for an on-demand logistics service. Transp Res Part C: Emerg Technol 103: 328–351. href224href338 
    href306 
                Zhang Z, Sun Y, Xie H, Teng Y, Wang J (2018) Gmma: Gpu-based multiobjective memetic algorithms for vehicle routing problem with route balancing. Applied Intelligence 49:63–78. href289href375 
    href323 
                Vidal T, Laporte G, Matl P (2020) A concise guide to existing and emerging vehicle routing problem variants. Eur J Oper Res 286:401–416. href70href81 
    href360 
    href50 
    href73 
                Nasri M, Metrane A, Hafidi I, Jamali A (2020) A robust approach for solving a vehicle routing problem with time windows with uncertain service and travel times. Int J Ind Eng Comput 11:1–16. href260href162 
    href291 
                Mazyavkina N, Sviridov S, Ivanov S, Burnaev E. (2021) Reinforcement learning for combinatorial optimization: A survey. Comput Oper Res 134:105400. href180href190 
    href310 
    href263 
    href292 
                Karimi-Mamaghan M, Mohammadi M, Meyer P, Karimi-Mamaghan AM, Talbi E-G (2022) Machine learning at the service of meta-heuristics for solving combinatorial optimization problems: a state-of-the-art. Eur J Oper Res 296:393–422. href38href317 
    href117 
    href114 
    href170 
                Wang Q, Tang C (2021) Deep reinforcement learning for transportation network combinatorial optimization: a survey. Knowl-Based Syst 233:107526. href286href90 
    href318 
                Parvez Farazi N, Zou B, Ahamed T, Barua L (2021) Deep reinforcement learning in transportation research: a review. Transp Res Interdiscip Perspect 11:100425. href144href264 
    href168 
                Ulmer MW, Thomas BW (2020) Meso-parametric value function approximation for dynamic customer acceptances in delivery routing. Eur J Oper Res 285:183–195. href102href156 
    href33 
    href115 
    href47 
                Ning C, You F (2019) Optimization under uncertainty in the era of big data and deep learning: when machine learning meets mathematical programming. Comput Chem Eng 125:434–448. href222href339 
    href154 
                Sutton RS, Barto AG (2018) Reinforcement learning part II an introduction, 2nd edition. MIT press, Cambridgehref133 
    href157 
                Schrittwieser J, Antonoglou I, Hubert T, Simonyan K, Sifre L, Schmitt S, Guez A, Lockhart E, Hassabis D, Graepel T, Lillicrap T, Silver D (2020) Mastering atari, go, chess and shogi by planning with a learned model. Nature 588:604–609. href266href174 
    href345 
                Hubmann C, Schulz J, Becker M, Althoff D, Stiller C (2018) Automated driving in uncertain environments: planning with interaction and uncertain maneuver prediction. IEEE Trans Intell Veh 3:5–17. href92href53 
    href238 
                Pouya P, Madni AM (2021) Expandable-partially observable Markov decision-process framework for modeling and analysis of autonomous vehicle behavior. IEEE Syst J 15:3714–3725. href252href49 
    href163 
                Bengio Y, Lodi A, Prouvost A (2021) Machine learning for combinatorial optimization: a methodological tour d’horizon. Eur J Oper Res 290:405–421. href373href288 
    href98 
    href179 
    href308 
                Bui Khac Hoai N, Cho J, Yi H (2021) Spatial-temporal graph neural network for traffic forecasting: an overview and open research issues. Applied Intelligence 52:2763–2774. href60href248 
    href64 
                Vesselinova N, Steinert R, Perez-Ramirez DF, Boman M (2020) Learning combinatorial optimization on graphs: a survey with applications to networking. IEEE Access 8:120388–120416. href347href229 
    href329 
                Sun P, Hu Y, Lan J, Tian L, Chen M (2019) Tide: time-relevant deep reinforcement learning for routing optimization. Futur Gener Comput Syst 99:401–409. href283href365 
    href89 
                Huynh TT, Lin CM, Lee K, The Vu M, Nguyen N, Chao F (2021) Intelligent wavelet fuzzy brain emotional controller using dual function-link network for uncertain nonlinear control systems. Applied Intelligence 52:2720–2744. href113href270 
    href43 
                Xu R, Li M, Yang Z, Yang L, Qiao K, Shang Z (2021) Dynamic feature selection algorithm based on q-learning mechanism. Appl Intell 51:1–12. href103href370 
    href319 
                Wang Q (2021) Varl: a variational autoencoder-based reinforcement learning framework for vehicle routing problems. Appl Intell. href227Chen L, Cui J, Tang X, Qian Y, Li Y, Zhang Y (2021) Rlpath: a knowledge graph link prediction method using reinforcement learning based attentive relation path searching and representation learning. Applied Intelligence 52:4715–4726. href1href253 
    href134 
                Zhang K, He F, Zhang Z, Lin X, Li M (2020) Multi-vehicle routing problems with soft time windows: a multi-agent reinforcement learning approach. Transportation Research Part C: Emerging Technologies 121:102861. href137href294 
    href93 
                Nazari M, Oroojlooy A, Snyder LV, Takáč M (2018) Reinforcement learning for solving the vehicle routing problem. In: Proceedings of the 32nd international conference on neural information processing systems (NeurIPS 2018). Montréal, pp 9861–9871. href341https://dl.acm.org/doi/10.5555/3327546.3327651Zhao J, Mao M, Zhao X, Zou J (2021) A hybrid of deep reinforcement learning and local search for the vehicle routing problems. IEEE Trans Intell Transp Syst 22:7208–7218. href214href228 
    href97 
                Dai H, Khalil EB, Zhang Y, Dilkina B, Song L (2017) Learning combinatorial optimization algorithms over graphs. Adv Neural Inf Process Syst. Red Hook, pp 6349–6359. href256https://dl.acm.org/doi/ https://dl.acm.org/doi/10.5555/3295222.329538210.5555/3295222.3295382Csy A, Ahfc B, Ksc A (2020) An actor-critic deep reinforcement learning approach for metro train scheduling with rolling stock circulation under stochastic demand. Transp Res B Methodol 140 :210–235. href262href324 
    href56 
                Kullman ND, Cousineau M, Goodson JC, Mendoza JE (2021) Dynamic ride-hailing with electric vehicles. Transp Sci 1–20. href149Ulmer MW, Goodson JC, Mattfeld DC, Thomas BW (2020) On modeling stochastic dynamic vehicle routing problems. EURO J Transp Logist 9:100008. href19110.1016/j.ejtl.2020.100008href343 
    href169 
                Mnih V, Badia AP, Mirza M, Graves A, Harley T, Lillicrap TP, Silver D, Kavukcuoglu K (2016) Asynchronous methods for deep reinforcement learning. In: Proceedings of the 33rd international conference on international conference on machine learning ICML 2016. New York, pp 1928–1937. href305Archetti C, Feillet D, Mor A, Speranza MG (2020) Dynamic traveling salesman problem with stochastic release dates. Eur J Oper Res 280:832–844. href110href172 
    href14 
    href348 
    href77 
                Qiu H, Wang S, Yin Y, Wang D, Wang Y (2022) A deep reinforcement learning-based approach for the home delivery and installation routing problem. Int J Prod Econ 244:108362. href369href371 
    href299 
                Yu JJQ, Yu W, Gu J (2019) Online vehicle routing with neural combinatorial optimization and deep reinforcement learning. IEEE Trans Intell Transp Syst 20:3806–3817. href105href147 
    href334 
                Ahamed T, Zou B, Farazi NP, Tulabandhula T (2021) Deep reinforcement learning for crowdsourced urban delivery. Transp Res B Methodol 152:227–257. href5910.1016/j.trb.2021.08.015href245 
    href173 
                Silva ML, Souza SD, Souza MF, Bazzan A (2019) A reinforcement learning-based multi-agent framework applied for solving routing and scheduling problems. Exp Syst Applic 131:148–171. href24010.1016/j.eswa.2019.04.056href282 
    href293 
                Kang Y, Lee S, Chung BD (2019) Learning-based logistics planning and scheduling for crowdsourced parcel delivery. Comput Industr Eng 132:271–279. href35810.1016/j.cie.2019.04.044href217 
    href287 
                Zou G, Tang J, Yilmaz L, Kong X (2021) Online food ordering delivery strategies based on deep reinforcement learning. Appl Intell. href36110.1007/s10489-021-02750-3Wang Z, Qin Z, Tang X, Ye J, Zhu H (2018) Deep reinforcement learning with knowledge transfer for online rides order dispatching. In: Proceedings IEEE international conference data mining. ICDM, pp 617–626, DOI href377, (to appear in print)Liang E, Wen K, Lam WHK, Sumalee A, Zhong R (2021) An integrated reinforcement learning and centralized programming approach for online taxi dispatching. IEEE Trans Neural Networks Learn Syst. href35Turan B, Pedarsani R, Alizadeh M (2020) Dynamic pricing and fleet management for electric autonomous mobility on demand systems. Transp Res Part C: Emerg Technol 121:102829. href153href99 
    href374 
                Chen X, Ulmer MW, Thomas BW (2022) Deep q-learning for same-day delivery with vehicles and drones. Eur J Oper Res 298 :939–952. href160href69 
    href195 
    href161 
    href187 
                Liu Z, Li J, Wu K (2020) Context-aware taxi dispatching at city-scale using deep reinforcement learning. IEEE Trans Intell Transp Syst 23:1–14. href72href181 
    href320 
                Liu X, Zhang D, Zhang T, Cui Y, Chen L, Liu S (2021) Novel best path selection approach based on hybrid improved a* algorithm and reinforcement learning. Applied Intelligence 51:9015–9029. href280href325 
    href199 
                Tang X, Li M, Lin X, He F (2020) Online operations of automated electric taxi fleets: an advisor-student reinforcement learning framework. Transp Res Part C Emerg Technol 121:102844. href182href304 
    href87 
                Koh S, Zhou B, Fang H, Yang P, Ji Z (2020) Real-time deep reinforcement learning based vehicle routing and navigation. Appl Soft Comput 96:106694. href278href300 
    href285 
                Mao C, Liu Y, Shen Z (2020) Dispatch of autonomous vehicles for taxi services: a deep reinforcement learning approach. Transportation Research Part C Emerging Technologies 115:102626. href15810.1016/j.trc.2020.102626href316 
    href32 
                Al-Abbasi AO, Ghosh A, Aggarwal V (2019) Deeppool: distributed model-free algorithm for ride-sharing using deep reinforcement learning. IEEE Trans Intell Transp Syst 20:4714–4727. href33210.1109/TITS.2019.2931830href255 
    href196 
                Basso R, Kulcsár B, Sanchez-Diaz I, Qu X (2022) Dynamic stochastic electric vehicle routing with safe reinforcement learning. Transp Res Part E: Logist Transp Rev 157:102496. href44href62 
    href242 
                Guo P, Xiao K, Ye Z, Zhu W (2021) Route optimization via environment-aware deep network and reinforcement learning. ACM Transactions on Intelligent System and Technology 12:1–21. href19710.1145/3461645href4 
    href130 
                Hoel C-J, Driggs-Campbell K, Wolff K, Laine L, Kochenderfer MJ (2020) Combining planning and deep reinforcement learning in tactical decision making for autonomous driving. IEEE Trans Intell Veh 5:294–305. href24110.1109/TIV.2019.2955905href96 
    href225 
                Nguyen Q, Vien N, Dang V. -H., Chung T (2020) Asynchronous framework with reptile+ algorithm to meta learn partially observable Markov decision process. Appl Intell 50:4050–4062. href71href5 
    href106 
                Wu X, Du Z, Guo Y, Fujita H (2019) Hierarchical attention based long short-term memory for chinese lyric generation. Appl Intell 49:44–52. href142href30 
    href78 
                Boeing G (2017) Osmnx: new methods for acquiring, constructing, analyzing, and visualizing complex street networks. Comput Environ Urban Syst 65:126–139. href356href215 
    href210 
                Melinte O, Vladareanu L (2020) Facial expressions recognition for human-robot interaction using deep convolutional neural networks with rectified adam optimizer. Sensors 20:2393. href165href100 
    href79 
                Lesch V, König M, Kounev S et al (2022) Tackling the rich vehicle routing problem with nature-inspired algorithms. Appl Intell. href307Zhang Q, Liu SQ, Masoud M (2020) A traffic congestion analysis by user equilibrium and system optimum with incomplete information. J Comb Optim. In press. href139Liu CL, Chang CC, Tseng CJ (2020) Actor-critic deep reinforcement learning for solving job shop scheduling problems. IEEE Access 8:71752–71762. href3href234 
    href175 
                Liu SQ, Kozan E (2016) Parallel-identical-machine job-shop scheduling with different stage-dependent buffering requirements. Comput Oper Res 74:31–41. href281href46 
    href121 
    href91 
    href376 
                Kozan E, Liu SQ (2017) An operational-level multi-stage mine production timetabling model for optimally synchronising drilling, blasting and excavating operations. Int J Mining, Reclam Environ 31:457–474. href268href86 
    href84 
                Liu SQ, Kozan E (2019) Integration of mathematical models for ore mining industry. Int J Syst Sci Oper Logist 6:55–68. href296href74 
    href207 
                Zeng L, Liu SQ, Kozan E et al (2021) A comprehensive interdisciplinary review of mine supply chain management. Resour Policy 74:102274. href271href290 
    href368 
                href246AcknowledgmentsThis work is supported by the National Natural Science Foundation of China under Grant No. 71871064.Author informationAuthors and AffiliationsSchool of Economics and Management, Fuzhou University, Fuzhou, 350108, Fujian, ChinaWeixu Pan & Shi Qiang LiuAuthorsWeixu Panhref82You can also search for this author in
                        href297 href48Shi Qiang Liuhref230You can also search for this author in
                        href36 href146Corresponding authorCorrespondence to
                href202.Ethics declarations

Conflict of Interests

The authors declared that they have no conflicts of interest to this work.
Additional informationPublisher’s noteSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.Weixu Pan and Shi Qiang Liu contributed equally to this work.Rights and permissionshref29About this articlehref267Cite this articlePan, W., Liu, S. Deep reinforcement learning for the dynamic and uncertain vehicle routing problem.
                    Appl Intell 53, 405–422 (2023). https://doi.org/10.1007/s10489-022-03456-whref206Accepted: 01 March 2022Published: 18 April 2022Issue Date: January 2023DOI: https://doi.org/10.1007/s10489-022-03456-wShare this articleAnyone you share the following link with will be able to read this content:Get shareable linkSorry, a shareable link is not currently available for this article.Copy to clipboard
                            Provided by the Springer Nature SharedIt content-sharing initiative
                        Keywordshref188href66href116href333







Use our pre-submission checklist


Avoid common mistakes on your manuscript.
















Advertisement




















Discover content

href302
href135



Publish with us

href378
href254
href380



Products and services

href186
href258
href303
href119



Our imprints

href39
href10
href131
href17
href203








Your privacy choices/Manage cookies


href226


href94


href12


href101


href335


href25





165.194.104.233

South Korea Trial Consortium (3001948335)  - Chung Ang University (3000120396)  - 7029 SpringerLink South Korea KESLI Korean Journal Package (3000256740)  - South Korea Trial Consortium (3000522537)  - 5539 SpringerLink South Korea KESLI Full OJA Consortium - (3000171421)  - 9823  SpringerLink South Korea Shinwon (3000699960)  - South Korea Full eJournal Con - Academic 2015-2017 (3991460179)  - 8064 South Korea KESLI Korean Consortium (3000251005)  - 5539 SpringerLink South Korea KESLI Full eJournal Consortium - Academic (3000175460) 

href236
© 2024 Springer Nature






