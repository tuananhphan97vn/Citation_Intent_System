<html class="js" lang="en"><head>
<meta charset="utf-8"/>
<meta content="IE=edge" http-equiv="X-UA-Compatible"/>
<meta content="pc,mobile" name="applicable-device"/>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="max-image-preview:large" name="robots"/>
<meta content="Yes" name="access"/>
<meta content="1268d79b5e96aecf3ff2a7dac04ad990" name="360-site-verification"/>
<title>Heterogeneous graphormer for extractive multimodal summarization | Journal of Intelligent Information Systems</title>
<meta content="@SpringerLink" name="twitter:site"/>
<meta content="summary_large_image" name="twitter:card"/>
<meta content="Content cover image" name="twitter:image:alt"/>
<meta content="Heterogeneous graphormer for extractive multimodal summarization" name="twitter:title"/>
<meta content="Journal of Intelligent Information Systems - Multimodal summarization with multimodal output (MSMO) aims to generate summaries that incorporate both text and images. Existing methods have not..." name="twitter:description"/>
<meta content="https://static-content.springer.com/image/art%3A10.1007%2Fs10844-024-00886-5/MediaObjects/10844_2024_886_Fig1_HTML.png" name="twitter:image"/>
<meta content="10844" name="journal_id"/>
<meta content="Heterogeneous graphormer for extractive multimodal summarization" name="dc.title"/>
<meta content="Journal of Intelligent Information Systems 2024" name="dc.source"/>
<meta content="text/html" name="dc.format"/>
<meta content="Springer" name="dc.publisher"/>
<meta content="2024-09-30" name="dc.date"/>
<meta content="OriginalPaper" name="dc.type"/>
<meta content="En" name="dc.language"/>
<meta content="2024 The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature" name="dc.copyright"/>
<meta content="2024 The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature" name="dc.rights"/>
<meta content="journalpermissions@springernature.com" name="dc.rightsAgent"/>
<meta content="Multimodal summarization with multimodal output (MSMO) aims to generate summaries that incorporate both text and images. Existing methods have not effectively leveraged intermodal relationships, such as sentence-image relationships, which are crucial for generating high-quality multimodal summaries. In this paper, we propose a heterogeneous graph-based model for multimodal summarization (HGMS) designed to efficiently leverage intermodal relationships within multimodal data. The model constructs a heterogeneous graph based on the relationships between modalities, containing nodes for words, sentences and images. An enhanced Graphormer is then proposed to update node representations, aiming to more effectively model intricate relationships between multiple modalities. To the best of our knowledge, we are the first to apply Graphormer in the field of graph-based summarization. Experimental results on a large-scale benchmark dataset demonstrate that HGMS achieves state-of-the-art performance in terms of automatic metrics and human evaluations." name="dc.description"/>
<meta content="1573-7675" name="prism.issn"/>
<meta content="Journal of Intelligent Information Systems" name="prism.publicationName"/>
<meta content="2024-09-30" name="prism.publicationDate"/>
<meta content="OriginalPaper" name="prism.section"/>
<meta content="1" name="prism.startingPage"/>
<meta content="19" name="prism.endingPage"/>
<meta content="2024 The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature" name="prism.copyright"/>
<meta content="journalpermissions@springernature.com" name="prism.rightsAgent"/>
<meta content="https://link.springer.com/article/10.1007/s10844-024-00886-5" name="prism.url"/>
<meta content="doi:10.1007/s10844-024-00886-5" name="prism.doi"/>
<meta content="https://link.springer.com/content/pdf/10.1007/s10844-024-00886-5.pdf" name="citation_pdf_url"/>
<meta content="https://link.springer.com/article/10.1007/s10844-024-00886-5" name="citation_fulltext_html_url"/>
<meta content="Journal of Intelligent Information Systems" name="citation_journal_title"/>
<meta content="J Intell Inf Syst" name="citation_journal_abbrev"/>
<meta content="Springer US" name="citation_publisher"/>
<meta content="1573-7675" name="citation_issn"/>
<meta content="Heterogeneous graphormer for extractive multimodal summarization" name="citation_title"/>
<meta content="2024/09/30" name="citation_online_date"/>
<meta content="1" name="citation_firstpage"/>
<meta content="19" name="citation_lastpage"/>
<meta content="Research" name="citation_article_type"/>
<meta content="en" name="citation_language"/>
<meta content="doi:10.1007/s10844-024-00886-5" name="dc.identifier"/>
<meta content="10.1007/s10844-024-00886-5" name="DOI"/>
<meta content="250995" name="size"/>
<meta content="10.1007/s10844-024-00886-5" name="citation_doi"/>
<meta content="http://api.springer.com/xmldata/jats?q=doi:10.1007/s10844-024-00886-5&amp;api_key=" name="citation_springer_api_url"/>
<meta content="Multimodal summarization with multimodal output (MSMO) aims to generate summaries that incorporate both text and images. Existing methods have not effectiv" name="description"/>
<meta content="Jiang, Xiankai" name="dc.creator"/>
<meta content="Chen, Jingqiang" name="dc.creator"/>
<meta content="Information Storage and Retrieval" name="dc.subject"/>
<meta content="Data Structures and Information Theory" name="dc.subject"/>
<meta content="Artificial Intelligence" name="dc.subject"/>
<meta content="IT in Business" name="dc.subject"/>
<meta content="Natural Language Processing (NLP)" name="dc.subject"/>
<meta content="An, C., Zhong, M., Wu, Z., et al. (2022). CoLo: A contrastive learning based re-ranking framework for one-stage summarization. In: Calzolari N, Huang CR, Kim H, et al (Eds.), Proceedings of the 29th international conference on computational linguistics. International Committee on Computational Linguistics, Gyeongju, Republic of Korea, pp 5783–5793. 
                https://aclanthology.org/2022.coling-1.508
                
              " name="citation_reference"/>
<meta content="Chen, J., &amp; Zhuge, H. (2018). Abstractive text-image summarization using multi-modal attentional hierarchical RNN. In: Riloff E, Chiang D, Hockenmaier J, et al (Eds.), Proceedings of the 2018 conference on empirical methods in natural language processing. Association for Computational Linguistics, Brussels, Belgium, pp 4046–4056. 
                https://doi.org/10.18653/v1/D18-1438
                
              " name="citation_reference"/>
<meta content="Cheng, J., &amp; Lapata, M. (2016). Neural summarization by extracting sentences and words. In: Erk K, Smith NA (Eds.), Proceedings of the 54th annual meeting of the association for computational linguistics (Volume 1: Long Papers). Association for Computational Linguistics, Berlin, Germany, pp 484–494. 
                https://doi.org/10.18653/v1/P16-1046
                
              " name="citation_reference"/>
<meta content="Cheng, X., Shen, Y., &amp; Lu, W. (2023). A set prediction network for extractive summarization. In: Rogers A, Boyd-Graber J, Okazaki N (Eds.), Findings of the association for computational linguistics: ACL 2023. Association for Computational Linguistics, Toronto, Canada, pp 4766–4777. 
                https://doi.org/10.18653/v1/2023.findings-acl.293
                
              " name="citation_reference"/>
<meta content="Cui, P., Hu, L., &amp; Liu, Y. (2020). Enhancing extractive text summarization with topic-aware graph neural networks. In: Scott D, Bel N, Zong C (Eds.), Proceedings of the 28th international conference on computational linguistics. International Committee on Computational Linguistics, Barcelona, Spain (Online), pp 5360–537. 
                https://doi.org/10.18653/v1/2020.coling-main.468
                
              " name="citation_reference"/>
<meta content="citation_journal_title=J Artif Intell Res; citation_title=Lexrank: Graph-based lexical centrality as salience in text summarization; citation_author=G Erkan, DR Radev; citation_volume=22; citation_publication_date=2004; citation_pages=457-47; citation_doi=10.1613/JAIR.1523; citation_id=CR6" name="citation_reference"/>
<meta content="Jia, R., Cao, Y., Tang, H., et al. (2020). Neural extractive summarization with hierarchical attentive heterogeneous graph network. In: Webber B, Cohn T, He Y, et al (Eds.), Proceedings of the 2020 conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics, Online, pp 3622–363. 
                https://doi.org/10.18653/v1/2020.emnlp-main.295
                
              " name="citation_reference"/>
<meta content="Jiang, C., Xie, R., Ye, W., et al. (2023). Exploiting pseudo image captions for multimodal summarization. In: Rogers A, Boyd-Graber J, Okazaki N (Eds.), Findings of the association for computational linguistics: ACL 2023. Association for Computational Linguistics, Toronto, Canada, pp 161–175. 
                https://doi.org/10.18653/v1/2023.findings-acl.12
                
              " name="citation_reference"/>
<meta content="citation_journal_title=J Intell Inf Syst; citation_title=Self-supervised opinion summarization with multi-modal knowledge graph; citation_author=L Jin, J Chen; citation_volume=62; citation_issue=1; citation_publication_date=2024; citation_pages=191-208; citation_doi=10.1007/S10844-023-00812-1; citation_id=CR9" name="citation_reference"/>
<meta content="Jing, B., You, Z., Yang, T., et al. (2021). Multiplex graph neural network for extractive text summarization. In: Moens MF, Huang X, Specia L, et al. (Eds.), Proceedings of the 2021 conference on empirical methods in natural language processing. Association for Computational Linguistics, Online and Punta Cana, Dominican Republic, pp 133–139. 
                https://doi.org/10.18653/v1/2021.emnlp-main.11
                
              " name="citation_reference"/>
<meta content="Kingma, D.P., &amp; Ba, J (2015). Adam: A method for stochastic optimization. In: Bengio Y, LeCun Y (Eds.), 3rd International conference on learning representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings,[SPACE]
                http://arxiv.org/abs/1412.6980
                
              " name="citation_reference"/>
<meta content="Lewis, M., Liu, Y., Goyal, N., et al. (2020). BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In: Jurafsky D, Chai J, Schluter N, et al (Eds.), Proceedings of the 58th annual meeting of the association for computational linguistics. Association for Computational Linguistics, Online, pp 7871–7880. 
                https://doi.org/10.18653/v1/2020.acl-main.703
                
              " name="citation_reference"/>
<meta content="Li, H., Zhu, J., Liu, T., et al. (2018). Multi-modal sentence summarization with modality attention and image filtering. In: Lang J (Ed.), Proceedings of the twenty-seventh international joint conference on artificial intelligence, IJCAI 2018, July 13-19, 2018, Stockholm, Sweden. ijcai.org, Stockholm, Sweden, pp 4152–4158. 
                https://doi.org/10.24963/ijcai.2018/577
                
              " name="citation_reference"/>
<meta content="Li, J., Li, D., Xiong, C., et al. (2022). BLIP: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In: Chaudhuri K, Jegelka S, Song L, et al (Eds.), International conference on machine learning, ICML 2022, 17-23 July 2022, PMLR, Baltimore, Maryland, USA, Proceedings of Machine Learning Research, 162 pp 12888–12900" name="citation_reference"/>
<meta content="Li, M., Chen, X., Gao, S., et al (2020) VMSMO: Learning to generate multimodal summary for video-based news articles. In: Webber B, Cohn T, He Y, et al (Eds.), Proceedings of the 2020 conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics, Online, pp 9360–9369. 
                https://doi.org/10.18653/v1/2020.emnlp-main.752
                
              " name="citation_reference"/>
<meta content="Liang, Y., Meng, F., Xu, J., et al. (2023). Summary-oriented vision modeling for multimodal abstractive summarization. In: Rogers A, Boyd-Graber J, Okazaki N (Eds.), Proceedings of the 61st annual meeting of the association for computational linguistics (Volume 1: Long Papers). Association for Computational Linguistics, Toronto, Canada, pp 2934–2951. 
                https://doi.org/10.18653/v1/2023.acl-long.165
                
              " name="citation_reference"/>
<meta content="Liu Y, &amp; Lapata M. (2019). Text summarization with pretrained encoders. In: Inui K, Jiang J, Ng V, et al (Eds.), Proceedings of the 2019 conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). Association for Computational Linguistics, Hong Kong, China, pp 3730–3740. 
                https://doi.org/10.18653/v1/D19-1387
                
              " name="citation_reference"/>
<meta content="Liu, Y., Ott, M., Goyal, N., et al. (2019). Roberta: A robustly optimized BERT pretraining approach. 
                arXiv:1907.11692
                
              
                https://arxiv.org/abs/1907.11692
                
              " name="citation_reference"/>
<meta content="Liu, Y., Liu, P., Radev, D., et al. (2022). BRIO: Bringing order to abstractive summarization. In: Muresan S, Nakov P, Villavicencio A (Eds.), Proceedings of the 60th annual meeting of the association for computational linguistics (Volume 1: Long Papers). Association for Computational Linguistics, Dublin, Ireland, pp 2890–2903. 
                https://doi.org/10.18653/v1/2022.acl-long.207
                
              " name="citation_reference"/>
<meta content="van der Maaten L, &amp; Hinton G (2008) Visualizing data using t-sne. Journal of Machine Learning Research 9(86), 2579–2605. 
                http://jmlr.org/papers/v9/vandermaaten08a.html
                
              " name="citation_reference"/>
<meta content="Mihalcea, R., Tarau, P (2004) TextRank: Bringing order into text. In: Lin D, Wu D (Eds.), Proceedings of the 2004 conference on empirical methods in natural language processing. Association for Computational Linguistics, Barcelona, Spain, pp 404–411. 
                https://aclanthology.org/W04-3252
                
              " name="citation_reference"/>
<meta content="Nallapati R, Zhai F, &amp; Zhou B. (2017) .Summarunner: A recurrent neural network based sequence model for extractive summarization of documents. Proceedings of the AAAI Conference on Artificial Intelligence 31(1). 
                https://doi.org/10.1609/aaai.v31i1.10958
                
              " name="citation_reference"/>
<meta content="Narayan S, Cohen SB, &amp; Lapata M. (2018). Ranking sentences for extractive summarization with reinforcement learning. In: Walker M, Ji H, Stent A (Eds.) Proceedings of the 2018 conference of the North American chapter of the association for computational linguistics: Human language technologies, Volume 1 (Long Papers). Association for Computational Linguistics, New Orleans, Louisiana, pp 1747–1759. 
                https://doi.org/10.18653/v1/N18-1158
                
              " name="citation_reference"/>
<meta content="Overbay, K., Ahn, J., Pesaran zadeh, F., et al. (2023). mRedditSum: A multimodal abstractive summarization dataset of Reddit threads with images. In: Bouamor H, Pino J, Bali K (Eds.), Proceedings of the 2023 conference on empirical methods in natural language processing. Association for Computational Linguistics, Singapore, pp 4117–4132. 
                https://doi.org/10.18653/v1/2023.emnlp-main.251
                
              " name="citation_reference"/>
<meta content="Paulus, R., Xiong, C., &amp; Socher, R. (2018). A deep reinforced model for abstractive summarization. In: International conference on learning representations, 
                https://openreview.net/forum?id=HkAClQgA-
                
              " name="citation_reference"/>
<meta content="Pennington, J., Socher, R., &amp; Manning, C. (2014). GloVe: Global vectors for word representation. In: Moschitti A, Pang B, Daelemans W (Eds.) Proceedings of the 2014 conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics, Doha, Qatar, pp 1532–1543. 
                https://doi.org/10.3115/v1/D14-1162
                
              " name="citation_reference"/>
<meta content="Phan TA, Nguyen NDN, &amp; Bui KHN. (2022). HeterGraphLongSum: Heterogeneous graph neural network with passage aggregation for extractive long document summarization. In: Calzolari N, Huang CR, Kim H, et al (Eds.), Proceedings of the 29th international conference on computational linguistics. International Committee on Computational Linguistics, Gyeongju, Republic of Korea, pp 6248–6258. 
                https://aclanthology.org/2022.coling-1.545
                
              " name="citation_reference"/>
<meta content="Radford A, Kim JW, Hallacy C, et al (2021) Learning transferable visual models from natural language supervision. In: Meila M, Zhang T (Eds.), Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, pp 8748–8763. 
                http://proceedings.mlr.press/v139/radford21a.html
                
              " name="citation_reference"/>
<meta content="See, A., Liu, P.J., Manning, C.D. (2017). Get to the point: Summarization with pointer-generator networks. In: Barzilay R, Kan MY (Eds.), Proceedings of the 55th annual meeting of the association for computational linguistics (Volume 1: Long Papers). Association for Computational Linguistics, Vancouver, Canada, pp 1073–1083. 
                https://doi.org/10.18653/v1/P17-1099
                
              " name="citation_reference"/>
<meta content="citation_journal_title=Proceedings of the AAAI Conference on Artificial Intelligence; citation_title=Hierarchical heterogeneous graph attention network for syntax-aware summarization; citation_author=Z Song, I King; citation_volume=36; citation_issue=10; citation_publication_date=2022; citation_pages=11340-1134; citation_doi=10.1609/aaai.v36i10.21385; citation_id=CR30" name="citation_reference"/>
<meta content="Sutskever, I., Vinyals, O., &amp; Le, Q.V. (2014). Sequence to sequence learning with neural networks. In: Ghahramani Z, Welling M, Cortes C, et al (Eds.), Advances in Neural Information Processing Systems,[SPACE]
                https://proceedings.neurips.cc/paper/2014/hash/a14ac55a4f27472c5d894ec1c3c743d2-Abstract.html
                
              " name="citation_reference"/>
<meta content="Tang, P., Hu, K., Yan, R., et al. (2022). OTExtSum: Extractive text summarisation with optimal transport. In: Carpuat M, de Marneffe MC, Meza Ruiz IV (eds) Findings of the association for computational linguistics: NAACL 2022. Association for Computational Linguistics, Seattle, United States, pp 1128–1141. 
                https://doi.org/10.18653/v1/2022.findings-naacl.85
                
              " name="citation_reference"/>
<meta content="Vaswani A, Shazeer N, Parmar N, et al. (2017). Attention is all you need. In: Guyon I, Luxburg UV, Bengio S, et al (Eds.), Advances in Neural Information Processing Systems,[SPACE]
                https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html
                
              " name="citation_reference"/>
<meta content="citation_journal_title=Graph attention networks. stat; citation_author=P Velickovic, G Cucurull, A Casanova; citation_volume=1050; citation_issue=20; citation_publication_date=2017; citation_pages=10-48550; citation_id=CR34" name="citation_reference"/>
<meta content="Wang, D., Liu, P., Zheng, Y., et al. (2020). Heterogeneous graph neural networks for extractive document summarization. In: Jurafsky D, Chai J, Schluter N, et al (Eds.), Proceedings of the 58th annual meeting of the association for computational linguistics. Association for Computational Linguistics, Online, pp 6209–6219. 
                https://doi.org/10.18653/v1/2020.acl-main.553
                
              " name="citation_reference"/>
<meta content="Wu W, Li W, Xiao X, et al. (2021). BASS: Boosting abstractive summarization with unified semantic graph. In: Proceedings of the 59th annual meeting of the association for computational linguistics and the 11th international joint conference on natural language processing (Volume 1: Long Papers). Association for Computational Linguistics, Online, pp 6052–6067.
                https://doi.org/10.18653/v1/2021.acl-long.472
                
              " name="citation_reference"/>
<meta content="Xiao M, Zhu J, Lin H, et al. (2023). CFSum coarse-to-fine contribution network for multimodal summarization. In: Rogers A, Boyd-Graber J, Okazaki N (Eds.), Proceedings of the 61st annual meeting of the association for computational linguistics (Volume 1: Long Papers). Association for Computational Linguistics, Toronto, Canada, pp 8538–8553. 
                https://doi.org/10.18653/v1/2023.acl-long.476
                
              " name="citation_reference"/>
<meta content="citation_journal_title=J Intell Inf Syst; citation_title=Extractive text-image summarization with relation-enhanced graph attention network; citation_author=F Xie, J Chen, K Chen; citation_volume=61; citation_issue=2; citation_publication_date=2023; citation_pages=325-341; citation_doi=10.1007/S10844-022-00757-X; citation_id=CR38" name="citation_reference"/>
<meta content="Ying, C., Cai, T., Luo, S., et al. (2021). Do transformers really perform badly for graph representation? In: Ranzato M, Beygelzimer A, Dauphin YN, et al (Eds.), Advances in Neural Information Processing Systems, pp 28877–28888. 
                https://proceedings.neurips.cc/paper/2021/hash/f1c1592588411002af340cbaedd6fc33-Abstract.html
                
              " name="citation_reference"/>
<meta content="Zhang, J., Zhao, Y., Saleh, M., et al. (2020). PEGASUS: pre-training with extracted gap-sentences for abstractive summarization. In: Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, pp 11328–11339. 
                http://proceedings.mlr.press/v119/zhang20ae.html
                
              " name="citation_reference"/>
<meta content="Zhang, L., Zhang, X., &amp; Pan, J. (2022a). Hierarchical cross-modality semantic correlation learning model for multimodal summarization. Proceedings of the AAAI Conference on Artificial Intelligence 36(10) 11676–11684. 
                https://doi.org/10.1609/aaai.v36i10.21422
                
              " name="citation_reference"/>
<meta content="Zhang, M., Zhou, G., Yu, W., et al. (2022b). MAA-PTG: multimodal aspect-aware product title generation. J Intell Inf Syst 59(1) 213–235. 
                https://doi.org/10.1007/S10844-022-00695-8
                
              " name="citation_reference"/>
<meta content="Zhang, Z., Meng, X., Wang, Y., et al. (2022c). Unims: A unified framework for multimodal summarization with knowledge distillation. Proceedings of the AAAI Conference on Artificial Intelligence 36(10) 11757–11764. 
                https://doi.org/10.1609/aaai.v36i10.21431
                
              " name="citation_reference"/>
<meta content="Zhong, M., Liu, P., Chen, Y., et al. (2020). Extractive summarization as text matching. In: Jurafsky D, Chai J, Schluter N, et al (Eds.) Proceedings of the 58th annual meeting of the association for computational linguistics. Association for Computational Linguistics, Online, pp 6197–6208. 
                https://doi.org/10.18653/v1/2020.acl-main.552
                
              " name="citation_reference"/>
<meta content="Zhu, J., Li, H., Liu, T., et al. (2018). MSMO: Multimodal summarization with multimodal output. In: Riloff E, Chiang D, Hockenmaier J, et al (eds) Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Brussels, Belgium, pp 4154–4164. 
                https://doi.org/10.18653/v1/D18-1448
                
              " name="citation_reference"/>
<meta content="citation_journal_title=Proceedings of the AAAI Conference on Artificial Intelligence; citation_title=Multimodal summarization with guidance of multimodal reference; citation_author=J Zhu, Y Zhou, J Zhang; citation_volume=34; citation_issue=05; citation_publication_date=2020; citation_pages=9749-975; citation_doi=10.1609/aaai.v34i05.6525; citation_id=CR46" name="citation_reference"/>
<meta content="Jiang, Xiankai" name="citation_author"/>
<meta content="1222045619@njupt.edu.cn" name="citation_author_email"/>
<meta content="School of Computer Science, Nanjing University of Posts and Telecommunications, Nanjing, China" name="citation_author_institution"/>
<meta content="Chen, Jingqiang" name="citation_author"/>
<meta content="cjq@njupt.edu.cn" name="citation_author_email"/>
<meta content="School of Computer Science, Nanjing University of Posts and Telecommunications, Nanjing, China" name="citation_author_institution"/>
<meta content="telephone=no" name="format-detection"/>
<meta content="https://link.springer.com/article/10.1007/s10844-024-00886-5" property="og:url"/>
<meta content="article" property="og:type"/>
<meta content="SpringerLink" property="og:site_name"/>
<meta content="Heterogeneous graphormer for extractive multimodal summarization - Journal of Intelligent Information Systems" property="og:title"/>
<meta content="Multimodal summarization with multimodal output (MSMO) aims to generate summaries that incorporate both text and images. Existing methods have not effectively leveraged intermodal relationships, such as sentence-image relationships, which are crucial for generating high-quality multimodal summaries. In this paper, we propose a heterogeneous graph-based model for multimodal summarization (HGMS) designed to efficiently leverage intermodal relationships within multimodal data. The model constructs a heterogeneous graph based on the relationships between modalities, containing nodes for words, sentences and images. An enhanced Graphormer is then proposed to update node representations, aiming to more effectively model intricate relationships between multiple modalities. To the best of our knowledge, we are the first to apply Graphormer in the field of graph-based summarization. Experimental results on a large-scale benchmark dataset demonstrate that HGMS achieves state-of-the-art performance in terms of automatic metrics and human evaluations." property="og:description"/>
<meta content="https://static-content.springer.com/image/art%3A10.1007%2Fs10844-024-00886-5/MediaObjects/10844_2024_886_Fig1_HTML.png" property="og:image"/>
<meta content="telephone=no" name="format-detection"/>
<link href="/oscar-static/img/favicons/darwin/apple-touch-icon-92e819bf8a.png" rel="apple-touch-icon" sizes="180x180"/>
<link href="/oscar-static/img/favicons/darwin/android-chrome-192x192-6f081ca7e5.png" rel="icon" sizes="192x192" type="image/png"/>
<link href="/oscar-static/img/favicons/darwin/favicon-32x32-1435da3e82.png" rel="icon" sizes="32x32" type="image/png"/>
<link href="/oscar-static/img/favicons/darwin/favicon-16x16-ed57f42bd2.png" rel="icon" sizes="16x16" type="image/png"/>
<link data-test="shortcut-icon" href="/oscar-static/img/favicons/darwin/favicon-c6d59aafac.ico" rel="shortcut icon"/>
<meta content="#e6e6e6" name="theme-color"/>
<!-- Please see discussion: https://github.com/springernature/frontend-open-space/issues/316-->
<!--TODO: Implement alternative to CTM in here if the discussion concludes we do not continue with CTM as a practice-->
<link href="/oscar-static/app-springerlink/css/print-b8af42253b.css" media="print" rel="stylesheet"/>
<style> html{text-size-adjust:100%;line-height:1.15}body{font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;line-height:1.8;margin:0}details,main{display:block}h1{font-size:2em;margin:.67em 0}a{background-color:transparent;color:#025e8d}sub{bottom:-.25em;font-size:75%;line-height:0;position:relative;vertical-align:baseline}img{border:0;height:auto;max-width:100%;vertical-align:middle}button,input{font-family:inherit;font-size:100%;line-height:1.15;margin:0;overflow:visible}button{text-transform:none}[type=button],[type=submit],button{-webkit-appearance:button}[type=search]{-webkit-appearance:textfield;outline-offset:-2px}summary{display:list-item}[hidden]{display:none}button{cursor:pointer}svg{height:1rem;width:1rem} </style>
<style>@media only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark) {  body{background:#fff;color:#222;font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;line-height:1.8;min-height:100%}a{color:#025e8d;text-decoration:underline;text-decoration-skip-ink:auto}button{cursor:pointer}img{border:0;height:auto;max-width:100%;vertical-align:middle}html{box-sizing:border-box;font-size:100%;height:100%;overflow-y:scroll}h1{font-size:2.25rem}h2{font-size:1.75rem}h1,h2,h4{font-weight:700;line-height:1.2}h4{font-size:1.25rem}body{font-size:1.125rem}*{box-sizing:inherit}p{margin-bottom:2rem;margin-top:0}p:last-of-type{margin-bottom:0}.c-ad{text-align:center}@media only screen and (min-width:480px){.c-ad{padding:8px}}.c-ad--728x90{display:none}.c-ad--728x90 .c-ad__inner{min-height:calc(1.5em + 94px)}@media only screen and (min-width:876px){.js .c-ad--728x90{display:none}}.c-ad__label{color:#333;font-size:.875rem;font-weight:400;line-height:1.5;margin-bottom:4px}.c-ad__label,.c-status-message{font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif}.c-status-message{align-items:center;box-sizing:border-box;display:flex;position:relative;width:100%}.c-status-message :last-child{margin-bottom:0}.c-status-message--boxed{background-color:#fff;border:1px solid #ccc;line-height:1.4;padding:16px}.c-status-message__heading{font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;font-size:.875rem;font-weight:700}.c-status-message__icon{fill:currentcolor;display:inline-block;flex:0 0 auto;height:1.5em;margin-right:8px;transform:translate(0);vertical-align:text-top;width:1.5em}.c-status-message__icon--top{align-self:flex-start}.c-status-message--info .c-status-message__icon{color:#003f8d}.c-status-message--boxed.c-status-message--info{border-bottom:4px solid #003f8d}.c-status-message--error .c-status-message__icon{color:#c40606}.c-status-message--boxed.c-status-message--error{border-bottom:4px solid #c40606}.c-status-message--success .c-status-message__icon{color:#00b8b0}.c-status-message--boxed.c-status-message--success{border-bottom:4px solid #00b8b0}.c-status-message--warning .c-status-message__icon{color:#edbc53}.c-status-message--boxed.c-status-message--warning{border-bottom:4px solid #edbc53}.eds-c-header{background-color:#fff;border-bottom:2px solid #01324b;font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;font-size:1rem;line-height:1.5;padding:8px 0 0}.eds-c-header__container{align-items:center;display:flex;flex-wrap:nowrap;gap:8px 16px;justify-content:space-between;margin:0 auto 8px;max-width:1280px;padding:0 8px;position:relative}.eds-c-header__nav{border-top:2px solid #c5e0f4;padding-top:4px;position:relative}.eds-c-header__nav-container{align-items:center;display:flex;flex-wrap:wrap;margin:0 auto 4px;max-width:1280px;padding:0 8px;position:relative}.eds-c-header__nav-container>:not(:last-child){margin-right:32px}.eds-c-header__link-container{align-items:center;display:flex;flex:1 0 auto;gap:8px 16px;justify-content:space-between}.eds-c-header__list{list-style:none;margin:0;padding:0}.eds-c-header__list-item{font-weight:700;margin:0 auto;max-width:1280px;padding:8px}.eds-c-header__list-item:not(:last-child){border-bottom:2px solid #c5e0f4}.eds-c-header__item{color:inherit}@media only screen and (min-width:768px){.eds-c-header__item--menu{display:none;visibility:hidden}.eds-c-header__item--menu:first-child+*{margin-block-start:0}}.eds-c-header__item--inline-links{display:none;visibility:hidden}@media only screen and (min-width:768px){.eds-c-header__item--inline-links{display:flex;gap:16px 16px;visibility:visible}}.eds-c-header__item--divider:before{border-left:2px solid #c5e0f4;content:"";height:calc(100% - 16px);margin-left:-15px;position:absolute;top:8px}.eds-c-header__brand{padding:16px 8px}.eds-c-header__brand a{display:block;line-height:1;text-decoration:none}.eds-c-header__brand img{height:1.5rem;width:auto}.eds-c-header__link{color:inherit;display:inline-block;font-weight:700;padding:16px 8px;position:relative;text-decoration-color:transparent;white-space:nowrap;word-break:normal}.eds-c-header__icon{fill:currentcolor;display:inline-block;font-size:1.5rem;height:1em;transform:translate(0);vertical-align:bottom;width:1em}.eds-c-header__icon+*{margin-left:8px}.eds-c-header__expander{background-color:#f0f7fc}.eds-c-header__search{display:block;padding:24px 0}@media only screen and (min-width:768px){.eds-c-header__search{max-width:70%}}.eds-c-header__search-container{position:relative}.eds-c-header__search-label{color:inherit;display:inline-block;font-weight:700;margin-bottom:8px}.eds-c-header__search-input{background-color:#fff;border:1px solid #000;padding:8px 48px 8px 8px;width:100%}.eds-c-header__search-button{background-color:transparent;border:0;color:inherit;height:100%;padding:0 8px;position:absolute;right:0}.has-tethered.eds-c-header__expander{border-bottom:2px solid #01324b;left:0;margin-top:-2px;top:100%;width:100%;z-index:10}@media only screen and (min-width:768px){.has-tethered.eds-c-header__expander--menu{display:none;visibility:hidden}}.has-tethered .eds-c-header__heading{display:none;visibility:hidden}.has-tethered .eds-c-header__heading:first-child+*{margin-block-start:0}.has-tethered .eds-c-header__search{margin:auto}.eds-c-header__heading{margin:0 auto;max-width:1280px;padding:16px 16px 0}.eds-c-pagination{align-items:center;display:flex;flex-wrap:wrap;font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;font-size:.875rem;gap:16px 0;justify-content:center;line-height:1.4;list-style:none;margin:0;padding:32px 0}@media only screen and (min-width:480px){.eds-c-pagination{padding:32px 16px}}.eds-c-pagination__item{margin-right:8px}.eds-c-pagination__item--prev{margin-right:16px}.eds-c-pagination__item--next .eds-c-pagination__link,.eds-c-pagination__item--prev .eds-c-pagination__link{padding:16px 8px}.eds-c-pagination__item--next{margin-left:8px}.eds-c-pagination__item:last-child{margin-right:0}.eds-c-pagination__link{align-items:center;color:#222;cursor:pointer;display:inline-block;font-size:1rem;margin:0;padding:16px 24px;position:relative;text-align:center;transition:all .2s ease 0s}.eds-c-pagination__link:visited{color:#222}.eds-c-pagination__link--disabled{border-color:#555;color:#555;cursor:default}.eds-c-pagination__link--active{background-color:#01324b;background-image:none;border-radius:8px;color:#fff}.eds-c-pagination__link--active:focus,.eds-c-pagination__link--active:hover,.eds-c-pagination__link--active:visited{color:#fff}.eds-c-pagination__link-container{align-items:center;display:flex}.eds-c-pagination__icon{fill:#222;height:1.5rem;width:1.5rem}.eds-c-pagination__icon--disabled{fill:#555}.eds-c-pagination__visually-hidden{clip:rect(0,0,0,0);border:0;clip-path:inset(50%);height:1px;overflow:hidden;padding:0;position:absolute!important;white-space:nowrap;width:1px}.c-breadcrumbs{color:#333;font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;font-size:1rem;list-style:none;margin:0;padding:0}.c-breadcrumbs>li{display:inline}svg.c-breadcrumbs__chevron{fill:#333;height:10px;margin:0 .25rem;width:10px}.c-breadcrumbs--contrast,.c-breadcrumbs--contrast .c-breadcrumbs__link{color:#fff}.c-breadcrumbs--contrast svg.c-breadcrumbs__chevron{fill:#fff}@media only screen and (max-width:479px){.c-breadcrumbs .c-breadcrumbs__item{display:none}.c-breadcrumbs .c-breadcrumbs__item:last-child,.c-breadcrumbs .c-breadcrumbs__item:nth-last-child(2){display:inline}}.c-skip-link{background:#01324b;bottom:auto;color:#fff;font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;font-size:1rem;padding:8px;position:absolute;text-align:center;transform:translateY(-100%);width:100%;z-index:9999}@media (prefers-reduced-motion:reduce){.c-skip-link{transition:top .3s ease-in-out 0s}}@media print{.c-skip-link{display:none}}.c-skip-link:active,.c-skip-link:hover,.c-skip-link:link,.c-skip-link:visited{color:#fff}.c-skip-link:focus{transform:translateY(0)}.l-with-sidebar{display:flex;flex-wrap:wrap}.l-with-sidebar>*{margin:0}.l-with-sidebar__sidebar{flex-basis:var(--with-sidebar--basis,400px);flex-grow:1}.l-with-sidebar>:not(.l-with-sidebar__sidebar){flex-basis:0px;flex-grow:999;min-width:var(--with-sidebar--min,53%)}.l-with-sidebar>:first-child{padding-right:4rem}@supports (gap:1em){.l-with-sidebar>:first-child{padding-right:0}.l-with-sidebar{gap:var(--with-sidebar--gap,4rem)}}.c-header__link{color:inherit;display:inline-block;font-weight:700;padding:16px 8px;position:relative;text-decoration-color:transparent;white-space:nowrap;word-break:normal}.app-masthead__colour-4{--background-color:#ff9500;--gradient-light:rgba(0,0,0,.5);--gradient-dark:rgba(0,0,0,.8)}.app-masthead{background:var(--background-color,#0070a8);position:relative}.app-masthead:after{background:radial-gradient(circle at top right,var(--gradient-light,rgba(0,0,0,.4)),var(--gradient-dark,rgba(0,0,0,.7)));bottom:0;content:"";left:0;position:absolute;right:0;top:0}@media only screen and (max-width:479px){.app-masthead:after{background:linear-gradient(225deg,var(--gradient-light,rgba(0,0,0,.4)),var(--gradient-dark,rgba(0,0,0,.7)))}}.app-masthead__container{color:var(--masthead-color,#fff);margin:0 auto;max-width:1280px;padding:0 16px;position:relative;z-index:1}.u-button{align-items:center;background-color:#01324b;background-image:none;border:4px solid transparent;border-radius:32px;cursor:pointer;display:inline-flex;font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;font-size:.875rem;font-weight:700;justify-content:center;line-height:1.3;margin:0;padding:16px 32px;position:relative;transition:all .2s ease 0s;width:auto}.u-button svg,.u-button--contrast svg,.u-button--primary svg,.u-button--secondary svg,.u-button--tertiary svg{fill:currentcolor}.u-button,.u-button:visited{color:#fff}.u-button,.u-button:hover{box-shadow:0 0 0 1px #01324b;text-decoration:none}.u-button:hover{border:4px solid #fff}.u-button:focus{border:4px solid #fc0;box-shadow:none;outline:0;text-decoration:none}.u-button:focus,.u-button:hover{background-color:#fff;background-image:none;color:#01324b}.app-masthead--pastel .c-pdf-download .u-button--primary:focus svg path,.app-masthead--pastel .c-pdf-download .u-button--primary:hover svg path,.c-context-bar--sticky .c-context-bar__container .c-pdf-download .u-button--primary:focus svg path,.c-context-bar--sticky .c-context-bar__container .c-pdf-download .u-button--primary:hover svg path,.u-button--primary:focus svg path,.u-button--primary:hover svg path,.u-button:focus svg path,.u-button:hover svg path{fill:#01324b}.u-button--primary{background-color:#01324b;background-image:none;border:4px solid transparent;box-shadow:0 0 0 1px #01324b;color:#fff;font-weight:700}.u-button--primary:visited{color:#fff}.u-button--primary:hover{border:4px solid #fff;box-shadow:0 0 0 1px #01324b;text-decoration:none}.u-button--primary:focus{border:4px solid #fc0;box-shadow:none;outline:0;text-decoration:none}.u-button--primary:focus,.u-button--primary:hover{background-color:#fff;background-image:none;color:#01324b}.u-button--secondary{background-color:#fff;border:4px solid #fff;color:#01324b;font-weight:700}.u-button--secondary:visited{color:#01324b}.u-button--secondary:hover{border:4px solid #01324b;box-shadow:none}.u-button--secondary:focus,.u-button--secondary:hover{background-color:#01324b;color:#fff}.app-masthead--pastel .c-pdf-download .u-button--secondary:focus svg path,.app-masthead--pastel .c-pdf-download .u-button--secondary:hover svg path,.c-context-bar--sticky .c-context-bar__container .c-pdf-download .u-button--secondary:focus svg path,.c-context-bar--sticky .c-context-bar__container .c-pdf-download .u-button--secondary:hover svg path,.u-button--secondary:focus svg path,.u-button--secondary:hover svg path,.u-button--tertiary:focus svg path,.u-button--tertiary:hover svg path{fill:#fff}.u-button--tertiary{background-color:#ebf1f5;border:4px solid transparent;box-shadow:none;color:#666;font-weight:700}.u-button--tertiary:visited{color:#666}.u-button--tertiary:hover{border:4px solid #01324b;box-shadow:none}.u-button--tertiary:focus,.u-button--tertiary:hover{background-color:#01324b;color:#fff}.u-button--contrast{background-color:transparent;background-image:none;color:#fff;font-weight:400}.u-button--contrast:visited{color:#fff}.u-button--contrast,.u-button--contrast:focus,.u-button--contrast:hover{border:4px solid #fff}.u-button--contrast:focus,.u-button--contrast:hover{background-color:#fff;background-image:none;color:#000}.u-button--contrast:focus svg path,.u-button--contrast:hover svg path{fill:#000}.u-button--disabled,.u-button:disabled{background-color:transparent;background-image:none;border:4px solid #ccc;color:#000;cursor:default;font-weight:400;opacity:.7}.u-button--disabled svg,.u-button:disabled svg{fill:currentcolor}.u-button--disabled:visited,.u-button:disabled:visited{color:#000}.u-button--disabled:focus,.u-button--disabled:hover,.u-button:disabled:focus,.u-button:disabled:hover{border:4px solid #ccc;text-decoration:none}.u-button--disabled:focus,.u-button--disabled:hover,.u-button:disabled:focus,.u-button:disabled:hover{background-color:transparent;background-image:none;color:#000}.u-button--disabled:focus svg path,.u-button--disabled:hover svg path,.u-button:disabled:focus svg path,.u-button:disabled:hover svg path{fill:#000}.u-button--small,.u-button--xsmall{font-size:.875rem;padding:2px 8px}.u-button--small{padding:8px 16px}.u-button--large{font-size:1.125rem;padding:10px 35px}.u-button--full-width{display:flex;width:100%}.u-button--icon-left svg{margin-right:8px}.u-button--icon-right svg{margin-left:8px}.u-clear-both{clear:both}.u-container{margin:0 auto;max-width:1280px;padding:0 16px}.u-justify-content-space-between{justify-content:space-between}.u-display-none{display:none}.js .u-js-hide,.u-hide{display:none;visibility:hidden}.u-visually-hidden{clip:rect(0,0,0,0);border:0;clip-path:inset(50%);height:1px;overflow:hidden;padding:0;position:absolute!important;white-space:nowrap;width:1px}.u-icon{fill:currentcolor;display:inline-block;height:1em;transform:translate(0);vertical-align:text-top;width:1em}.u-list-reset{list-style:none;margin:0;padding:0}.u-ma-16{margin:16px}.u-mt-0{margin-top:0}.u-mt-24{margin-top:24px}.u-mt-32{margin-top:32px}.u-mb-8{margin-bottom:8px}.u-mb-32{margin-bottom:32px}.u-button-reset{background-color:transparent;border:0;padding:0}.u-sans-serif{font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif}.u-serif{font-family:Merriweather,serif}h1,h2,h4{-webkit-font-smoothing:antialiased}p{overflow-wrap:break-word;word-break:break-word}.u-h4{font-size:1.25rem;font-weight:700;line-height:1.2}.u-mbs-0{margin-block-start:0!important}.c-article-header{font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif}.c-article-identifiers{color:#6f6f6f;display:flex;flex-wrap:wrap;font-size:1rem;line-height:1.3;list-style:none;margin:0 0 8px;padding:0}.c-article-identifiers__item{border-right:1px solid #6f6f6f;list-style:none;margin-right:8px;padding-right:8px}.c-article-identifiers__item:last-child{border-right:0;margin-right:0;padding-right:0}@media only screen and (min-width:876px){.c-article-title{font-size:1.875rem;line-height:1.2}}.c-article-author-list{display:inline;font-size:1rem;list-style:none;margin:0 8px 0 0;padding:0;width:100%}.c-article-author-list__item{display:inline;padding-right:0}.c-article-author-list__show-more{display:none;margin-right:4px}.c-article-author-list__button,.js .c-article-author-list__item--hide,.js .c-article-author-list__show-more{display:none}.js .c-article-author-list--long .c-article-author-list__show-more,.js .c-article-author-list--long+.c-article-author-list__button{display:inline}@media only screen and (max-width:767px){.js .c-article-author-list__item--hide-small-screen{display:none}.js .c-article-author-list--short .c-article-author-list__show-more,.js .c-article-author-list--short+.c-article-author-list__button{display:inline}}#uptodate-client,.js .c-article-author-list--expanded .c-article-author-list__show-more{display:none!important}.js .c-article-author-list--expanded .c-article-author-list__item--hide-small-screen{display:inline!important}.c-article-author-list__button,.c-button-author-list{background:#ebf1f5;border:4px solid #ebf1f5;border-radius:20px;color:#666;font-size:.875rem;line-height:1.4;padding:2px 11px 2px 8px;text-decoration:none}.c-article-author-list__button svg,.c-button-author-list svg{margin:1px 4px 0 0}.c-article-author-list__button:hover,.c-button-author-list:hover{background:#025e8d;border-color:transparent;color:#fff}.c-article-body .c-article-access-provider{padding:8px 16px}.c-article-body .c-article-access-provider,.c-notes{border:1px solid #d5d5d5;border-image:initial;border-left:none;border-right:none;margin:24px 0}.c-article-body .c-article-access-provider__text{color:#555}.c-article-body .c-article-access-provider__text,.c-notes__text{font-size:1rem;margin-bottom:0;padding-bottom:2px;padding-top:2px;text-align:center}.c-article-body .c-article-author-affiliation__address{color:inherit;font-weight:700;margin:0}.c-article-body .c-article-author-affiliation__authors-list{list-style:none;margin:0;padding:0}.c-article-body .c-article-author-affiliation__authors-item{display:inline;margin-left:0}.c-article-authors-search{margin-bottom:24px;margin-top:0}.c-article-authors-search__item,.c-article-authors-search__title{font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif}.c-article-authors-search__title{color:#626262;font-size:1.05rem;font-weight:700;margin:0;padding:0}.c-article-authors-search__item{font-size:1rem}.c-article-authors-search__text{margin:0}.c-code-block{border:1px solid #fff;font-family:monospace;margin:0 0 24px;padding:20px}.c-code-block__heading{font-weight:400;margin-bottom:16px}.c-code-block__line{display:block;overflow-wrap:break-word;white-space:pre-wrap}.c-article-share-box{font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;margin-bottom:24px}.c-article-share-box__description{font-size:1rem;margin-bottom:8px}.c-article-share-box__no-sharelink-info{font-size:.813rem;font-weight:700;margin-bottom:24px;padding-top:4px}.c-article-share-box__only-read-input{border:1px solid #d5d5d5;box-sizing:content-box;display:inline-block;font-size:.875rem;font-weight:700;height:24px;margin-bottom:8px;padding:8px 10px}.c-article-share-box__additional-info{color:#626262;font-size:.813rem}.c-article-share-box__button{background:#fff;box-sizing:content-box;text-align:center}.c-article-share-box__button--link-like{background-color:transparent;border:0;color:#025e8d;cursor:pointer;font-size:.875rem;margin-bottom:8px;margin-left:10px}.c-article-associated-content__container .c-article-associated-content__collection-label{font-size:.875rem;line-height:1.4}.c-article-associated-content__container .c-article-associated-content__collection-title{line-height:1.3}.c-reading-companion{clear:both;min-height:389px}.c-reading-companion__figures-list,.c-reading-companion__references-list{list-style:none;min-height:389px;padding:0}.c-reading-companion__references-list--numeric{list-style:decimal inside}.c-reading-companion__figure-item{border-top:1px solid #d5d5d5;font-size:1rem;padding:16px 8px 16px 0}.c-reading-companion__figure-item:first-child{border-top:none;padding-top:8px}.c-reading-companion__reference-item{font-size:1rem}.c-reading-companion__reference-item:first-child{border-top:none}.c-reading-companion__reference-item a{word-break:break-word}.c-reading-companion__reference-citation{display:inline}.c-reading-companion__reference-links{font-size:.813rem;font-weight:700;list-style:none;margin:8px 0 0;padding:0;text-align:right}.c-reading-companion__reference-links>a{display:inline-block;padding-left:8px}.c-reading-companion__reference-links>a:first-child{display:inline-block;padding-left:0}.c-reading-companion__figure-title{display:block;font-size:1.25rem;font-weight:700;line-height:1.2;margin:0 0 8px}.c-reading-companion__figure-links{display:flex;justify-content:space-between;margin:8px 0 0}.c-reading-companion__figure-links>a{align-items:center;display:flex}.c-article-section__figure-caption{display:block;margin-bottom:8px;word-break:break-word}.c-article-section__figure .video,p.app-article-masthead__access--above-download{margin:0 0 16px}.c-article-section__figure-description{font-size:1rem}.c-article-section__figure-description>*{margin-bottom:0}.c-cod{display:block;font-size:1rem;width:100%}.c-cod__form{background:#ebf0f3}.c-cod__prompt{font-size:1.125rem;line-height:1.3;margin:0 0 24px}.c-cod__label{display:block;margin:0 0 4px}.c-cod__row{display:flex;margin:0 0 16px}.c-cod__row:last-child{margin:0}.c-cod__input{border:1px solid #d5d5d5;border-radius:2px;flex-shrink:0;margin:0;padding:13px}.c-cod__input--submit{background-color:#025e8d;border:1px solid #025e8d;color:#fff;flex-shrink:1;margin-left:8px;transition:background-color .2s ease-out 0s,color .2s ease-out 0s}.c-cod__input--submit-single{flex-basis:100%;flex-shrink:0;margin:0}.c-cod__input--submit:focus,.c-cod__input--submit:hover{background-color:#fff;color:#025e8d}.save-data .c-article-author-institutional-author__sub-division,.save-data .c-article-equation__number,.save-data .c-article-figure-description,.save-data .c-article-fullwidth-content,.save-data .c-article-main-column,.save-data .c-article-satellite-article-link,.save-data .c-article-satellite-subtitle,.save-data .c-article-table-container,.save-data .c-blockquote__body,.save-data .c-code-block__heading,.save-data .c-reading-companion__figure-title,.save-data .c-reading-companion__reference-citation,.save-data .c-site-messages--nature-briefing-email-variant .serif,.save-data .c-site-messages--nature-briefing-email-variant.serif,.save-data .serif,.save-data .u-serif,.save-data h1,.save-data h2,.save-data h3{font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif}.c-pdf-download__link{display:flex;flex:1 1 0%;padding:13px 24px}.c-pdf-download__link:hover{text-decoration:none}@media only screen and (min-width:768px){.c-context-bar--sticky .c-pdf-download__link{align-items:center;flex:1 1 183px}}@media only screen and (max-width:320px){.c-context-bar--sticky .c-pdf-download__link{padding:16px}}.c-article-body .c-article-recommendations-list,.c-book-body .c-article-recommendations-list{display:flex;flex-direction:row;gap:16px 16px;margin:0;max-width:100%;padding:16px 0 0}.c-article-body .c-article-recommendations-list__item,.c-book-body .c-article-recommendations-list__item{flex:1 1 0%}@media only screen and (max-width:767px){.c-article-body .c-article-recommendations-list,.c-book-body .c-article-recommendations-list{flex-direction:column}}.c-article-body .c-article-recommendations-card__authors{display:none;font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;font-size:.875rem;line-height:1.5;margin:0 0 8px}@media only screen and (max-width:767px){.c-article-body .c-article-recommendations-card__authors{display:block;margin:0}}.c-article-body .c-article-history{margin-top:24px}.app-article-metrics-bar p{margin:0}.app-article-masthead{display:flex;flex-direction:column;gap:16px 16px;padding:16px 0 24px}.app-article-masthead__info{display:flex;flex-direction:column;flex-grow:1}.app-article-masthead__brand{border-top:1px solid hsla(0,0%,100%,.8);display:flex;flex-direction:column;flex-shrink:0;gap:8px 8px;min-height:96px;padding:16px 0 0}.app-article-masthead__brand img{border:1px solid #fff;border-radius:8px;box-shadow:0 4px 15px 0 hsla(0,0%,50%,.25);height:auto;left:0;position:absolute;width:72px}.app-article-masthead__journal-link{display:block;font-size:1.125rem;font-weight:700;margin:0 0 8px;max-width:400px;padding:0 0 0 88px;position:relative}.app-article-masthead__journal-title{-webkit-box-orient:vertical;-webkit-line-clamp:3;display:-webkit-box;overflow:hidden}.app-article-masthead__submission-link{align-items:center;display:flex;font-size:1rem;gap:4px 4px;margin:0 0 0 88px}.app-article-masthead__access{align-items:center;display:flex;flex-wrap:wrap;font-size:.875rem;font-weight:300;gap:4px 4px;margin:0}.app-article-masthead__buttons{display:flex;flex-flow:column wrap;gap:16px 16px}.app-article-masthead__access svg,.app-masthead--pastel .c-pdf-download .u-button--primary svg,.app-masthead--pastel .c-pdf-download .u-button--secondary svg,.c-context-bar--sticky .c-context-bar__container .c-pdf-download .u-button--primary svg,.c-context-bar--sticky .c-context-bar__container .c-pdf-download .u-button--secondary svg{fill:currentcolor}.app-article-masthead a{color:#fff}.app-masthead--pastel .c-pdf-download .u-button--primary,.c-context-bar--sticky .c-context-bar__container .c-pdf-download .u-button--primary{background-color:#025e8d;background-image:none;border:2px solid transparent;box-shadow:none;color:#fff;font-weight:700}.app-masthead--pastel .c-pdf-download .u-button--primary:visited,.c-context-bar--sticky .c-context-bar__container .c-pdf-download .u-button--primary:visited{color:#fff}.app-masthead--pastel .c-pdf-download .u-button--primary:hover,.c-context-bar--sticky .c-context-bar__container .c-pdf-download .u-button--primary:hover{text-decoration:none}.app-masthead--pastel .c-pdf-download .u-button--primary:focus,.c-context-bar--sticky .c-context-bar__container .c-pdf-download .u-button--primary:focus{border:4px solid #fc0;box-shadow:none;outline:0;text-decoration:none}.app-masthead--pastel .c-pdf-download .u-button--primary:focus,.app-masthead--pastel .c-pdf-download .u-button--primary:hover,.c-context-bar--sticky .c-context-bar__container .c-pdf-download .u-button--primary:focus,.c-context-bar--sticky .c-context-bar__container .c-pdf-download .u-button--primary:hover{background-color:#fff;background-image:none;color:#01324b}.app-masthead--pastel .c-pdf-download .u-button--primary:hover,.c-context-bar--sticky .c-context-bar__container .c-pdf-download .u-button--primary:hover{background:0 0;border:2px solid #025e8d;box-shadow:none;color:#025e8d}.app-masthead--pastel .c-pdf-download .u-button--secondary,.c-context-bar--sticky .c-context-bar__container .c-pdf-download .u-button--secondary{background:0 0;border:2px solid #025e8d;color:#025e8d;font-weight:700}.app-masthead--pastel .c-pdf-download .u-button--secondary:visited,.c-context-bar--sticky .c-context-bar__container .c-pdf-download .u-button--secondary:visited{color:#01324b}.app-masthead--pastel .c-pdf-download .u-button--secondary:hover,.c-context-bar--sticky .c-context-bar__container .c-pdf-download .u-button--secondary:hover{background-color:#01324b;background-color:#025e8d;border:2px solid transparent;box-shadow:none;color:#fff}.app-masthead--pastel .c-pdf-download .u-button--secondary:focus,.c-context-bar--sticky .c-context-bar__container .c-pdf-download .u-button--secondary:focus{background-color:#fff;background-image:none;border:4px solid #fc0;color:#01324b}@media only screen and (min-width:768px){.app-article-masthead{flex-direction:row;gap:64px 64px;padding:24px 0}.app-article-masthead__brand{border:0;padding:0}.app-article-masthead__brand img{height:auto;position:static;width:auto}.app-article-masthead__buttons{align-items:center;flex-direction:row;margin-top:auto}.app-article-masthead__journal-link{display:flex;flex-direction:column;gap:24px 24px;margin:0 0 8px;padding:0}.app-article-masthead__submission-link{margin:0}}@media only screen and (min-width:1024px){.app-article-masthead__brand{flex-basis:400px}}.app-article-masthead .c-article-identifiers{font-size:.875rem;font-weight:300;line-height:1;margin:0 0 8px;overflow:hidden;padding:0}.app-article-masthead .c-article-identifiers--cite-list{margin:0 0 16px}.app-article-masthead .c-article-identifiers *{color:#fff}.app-article-masthead .c-cod{display:none}.app-article-masthead .c-article-identifiers__item{border-left:1px solid #fff;border-right:0;margin:0 17px 8px -9px;padding:0 0 0 8px}.app-article-masthead .c-article-identifiers__item--cite{border-left:0}.app-article-metrics-bar{display:flex;flex-wrap:wrap;font-size:1rem;padding:16px 0 0;row-gap:24px}.app-article-metrics-bar__item{padding:0 16px 0 0}.app-article-metrics-bar__count{font-weight:700}.app-article-metrics-bar__label{font-weight:400;padding-left:4px}.app-article-metrics-bar__icon{height:auto;margin-right:4px;margin-top:-4px;width:auto}.app-article-metrics-bar__arrow-icon{margin:4px 0 0 4px}.app-article-metrics-bar a{color:#000}.app-article-metrics-bar .app-article-metrics-bar__item--metrics{padding-right:0}.app-overview-section .c-article-author-list,.app-overview-section__authors{line-height:2}.app-article-metrics-bar{margin-top:8px}.c-book-toc-pagination+.c-book-section__back-to-top{margin-top:0}.c-article-body .c-article-access-provider__text--chapter{color:#222;font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;padding:20px 0}.c-article-body .c-article-access-provider__text--chapter svg.c-status-message__icon{fill:#003f8d;vertical-align:middle}.c-article-body-section__content--separator{padding-top:40px}.c-pdf-download__link{max-height:44px}.app-article-access .u-button--primary,.app-article-access .u-button--primary:visited{color:#fff}.c-article-sidebar{display:none}@media only screen and (min-width:1024px){.c-article-sidebar{display:block}}.c-cod__form{border-radius:12px}.c-cod__label{font-size:.875rem}.c-cod .c-status-message{align-items:center;justify-content:center;margin-bottom:16px;padding-bottom:16px}@media only screen and (min-width:1024px){.c-cod .c-status-message{align-items:inherit}}.c-cod .c-status-message__icon{margin-top:4px}.c-cod .c-cod__prompt{font-size:1rem;margin-bottom:16px}.c-article-body .app-article-access,.c-book-body .app-article-access{display:block}@media only screen and (min-width:1024px){.c-article-body .app-article-access,.c-book-body .app-article-access{display:none}}.c-article-body .app-card-service{margin-bottom:32px}@media only screen and (min-width:1024px){.c-article-body .app-card-service{display:none}}.app-article-access .buybox__buy .u-button--secondary,.app-article-access .u-button--primary,.c-cod__row .u-button--primary{background-color:#025e8d;border:2px solid #025e8d;box-shadow:none;font-size:1rem;font-weight:700;gap:8px 8px;justify-content:center;line-height:1.5;padding:8px 24px}.app-article-access .buybox__buy .u-button--secondary,.app-article-access .u-button--primary:hover,.c-cod__row .u-button--primary:hover{background-color:#fff;color:#025e8d}.app-article-access .buybox__buy .u-button--secondary:hover{background-color:#025e8d;color:#fff}.buybox__buy .c-notes__text{color:#666;font-size:.875rem;padding:0 16px 8px}.c-cod__input{flex-basis:auto;width:100%}.c-article-title{font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;font-size:2.25rem;font-weight:700;line-height:1.2;margin:12px 0}.c-reading-companion__figure-item figure{margin:0}@media only screen and (min-width:768px){.c-article-title{margin:16px 0}}.app-article-access{border:1px solid #c5e0f4;border-radius:12px}.app-article-access__heading{border-bottom:1px solid #c5e0f4;font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;font-size:1.125rem;font-weight:700;margin:0;padding:16px;text-align:center}.app-article-access .buybox__info svg{vertical-align:middle}.c-article-body .app-article-access p{margin-bottom:0}.app-article-access .buybox__info{font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;font-size:1rem;margin:0}.app-article-access{margin:0 0 32px}@media only screen and (min-width:1024px){.app-article-access{margin:0 0 24px}}.c-status-message{font-size:1rem}.c-article-body{font-size:1.125rem}.c-article-body dl,.c-article-body ol,.c-article-body p,.c-article-body ul{margin-bottom:32px;margin-top:0}.c-article-access-provider__text:last-of-type,.c-article-body .c-notes__text:last-of-type{margin-bottom:0}.c-article-body ol p,.c-article-body ul p{margin-bottom:16px}.c-article-section__figure-caption{font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif}.c-reading-companion__figure-item{border-top-color:#c5e0f4}.c-reading-companion__sticky{max-width:400px}.c-article-section .c-article-section__figure-description>*{font-size:1rem;margin-bottom:16px}.c-reading-companion__reference-item{border-top:1px solid #d5d5d5;padding:16px 0}.c-reading-companion__reference-item:first-child{padding-top:0}.c-article-share-box__button,.js .c-article-authors-search__item .c-article-button{background:0 0;border:2px solid #025e8d;border-radius:32px;box-shadow:none;color:#025e8d;font-size:1rem;font-weight:700;line-height:1.5;margin:0;padding:8px 24px;transition:all .2s ease 0s}.c-article-authors-search__item .c-article-button{width:100%}.c-pdf-download .u-button{background-color:#fff;border:2px solid #fff;color:#01324b;justify-content:center}.c-context-bar__container .c-pdf-download .u-button svg,.c-pdf-download .u-button svg{fill:currentcolor}.c-pdf-download .u-button:visited{color:#01324b}.c-pdf-download .u-button:hover{border:4px solid #01324b;box-shadow:none}.c-pdf-download .u-button:focus,.c-pdf-download .u-button:hover{background-color:#01324b}.c-pdf-download .u-button:focus svg path,.c-pdf-download .u-button:hover svg path{fill:#fff}.c-context-bar__container .c-pdf-download .u-button{background-image:none;border:2px solid;color:#fff}.c-context-bar__container .c-pdf-download .u-button:visited{color:#fff}.c-context-bar__container .c-pdf-download .u-button:hover{text-decoration:none}.c-context-bar__container .c-pdf-download .u-button:focus{box-shadow:none;outline:0;text-decoration:none}.c-context-bar__container .c-pdf-download .u-button:focus,.c-context-bar__container .c-pdf-download .u-button:hover{background-color:#fff;background-image:none;color:#01324b}.c-context-bar__container .c-pdf-download .u-button:focus svg path,.c-context-bar__container .c-pdf-download .u-button:hover svg path{fill:#01324b}.c-context-bar__container .c-pdf-download .u-button,.c-pdf-download .u-button{box-shadow:none;font-size:1rem;font-weight:700;line-height:1.5;padding:8px 24px}.c-context-bar__container .c-pdf-download .u-button{background-color:#025e8d}.c-pdf-download .u-button:hover{border:2px solid #fff}.c-pdf-download .u-button:focus,.c-pdf-download .u-button:hover{background:0 0;box-shadow:none;color:#fff}.c-context-bar__container .c-pdf-download .u-button:hover{border:2px solid #025e8d;box-shadow:none;color:#025e8d}.c-context-bar__container .c-pdf-download .u-button:focus,.c-pdf-download .u-button:focus{border:2px solid #025e8d}.c-article-share-box__button:focus:focus,.c-article__pill-button:focus:focus,.c-context-bar__container .c-pdf-download .u-button:focus:focus,.c-pdf-download .u-button:focus:focus{outline:3px solid #08c;will-change:transform}.c-pdf-download__link .u-icon{padding-top:0}.c-bibliographic-information__column button{margin-bottom:16px}.c-article-body .c-article-author-affiliation__list p,.c-article-body .c-article-author-information__list p,figure{margin:0}.c-article-share-box__button{margin-right:16px}.c-status-message--boxed{border-radius:12px}.c-article-associated-content__collection-title{font-size:1rem}.app-card-service__description,.c-article-body .app-card-service__description{color:#222;margin-bottom:0;margin-top:8px}.app-article-access__subscriptions a,.app-article-access__subscriptions a:visited,.app-book-series-listing__item a,.app-book-series-listing__item a:hover,.app-book-series-listing__item a:visited,.c-article-author-list a,.c-article-author-list a:visited,.c-article-buy-box a,.c-article-buy-box a:visited,.c-article-peer-review a,.c-article-peer-review a:visited,.c-article-satellite-subtitle a,.c-article-satellite-subtitle a:visited,.c-breadcrumbs__link,.c-breadcrumbs__link:hover,.c-breadcrumbs__link:visited{color:#000}.c-article-author-list svg{height:24px;margin:0 0 0 6px;width:24px}.c-article-header{margin-bottom:32px}@media only screen and (min-width:876px){.js .c-ad--conditional{display:block}}.u-lazy-ad-wrapper{background-color:#fff;display:none;min-height:149px}@media only screen and (min-width:876px){.u-lazy-ad-wrapper{display:block}}p.c-ad__label{margin-bottom:4px}.c-ad--728x90{background-color:#fff;border-bottom:2px solid #cedbe0} } </style>
<style>@media only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark) {  .eds-c-header__brand img{height:24px;width:203px}.app-article-masthead__journal-link img{height:93px;width:72px}@media only screen and (min-width:769px){.app-article-masthead__journal-link img{height:161px;width:122px}} } </style>
<link data-inline-css-source="critical-css" data-test="critical-css-handler" href="/oscar-static/app-springerlink/css/core-darwin-3c86549cfc.css" media="all" onload="this.media='all';this.onload=null" rel="stylesheet"/>
<link data-inline-css-source="critical-css" data-test="critical-css-handler" href="/oscar-static/app-springerlink/css/enhanced-darwin-article-0fd044d255.css" media="only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark)" onload="this.media='only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark)';this.onload=null" rel="stylesheet"/>
<script async="" src="https://pagead2.googlesyndication.com/tag/js/gpt.js" type="text/javascript"></script><script async="" src="//cdn.jsdelivr.net/npm/mathjax@2.7.5/MathJax.js?config=TeX-AMS-MML_SVG.js"></script><script async="" src="https://www.googletagmanager.com/gtm.js?id=GTM-MRVXSHQ"></script><script type="text/javascript">
        config = {
            env: 'live',
            site: '10844.springer.com',
            siteWithPath: '10844.springer.com' + window.location.pathname,
            twitterHashtag: '10844',
            cmsPrefix: 'https://studio-cms.springernature.com/studio/',
            
            
            
            
            publisherBrand: 'Springer',
            mustardcut: false
        };
    </script><script onload="initGTM(window,document,'script','dataLayer','GTM-MRVXSHQ')" src="https://cmp.springer.com/production_live/en/consent-bundle-17-52.js"></script>
<script>
        window.dataLayer = [{"GA Key":"UA-26408784-1","DOI":"10.1007/s10844-024-00886-5","Page":"article","springerJournal":true,"Publishing Model":"Hybrid Access","page":{"attributes":{"environment":"live"}},"Country":"KR","japan":false,"doi":"10.1007-s10844-024-00886-5","Journal Id":10844,"Journal Title":"Journal of Intelligent Information Systems","imprint":"Springer","Keywords":"Summarization, Extractive summarization, Multi-modal summarization, Graph neural networks","kwrd":["Summarization","Extractive_summarization","Multi-modal_summarization","Graph_neural_networks"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["cobranding"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["3001948335","3000120396","3000256740","3000522537","3000171421","3000699960","3991460179","3000251005","3000175460"],"businessPartnerIDString":"3001948335|3000120396|3000256740|3000522537|3000171421|3000699960|3991460179|3000251005|3000175460"}},"Access Type":"subscription","Bpids":"3001948335, 3000120396, 3000256740, 3000522537, 3000171421, 3000699960, 3991460179, 3000251005, 3000175460","Bpnames":"South Korea Trial Consortium, Chung Ang University, 7029 SpringerLink South Korea KESLI Korean Journal Package, South Korea Trial Consortium, 5539 SpringerLink South Korea KESLI Full OJA Consortium -, 9823  SpringerLink South Korea Shinwon, South Korea Full eJournal Con - Academic 2015-2017, 8064 South Korea KESLI Korean Consortium, 5539 SpringerLink South Korea KESLI Full eJournal Consortium - Academic","BPID":["3001948335","3000120396","3000256740","3000522537","3000171421","3000699960","3991460179","3000251005","3000175460"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10844-024-00886-5","Full HTML":"Y","Subject Codes":["SCI","SCI18032","SCI15009","SCI21000","SC522000","SCI21040"],"pmc":["I","I18032","I15009","I21000","522000","I21040"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1573-7675","pissn":"0925-9902"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Information Storage and Retrieval","2":"Data Structures and Information Theory","3":"Artificial Intelligence","4":"IT in Business","5":"Natural Language Processing (NLP)"},"secondarySubjectCodes":{"1":"I18032","2":"I15009","3":"I21000","4":"522000","5":"I21040"}},"sucode":"SC6","articleType":"Research"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article"}];
    </script>
<script data-test="springer-link-article-datalayer">
        window.dataLayer = window.dataLayer || [];
        window.dataLayer.push({
            ga4MeasurementId: 'G-B3E4QL2TPR',
            ga360TrackingId: 'UA-26408784-1',
            twitterId: 'o47a7',
            baiduId: 'aef3043f025ccf2305af8a194652d70b',
            ga4ServerUrl: 'https://collect.springer.com',
            imprint: 'springerlink',
                page: {
                    attributes:{
                        featureFlags: [{ name: 'darwin-orion', active: true }, { name: 'chapter-books-recs', active: true } ],
                        darwinAvailable: true
                    }
                }
            
        });
    </script>
<script>
    (function(w, d) {
        w.config = w.config || {};
        w.config.mustardcut = false;

        
        if (w.matchMedia && w.matchMedia('only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark)').matches) {
            w.config.mustardcut = true;
            d.classList.add('js');
            d.classList.remove('grade-c');
            d.classList.remove('no-js');
        }
    })(window, document.documentElement);
</script>
<script class="js-entry">
    if (window.config.mustardcut) {
        (function(w, d) {
            
            
            
                window.Component = {};
                window.suppressShareButton = false;
                window.onArticlePage = true;
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                {'src': '/oscar-static/js/polyfill-es5-bundle-9246e1269f.js', 'async': false}
            ];

            var bodyScripts = [
                
                    {'src': '/oscar-static/js/global-article-es5-bundle-dad1690b0d.js', 'async': false, 'module': false},
                    {'src': '/oscar-static/js/global-article-es6-bundle-e7d03c4cb3.js', 'async': false, 'module': true}
                
                
                    
                
                
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i = 0; i < headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i = 0; i < bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        })(window, document);
    }
</script><script src="/oscar-static/js/polyfill-es5-bundle-9246e1269f.js"></script>
<script data-cc-script="C03" data-src="https://cdn.optimizely.com/js/27195530232.js"></script>
<script data-test="gtm-head">
    window.initGTM = function() {
        if (window.config.mustardcut) {
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                    j = d.createElement(s),
                    dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-MRVXSHQ');
        }
    }
</script>
<script>
(function (w, d, t) {
    function cc() {
        var h = w.location.hostname;
        var e = d.createElement(t),
        s = d.getElementsByTagName(t)[0];

        
        if (h.indexOf('springer.com') > -1 && h.indexOf('biomedcentral.com') === -1 && h.indexOf('springeropen.com') === -1) {
            if (h.indexOf('link-qa.springer.com') > -1 || h.indexOf('test-www.springer.com') > -1) {
                e.src = 'https://cmp.springer.com/production_live/en/consent-bundle-17-52.js';
                e.setAttribute('onload', "initGTM(window,document,'script','dataLayer','GTM-MRVXSHQ')");
            } else {
                e.src = 'https://cmp.springer.com/production_live/en/consent-bundle-17-52.js';
                e.setAttribute('onload', "initGTM(window,document,'script','dataLayer','GTM-MRVXSHQ')");
            }
        } else if (h.indexOf('biomedcentral.com') > -1) {
            if (h.indexOf('biomedcentral.com.qa') > -1) {
                e.src = 'https://cmp.biomedcentral.com/production_live/en/consent-bundle-15-36.js';
                e.setAttribute('onload', "initGTM(window,document,'script','dataLayer','GTM-MRVXSHQ')");
            } else {
                e.src = 'https://cmp.biomedcentral.com/production_live/en/consent-bundle-15-36.js';
                e.setAttribute('onload', "initGTM(window,document,'script','dataLayer','GTM-MRVXSHQ')");
            }
        } else if (h.indexOf('springeropen.com') > -1) {
            if (h.indexOf('springeropen.com.qa') > -1) {
                e.src = 'https://cmp.springernature.com/production_live/en/consent-bundle-16-34.js';
                e.setAttribute('onload', "initGTM(window,document,'script','dataLayer','GTM-MRVXSHQ')");
            } else {
                e.src = 'https://cmp.springernature.com/production_live/en/consent-bundle-16-34.js';
                e.setAttribute('onload', "initGTM(window,document,'script','dataLayer','GTM-MRVXSHQ')");
            }
        } else if (h.indexOf('springernature.com') > -1) {
            if (h.indexOf('beta-qa.springernature.com') > -1) {
                e.src = 'https://cmp.springernature.com/production_live/en/consent-bundle-49-43.js';
                e.setAttribute('onload', "initGTM(window,document,'script','dataLayer','GTM-NK22KLS')");
            } else {
                e.src = 'https://cmp.springernature.com/production_live/en/consent-bundle-49-43.js';
                e.setAttribute('onload', "initGTM(window,document,'script','dataLayer','GTM-NK22KLS')");
            }
        } else {
            e.src = '/oscar-static/js/cookie-consent-es5-bundle-cb57c2c98a.js';
            e.setAttribute('data-consent', h);
        }
        s.insertAdjacentElement('afterend', e);
    }

    cc();
})(window, document, 'script');
</script>
<link href="https://link.springer.com/article/10.1007/s10844-024-00886-5" rel="canonical"/>
<script type="application/ld+json">{"mainEntity":{"headline":"Heterogeneous graphormer for extractive multimodal summarization","description":"Multimodal summarization with multimodal output (MSMO) aims to generate summaries that incorporate both text and images. Existing methods have not effectively leveraged intermodal relationships, such as sentence-image relationships, which are crucial for generating high-quality multimodal summaries. In this paper, we propose a heterogeneous graph-based model for multimodal summarization (HGMS) designed to efficiently leverage intermodal relationships within multimodal data. The model constructs a heterogeneous graph based on the relationships between modalities, containing nodes for words, sentences and images. An enhanced Graphormer is then proposed to update node representations, aiming to more effectively model intricate relationships between multiple modalities. To the best of our knowledge, we are the first to apply Graphormer in the field of graph-based summarization. Experimental results on a large-scale benchmark dataset demonstrate that HGMS achieves state-of-the-art performance in terms of automatic metrics and human evaluations.","datePublished":"2024-09-30T00:00:00Z","dateModified":"2024-09-30T00:00:00Z","pageStart":"1","pageEnd":"19","sameAs":"https://doi.org/10.1007/s10844-024-00886-5","keywords":["Summarization","Extractive summarization","Multi-modal summarization","Graph neural networks","Information Storage and Retrieval","Data Structures and Information Theory","Artificial Intelligence","IT in Business","Natural Language Processing (NLP)"],"image":["https://media.springernature.com/lw1200/springer-static/image/art%3A10.1007%2Fs10844-024-00886-5/MediaObjects/10844_2024_886_Fig1_HTML.png","https://media.springernature.com/lw1200/springer-static/image/art%3A10.1007%2Fs10844-024-00886-5/MediaObjects/10844_2024_886_Fig2_HTML.png","https://media.springernature.com/lw1200/springer-static/image/art%3A10.1007%2Fs10844-024-00886-5/MediaObjects/10844_2024_886_Fig3_HTML.png","https://media.springernature.com/lw1200/springer-static/image/art%3A10.1007%2Fs10844-024-00886-5/MediaObjects/10844_2024_886_Fig4_HTML.png"],"isPartOf":{"name":"Journal of Intelligent Information Systems","issn":["1573-7675","0925-9902"],"@type":["Periodical"]},"publisher":{"name":"Springer US","logo":{"url":"https://www.springernature.com/app-sn/public/images/logo-springernature.png","@type":"ImageObject"},"@type":"Organization"},"author":[{"name":"Xiankai Jiang","affiliation":[{"name":"Nanjing University of Posts and Telecommunications","address":{"name":"School of Computer Science, Nanjing University of Posts and Telecommunications, Nanjing, China","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Jingqiang Chen","affiliation":[{"name":"Nanjing University of Posts and Telecommunications","address":{"name":"School of Computer Science, Nanjing University of Posts and Telecommunications, Nanjing, China","@type":"PostalAddress"},"@type":"Organization"}],"email":"cjq@njupt.edu.cn","@type":"Person"}],"isAccessibleForFree":false,"hasPart":{"isAccessibleForFree":false,"cssSelector":".main-content","@type":"WebPageElement"},"@type":"ScholarlyArticle"},"@context":"https://schema.org","@type":"WebPage"}</script>
<style type="text/css">.cc-banner{background-color:#01324b;border:none!important;bottom:0;box-sizing:border-box;color:#fff!important;left:0;line-height:1.3;margin:auto 0 0;max-width:100%;outline:0;overflow:visible;padding:0;position:fixed;right:0;width:100%;z-index:99999}.cc-banner::backdrop{background-color:#0000004d}.cc-banner *{color:inherit!important}.cc-banner:focus{box-shadow:none;outline:0}.cc-banner a{color:#fff!important;text-decoration:underline}.cc-banner a:active,.cc-banner a:focus,.cc-banner a:hover{color:inherit;text-decoration:none}.cc-banner a:focus{outline:3px solid #08c!important}.cc-banner h2,.cc-banner h3,.cc-banner h4,.cc-banner h5,.cc-banner h6{font-family:sans-serif;font-style:normal;font-weight:700;margin:0 0 .5em}.cc-banner .cc-h2,.cc-banner h2{font-size:18px}.cc-banner .cc-h3,.cc-banner h3{font-size:16px}.cc-banner .cc-h4,.cc-banner .cc-h5,.cc-banner .cc-h6,.cc-banner h4,.cc-banner h5,.cc-banner h6{font-size:14px}.cc-banner .cc-button{font-size:16px}.cc-banner__content{background-color:#01324b;display:flex;flex-direction:column;margin:0 auto;max-height:90vh;max-width:100%;padding:16px;position:relative}.cc-banner__content:focus{outline:0}@media (min-width:680px){.cc-banner__content{padding:12px}}@media (min-width:980px){.cc-banner__content{max-height:60vh;padding-bottom:20px;padding-top:20px}}@media (min-width:1320px){.cc-banner__content{padding-bottom:40px;padding-top:40px}}.cc-banner__container{display:flex;flex-direction:column;margin:auto;max-width:1320px;overflow:auto}.cc-banner__title{background:none!important;flex-shrink:0;font-size:18px!important;font-size:22px!important;font-weight:700!important;letter-spacing:normal;margin:0 0 12px!important}@media (min-width:680px){.cc-banner__title{font-size:24px!important;margin:0 0 16px!important}}@media (min-width:1320px){.cc-banner__title{font-size:26px!important;margin:0 0 24px!important}}.cc-banner__body{display:flex;flex-direction:column;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif!important;overflow-x:hidden;overflow-y:auto;padding:3px 3px 16px}@media (min-width:980px){.cc-banner__body{flex-direction:row}}.cc-banner__policy p{font-size:16px!important;margin:0;max-width:none}.cc-banner__policy p:not(:last-child){margin:0 0 16px}@media (min-width:980px){.cc-banner__policy p:not(:last-child){margin:0 0 24px}}.cc-banner__policy p a{font-size:16px!important;font-weight:700}.cc-banner__footer{box-shadow:none;flex-shrink:0;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif !important!important;margin:0;padding:12px 0 3px 3px;position:relative}@media (min-width:380px){.cc-banner__footer{align-items:stretch;display:flex;flex-wrap:wrap}}@media (min-width:680px){.cc-banner__footer{box-shadow:none;flex-wrap:nowrap;width:40%}}.cc-banner__footer .cc-banner__button{flex:1 1 auto;font-weight:700;overflow:hidden;padding:.5em 1em;width:100%}@media (min-width:680px){.cc-banner__footer .cc-banner__button{max-width:275px}}.cc-banner__footer .cc-banner__button:not(:last-child){margin-bottom:12px}@media (min-width:680px){.cc-banner__footer .cc-banner__button:not(:last-child){margin-bottom:0;margin-right:16px}}@media (min-width:980px){.cc-banner__footer .cc-banner__button:not(:last-child){margin-right:24px}}.cc-banner__button-preferences{padding-left:0;padding-right:0}@media (min-width:380px){.cc-banner__button-preferences{flex:0 0 auto;margin:auto}}@media (min-width:680px){.cc-banner__button-preferences{margin:0}}@media (min-width:380px) and (max-width:680px){.cc-banner__button-break{display:block}}@media (min-width:680px){.cc-banner--is-tcf .cc-banner__footer{width:auto}}@media (min-width:980px){.cc-banner--is-tcf .cc-banner__footer{padding-right:48px;width:66%}}.cc-banner--is-tcf .cc-banner__stacks-intro{font-size:16px!important}.cc-banner--is-tcf .cc-banner__stacks{margin:16px 0 0}@media (min-width:980px){.cc-banner--is-tcf .cc-banner__stacks{display:flex;flex:0 0 33%;flex-direction:column;margin:0 0 0 48px;overflow:auto}}.cc-banner--is-tcf .cc-banner__stacks-intro{font-size:16px;margin:0 0 12px}.cc-banner--is-tcf .cc-banner__stacks-details{padding:0 3px}.cc-banner--is-tcf .cc-banner .cc-details{border-color:#fff3!important}.cc-box{border-radius:10px;padding:12px}.cc-box--info{background-color:#eff6fb}.cc-box--light{background-color:#faf9f6}.cc-button{border:2px solid #0000;border-radius:32px!important;color:inherit;cursor:pointer;font-size:14px;font-weight:700!important;left:auto;line-height:1.2;margin:0;padding:.5em 1em;right:auto;transition:all .2s}@media (min-width:680px){.cc-button{font-size:14px;padding:.75em 1em}}.cc-button--sm{font-size:12px;padding:2px 8px!important}.cc-button--primary{background-color:#025e8d!important;border-color:#025e8d!important;color:#fff!important}.cc-button--primary:focus{outline:3px solid #08c}.cc-button--primary:focus,.cc-button--primary:hover,.cc-button--secondary{background-color:#fff!important;border-color:#025e8d!important;color:#025e8d!important}.cc-button--secondary:focus{background-color:#025e8d!important;border-color:#fff!important;color:#fff!important;outline:3px solid #08c!important}.cc-button--secondary:hover{background-color:#025e8d!important;border-color:#025e8d!important;color:#fff!important}.cc-button--secondary:active{border:2px solid #01324b!important}.cc-button--secondary.cc-button--contrast{border-color:#fff!important}.cc-button--secondary.cc-button--contrast:hover{background-color:initial!important;color:#fff!important}.cc-button--tertiary{background-color:#f0f7fc!important;background-image:none;border:2px solid #f0f7fc;color:#025e8d!important;text-decoration:underline!important;text-underline-offset:.25em}.cc-button--tertiary:focus{outline:3px solid #08c!important}.cc-button--tertiary:hover{text-decoration-thickness:.25em}.cc-button--tertiary:active{color:#013c5b!important}.cc-button--link{background-color:initial!important;border-color:#0000!important;border-radius:0!important;color:inherit!important;padding:0!important;text-decoration:underline!important}.cc-button--link:focus{outline:3px solid #08c}.cc-button--link:hover{background-color:initial!important;box-shadow:none;text-decoration:none}.cc-button--text{border-radius:0;padding:0}.cc-button--details{padding-right:24px!important;position:relative}.cc-button--details:after,.cc-button--details:before{background-color:currentColor;content:"";position:absolute;transition:transform .25s ease-out}.cc-button--details:before{height:10px;margin-top:-5px;right:12px;top:50%;width:2px}.cc-button--details:after{height:2px;margin-top:-1px;right:8px;top:50%;width:10px}.cc-button--details.cc-active:before{transform:rotate(90deg);transform-origin:center}.cc-button--details.cc-active:after{display:none}.cc-details{border-bottom:1px solid #0000001a;padding:12px 0;position:relative;width:100%}.cc-details__summary{align-items:center;display:flex;font-size:14px;font-weight:700;list-style-type:none}.cc-details__summary:focus,.cc-details__summary:focus-visible{outline:3px solid #08c!important;will-change:auto}.cc-details__title{align-items:baseline;display:flex}.cc-details__title h2,.cc-details__title h3,.cc-details__title h4,.cc-details__title h5{line-height:1.4;margin:0!important}.cc-details__title svg{flex-shrink:0;margin-right:8px;position:relative;top:-1px;transition:all .2s}.cc-details[open] .cc-details__title svg{top:2px;transform:rotate(90deg)}.cc-details__switch{margin-left:auto}.cc-details__section{padding:16px 0 0 18px}.cc-details__section p{margin:0}.cc-details__section p:not(:last-child){margin-bottom:12px}details summary::-webkit-details-marker{display:none}.cc-radio{align-items:center;display:flex;position:relative}.cc-radio *{cursor:pointer}.cc-radio__input{height:22px;left:0;position:absolute;top:0;width:22px}.cc-radio__input:focus{outline:none}.cc-radio__label{color:inherit;font-size:14px;font-weight:700;line-height:23px;margin:0;padding-left:28px}.cc-radio__label:after,.cc-radio__label:before{background-color:#fff;content:"";display:block;position:absolute;transition:transform .25s ease-out}.cc-radio__label:before{border:1px solid #777;border-radius:50%;height:22px;left:0;top:0;width:22px}.cc-radio__label:after{border:7px solid #025e8d;border-radius:50%;height:0;left:4px;opacity:0;top:4px;width:0}.cc-radio__input:focus+.cc-radio__label:before{box-shadow:0 0 0 2px #08c;outline:none}.cc-radio__label--hidden{display:none}.cc-radio__input:checked+.cc-radio__label:after{opacity:1}.cc-radio__input:disabled{cursor:default}.cc-radio__input:disabled+.cc-radio__label{cursor:default;opacity:.5}.cc-switch *{cursor:pointer}.cc-switch{align-items:center;display:flex;line-height:1}.cc-switch__label{cursor:pointer;display:inline-block;font-size:14px;font-weight:700;margin:0;-webkit-user-select:none;user-select:none}.cc-switch__input{-webkit-appearance:none;appearance:none;background-color:#fff;background-image:url("data:image/svg+xml;charset=utf-8,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='-4 -4 8 8'%3E%3Ccircle r='3' fill='rgba(0, 0, 0, 0.25)'/%3E%3C/svg%3E");background-position:0;background-repeat:no-repeat;border:1px solid #777;border-radius:16px;height:18px!important;margin:0 8px 0 0;transition:background-position .2s ease-in-out;width:32px!important}.cc-switch__input:focus{background-image:url("data:image/svg+xml;charset=utf-8,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='-4 -4 8 8'%3E%3Ccircle r='3' fill='%2380b3cc'/%3E%3C/svg%3E")}.cc-switch__input:checked{background-color:#025e8d;background-image:url("data:image/svg+xml;charset=utf-8,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='-4 -4 8 8'%3E%3Ccircle r='3' fill='%23fff'/%3E%3C/svg%3E");background-position:100%}.cc-switch__input:checked~.cc-switch__label>.cc-switch__text-off,.cc-switch__input:not(:checked)~.cc-switch__label>.cc-switch__text-on{display:none}.cc-switch__input:hover{cursor:pointer;outline:none}.cc-switch__input:focus-visible{cursor:pointer;outline:3px solid #fc0}.cc-list>ul,ul.cc-list{list-style-type:disc;margin-left:0;padding-left:0}.cc-list>ul>li,ul.cc-list>li{margin-left:1em}.cc-list>ul>li:not(:last-child),ul.cc-list>li:not(:last-child){margin-bottom:4px}.cc-list__title{margin-bottom:8px!important}.cc-list--inline>ul,ul.cc-list--inline{display:flex;flex-wrap:wrap;list-style-type:none}.cc-list--inline>ul>li,ul.cc-list--inline>li{margin:0 .5em .5em 0}.cc-overlay{background-color:#0000004d;z-index:99998}.cc-overlay,.cc-preferences{bottom:0;left:0;position:fixed;right:0;top:0}.cc-preferences{background-color:#050a14f2!important;border:0;box-sizing:border-box;color:#111;font-family:sans-serif!important;line-height:1.4;margin:auto;max-height:100vh;overflow:auto;padding:0;z-index:100000}.cc-preferences:focus{outline:none}.cc-preferences *,.cc-preferences :after,.cc-preferences :before{box-sizing:inherit!important}.cc-preferences h2,.cc-preferences h3,.cc-preferences h4,.cc-preferences h5,.cc-preferences h6{font-family:sans-serif;font-style:normal;font-weight:700;margin:0 0 .5em}.cc-preferences .cc-h2,.cc-preferences h2{font-size:18px}.cc-preferences .cc-h3,.cc-preferences h3{font-size:16px}.cc-preferences .cc-h4,.cc-preferences .cc-h5,.cc-preferences .cc-h6,.cc-preferences h4,.cc-preferences h5,.cc-preferences h6{font-size:14px}.cc-preferences a{color:#025e8d;text-decoration:underline}.cc-preferences a:hover{color:inherit;text-decoration:none}.cc-preferences h3{background:none;color:#111;text-transform:none}dialog.cc-preferences{background-color:initial}dialog.cc-preferences::backdrop{background-color:#000000e6}.cc-preferences__dialog{display:flex;flex-direction:column;margin:auto;max-height:100vh;max-width:860px;padding:12px;position:relative}.cc-preferences__dialog>:last-child{border-bottom-left-radius:10px;border-bottom-right-radius:10px}@media (min-width:980px){.cc-preferences__dialog{padding:16px}}.cc-preferences__close{background:#0000!important;border:1px solid #ececec;border-radius:50%;color:#111!important;cursor:pointer;font-family:Times New Roman,serif;font-size:40px;height:40px;left:auto;line-height:1;margin-top:-20px;padding:0!important;position:absolute;right:20px;top:50%;width:40px}.cc-preferences__close:focus{outline:3px solid #08c}.cc-preferences__close-label{border:0;clip:rect(0,0,0,0);height:1px;margin:-100%;overflow:hidden;padding:0;position:absolute!important;width:1px}.cc-preferences__header{background:#fff!important;border-bottom:1px solid #0000001a;border-top-left-radius:10px;border-top-right-radius:10px;padding:16px;position:relative;text-align:center}.cc-preferences__title{background:none!important;color:#111!important;font-family:sans-serif!important;font-size:18px!important;font-weight:700!important;margin:0!important;padding-right:16px!important}@media (min-width:480px){.cc-preferences__title{padding-right:0!important}}@media (min-width:980px){.cc-preferences__title{font-size:22px!important}}.cc-preferences__body{background:#fff!important;flex:1 1 auto;min-height:200px;overflow-x:hidden;overflow-y:auto;padding:16px}.cc-preferences__footer{background:#fff!important;border-top:1px solid #d0d0d0;box-shadow:0 0 5px 0 #0003;margin-bottom:0;padding:12px;position:relative}@media (min-width:480px){.cc-preferences__footer{align-items:stretch;display:flex}}.cc-preferences__footer>.cc-button{display:block;width:100%}@media (min-width:480px){.cc-preferences__footer>.cc-button{flex:1 1 auto}}@media (min-width:980px){.cc-preferences__footer>.cc-button{flex-basis:auto}}@media (min-width:480px){.cc-preferences__footer>.cc-button:not(:first-child){margin-left:12px}}.cc-preferences__footer>.cc-button:not(:last-child){margin-bottom:8px}@media (min-width:480px){.cc-preferences__footer>.cc-button:not(:last-child){margin-bottom:0}}.cc-preferences__categories{list-style:none;margin:0;padding:0}.cc-preferences__category:not(:last-child){border-bottom:1px solid #0000001a;margin-bottom:12px;padding-bottom:12px}.cc-preferences__category-description{font-size:14px;margin:0 0 8px}.cc-preferences__category-footer{align-items:center;display:flex;justify-content:space-between}.cc-preferences__status{border:0;clip:rect(0,0,0,0);height:1px;margin:-100%;overflow:hidden;padding:0;position:absolute!important;width:1px}.cc-preferences__controls{display:flex;margin:0}.cc-preferences__controls>:not(:last-child){margin-right:12px}.cc-preferences__always-on{font-size:14px;font-weight:700;padding-left:26px;position:relative}.cc-preferences__always-on:before{background:#0000!important;border:solid;border-top-color:#0000;border-width:0 0 4px 4px;content:"";display:block;height:10px;left:0;position:absolute;top:2px;transform:rotate(-45deg);width:18px}.cc-preferences__details{background-color:#eff6fb!important;border-radius:10px;font-size:14px;margin:12px 0 0;padding:12px}.cc-preferences__cookie-list,.cc-preferences__provider-list{list-style:none;margin:0;padding:0}.cc-preferences__provider-list{columns:170px}.cc-preferences__cookie-title{font-size:1em;margin:0}.cc-preferences__cookie-description{font-size:1em;margin:0 0 8px}.cc-preferences__cookie-domain,.cc-preferences__cookie-lifespan{border-left:1px solid #999;color:#666;margin-left:8px;padding-left:4px}body.cc-has-preferences-open{overflow:hidden;position:relative}.cc-table{border-collapse:collapse;width:100%}.cc-table tbody tr{border-top:1px solid #0000001a}.cc-table td,.cc-table th{font-size:14px;padding:4px 8px;vertical-align:top}.cc-table+.cc-table{margin-top:8px}.cc-table--purposes td:first-child,.cc-table--purposes th:first-child{width:70%}@media (min-width:680px){.cc-table--purposes td:first-child,.cc-table--purposes th:first-child{width:80%}}.cc-table--purposes td:last-child,.cc-table--purposes th:last-child{padding-left:8px;width:40%}@media (min-width:680px){.cc-table--purposes td:last-child,.cc-table--purposes th:last-child{width:30%}}.cc-tabs,.cc-tabs>ul{background-color:#faf9f6;display:flex;list-style:none;margin:0;padding:0}.cc-tabs__panel{border-top:1px solid #ececec;margin-top:-1px}.cc-tabs__button{background-color:initial;border:0;border-left:1px solid #0000;border-right:1px solid #0000;font-size:16px;font-weight:700;padding:15px 16px 12px;position:relative}.cc-tabs__button:before{background-color:initial;content:"";display:block;height:3px;left:0;position:absolute;top:0;width:100%}.cc-tabs__button.cc-active{background-color:#fff;border-color:#025e8d #ececec #ececec}.cc-tabs__button.cc-active:before{background-color:#025e8d}.cc-tabs__content{background-color:#fff;display:none;font-size:14px;padding:16px}.cc-tabs__section:not(:first-child){padding-top:24px}.cc-tcf{font-size:14px;margin-top:16px}.cc-tcf__list{list-style:none;margin:0;padding:0}.cc-vendor-count{color:#666;font-size:16px;margin:0 0 8px}.cc-hide{display:none!important}.cc-show{display:block!important}.cc-external-link{background-color:#ececec;background-image:url("data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyMCIgaGVpZ2h0PSIyMCI+PHBhdGggZD0iTTcgMGExIDEgMCAxIDEgMCAySDIuNmMtLjM3MSAwLS42LjIwOS0uNi41djE1YzAgLjI5MS4yMjkuNS42LjVoMTQuOGMuMzcxIDAgLjYtLjIwOS42LS41VjEzYTEgMSAwIDAgMSAyIDB2NC41YzAgMS40MzgtMS4xNjIgMi41LTIuNiAyLjVIMi42QzEuMTYyIDIwIDAgMTguOTM4IDAgMTcuNXYtMTVDMCAxLjA2MiAxLjE2MiAwIDIuNiAwem02IDBoNmwuMDc1LjAwMy4xMjYuMDE3LjExMS4wMy4xMTEuMDQ0LjA5OC4wNTIuMDk2LjA2Ny4wOS4wOGExIDEgMCAwIDEgLjA5Ny4xMTJsLjA3MS4xMS4wNTQuMTE0LjAzNS4xMDUuMDMuMTQ4TDIwIDF2NmExIDEgMCAwIDEtMiAwVjMuNDE0bC02LjY5MyA2LjY5M2ExIDEgMCAwIDEtMS40MTQtMS40MTRMMTYuNTg0IDJIMTNhMSAxIDAgMCAxLS45OTMtLjg4M0wxMiAxYTEgMSAwIDAgMSAxLTEiLz48L3N2Zz4=");background-position:right 8px center;background-repeat:no-repeat;background-size:10px auto;border-radius:3px;box-shadow:0 1px 0 #0003;color:#111!important;display:inline-block;font-size:12px;padding:4px 26px 4px 8px;text-decoration:none!important}.cc-external-link:hover{text-decoration:underline!important}.cc-m-0{margin:0!important}.cc-grey{color:#666}</style><style type="text/css">.MathJax_Hover_Frame {border-radius: .25em; -webkit-border-radius: .25em; -moz-border-radius: .25em; -khtml-border-radius: .25em; box-shadow: 0px 0px 15px #83A; -webkit-box-shadow: 0px 0px 15px #83A; -moz-box-shadow: 0px 0px 15px #83A; -khtml-box-shadow: 0px 0px 15px #83A; border: 1px solid #A6D ! important; display: inline-block; position: absolute}
.MathJax_Menu_Button .MathJax_Hover_Arrow {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 4px; -webkit-border-radius: 4px; -moz-border-radius: 4px; -khtml-border-radius: 4px; font-family: 'Courier New',Courier; font-size: 9px; color: #F0F0F0}
.MathJax_Menu_Button .MathJax_Hover_Arrow span {display: block; background-color: #AAA; border: 1px solid; border-radius: 3px; line-height: 0; padding: 4px}
.MathJax_Hover_Arrow:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_Hover_Arrow:hover span {background-color: #CCC!important}
</style><style type="text/css">#MathJax_About {position: fixed; left: 50%; width: auto; text-align: center; border: 3px outset; padding: 1em 2em; background-color: #DDDDDD; color: black; cursor: default; font-family: message-box; font-size: 120%; font-style: normal; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 15px; -webkit-border-radius: 15px; -moz-border-radius: 15px; -khtml-border-radius: 15px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_About.MathJax_MousePost {outline: none}
.MathJax_Menu {position: absolute; background-color: white; color: black; width: auto; padding: 5px 0px; border: 1px solid #CCCCCC; margin: 0; cursor: default; font: menu; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 5px; -webkit-border-radius: 5px; -moz-border-radius: 5px; -khtml-border-radius: 5px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
.MathJax_MenuItem {padding: 1px 2em; background: transparent}
.MathJax_MenuArrow {position: absolute; right: .5em; padding-top: .25em; color: #666666; font-size: .75em}
.MathJax_MenuActive .MathJax_MenuArrow {color: white}
.MathJax_MenuArrow.RTL {left: .5em; right: auto}
.MathJax_MenuCheck {position: absolute; left: .7em}
.MathJax_MenuCheck.RTL {right: .7em; left: auto}
.MathJax_MenuRadioCheck {position: absolute; left: .7em}
.MathJax_MenuRadioCheck.RTL {right: .7em; left: auto}
.MathJax_MenuLabel {padding: 1px 2em 3px 1.33em; font-style: italic}
.MathJax_MenuRule {border-top: 1px solid #DDDDDD; margin: 4px 3px}
.MathJax_MenuDisabled {color: GrayText}
.MathJax_MenuActive {background-color: #606872; color: white}
.MathJax_MenuDisabled:focus, .MathJax_MenuLabel:focus {background-color: #E8E8E8}
.MathJax_ContextMenu:focus {outline: none}
.MathJax_ContextMenu .MathJax_MenuItem:focus {outline: none}
#MathJax_AboutClose {top: .2em; right: .2em}
.MathJax_Menu .MathJax_MenuClose {top: -10px; left: -10px}
.MathJax_MenuClose {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; font-family: 'Courier New',Courier; font-size: 24px; color: #F0F0F0}
.MathJax_MenuClose span {display: block; background-color: #AAA; border: 1.5px solid; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; line-height: 0; padding: 8px 0 6px}
.MathJax_MenuClose:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_MenuClose:hover span {background-color: #CCC!important}
.MathJax_MenuClose:hover:focus {outline: none}
</style><style type="text/css">.MJX_Assistive_MathML {position: absolute!important; top: 0; left: 0; clip: rect(1px, 1px, 1px, 1px); padding: 1px 0 0 0!important; border: 0!important; height: 1px!important; width: 1px!important; overflow: hidden!important; display: block!important; -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none}
.MJX_Assistive_MathML.MJX_Assistive_MathML_Block {width: 100%!important}
</style><style type="text/css">#MathJax_Zoom {position: absolute; background-color: #F0F0F0; overflow: auto; display: block; z-index: 301; padding: .5em; border: 1px solid black; margin: 0; font-weight: normal; font-style: normal; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; -webkit-box-sizing: content-box; -moz-box-sizing: content-box; box-sizing: content-box; box-shadow: 5px 5px 15px #AAAAAA; -webkit-box-shadow: 5px 5px 15px #AAAAAA; -moz-box-shadow: 5px 5px 15px #AAAAAA; -khtml-box-shadow: 5px 5px 15px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_ZoomOverlay {position: absolute; left: 0; top: 0; z-index: 300; display: inline-block; width: 100%; height: 100%; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
#MathJax_ZoomFrame {position: relative; display: inline-block; height: 0; width: 0}
#MathJax_ZoomEventTrap {position: absolute; left: 0; top: 0; z-index: 302; display: inline-block; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
</style><style type="text/css">.MathJax_Preview {color: #888}
#MathJax_Message {position: fixed; left: 1em; bottom: 1.5em; background-color: #E6E6E6; border: 1px solid #959595; margin: 0px; padding: 2px 8px; z-index: 102; color: black; font-size: 80%; width: auto; white-space: nowrap}
#MathJax_MSIE_Frame {position: absolute; top: 0; left: 0; width: 0px; z-index: 101; border: 0px; margin: 0px; padding: 0px}
.MathJax_Error {color: #CC0000; font-style: italic}
</style><style type="text/css">.MJXp-script {font-size: .8em}
.MJXp-right {-webkit-transform-origin: right; -moz-transform-origin: right; -ms-transform-origin: right; -o-transform-origin: right; transform-origin: right}
.MJXp-bold {font-weight: bold}
.MJXp-italic {font-style: italic}
.MJXp-scr {font-family: MathJax_Script,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-frak {font-family: MathJax_Fraktur,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-sf {font-family: MathJax_SansSerif,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-cal {font-family: MathJax_Caligraphic,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-mono {font-family: MathJax_Typewriter,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-largeop {font-size: 150%}
.MJXp-largeop.MJXp-int {vertical-align: -.2em}
.MJXp-math {display: inline-block; line-height: 1.2; text-indent: 0; font-family: 'Times New Roman',Times,STIXGeneral,serif; white-space: nowrap; border-collapse: collapse}
.MJXp-display {display: block; text-align: center; margin: 1em 0}
.MJXp-math span {display: inline-block}
.MJXp-box {display: block!important; text-align: center}
.MJXp-box:after {content: " "}
.MJXp-rule {display: block!important; margin-top: .1em}
.MJXp-char {display: block!important}
.MJXp-mo {margin: 0 .15em}
.MJXp-mfrac {margin: 0 .125em; vertical-align: .25em}
.MJXp-denom {display: inline-table!important; width: 100%}
.MJXp-denom > * {display: table-row!important}
.MJXp-surd {vertical-align: top}
.MJXp-surd > * {display: block!important}
.MJXp-script-box > *  {display: table!important; height: 50%}
.MJXp-script-box > * > * {display: table-cell!important; vertical-align: top}
.MJXp-script-box > *:last-child > * {vertical-align: bottom}
.MJXp-script-box > * > * > * {display: block!important}
.MJXp-mphantom {visibility: hidden}
.MJXp-munderover, .MJXp-munder {display: inline-table!important}
.MJXp-over {display: inline-block!important; text-align: center}
.MJXp-over > * {display: block!important}
.MJXp-munderover > *, .MJXp-munder > * {display: table-row!important}
.MJXp-mtable {vertical-align: .25em; margin: 0 .125em}
.MJXp-mtable > * {display: inline-table!important; vertical-align: middle}
.MJXp-mtr {display: table-row!important}
.MJXp-mtd {display: table-cell!important; text-align: center; padding: .5em 0 0 .5em}
.MJXp-mtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-mlabeledtr {display: table-row!important}
.MJXp-mlabeledtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mlabeledtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MJXp-scale0 {-webkit-transform: scaleX(.0); -moz-transform: scaleX(.0); -ms-transform: scaleX(.0); -o-transform: scaleX(.0); transform: scaleX(.0)}
.MJXp-scale1 {-webkit-transform: scaleX(.1); -moz-transform: scaleX(.1); -ms-transform: scaleX(.1); -o-transform: scaleX(.1); transform: scaleX(.1)}
.MJXp-scale2 {-webkit-transform: scaleX(.2); -moz-transform: scaleX(.2); -ms-transform: scaleX(.2); -o-transform: scaleX(.2); transform: scaleX(.2)}
.MJXp-scale3 {-webkit-transform: scaleX(.3); -moz-transform: scaleX(.3); -ms-transform: scaleX(.3); -o-transform: scaleX(.3); transform: scaleX(.3)}
.MJXp-scale4 {-webkit-transform: scaleX(.4); -moz-transform: scaleX(.4); -ms-transform: scaleX(.4); -o-transform: scaleX(.4); transform: scaleX(.4)}
.MJXp-scale5 {-webkit-transform: scaleX(.5); -moz-transform: scaleX(.5); -ms-transform: scaleX(.5); -o-transform: scaleX(.5); transform: scaleX(.5)}
.MJXp-scale6 {-webkit-transform: scaleX(.6); -moz-transform: scaleX(.6); -ms-transform: scaleX(.6); -o-transform: scaleX(.6); transform: scaleX(.6)}
.MJXp-scale7 {-webkit-transform: scaleX(.7); -moz-transform: scaleX(.7); -ms-transform: scaleX(.7); -o-transform: scaleX(.7); transform: scaleX(.7)}
.MJXp-scale8 {-webkit-transform: scaleX(.8); -moz-transform: scaleX(.8); -ms-transform: scaleX(.8); -o-transform: scaleX(.8); transform: scaleX(.8)}
.MJXp-scale9 {-webkit-transform: scaleX(.9); -moz-transform: scaleX(.9); -ms-transform: scaleX(.9); -o-transform: scaleX(.9); transform: scaleX(.9)}
.MathJax_PHTML .noError {vertical-align: ; font-size: 90%; text-align: left; color: black; padding: 1px 3px; border: 1px solid}
</style><meta content="AlK2UR5SkAlj8jjdEc9p3F3xuFYlF6LYjAML3EOqw1g26eCwWPjdmecULvBH5MVPoqKYrOfPhYVL71xAXI1IBQoAAAB8eyJvcmlnaW4iOiJodHRwczovL2RvdWJsZWNsaWNrLm5ldDo0NDMiLCJmZWF0dXJlIjoiV2ViVmlld1hSZXF1ZXN0ZWRXaXRoRGVwcmVjYXRpb24iLCJleHBpcnkiOjE3NTgwNjcxOTksImlzU3ViZG9tYWluIjp0cnVlfQ==" http-equiv="origin-trial"/><meta content="Amm8/NmvvQfhwCib6I7ZsmUxiSCfOxWxHayJwyU1r3gRIItzr7bNQid6O8ZYaE1GSQTa69WwhPC9flq/oYkRBwsAAACCeyJvcmlnaW4iOiJodHRwczovL2dvb2dsZXN5bmRpY2F0aW9uLmNvbTo0NDMiLCJmZWF0dXJlIjoiV2ViVmlld1hSZXF1ZXN0ZWRXaXRoRGVwcmVjYXRpb24iLCJleHBpcnkiOjE3NTgwNjcxOTksImlzU3ViZG9tYWluIjp0cnVlfQ==" http-equiv="origin-trial"/><meta content="A9wSqI5i0iwGdf6L1CERNdmsTPgVu44ewj8QxTBYgsv1LCPUVF7YmWOvTappqB1139jAymxUW/RO8zmMqo4zlAAAAACNeyJvcmlnaW4iOiJodHRwczovL2RvdWJsZWNsaWNrLm5ldDo0NDMiLCJmZWF0dXJlIjoiRmxlZGdlQmlkZGluZ0FuZEF1Y3Rpb25TZXJ2ZXIiLCJleHBpcnkiOjE3MzY4MTI4MDAsImlzU3ViZG9tYWluIjp0cnVlLCJpc1RoaXJkUGFydHkiOnRydWV9" http-equiv="origin-trial"/><meta content="A+d7vJfYtay4OUbdtRPZA3y7bKQLsxaMEPmxgfhBGqKXNrdkCQeJlUwqa6EBbSfjwFtJWTrWIioXeMW+y8bWAgQAAACTeyJvcmlnaW4iOiJodHRwczovL2dvb2dsZXN5bmRpY2F0aW9uLmNvbTo0NDMiLCJmZWF0dXJlIjoiRmxlZGdlQmlkZGluZ0FuZEF1Y3Rpb25TZXJ2ZXIiLCJleHBpcnkiOjE3MzY4MTI4MDAsImlzU3ViZG9tYWluIjp0cnVlLCJpc1RoaXJkUGFydHkiOnRydWV9" http-equiv="origin-trial"/><script async="" src="https://pagead2.googlesyndication.com/pagead/managed/js/gpt/m202410080101/pubads_impl.js"></script><script async="" src="https://cdn.pbgrd.com/core-spl.js"></script></head>
<body class=""><div id="MathJax_Message" style="display: none;"></div><dialog aria-labelledby="cc-banner-label" class="cc-banner" data-cc-banner="" data-nosnippet="" open="">
<div autofocus="" class="cc-banner__content" tabindex="-1">
<div class="cc-banner__container">
<div class="cc-banner__header">
<h2 class="cc-banner__title" id="cc-banner-label">Your privacy, your choice</h2>
</div>
<div class="cc-banner__body">
<div class="cc-banner__policy">
<p>We use essential cookies to make sure the site can function. We also use optional cookies for advertising, personalisation of content, usage analysis, and social media.</p><p>By accepting optional cookies, you consent to the processing of your personal data - including transfers to third parties. Some third parties are outside of the European Economic Area, with varying standards of data protection.</p><p>See our <a data-cc-action="privacy" href="https://link.springer.com/privacystatement">privacy policy</a> for more information on the use of your personal data.</p><p><button class="cc-button cc-button--link cc-button--text" data-cc-action="preferences" type="button">Manage preferences</button> for further information and to change your choices.</p>
</div>
</div>
<div class="cc-banner__footer">
<button class="cc-button cc-button--secondary cc-button--contrast cc-banner__button cc-banner__button-accept" data-cc-action="accept">Accept all cookies</button>
</div>
</div>
</div>
</dialog>
<!-- Google Tag Manager (noscript) -->
<noscript>
<iframe height="0" src="https://www.googletagmanager.com/ns.html?id=GTM-MRVXSHQ" style="display:none;visibility:hidden" width="0"></iframe>
</noscript>
<!-- End Google Tag Manager (noscript) -->
<!-- Google Tag Manager (noscript) -->
<noscript data-test="gtm-body">
<iframe height="0" src="https://www.googletagmanager.com/ns.html?id=GTM-MRVXSHQ" style="display:none;visibility:hidden" width="0"></iframe>
</noscript>
<!-- End Google Tag Manager (noscript) -->
<div aria-hidden="true" class="u-visually-hidden" data-test="darwin-icons">
<!--?xml version="1.0" encoding="UTF-8"?--><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><symbol id="icon-eds-i-accesses-medium" viewbox="0 0 24 24"><path d="M15.59 1a1 1 0 0 1 .706.291l5.41 5.385a1 1 0 0 1 .294.709v13.077c0 .674-.269 1.32-.747 1.796a2.549 2.549 0 0 1-1.798.742H15a1 1 0 0 1 0-2h4.455a.549.549 0 0 0 .387-.16.535.535 0 0 0 .158-.378V7.8L15.178 3H5.545a.543.543 0 0 0-.538.451L5 3.538v8.607a1 1 0 0 1-2 0V3.538A2.542 2.542 0 0 1 5.545 1h10.046ZM8 13c2.052 0 4.66 1.61 6.36 3.4l.124.141c.333.41.516.925.516 1.459 0 .6-.232 1.178-.64 1.599C12.666 21.388 10.054 23 8 23c-2.052 0-4.66-1.61-6.353-3.393A2.31 2.31 0 0 1 1 18c0-.6.232-1.178.64-1.6C3.34 14.61 5.948 13 8 13Zm0 2c-1.369 0-3.552 1.348-4.917 2.785A.31.31 0 0 0 3 18c0 .083.031.161.09.222C4.447 19.652 6.631 21 8 21c1.37 0 3.556-1.35 4.917-2.785A.31.31 0 0 0 13 18a.32.32 0 0 0-.048-.17l-.042-.052C11.553 16.348 9.369 15 8 15Zm0 1a2 2 0 1 1 0 4 2 2 0 0 1 0-4Z"></path></symbol><symbol id="icon-eds-i-altmetric-medium" viewbox="0 0 24 24"><path d="M12 1c5.978 0 10.843 4.77 10.996 10.712l.004.306-.002.022-.002.248C22.843 18.23 17.978 23 12 23 5.925 23 1 18.075 1 12S5.925 1 12 1Zm-1.726 9.246L8.848 12.53a1 1 0 0 1-.718.461L8.003 13l-4.947.014a9.001 9.001 0 0 0 17.887-.001L16.553 13l-2.205 3.53a1 1 0 0 1-1.735-.068l-.05-.11-2.289-6.106ZM12 3a9.001 9.001 0 0 0-8.947 8.013l4.391-.012L9.652 7.47a1 1 0 0 1 1.784.179l2.288 6.104 1.428-2.283a1 1 0 0 1 .722-.462l.129-.008 4.943.012A9.001 9.001 0 0 0 12 3Z"></path></symbol><symbol id="icon-eds-i-arrow-bend-down-medium" viewbox="0 0 24 24"><path d="m11.852 20.989.058.007L12 21l.075-.003.126-.017.111-.03.111-.044.098-.052.104-.074.082-.073 6-6a1 1 0 0 0-1.414-1.414L13 17.585v-12.2C13 4.075 11.964 3 10.667 3H4a1 1 0 1 0 0 2h6.667c.175 0 .333.164.333.385v12.2l-4.293-4.292a1 1 0 0 0-1.32-.083l-.094.083a1 1 0 0 0 0 1.414l6 6c.035.036.073.068.112.097l.11.071.114.054.105.035.118.025Z"></path></symbol><symbol id="icon-eds-i-arrow-bend-down-small" viewbox="0 0 16 16"><path d="M1 2a1 1 0 0 0 1 1h5v8.585L3.707 8.293a1 1 0 0 0-1.32-.083l-.094.083a1 1 0 0 0 0 1.414l5 5 .063.059.093.069.081.048.105.048.104.035.105.022.096.01h.136l.122-.018.113-.03.103-.04.1-.053.102-.07.052-.043 5.04-5.037a1 1 0 1 0-1.415-1.414L9 11.583V3a2 2 0 0 0-2-2H2a1 1 0 0 0-1 1Z"></path></symbol><symbol id="icon-eds-i-arrow-bend-up-medium" viewbox="0 0 24 24"><path d="m11.852 3.011.058-.007L12 3l.075.003.126.017.111.03.111.044.098.052.104.074.082.073 6 6a1 1 0 1 1-1.414 1.414L13 6.415v12.2C13 19.925 11.964 21 10.667 21H4a1 1 0 0 1 0-2h6.667c.175 0 .333-.164.333-.385v-12.2l-4.293 4.292a1 1 0 0 1-1.32.083l-.094-.083a1 1 0 0 1 0-1.414l6-6c.035-.036.073-.068.112-.097l.11-.071.114-.054.105-.035.118-.025Z"></path></symbol><symbol id="icon-eds-i-arrow-bend-up-small" viewbox="0 0 16 16"><path d="M1 13.998a1 1 0 0 1 1-1h5V4.413L3.707 7.705a1 1 0 0 1-1.32.084l-.094-.084a1 1 0 0 1 0-1.414l5-5 .063-.059.093-.068.081-.05.105-.047.104-.035.105-.022L7.94 1l.136.001.122.017.113.03.103.04.1.053.102.07.052.043 5.04 5.037a1 1 0 1 1-1.415 1.414L9 4.415v8.583a2 2 0 0 1-2 2H2a1 1 0 0 1-1-1Z"></path></symbol><symbol id="icon-eds-i-arrow-diagonal-medium" viewbox="0 0 24 24"><path d="M14 3h6l.075.003.126.017.111.03.111.044.098.052.096.067.09.08c.036.035.068.073.097.112l.071.11.054.114.035.105.03.148L21 4v6a1 1 0 0 1-2 0V6.414l-4.293 4.293a1 1 0 0 1-1.414-1.414L17.584 5H14a1 1 0 0 1-.993-.883L13 4a1 1 0 0 1 1-1ZM4 13a1 1 0 0 1 1 1v3.584l4.293-4.291a1 1 0 1 1 1.414 1.414L6.414 19H10a1 1 0 0 1 .993.883L11 20a1 1 0 0 1-1 1l-6.075-.003-.126-.017-.111-.03-.111-.044-.098-.052-.096-.067-.09-.08a1.01 1.01 0 0 1-.097-.112l-.071-.11-.054-.114-.035-.105-.025-.118-.007-.058L3 20v-6a1 1 0 0 1 1-1Z"></path></symbol><symbol id="icon-eds-i-arrow-diagonal-small" viewbox="0 0 16 16"><path d="m2 15-.082-.004-.119-.016-.111-.03-.111-.044-.098-.052-.096-.067-.09-.08a1.008 1.008 0 0 1-.097-.112l-.071-.11-.031-.062-.034-.081-.024-.076-.025-.118-.007-.058L1 14.02V9a1 1 0 1 1 2 0v2.584l2.793-2.791a1 1 0 1 1 1.414 1.414L4.414 13H7a1 1 0 0 1 .993.883L8 14a1 1 0 0 1-1 1H2ZM14 1l.081.003.12.017.111.03.111.044.098.052.096.067.09.08c.036.035.068.073.097.112l.071.11.031.062.034.081.024.076.03.148L15 2v5a1 1 0 0 1-2 0V4.414l-2.96 2.96A1 1 0 1 1 8.626 5.96L11.584 3H9a1 1 0 0 1-.993-.883L8 2a1 1 0 0 1 1-1h5Z"></path></symbol><symbol id="icon-eds-i-arrow-down-medium" viewbox="0 0 24 24"><path d="m20.707 12.728-7.99 7.98a.996.996 0 0 1-.561.281l-.157.011a.998.998 0 0 1-.788-.384l-7.918-7.908a1 1 0 0 1 1.414-1.416L11 17.576V4a1 1 0 0 1 2 0v13.598l6.293-6.285a1 1 0 0 1 1.32-.082l.095.083a1 1 0 0 1-.001 1.414Z"></path></symbol><symbol id="icon-eds-i-arrow-down-small" viewbox="0 0 16 16"><path d="m1.293 8.707 6 6 .063.059.093.069.081.048.105.049.104.034.056.013.118.017L8 15l.076-.003.122-.017.113-.03.085-.032.063-.03.098-.058.06-.043.05-.043 6.04-6.037a1 1 0 0 0-1.414-1.414L9 11.583V2a1 1 0 1 0-2 0v9.585L2.707 7.293a1 1 0 0 0-1.32-.083l-.094.083a1 1 0 0 0 0 1.414Z"></path></symbol><symbol id="icon-eds-i-arrow-left-medium" viewbox="0 0 24 24"><path d="m11.272 3.293-7.98 7.99a.996.996 0 0 0-.281.561L3 12.001c0 .32.15.605.384.788l7.908 7.918a1 1 0 0 0 1.416-1.414L6.424 13H20a1 1 0 0 0 0-2H6.402l6.285-6.293a1 1 0 0 0 .082-1.32l-.083-.095a1 1 0 0 0-1.414.001Z"></path></symbol><symbol id="icon-eds-i-arrow-left-small" viewbox="0 0 16 16"><path d="m7.293 1.293-6 6-.059.063-.069.093-.048.081-.049.105-.034.104-.013.056-.017.118L1 8l.003.076.017.122.03.113.032.085.03.063.058.098.043.06.043.05 6.037 6.04a1 1 0 0 0 1.414-1.414L4.417 9H14a1 1 0 0 0 0-2H4.415l4.292-4.293a1 1 0 0 0 .083-1.32l-.083-.094a1 1 0 0 0-1.414 0Z"></path></symbol><symbol id="icon-eds-i-arrow-right-medium" viewbox="0 0 24 24"><path d="m12.728 3.293 7.98 7.99a.996.996 0 0 1 .281.561l.011.157c0 .32-.15.605-.384.788l-7.908 7.918a1 1 0 0 1-1.416-1.414L17.576 13H4a1 1 0 0 1 0-2h13.598l-6.285-6.293a1 1 0 0 1-.082-1.32l.083-.095a1 1 0 0 1 1.414.001Z"></path></symbol><symbol id="icon-eds-i-arrow-right-small" viewbox="0 0 16 16"><path d="m8.707 1.293 6 6 .059.063.069.093.048.081.049.105.034.104.013.056.017.118L15 8l-.003.076-.017.122-.03.113-.032.085-.03.063-.058.098-.043.06-.043.05-6.037 6.04a1 1 0 0 1-1.414-1.414L11.583 9H2a1 1 0 1 1 0-2h9.585L7.293 2.707a1 1 0 0 1-.083-1.32l.083-.094a1 1 0 0 1 1.414 0Z"></path></symbol><symbol id="icon-eds-i-arrow-up-medium" viewbox="0 0 24 24"><path d="m3.293 11.272 7.99-7.98a.996.996 0 0 1 .561-.281L12.001 3c.32 0 .605.15.788.384l7.918 7.908a1 1 0 0 1-1.414 1.416L13 6.424V20a1 1 0 0 1-2 0V6.402l-6.293 6.285a1 1 0 0 1-1.32.082l-.095-.083a1 1 0 0 1 .001-1.414Z"></path></symbol><symbol id="icon-eds-i-arrow-up-small" viewbox="0 0 16 16"><path d="m1.293 7.293 6-6 .063-.059.093-.069.081-.048.105-.049.104-.034.056-.013.118-.017L8 1l.076.003.122.017.113.03.085.032.063.03.098.058.06.043.05.043 6.04 6.037a1 1 0 0 1-1.414 1.414L9 4.417V14a1 1 0 0 1-2 0V4.415L2.707 8.707a1 1 0 0 1-1.32.083l-.094-.083a1 1 0 0 1 0-1.414Z"></path></symbol><symbol id="icon-eds-i-article-medium" viewbox="0 0 24 24"><path d="M8 7a1 1 0 0 0 0 2h4a1 1 0 1 0 0-2H8ZM8 11a1 1 0 1 0 0 2h8a1 1 0 1 0 0-2H8ZM7 16a1 1 0 0 1 1-1h8a1 1 0 1 1 0 2H8a1 1 0 0 1-1-1Z"></path><path clip-rule="evenodd" d="M5.545 1A2.542 2.542 0 0 0 3 3.538v16.924A2.542 2.542 0 0 0 5.545 23h12.91A2.542 2.542 0 0 0 21 20.462V3.5A2.5 2.5 0 0 0 18.5 1H5.545ZM5 3.538C5 3.245 5.24 3 5.545 3H18.5a.5.5 0 0 1 .5.5v16.962c0 .293-.24.538-.546.538H5.545A.542.542 0 0 1 5 20.462V3.538Z"></path></symbol><symbol id="icon-eds-i-book-medium" viewbox="0 0 24 24"><path d="M18.5 1A2.5 2.5 0 0 1 21 3.5v12c0 1.16-.79 2.135-1.86 2.418l-.14.031V21h1a1 1 0 0 1 .993.883L21 22a1 1 0 0 1-1 1H6.5A3.5 3.5 0 0 1 3 19.5v-15A3.5 3.5 0 0 1 6.5 1h12ZM17 18H6.5a1.5 1.5 0 0 0-1.493 1.356L5 19.5A1.5 1.5 0 0 0 6.5 21H17v-3Zm1.5-15h-12A1.5 1.5 0 0 0 5 4.5v11.837l.054-.025a3.481 3.481 0 0 1 1.254-.307L6.5 16h12a.5.5 0 0 0 .492-.41L19 15.5v-12a.5.5 0 0 0-.5-.5ZM15 6a1 1 0 0 1 0 2H9a1 1 0 1 1 0-2h6Z"></path></symbol><symbol id="icon-eds-i-book-series-medium" viewbox="0 0 24 24"><path clip-rule="evenodd" d="M1 3.786C1 2.759 1.857 2 2.82 2H6.18c.964 0 1.82.759 1.82 1.786V4h3.168c.668 0 1.298.364 1.616.938.158-.109.333-.195.523-.252l3.216-.965c.923-.277 1.962.204 2.257 1.187l4.146 13.82c.296.984-.307 1.957-1.23 2.234l-3.217.965c-.923.277-1.962-.203-2.257-1.187L13 10.005v10.21c0 1.04-.878 1.785-1.834 1.785H7.833c-.291 0-.575-.07-.83-.195A1.849 1.849 0 0 1 6.18 22H2.821C1.857 22 1 21.241 1 20.214V3.786ZM3 4v11h3V4H3Zm0 16v-3h3v3H3Zm15.075-.04-.814-2.712 2.874-.862.813 2.712-2.873.862Zm1.485-5.49-2.874.862-2.634-8.782 2.873-.862 2.635 8.782ZM8 20V6h3v14H8Z" fill-rule="evenodd"></path></symbol><symbol id="icon-eds-i-calendar-acceptance-medium" viewbox="0 0 24 24"><path d="M17 2a1 1 0 0 1 1 1v1h1.5C20.817 4 22 5.183 22 6.5v13c0 1.317-1.183 2.5-2.5 2.5h-15C3.183 22 2 20.817 2 19.5v-13C2 5.183 3.183 4 4.5 4a1 1 0 1 1 0 2c-.212 0-.5.288-.5.5v13c0 .212.288.5.5.5h15c.212 0 .5-.288.5-.5v-13c0-.212-.288-.5-.5-.5H18v1a1 1 0 0 1-2 0V3a1 1 0 0 1 1-1Zm-.534 7.747a1 1 0 0 1 .094 1.412l-4.846 5.538a1 1 0 0 1-1.352.141l-2.77-2.076a1 1 0 0 1 1.2-1.6l2.027 1.519 4.236-4.84a1 1 0 0 1 1.411-.094ZM7.5 2a1 1 0 0 1 1 1v1H14a1 1 0 0 1 0 2H8.5v1a1 1 0 1 1-2 0V3a1 1 0 0 1 1-1Z"></path></symbol><symbol id="icon-eds-i-calendar-date-medium" viewbox="0 0 24 24"><path d="M17 2a1 1 0 0 1 1 1v1h1.5C20.817 4 22 5.183 22 6.5v13c0 1.317-1.183 2.5-2.5 2.5h-15C3.183 22 2 20.817 2 19.5v-13C2 5.183 3.183 4 4.5 4a1 1 0 1 1 0 2c-.212 0-.5.288-.5.5v13c0 .212.288.5.5.5h15c.212 0 .5-.288.5-.5v-13c0-.212-.288-.5-.5-.5H18v1a1 1 0 0 1-2 0V3a1 1 0 0 1 1-1ZM8 15a1 1 0 1 1 0 2 1 1 0 0 1 0-2Zm4 0a1 1 0 1 1 0 2 1 1 0 0 1 0-2Zm-4-4a1 1 0 1 1 0 2 1 1 0 0 1 0-2Zm4 0a1 1 0 1 1 0 2 1 1 0 0 1 0-2Zm4 0a1 1 0 1 1 0 2 1 1 0 0 1 0-2ZM7.5 2a1 1 0 0 1 1 1v1H14a1 1 0 0 1 0 2H8.5v1a1 1 0 1 1-2 0V3a1 1 0 0 1 1-1Z"></path></symbol><symbol id="icon-eds-i-calendar-decision-medium" viewbox="0 0 24 24"><path d="M17 2a1 1 0 0 1 1 1v1h1.5C20.817 4 22 5.183 22 6.5v13c0 1.317-1.183 2.5-2.5 2.5h-15C3.183 22 2 20.817 2 19.5v-13C2 5.183 3.183 4 4.5 4a1 1 0 1 1 0 2c-.212 0-.5.288-.5.5v13c0 .212.288.5.5.5h15c.212 0 .5-.288.5-.5v-13c0-.212-.288-.5-.5-.5H18v1a1 1 0 0 1-2 0V3a1 1 0 0 1 1-1Zm-2.935 8.246 2.686 2.645c.34.335.34.883 0 1.218l-2.686 2.645a.858.858 0 0 1-1.213-.009.854.854 0 0 1 .009-1.21l1.05-1.035H7.984a.992.992 0 0 1-.984-1c0-.552.44-1 .984-1h5.928l-1.051-1.036a.854.854 0 0 1-.085-1.121l.076-.088a.858.858 0 0 1 1.213-.009ZM7.5 2a1 1 0 0 1 1 1v1H14a1 1 0 0 1 0 2H8.5v1a1 1 0 1 1-2 0V3a1 1 0 0 1 1-1Z"></path></symbol><symbol id="icon-eds-i-calendar-impact-factor-medium" viewbox="0 0 24 24"><path d="M17 2a1 1 0 0 1 1 1v1h1.5C20.817 4 22 5.183 22 6.5v13c0 1.317-1.183 2.5-2.5 2.5h-15C3.183 22 2 20.817 2 19.5v-13C2 5.183 3.183 4 4.5 4a1 1 0 1 1 0 2c-.212 0-.5.288-.5.5v13c0 .212.288.5.5.5h15c.212 0 .5-.288.5-.5v-13c0-.212-.288-.5-.5-.5H18v1a1 1 0 0 1-2 0V3a1 1 0 0 1 1-1Zm-3.2 6.924a.48.48 0 0 1 .125.544l-1.52 3.283h2.304c.27 0 .491.215.491.483a.477.477 0 0 1-.13.327l-4.18 4.484a.498.498 0 0 1-.69.031.48.48 0 0 1-.125-.544l1.52-3.284H9.291a.487.487 0 0 1-.491-.482c0-.121.047-.238.13-.327l4.18-4.484a.498.498 0 0 1 .69-.031ZM7.5 2a1 1 0 0 1 1 1v1H14a1 1 0 0 1 0 2H8.5v1a1 1 0 1 1-2 0V3a1 1 0 0 1 1-1Z"></path></symbol><symbol id="icon-eds-i-call-papers-medium" viewbox="0 0 24 24"><g><path d="m20.707 2.883-1.414 1.414a1 1 0 0 0 1.414 1.414l1.414-1.414a1 1 0 0 0-1.414-1.414Z"></path><path clip-rule="evenodd" d="M6 16.054c0 2.026 1.052 2.943 3 2.943a1 1 0 1 1 0 2c-2.996 0-5-1.746-5-4.943v-1.227a4.068 4.068 0 0 1-1.83-1.189 4.553 4.553 0 0 1-.87-1.455 4.868 4.868 0 0 1-.3-1.686c0-1.17.417-2.298 1.17-3.14.38-.426.834-.767 1.338-1 .51-.237 1.06-.36 1.617-.36L6.632 6H7l7.932-2.895A2.363 2.363 0 0 1 18 5.36v9.28a2.36 2.36 0 0 1-3.069 2.25l.084.03L7 14.997H6v1.057Zm9.637-11.057a.415.415 0 0 0-.083.008L8 7.638v5.536l7.424 1.786.104.02c.035.01.072.02.109.02.2 0 .363-.16.363-.36V5.36c0-.2-.163-.363-.363-.363Zm-9.638 3h-.874a1.82 1.82 0 0 0-.625.111l-.15.063a2.128 2.128 0 0 0-.689.517c-.42.47-.661 1.123-.661 1.81 0 .34.06.678.176.992.114.308.28.585.485.816.4.447.925.691 1.464.691h.874v-5Z"></path><path d="M20 8.997h2a1 1 0 1 1 0 2h-2a1 1 0 1 1 0-2ZM20.707 14.293l1.414 1.414a1 1 0 0 1-1.414 1.414l-1.414-1.414a1 1 0 0 1 1.414-1.414Z"></path></g></symbol><symbol id="icon-eds-i-card-medium" viewbox="0 0 24 24"><path d="M19.615 2c.315 0 .716.067 1.14.279.76.38 1.245 1.107 1.245 2.106v15.23c0 .315-.067.716-.279 1.14-.38.76-1.107 1.245-2.106 1.245H4.385a2.56 2.56 0 0 1-1.14-.279C2.485 21.341 2 20.614 2 19.615V4.385c0-.315.067-.716.279-1.14C2.659 2.485 3.386 2 4.385 2h15.23Zm0 2H4.385c-.213 0-.265.034-.317.14A.71.71 0 0 0 4 4.385v15.23c0 .213.034.265.14.317a.71.71 0 0 0 .245.068h15.23c.213 0 .265-.034.317-.14a.71.71 0 0 0 .068-.245V4.385c0-.213-.034-.265-.14-.317A.71.71 0 0 0 19.615 4ZM17 16a1 1 0 0 1 0 2H7a1 1 0 0 1 0-2h10Zm0-3a1 1 0 0 1 0 2H7a1 1 0 0 1 0-2h10Zm-.5-7A1.5 1.5 0 0 1 18 7.5v3a1.5 1.5 0 0 1-1.5 1.5h-9A1.5 1.5 0 0 1 6 10.5v-3A1.5 1.5 0 0 1 7.5 6h9ZM16 8H8v2h8V8Z"></path></symbol><symbol id="icon-eds-i-cart-medium" viewbox="0 0 24 24"><path d="M5.76 1a1 1 0 0 1 .994.902L7.155 6h13.34c.18 0 .358.02.532.057l.174.045a2.5 2.5 0 0 1 1.693 3.103l-2.069 7.03c-.36 1.099-1.398 1.823-2.49 1.763H8.65c-1.272.015-2.352-.927-2.546-2.244L4.852 3H2a1 1 0 0 1-.993-.883L1 2a1 1 0 0 1 1-1h3.76Zm2.328 14.51a.555.555 0 0 0 .55.488l9.751.001a.533.533 0 0 0 .527-.357l2.059-7a.5.5 0 0 0-.48-.642H7.351l.737 7.51ZM18 19a2 2 0 1 1 0 4 2 2 0 0 1 0-4ZM8 19a2 2 0 1 1 0 4 2 2 0 0 1 0-4Z"></path></symbol><symbol id="icon-eds-i-check-circle-medium" viewbox="0 0 24 24"><path d="M12 1c6.075 0 11 4.925 11 11s-4.925 11-11 11S1 18.075 1 12 5.925 1 12 1Zm0 2a9 9 0 1 0 0 18 9 9 0 0 0 0-18Zm5.125 4.72a1 1 0 0 1 .156 1.405l-6 7.5a1 1 0 0 1-1.421.143l-3-2.5a1 1 0 0 1 1.28-1.536l2.217 1.846 5.362-6.703a1 1 0 0 1 1.406-.156Z"></path></symbol><symbol id="icon-eds-i-check-filled-medium" viewbox="0 0 24 24"><path d="M12 1c6.075 0 11 4.925 11 11s-4.925 11-11 11S1 18.075 1 12 5.925 1 12 1Zm5.125 6.72a1 1 0 0 0-1.406.155l-5.362 6.703-2.217-1.846a1 1 0 1 0-1.28 1.536l3 2.5a1 1 0 0 0 1.42-.143l6-7.5a1 1 0 0 0-.155-1.406Z"></path></symbol><symbol id="icon-eds-i-chevron-down-medium" viewbox="0 0 24 24"><path d="M3.305 8.28a1 1 0 0 0-.024 1.415l7.495 7.762c.314.345.757.543 1.224.543.467 0 .91-.198 1.204-.522l7.515-7.783a1 1 0 1 0-1.438-1.39L12 15.845l-7.28-7.54A1 1 0 0 0 3.4 8.2l-.096.082Z"></path></symbol><symbol id="icon-eds-i-chevron-down-small" viewbox="0 0 16 16"><path d="M13.692 5.278a1 1 0 0 1 .03 1.414L9.103 11.51a1.491 1.491 0 0 1-2.188.019L2.278 6.692a1 1 0 0 1 1.444-1.384L8 9.771l4.278-4.463a1 1 0 0 1 1.318-.111l.096.081Z"></path></symbol><symbol id="icon-eds-i-chevron-left-medium" viewbox="0 0 24 24"><path d="M15.72 3.305a1 1 0 0 0-1.415-.024l-7.762 7.495A1.655 1.655 0 0 0 6 12c0 .467.198.91.522 1.204l7.783 7.515a1 1 0 1 0 1.39-1.438L8.155 12l7.54-7.28A1 1 0 0 0 15.8 3.4l-.082-.096Z"></path></symbol><symbol id="icon-eds-i-chevron-left-small" viewbox="0 0 16 16"><path d="M10.722 2.308a1 1 0 0 0-1.414-.03L4.49 6.897a1.491 1.491 0 0 0-.019 2.188l4.838 4.637a1 1 0 1 0 1.384-1.444L6.229 8l4.463-4.278a1 1 0 0 0 .111-1.318l-.081-.096Z"></path></symbol><symbol id="icon-eds-i-chevron-right-medium" viewbox="0 0 24 24"><path d="M8.28 3.305a1 1 0 0 1 1.415-.024l7.762 7.495c.345.314.543.757.543 1.224 0 .467-.198.91-.522 1.204l-7.783 7.515a1 1 0 1 1-1.39-1.438L15.845 12l-7.54-7.28A1 1 0 0 1 8.2 3.4l.082-.096Z"></path></symbol><symbol id="icon-eds-i-chevron-right-small" viewbox="0 0 16 16"><path d="M5.278 2.308a1 1 0 0 1 1.414-.03l4.819 4.619a1.491 1.491 0 0 1 .019 2.188l-4.838 4.637a1 1 0 1 1-1.384-1.444L9.771 8 5.308 3.722a1 1 0 0 1-.111-1.318l.081-.096Z"></path></symbol><symbol id="icon-eds-i-chevron-up-medium" viewbox="0 0 24 24"><path d="M20.695 15.72a1 1 0 0 0 .024-1.415l-7.495-7.762A1.655 1.655 0 0 0 12 6c-.467 0-.91.198-1.204.522l-7.515 7.783a1 1 0 1 0 1.438 1.39L12 8.155l7.28 7.54a1 1 0 0 0 1.319.106l.096-.082Z"></path></symbol><symbol id="icon-eds-i-chevron-up-small" viewbox="0 0 16 16"><path d="M13.692 10.722a1 1 0 0 0 .03-1.414L9.103 4.49a1.491 1.491 0 0 0-2.188-.019L2.278 9.308a1 1 0 0 0 1.444 1.384L8 6.229l4.278 4.463a1 1 0 0 0 1.318.111l.096-.081Z"></path></symbol><symbol id="icon-eds-i-citations-medium" viewbox="0 0 24 24"><path d="M15.59 1a1 1 0 0 1 .706.291l5.41 5.385a1 1 0 0 1 .294.709v13.077c0 .674-.269 1.32-.747 1.796a2.549 2.549 0 0 1-1.798.742h-5.843a1 1 0 1 1 0-2h5.843a.549.549 0 0 0 .387-.16.535.535 0 0 0 .158-.378V7.8L15.178 3H5.545a.543.543 0 0 0-.538.451L5 3.538v8.607a1 1 0 0 1-2 0V3.538A2.542 2.542 0 0 1 5.545 1h10.046ZM5.483 14.35c.197.26.17.62-.049.848l-.095.083-.016.011c-.36.24-.628.45-.804.634-.393.409-.59.93-.59 1.562.077-.019.192-.028.345-.028.442 0 .84.158 1.195.474.355.316.532.716.532 1.2 0 .501-.173.9-.518 1.198-.345.298-.767.446-1.266.446-.672 0-1.209-.195-1.612-.585-.403-.39-.604-.976-.604-1.757 0-.744.11-1.39.33-1.938.222-.549.49-1.009.807-1.38a4.28 4.28 0 0 1 .992-.88c.07-.043.148-.087.232-.133a.881.881 0 0 1 1.121.245Zm5 0c.197.26.17.62-.049.848l-.095.083-.016.011c-.36.24-.628.45-.804.634-.393.409-.59.93-.59 1.562.077-.019.192-.028.345-.028.442 0 .84.158 1.195.474.355.316.532.716.532 1.2 0 .501-.173.9-.518 1.198-.345.298-.767.446-1.266.446-.672 0-1.209-.195-1.612-.585-.403-.39-.604-.976-.604-1.757 0-.744.11-1.39.33-1.938.222-.549.49-1.009.807-1.38a4.28 4.28 0 0 1 .992-.88c.07-.043.148-.087.232-.133a.881.881 0 0 1 1.121.245Z"></path></symbol><symbol id="icon-eds-i-clipboard-check-medium" viewbox="0 0 24 24"><path d="M14.4 1c1.238 0 2.274.865 2.536 2.024L18.5 3C19.886 3 21 4.14 21 5.535v14.93C21 21.86 19.886 23 18.5 23h-13C4.114 23 3 21.86 3 20.465V5.535C3 4.14 4.114 3 5.5 3h1.57c.27-1.147 1.3-2 2.53-2h4.8Zm4.115 4-1.59.024A2.601 2.601 0 0 1 14.4 7H9.6c-1.23 0-2.26-.853-2.53-2H5.5c-.27 0-.5.234-.5.535v14.93c0 .3.23.535.5.535h13c.27 0 .5-.234.5-.535V5.535c0-.3-.23-.535-.485-.535Zm-1.909 4.205a1 1 0 0 1 .19 1.401l-5.334 7a1 1 0 0 1-1.344.23l-2.667-1.75a1 1 0 1 1 1.098-1.672l1.887 1.238 4.769-6.258a1 1 0 0 1 1.401-.19ZM14.4 3H9.6a.6.6 0 0 0-.6.6v.8a.6.6 0 0 0 .6.6h4.8a.6.6 0 0 0 .6-.6v-.8a.6.6 0 0 0-.6-.6Z"></path></symbol><symbol id="icon-eds-i-clipboard-report-medium" viewbox="0 0 24 24"><path d="M14.4 1c1.238 0 2.274.865 2.536 2.024L18.5 3C19.886 3 21 4.14 21 5.535v14.93C21 21.86 19.886 23 18.5 23h-13C4.114 23 3 21.86 3 20.465V5.535C3 4.14 4.114 3 5.5 3h1.57c.27-1.147 1.3-2 2.53-2h4.8Zm4.115 4-1.59.024A2.601 2.601 0 0 1 14.4 7H9.6c-1.23 0-2.26-.853-2.53-2H5.5c-.27 0-.5.234-.5.535v14.93c0 .3.23.535.5.535h13c.27 0 .5-.234.5-.535V5.535c0-.3-.23-.535-.485-.535Zm-2.658 10.929a1 1 0 0 1 0 2H8a1 1 0 0 1 0-2h7.857Zm0-3.929a1 1 0 0 1 0 2H8a1 1 0 0 1 0-2h7.857ZM14.4 3H9.6a.6.6 0 0 0-.6.6v.8a.6.6 0 0 0 .6.6h4.8a.6.6 0 0 0 .6-.6v-.8a.6.6 0 0 0-.6-.6Z"></path></symbol><symbol id="icon-eds-i-close-medium" viewbox="0 0 24 24"><path d="M12 1c6.075 0 11 4.925 11 11s-4.925 11-11 11S1 18.075 1 12 5.925 1 12 1Zm0 2a9 9 0 1 0 0 18 9 9 0 0 0 0-18ZM8.707 7.293 12 10.585l3.293-3.292a1 1 0 0 1 1.414 1.414L13.415 12l3.292 3.293a1 1 0 0 1-1.414 1.414L12 13.415l-3.293 3.292a1 1 0 1 1-1.414-1.414L10.585 12 7.293 8.707a1 1 0 0 1 1.414-1.414Z"></path></symbol><symbol id="icon-eds-i-cloud-upload-medium" viewbox="0 0 24 24"><path d="m12.852 10.011.028-.004L13 10l.075.003.126.017.086.022.136.052.098.052.104.074.082.073 3 3a1 1 0 0 1 0 1.414l-.094.083a1 1 0 0 1-1.32-.083L14 13.416V20a1 1 0 0 1-2 0v-6.586l-1.293 1.293a1 1 0 0 1-1.32.083l-.094-.083a1 1 0 0 1 0-1.414l3-3 .112-.097.11-.071.114-.054.105-.035.118-.025Zm.587-7.962c3.065.362 5.497 2.662 5.992 5.562l.013.085.207.073c2.117.782 3.496 2.845 3.337 5.097l-.022.226c-.297 2.561-2.503 4.491-5.124 4.502a1 1 0 1 1-.009-2c1.619-.007 2.967-1.186 3.147-2.733.179-1.542-.86-2.979-2.487-3.353-.512-.149-.894-.579-.981-1.165-.21-2.237-2-4.035-4.308-4.308-2.31-.273-4.497 1.06-5.25 3.19l-.049.113c-.234.468-.718.756-1.176.743-1.418.057-2.689.857-3.32 2.084a3.668 3.668 0 0 0 .262 3.798c.796 1.136 2.169 1.764 3.583 1.635a1 1 0 1 1 .182 1.992c-2.125.194-4.193-.753-5.403-2.48a5.668 5.668 0 0 1-.403-5.86c.85-1.652 2.449-2.79 4.323-3.092l.287-.039.013-.028c1.207-2.741 4.125-4.404 7.186-4.042Z"></path></symbol><symbol id="icon-eds-i-collection-medium" viewbox="0 0 24 24"><path d="M21 7a1 1 0 0 1 1 1v12.5a2.5 2.5 0 0 1-2.5 2.5H8a1 1 0 0 1 0-2h11.5a.5.5 0 0 0 .5-.5V8a1 1 0 0 1 1-1Zm-5.5-5A2.5 2.5 0 0 1 18 4.5v12a2.5 2.5 0 0 1-2.5 2.5h-11A2.5 2.5 0 0 1 2 16.5v-12A2.5 2.5 0 0 1 4.5 2h11Zm0 2h-11a.5.5 0 0 0-.5.5v12a.5.5 0 0 0 .5.5h11a.5.5 0 0 0 .5-.5v-12a.5.5 0 0 0-.5-.5ZM13 13a1 1 0 0 1 0 2H7a1 1 0 0 1 0-2h6Zm0-3.5a1 1 0 0 1 0 2H7a1 1 0 0 1 0-2h6ZM13 6a1 1 0 0 1 0 2H7a1 1 0 1 1 0-2h6Z"></path></symbol><symbol id="icon-eds-i-conference-series-medium" viewbox="0 0 24 24"><path clip-rule="evenodd" d="M4.5 2A2.5 2.5 0 0 0 2 4.5v11A2.5 2.5 0 0 0 4.5 18h2.37l-2.534 2.253a1 1 0 0 0 1.328 1.494L9.88 18H11v3a1 1 0 1 0 2 0v-3h1.12l4.216 3.747a1 1 0 0 0 1.328-1.494L17.13 18h2.37a2.5 2.5 0 0 0 2.5-2.5v-11A2.5 2.5 0 0 0 19.5 2h-15ZM20 6V4.5a.5.5 0 0 0-.5-.5h-15a.5.5 0 0 0-.5.5V6h16ZM4 8v7.5a.5.5 0 0 0 .5.5h15a.5.5 0 0 0 .5-.5V8H4Z" fill-rule="evenodd"></path></symbol><symbol id="icon-eds-i-delivery-medium" viewbox="0 0 24 24"><path d="M8.51 20.598a3.037 3.037 0 0 1-3.02 0A2.968 2.968 0 0 1 4.161 19L3.5 19A2.5 2.5 0 0 1 1 16.5v-11A2.5 2.5 0 0 1 3.5 3h10a2.5 2.5 0 0 1 2.45 2.004L16 5h2.527c.976 0 1.855.585 2.27 1.49l2.112 4.62a1 1 0 0 1 .091.416v4.856C23 17.814 21.889 19 20.484 19h-.523a1.01 1.01 0 0 1-.121-.007 2.96 2.96 0 0 1-1.33 1.605 3.037 3.037 0 0 1-3.02 0A2.968 2.968 0 0 1 14.161 19H9.838a2.968 2.968 0 0 1-1.327 1.597Zm-2.024-3.462a.955.955 0 0 0-.481.73L5.999 18l.001.022a.944.944 0 0 0 .388.777l.098.065c.316.181.712.181 1.028 0A.97.97 0 0 0 8 17.978a.95.95 0 0 0-.486-.842 1.037 1.037 0 0 0-1.028 0Zm10 0a.955.955 0 0 0-.481.73l-.005.156a.944.944 0 0 0 .388.777l.098.065c.316.181.712.181 1.028 0a.97.97 0 0 0 .486-.886.95.95 0 0 0-.486-.842 1.037 1.037 0 0 0-1.028 0ZM21 12h-5v3.17a3.038 3.038 0 0 1 2.51.232 2.993 2.993 0 0 1 1.277 1.45l.058.155.058-.005.581-.002c.27 0 .516-.263.516-.618V12Zm-7.5-7h-10a.5.5 0 0 0-.5.5v11a.5.5 0 0 0 .5.5h.662a2.964 2.964 0 0 1 1.155-1.491l.172-.107a3.037 3.037 0 0 1 3.022 0A2.987 2.987 0 0 1 9.843 17H13.5a.5.5 0 0 0 .5-.5v-11a.5.5 0 0 0-.5-.5Zm5.027 2H16v3h4.203l-1.224-2.677a.532.532 0 0 0-.375-.316L18.527 7Z"></path></symbol><symbol id="icon-eds-i-download-medium" viewbox="0 0 24 24"><path d="M22 18.5a3.5 3.5 0 0 1-3.5 3.5h-13A3.5 3.5 0 0 1 2 18.5V18a1 1 0 0 1 2 0v.5A1.5 1.5 0 0 0 5.5 20h13a1.5 1.5 0 0 0 1.5-1.5V18a1 1 0 0 1 2 0v.5Zm-3.293-7.793-6 6-.063.059-.093.069-.081.048-.105.049-.104.034-.056.013-.118.017L12 17l-.076-.003-.122-.017-.113-.03-.085-.032-.063-.03-.098-.058-.06-.043-.05-.043-6.04-6.037a1 1 0 0 1 1.414-1.414l4.294 4.29L11 3a1 1 0 0 1 2 0l.001 10.585 4.292-4.292a1 1 0 0 1 1.32-.083l.094.083a1 1 0 0 1 0 1.414Z"></path></symbol><symbol id="icon-eds-i-edit-medium" viewbox="0 0 24 24"><path d="M17.149 2a2.38 2.38 0 0 1 1.699.711l2.446 2.46a2.384 2.384 0 0 1 .005 3.38L10.01 19.906a1 1 0 0 1-.434.257l-6.3 1.8a1 1 0 0 1-1.237-1.237l1.8-6.3a1 1 0 0 1 .257-.434L15.443 2.718A2.385 2.385 0 0 1 17.15 2Zm-3.874 5.689-7.586 7.536-1.234 4.319 4.318-1.234 7.54-7.582-3.038-3.039ZM17.149 4a.395.395 0 0 0-.286.126L14.695 6.28l3.029 3.029 2.162-2.173a.384.384 0 0 0 .106-.197L20 6.864c0-.103-.04-.2-.119-.278l-2.457-2.47A.385.385 0 0 0 17.149 4Z"></path></symbol><symbol id="icon-eds-i-education-medium" viewbox="0 0 24 24"><path clip-rule="evenodd" d="M12.41 2.088a1 1 0 0 0-.82 0l-10 4.5a1 1 0 0 0 0 1.824L3 9.047v7.124A3.001 3.001 0 0 0 4 22a3 3 0 0 0 1-5.83V9.948l1 .45V14.5a1 1 0 0 0 .087.408L7 14.5c-.913.408-.912.41-.912.41l.001.003.003.006.007.015a1.988 1.988 0 0 0 .083.16c.054.097.131.225.236.373.21.297.53.68.993 1.057C8.351 17.292 9.824 18 12 18c2.176 0 3.65-.707 4.589-1.476.463-.378.783-.76.993-1.057a4.162 4.162 0 0 0 .319-.533l.007-.015.003-.006v-.003h.002s0-.002-.913-.41l.913.408A1 1 0 0 0 18 14.5v-4.103l4.41-1.985a1 1 0 0 0 0-1.824l-10-4.5ZM16 11.297l-3.59 1.615a1 1 0 0 1-.82 0L8 11.297v2.94a3.388 3.388 0 0 0 .677.739C9.267 15.457 10.294 16 12 16s2.734-.543 3.323-1.024a3.388 3.388 0 0 0 .677-.739v-2.94ZM4.437 7.5 12 4.097 19.563 7.5 12 10.903 4.437 7.5ZM3 19a1 1 0 1 1 2 0 1 1 0 0 1-2 0Z" fill-rule="evenodd"></path></symbol><symbol id="icon-eds-i-error-diamond-medium" viewbox="0 0 24 24"><path d="M12.002 1c.702 0 1.375.279 1.871.775l8.35 8.353a2.646 2.646 0 0 1 .001 3.744l-8.353 8.353a2.646 2.646 0 0 1-3.742 0l-8.353-8.353a2.646 2.646 0 0 1 0-3.744l8.353-8.353.156-.142c.424-.362.952-.58 1.507-.625l.21-.008Zm0 2a.646.646 0 0 0-.38.123l-.093.08-8.34 8.34a.646.646 0 0 0-.18.355L3 12c0 .171.068.336.19.457l8.353 8.354a.646.646 0 0 0 .914 0l8.354-8.354a.646.646 0 0 0-.001-.914l-8.351-8.354A.646.646 0 0 0 12.002 3ZM12 14.5a1.5 1.5 0 0 1 .144 2.993L12 17.5a1.5 1.5 0 0 1 0-3ZM12 6a1 1 0 0 1 1 1v5a1 1 0 0 1-2 0V7a1 1 0 0 1 1-1Z"></path></symbol><symbol id="icon-eds-i-error-filled-medium" viewbox="0 0 24 24"><path d="M12.002 1c.702 0 1.375.279 1.871.775l8.35 8.353a2.646 2.646 0 0 1 .001 3.744l-8.353 8.353a2.646 2.646 0 0 1-3.742 0l-8.353-8.353a2.646 2.646 0 0 1 0-3.744l8.353-8.353.156-.142c.424-.362.952-.58 1.507-.625l.21-.008ZM12 14.5a1.5 1.5 0 0 0 0 3l.144-.007A1.5 1.5 0 0 0 12 14.5ZM12 6a1 1 0 0 0-1 1v5a1 1 0 0 0 2 0V7a1 1 0 0 0-1-1Z"></path></symbol><symbol id="icon-eds-i-external-link-medium" viewbox="0 0 24 24"><path d="M9 2a1 1 0 1 1 0 2H4.6c-.371 0-.6.209-.6.5v15c0 .291.229.5.6.5h14.8c.371 0 .6-.209.6-.5V15a1 1 0 0 1 2 0v4.5c0 1.438-1.162 2.5-2.6 2.5H4.6C3.162 22 2 20.938 2 19.5v-15C2 3.062 3.162 2 4.6 2H9Zm6 0h6l.075.003.126.017.111.03.111.044.098.052.096.067.09.08c.036.035.068.073.097.112l.071.11.054.114.035.105.03.148L22 3v6a1 1 0 0 1-2 0V5.414l-6.693 6.693a1 1 0 0 1-1.414-1.414L18.584 4H15a1 1 0 0 1-.993-.883L14 3a1 1 0 0 1 1-1Z"></path></symbol><symbol id="icon-eds-i-external-link-small" viewbox="0 0 16 16"><path d="M5 1a1 1 0 1 1 0 2l-2-.001V13L13 13v-2a1 1 0 0 1 2 0v2c0 1.15-.93 2-2.067 2H3.067C1.93 15 1 14.15 1 13V3c0-1.15.93-2 2.067-2H5Zm4 0h5l.075.003.126.017.111.03.111.044.098.052.096.067.09.08.044.047.073.093.051.083.054.113.035.105.03.148L15 2v5a1 1 0 0 1-2 0V4.414L9.107 8.307a1 1 0 0 1-1.414-1.414L11.584 3H9a1 1 0 0 1-.993-.883L8 2a1 1 0 0 1 1-1Z"></path></symbol><symbol id="icon-eds-i-file-download-medium" viewbox="0 0 24 24"><path d="M14.5 1a1 1 0 0 1 .707.293l5.5 5.5A1 1 0 0 1 21 7.5v12.962A2.542 2.542 0 0 1 18.455 23H5.545A2.542 2.542 0 0 1 3 20.462V3.538A2.542 2.542 0 0 1 5.545 1H14.5Zm-.415 2h-8.54A.542.542 0 0 0 5 3.538v16.924c0 .296.243.538.545.538h12.91a.542.542 0 0 0 .545-.538V7.915L14.085 3ZM12 7a1 1 0 0 1 1 1v6.585l2.293-2.292a1 1 0 0 1 1.32-.083l.094.083a1 1 0 0 1 0 1.414l-4 4a1.008 1.008 0 0 1-.112.097l-.11.071-.114.054-.105.035-.149.03L12 18l-.075-.003-.126-.017-.111-.03-.111-.044-.098-.052-.096-.067-.09-.08-4-4a1 1 0 0 1 1.414-1.414L11 14.585V8a1 1 0 0 1 1-1Z"></path></symbol><symbol id="icon-eds-i-file-report-medium" viewbox="0 0 24 24"><path d="M14.5 1a1 1 0 0 1 .707.293l5.5 5.5A1 1 0 0 1 21 7.5v12.962c0 .674-.269 1.32-.747 1.796a2.549 2.549 0 0 1-1.798.742H5.545c-.674 0-1.32-.267-1.798-.742A2.535 2.535 0 0 1 3 20.462V3.538A2.542 2.542 0 0 1 5.545 1H14.5Zm-.415 2h-8.54A.542.542 0 0 0 5 3.538v16.924c0 .142.057.278.158.379.102.102.242.159.387.159h12.91a.549.549 0 0 0 .387-.16.535.535 0 0 0 .158-.378V7.915L14.085 3ZM16 17a1 1 0 0 1 0 2H8a1 1 0 0 1 0-2h8Zm0-3a1 1 0 0 1 0 2H8a1 1 0 0 1 0-2h8Zm-4.793-6.207L13 9.585l1.793-1.792a1 1 0 0 1 1.32-.083l.094.083a1 1 0 0 1 0 1.414l-2.5 2.5a1 1 0 0 1-1.414 0L10.5 9.915l-1.793 1.792a1 1 0 0 1-1.32.083l-.094-.083a1 1 0 0 1 0-1.414l2.5-2.5a1 1 0 0 1 1.414 0Z"></path></symbol><symbol id="icon-eds-i-file-text-medium" viewbox="0 0 24 24"><path d="M14.5 1a1 1 0 0 1 .707.293l5.5 5.5A1 1 0 0 1 21 7.5v12.962A2.542 2.542 0 0 1 18.455 23H5.545A2.542 2.542 0 0 1 3 20.462V3.538A2.542 2.542 0 0 1 5.545 1H14.5Zm-.415 2h-8.54A.542.542 0 0 0 5 3.538v16.924c0 .296.243.538.545.538h12.91a.542.542 0 0 0 .545-.538V7.915L14.085 3ZM16 15a1 1 0 0 1 0 2H8a1 1 0 0 1 0-2h8Zm0-4a1 1 0 0 1 0 2H8a1 1 0 0 1 0-2h8Zm-5-4a1 1 0 0 1 0 2H8a1 1 0 1 1 0-2h3Z"></path></symbol><symbol id="icon-eds-i-file-upload-medium" viewbox="0 0 24 24"><path d="M14.5 1a1 1 0 0 1 .707.293l5.5 5.5A1 1 0 0 1 21 7.5v12.962A2.542 2.542 0 0 1 18.455 23H5.545A2.542 2.542 0 0 1 3 20.462V3.538A2.542 2.542 0 0 1 5.545 1H14.5Zm-.415 2h-8.54A.542.542 0 0 0 5 3.538v16.924c0 .296.243.538.545.538h12.91a.542.542 0 0 0 .545-.538V7.915L14.085 3Zm-2.233 4.011.058-.007L12 7l.075.003.126.017.111.03.111.044.098.052.104.074.082.073 4 4a1 1 0 0 1 0 1.414l-.094.083a1 1 0 0 1-1.32-.083L13 10.415V17a1 1 0 0 1-2 0v-6.585l-2.293 2.292a1 1 0 0 1-1.32.083l-.094-.083a1 1 0 0 1 0-1.414l4-4 .112-.097.11-.071.114-.054.105-.035.118-.025Z"></path></symbol><symbol id="icon-eds-i-filter-medium" viewbox="0 0 24 24"><path d="M21 2a1 1 0 0 1 .82 1.573L15 13.314V18a1 1 0 0 1-.31.724l-.09.076-4 3A1 1 0 0 1 9 21v-7.684L2.18 3.573a1 1 0 0 1 .707-1.567L3 2h18Zm-1.921 2H4.92l5.9 8.427a1 1 0 0 1 .172.45L11 13v6l2-1.5V13a1 1 0 0 1 .117-.469l.064-.104L19.079 4Z"></path></symbol><symbol id="icon-eds-i-funding-medium" viewbox="0 0 24 24"><path clip-rule="evenodd" d="M23 8A7 7 0 1 0 9 8a7 7 0 0 0 14 0ZM9.006 12.225A4.07 4.07 0 0 0 6.12 11.02H2a.979.979 0 1 0 0 1.958h4.12c.558 0 1.094.222 1.489.617l2.207 2.288c.27.27.27.687.012.944a.656.656 0 0 1-.928 0L7.744 15.67a.98.98 0 0 0-1.386 1.384l1.157 1.158c.535.536 1.244.791 1.946.765l.041.002h6.922c.874 0 1.597.748 1.597 1.688 0 .203-.146.354-.309.354H7.755c-.487 0-.96-.178-1.339-.504L2.64 17.259a.979.979 0 0 0-1.28 1.482L5.137 22c.733.631 1.66.979 2.618.979h9.957c1.26 0 2.267-1.043 2.267-2.312 0-2.006-1.584-3.646-3.555-3.646h-4.529a2.617 2.617 0 0 0-.681-2.509l-2.208-2.287ZM16 3a5 5 0 1 0 0 10 5 5 0 0 0 0-10Zm.979 3.5a.979.979 0 1 0-1.958 0v3a.979.979 0 1 0 1.958 0v-3Z" fill-rule="evenodd"></path></symbol><symbol id="icon-eds-i-hashtag-medium" viewbox="0 0 24 24"><path d="M12 1c6.075 0 11 4.925 11 11s-4.925 11-11 11S1 18.075 1 12 5.925 1 12 1Zm0 2a9 9 0 1 0 0 18 9 9 0 0 0 0-18ZM9.52 18.189a1 1 0 1 1-1.964-.378l.437-2.274H6a1 1 0 1 1 0-2h2.378l.592-3.076H6a1 1 0 0 1 0-2h3.354l.51-2.65a1 1 0 1 1 1.964.378l-.437 2.272h3.04l.51-2.65a1 1 0 1 1 1.964.378l-.438 2.272H18a1 1 0 0 1 0 2h-1.917l-.592 3.076H18a1 1 0 0 1 0 2h-2.893l-.51 2.652a1 1 0 1 1-1.964-.378l.437-2.274h-3.04l-.51 2.652Zm.895-4.652h3.04l.591-3.076h-3.04l-.591 3.076Z"></path></symbol><symbol id="icon-eds-i-home-medium" viewbox="0 0 24 24"><path d="M5 22a1 1 0 0 1-1-1v-8.586l-1.293 1.293a1 1 0 0 1-1.32.083l-.094-.083a1 1 0 0 1 0-1.414l10-10a1 1 0 0 1 1.414 0l10 10a1 1 0 0 1-1.414 1.414L20 12.415V21a1 1 0 0 1-1 1H5Zm7-17.585-6 5.999V20h5v-4a1 1 0 0 1 2 0v4h5v-9.585l-6-6Z"></path></symbol><symbol id="icon-eds-i-image-medium" viewbox="0 0 24 24"><path d="M19.615 2A2.385 2.385 0 0 1 22 4.385v15.23A2.385 2.385 0 0 1 19.615 22H4.385A2.385 2.385 0 0 1 2 19.615V4.385A2.385 2.385 0 0 1 4.385 2h15.23Zm0 2H4.385A.385.385 0 0 0 4 4.385v15.23c0 .213.172.385.385.385h1.244l10.228-8.76a1 1 0 0 1 1.254-.037L20 13.392V4.385A.385.385 0 0 0 19.615 4Zm-3.07 9.283L8.703 20h10.912a.385.385 0 0 0 .385-.385v-3.713l-3.455-2.619ZM9.5 6a3.5 3.5 0 1 1 0 7 3.5 3.5 0 0 1 0-7Zm0 2a1.5 1.5 0 1 0 0 3 1.5 1.5 0 0 0 0-3Z"></path></symbol><symbol id="icon-eds-i-impact-factor-medium" viewbox="0 0 24 24"><path d="M16.49 2.672c.74.694.986 1.765.632 2.712l-.04.1-1.549 3.54h1.477a2.496 2.496 0 0 1 2.485 2.34l.005.163c0 .618-.23 1.21-.642 1.675l-7.147 7.961a2.48 2.48 0 0 1-3.554.165 2.512 2.512 0 0 1-.633-2.712l.042-.103L9.108 15H7.46c-1.393 0-2.379-1.11-2.455-2.369L5 12.473c0-.593.142-1.145.628-1.692l7.307-7.944a2.48 2.48 0 0 1 3.555-.165ZM14.43 4.164l-7.33 7.97c-.083.093-.101.214-.101.34 0 .277.19.526.46.526h4.163l.097-.009c.015 0 .03.003.046.009.181.078.264.32.186.5l-2.554 5.817a.512.512 0 0 0 .127.552.48.48 0 0 0 .69-.033l7.155-7.97a.513.513 0 0 0 .13-.34.497.497 0 0 0-.49-.502h-3.988a.355.355 0 0 1-.328-.497l2.555-5.844a.512.512 0 0 0-.127-.552.48.48 0 0 0-.69.033Z"></path></symbol><symbol id="icon-eds-i-info-circle-medium" viewbox="0 0 24 24"><path d="M12 1c6.075 0 11 4.925 11 11s-4.925 11-11 11S1 18.075 1 12 5.925 1 12 1Zm0 2a9 9 0 1 0 0 18 9 9 0 0 0 0-18Zm0 7a1 1 0 0 1 1 1v5h1.5a1 1 0 0 1 0 2h-5a1 1 0 0 1 0-2H11v-4h-.5a1 1 0 0 1-.993-.883L9.5 11a1 1 0 0 1 1-1H12Zm0-4.5a1.5 1.5 0 0 1 .144 2.993L12 8.5a1.5 1.5 0 0 1 0-3Z"></path></symbol><symbol id="icon-eds-i-info-filled-medium" viewbox="0 0 24 24"><path d="M12 1c6.075 0 11 4.925 11 11s-4.925 11-11 11S1 18.075 1 12 5.925 1 12 1Zm0 9h-1.5a1 1 0 0 0-1 1l.007.117A1 1 0 0 0 10.5 12h.5v4H9.5a1 1 0 0 0 0 2h5a1 1 0 0 0 0-2H13v-5a1 1 0 0 0-1-1Zm0-4.5a1.5 1.5 0 0 0 0 3l.144-.007A1.5 1.5 0 0 0 12 5.5Z"></path></symbol><symbol id="icon-eds-i-journal-medium" viewbox="0 0 24 24"><path d="M18.5 1A2.5 2.5 0 0 1 21 3.5v14a2.5 2.5 0 0 1-2.5 2.5h-13a.5.5 0 1 0 0 1H20a1 1 0 0 1 0 2H5.5A2.5 2.5 0 0 1 3 20.5v-17A2.5 2.5 0 0 1 5.5 1h13ZM7 3H5.5a.5.5 0 0 0-.5.5v14.549l.016-.002c.104-.02.211-.035.32-.042L5.5 18H7V3Zm11.5 0H9v15h9.5a.5.5 0 0 0 .5-.5v-14a.5.5 0 0 0-.5-.5ZM16 5a1 1 0 0 1 1 1v4a1 1 0 0 1-1 1h-5a1 1 0 0 1-1-1V6a1 1 0 0 1 1-1h5Zm-1 2h-3v2h3V7Z"></path></symbol><symbol id="icon-eds-i-mail-medium" viewbox="0 0 24 24"><path d="M20.462 3C21.875 3 23 4.184 23 5.619v12.762C23 19.816 21.875 21 20.462 21H3.538C2.125 21 1 19.816 1 18.381V5.619C1 4.184 2.125 3 3.538 3h16.924ZM21 8.158l-7.378 6.258a2.549 2.549 0 0 1-3.253-.008L3 8.16v10.222c0 .353.253.619.538.619h16.924c.285 0 .538-.266.538-.619V8.158ZM20.462 5H3.538c-.264 0-.5.228-.534.542l8.65 7.334c.2.165.492.165.684.007l8.656-7.342-.001-.025c-.044-.3-.274-.516-.531-.516Z"></path></symbol><symbol id="icon-eds-i-mail-send-medium" viewbox="0 0 24 24"><path d="M20.444 5a2.562 2.562 0 0 1 2.548 2.37l.007.078.001.123v7.858A2.564 2.564 0 0 1 20.444 18H9.556A2.564 2.564 0 0 1 7 15.429l.001-7.977.007-.082A2.561 2.561 0 0 1 9.556 5h10.888ZM21 9.331l-5.46 3.51a1 1 0 0 1-1.08 0L9 9.332v6.097c0 .317.251.571.556.571h10.888a.564.564 0 0 0 .556-.571V9.33ZM20.444 7H9.556a.543.543 0 0 0-.32.105l5.763 3.706 5.766-3.706a.543.543 0 0 0-.32-.105ZM4.308 5a1 1 0 1 1 0 2H2a1 1 0 1 1 0-2h2.308Zm0 5.5a1 1 0 0 1 0 2H2a1 1 0 0 1 0-2h2.308Zm0 5.5a1 1 0 0 1 0 2H2a1 1 0 0 1 0-2h2.308Z"></path></symbol><symbol id="icon-eds-i-mentions-medium" viewbox="0 0 24 24"><path d="m9.452 1.293 5.92 5.92 2.92-2.92a1 1 0 0 1 1.415 1.414l-2.92 2.92 5.92 5.92a1 1 0 0 1 0 1.415 10.371 10.371 0 0 1-10.378 2.584l.652 3.258A1 1 0 0 1 12 23H2a1 1 0 0 1-.874-1.486l4.789-8.62C4.194 9.074 4.9 4.43 8.038 1.292a1 1 0 0 1 1.414 0Zm-2.355 13.59L3.699 21h7.081l-.689-3.442a10.392 10.392 0 0 1-2.775-2.396l-.22-.28Zm1.69-11.427-.07.09a8.374 8.374 0 0 0 11.737 11.737l.089-.071L8.787 3.456Z"></path></symbol><symbol id="icon-eds-i-menu-medium" viewbox="0 0 24 24"><path d="M21 4a1 1 0 0 1 0 2H3a1 1 0 1 1 0-2h18Zm-4 7a1 1 0 0 1 0 2H3a1 1 0 0 1 0-2h14Zm4 7a1 1 0 0 1 0 2H3a1 1 0 0 1 0-2h18Z"></path></symbol><symbol id="icon-eds-i-metrics-medium" viewbox="0 0 24 24"><path d="M3 22a1 1 0 0 1-1-1V3a1 1 0 0 1 1-1h6a1 1 0 0 1 1 1v7h4V8a1 1 0 0 1 1-1h6a1 1 0 0 1 1 1v13a1 1 0 0 1-.883.993L21 22H3Zm17-2V9h-4v11h4Zm-6-8h-4v8h4v-8ZM8 4H4v16h4V4Z"></path></symbol><symbol id="icon-eds-i-news-medium" viewbox="0 0 24 24"><path d="M17.384 3c.975 0 1.77.787 1.77 1.762v13.333c0 .462.354.846.815.899l.107.006.109-.006a.915.915 0 0 0 .809-.794l.006-.105V8.19a1 1 0 0 1 2 0v9.905A2.914 2.914 0 0 1 20.077 21H3.538a2.547 2.547 0 0 1-1.644-.601l-.147-.135A2.516 2.516 0 0 1 1 18.476V4.762C1 3.787 1.794 3 2.77 3h14.614Zm-.231 2H3v13.476c0 .11.035.216.1.304l.054.063c.101.1.24.157.384.157l13.761-.001-.026-.078a2.88 2.88 0 0 1-.115-.655l-.004-.17L17.153 5ZM14 15.021a.979.979 0 1 1 0 1.958H6a.979.979 0 1 1 0-1.958h8Zm0-8c.54 0 .979.438.979.979v4c0 .54-.438.979-.979.979H6A.979.979 0 0 1 5.021 12V8c0-.54.438-.979.979-.979h8Zm-.98 1.958H6.979v2.041h6.041V8.979Z"></path></symbol><symbol id="icon-eds-i-newsletter-medium" viewbox="0 0 24 24"><path d="M21 10a1 1 0 0 1 1 1v9.5a2.5 2.5 0 0 1-2.5 2.5h-15A2.5 2.5 0 0 1 2 20.5V11a1 1 0 0 1 2 0v.439l8 4.888 8-4.889V11a1 1 0 0 1 1-1Zm-1 3.783-7.479 4.57a1 1 0 0 1-1.042 0l-7.48-4.57V20.5a.5.5 0 0 0 .501.5h15a.5.5 0 0 0 .5-.5v-6.717ZM15 9a1 1 0 0 1 0 2H9a1 1 0 0 1 0-2h6Zm2.5-8A2.5 2.5 0 0 1 20 3.5V9a1 1 0 0 1-2 0V3.5a.5.5 0 0 0-.5-.5h-11a.5.5 0 0 0-.5.5V9a1 1 0 1 1-2 0V3.5A2.5 2.5 0 0 1 6.5 1h11ZM15 5a1 1 0 0 1 0 2H9a1 1 0 1 1 0-2h6Z"></path></symbol><symbol id="icon-eds-i-notifcation-medium" viewbox="0 0 24 24"><path d="M14 20a1 1 0 0 1 0 2h-4a1 1 0 0 1 0-2h4ZM3 18l-.133-.007c-1.156-.124-1.156-1.862 0-1.986l.3-.012C4.32 15.923 5 15.107 5 14V9.5C5 5.368 8.014 2 12 2s7 3.368 7 7.5V14c0 1.107.68 1.923 1.832 1.995l.301.012c1.156.124 1.156 1.862 0 1.986L21 18H3Zm9-14C9.17 4 7 6.426 7 9.5V14c0 .671-.146 1.303-.416 1.858L6.51 16h10.979l-.073-.142a4.192 4.192 0 0 1-.412-1.658L17 14V9.5C17 6.426 14.83 4 12 4Z"></path></symbol><symbol id="icon-eds-i-publish-medium" viewbox="0 0 24 24"><g><path d="M16.296 1.291A1 1 0 0 0 15.591 1H5.545A2.542 2.542 0 0 0 3 3.538V13a1 1 0 1 0 2 0V3.538l.007-.087A.543.543 0 0 1 5.545 3h9.633L20 7.8v12.662a.534.534 0 0 1-.158.379.548.548 0 0 1-.387.159H11a1 1 0 1 0 0 2h8.455c.674 0 1.32-.267 1.798-.742A2.534 2.534 0 0 0 22 20.462V7.385a1 1 0 0 0-.294-.709l-5.41-5.385Z"></path><path d="M10.762 16.647a1 1 0 0 0-1.525-1.294l-4.472 5.271-2.153-1.665a1 1 0 1 0-1.224 1.582l2.91 2.25a1 1 0 0 0 1.374-.144l5.09-6ZM16 10a1 1 0 1 1 0 2H8a1 1 0 1 1 0-2h8ZM12 7a1 1 0 0 0-1-1H8a1 1 0 1 0 0 2h3a1 1 0 0 0 1-1Z"></path></g></symbol><symbol id="icon-eds-i-refresh-medium" viewbox="0 0 24 24"><g><path d="M7.831 5.636H6.032A8.76 8.76 0 0 1 9 3.631 8.549 8.549 0 0 1 12.232 3c.603 0 1.192.063 1.76.182C17.979 4.017 21 7.632 21 12a1 1 0 1 0 2 0c0-5.296-3.674-9.746-8.591-10.776A10.61 10.61 0 0 0 5 3.851V2.805a1 1 0 0 0-.987-1H4a1 1 0 0 0-1 1v3.831a1 1 0 0 0 1 1h3.831a1 1 0 0 0 .013-2h-.013ZM17.968 18.364c-1.59 1.632-3.784 2.636-6.2 2.636C6.948 21 3 16.993 3 12a1 1 0 1 0-2 0c0 6.053 4.799 11 10.768 11 2.788 0 5.324-1.082 7.232-2.85v1.045a1 1 0 1 0 2 0v-3.831a1 1 0 0 0-1-1h-3.831a1 1 0 0 0 0 2h1.799Z"></path></g></symbol><symbol id="icon-eds-i-search-medium" viewbox="0 0 24 24"><path d="M11 1c5.523 0 10 4.477 10 10 0 2.4-.846 4.604-2.256 6.328l3.963 3.965a1 1 0 0 1-1.414 1.414l-3.965-3.963A9.959 9.959 0 0 1 11 21C5.477 21 1 16.523 1 11S5.477 1 11 1Zm0 2a8 8 0 1 0 0 16 8 8 0 0 0 0-16Z"></path></symbol><symbol id="icon-eds-i-settings-medium" viewbox="0 0 24 24"><path d="M11.382 1h1.24a2.508 2.508 0 0 1 2.334 1.63l.523 1.378 1.59.933 1.444-.224c.954-.132 1.89.3 2.422 1.101l.095.155.598 1.066a2.56 2.56 0 0 1-.195 2.848l-.894 1.161v1.896l.92 1.163c.6.768.707 1.812.295 2.674l-.09.17-.606 1.08a2.504 2.504 0 0 1-2.531 1.25l-1.428-.223-1.589.932-.523 1.378a2.512 2.512 0 0 1-2.155 1.625L12.65 23h-1.27a2.508 2.508 0 0 1-2.334-1.63l-.524-1.379-1.59-.933-1.443.225c-.954.132-1.89-.3-2.422-1.101l-.095-.155-.598-1.066a2.56 2.56 0 0 1 .195-2.847l.891-1.161v-1.898l-.919-1.162a2.562 2.562 0 0 1-.295-2.674l.09-.17.606-1.08a2.504 2.504 0 0 1 2.531-1.25l1.43.223 1.618-.938.524-1.375.07-.167A2.507 2.507 0 0 1 11.382 1Zm.003 2a.509.509 0 0 0-.47.338l-.65 1.71a1 1 0 0 1-.434.51L7.6 6.85a1 1 0 0 1-.655.123l-1.762-.275a.497.497 0 0 0-.498.252l-.61 1.088a.562.562 0 0 0 .04.619l1.13 1.43a1 1 0 0 1 .216.62v2.585a1 1 0 0 1-.207.61L4.15 15.339a.568.568 0 0 0-.036.634l.601 1.072a.494.494 0 0 0 .484.26l1.78-.278a1 1 0 0 1 .66.126l2.2 1.292a1 1 0 0 1 .43.507l.648 1.71a.508.508 0 0 0 .467.338h1.263a.51.51 0 0 0 .47-.34l.65-1.708a1 1 0 0 1 .428-.507l2.201-1.292a1 1 0 0 1 .66-.126l1.763.275a.497.497 0 0 0 .498-.252l.61-1.088a.562.562 0 0 0-.04-.619l-1.13-1.43a1 1 0 0 1-.216-.62v-2.585a1 1 0 0 1 .207-.61l1.105-1.437a.568.568 0 0 0 .037-.634l-.601-1.072a.494.494 0 0 0-.484-.26l-1.78.278a1 1 0 0 1-.66-.126l-2.2-1.292a1 1 0 0 1-.43-.507l-.649-1.71A.508.508 0 0 0 12.62 3h-1.234ZM12 8a4 4 0 1 1 0 8 4 4 0 0 1 0-8Zm0 2a2 2 0 1 0 0 4 2 2 0 0 0 0-4Z"></path></symbol><symbol id="icon-eds-i-shipping-medium" viewbox="0 0 24 24"><path d="M16.515 2c1.406 0 2.706.728 3.352 1.902l2.02 3.635.02.042.036.089.031.105.012.058.01.073.004.075v11.577c0 .64-.244 1.255-.683 1.713a2.356 2.356 0 0 1-1.701.731H4.386a2.356 2.356 0 0 1-1.702-.731 2.476 2.476 0 0 1-.683-1.713V7.948c.01-.217.083-.43.22-.6L4.2 3.905C4.833 2.755 6.089 2.032 7.486 2h9.029ZM20 9H4v10.556a.49.49 0 0 0 .075.26l.053.07a.356.356 0 0 0 .257.114h15.23c.094 0 .186-.04.258-.115a.477.477 0 0 0 .127-.33V9Zm-2 7.5a1 1 0 0 1 0 2h-4a1 1 0 0 1 0-2h4ZM16.514 4H13v3h6.3l-1.183-2.13c-.288-.522-.908-.87-1.603-.87ZM11 3.999H7.51c-.679.017-1.277.36-1.566.887L4.728 7H11V3.999Z"></path></symbol><symbol id="icon-eds-i-step-guide-medium" viewbox="0 0 24 24"><path d="M11.394 9.447a1 1 0 1 0-1.788-.894l-.88 1.759-.019-.02a1 1 0 1 0-1.414 1.415l1 1a1 1 0 0 0 1.601-.26l1.5-3ZM12 11a1 1 0 0 1 1-1h3a1 1 0 1 1 0 2h-3a1 1 0 0 1-1-1ZM12 17a1 1 0 0 1 1-1h3a1 1 0 1 1 0 2h-3a1 1 0 0 1-1-1ZM10.947 14.105a1 1 0 0 1 .447 1.342l-1.5 3a1 1 0 0 1-1.601.26l-1-1a1 1 0 1 1 1.414-1.414l.02.019.879-1.76a1 1 0 0 1 1.341-.447Z"></path><path clip-rule="evenodd" d="M5.545 1A2.542 2.542 0 0 0 3 3.538v16.924A2.542 2.542 0 0 0 5.545 23h12.91A2.542 2.542 0 0 0 21 20.462V7.5a1 1 0 0 0-.293-.707l-5.5-5.5A1 1 0 0 0 14.5 1H5.545ZM5 3.538C5 3.245 5.24 3 5.545 3h8.54L19 7.914v12.547c0 .294-.24.539-.546.539H5.545A.542.542 0 0 1 5 20.462V3.538Z"></path></symbol><symbol id="icon-eds-i-submission-medium" viewbox="0 0 24 24"><g><path d="M5 3.538C5 3.245 5.24 3 5.545 3h9.633L20 7.8v12.662a.535.535 0 0 1-.158.379.549.549 0 0 1-.387.159H6a1 1 0 0 1-1-1v-2.5a1 1 0 1 0-2 0V20a3 3 0 0 0 3 3h13.455c.673 0 1.32-.266 1.798-.742A2.535 2.535 0 0 0 22 20.462V7.385a1 1 0 0 0-.294-.709l-5.41-5.385A1 1 0 0 0 15.591 1H5.545A2.542 2.542 0 0 0 3 3.538V7a1 1 0 0 0 2 0V3.538Z"></path><path d="m13.707 13.707-4 4a1 1 0 0 1-1.414 0l-.083-.094a1 1 0 0 1 .083-1.32L10.585 14 2 14a1 1 0 1 1 0-2l8.583.001-2.29-2.294a1 1 0 0 1 1.414-1.414l4.037 4.04.043.05.043.06.059.098.03.063.031.085.03.113.017.122L14 13l-.004.087-.017.118-.013.056-.034.104-.049.105-.048.081-.07.093-.058.063Z"></path></g></symbol><symbol id="icon-eds-i-table-1-medium" viewbox="0 0 24 24"><path d="M4.385 22a2.56 2.56 0 0 1-1.14-.279C2.485 21.341 2 20.614 2 19.615V4.385c0-.315.067-.716.279-1.14C2.659 2.485 3.386 2 4.385 2h15.23c.315 0 .716.067 1.14.279.76.38 1.245 1.107 1.245 2.106v15.23c0 .315-.067.716-.279 1.14-.38.76-1.107 1.245-2.106 1.245H4.385ZM4 19.615c0 .213.034.265.14.317a.71.71 0 0 0 .245.068H8v-4H4v3.615ZM20 16H10v4h9.615c.213 0 .265-.034.317-.14a.71.71 0 0 0 .068-.245V16Zm0-2v-4H10v4h10ZM4 14h4v-4H4v4ZM19.615 4H10v4h10V4.385c0-.213-.034-.265-.14-.317A.71.71 0 0 0 19.615 4ZM8 4H4.385l-.082.002c-.146.01-.19.047-.235.138A.71.71 0 0 0 4 4.385V8h4V4Z"></path></symbol><symbol id="icon-eds-i-table-2-medium" viewbox="0 0 24 24"><path d="M4.384 22A2.384 2.384 0 0 1 2 19.616V4.384A2.384 2.384 0 0 1 4.384 2h15.232A2.384 2.384 0 0 1 22 4.384v15.232A2.384 2.384 0 0 1 19.616 22H4.384ZM10 15H4v4.616c0 .212.172.384.384.384H10v-5Zm5 0h-3v5h3v-5Zm5 0h-3v5h2.616a.384.384 0 0 0 .384-.384V15ZM10 9H4v4h6V9Zm5 0h-3v4h3V9Zm5 0h-3v4h3V9Zm-.384-5H4.384A.384.384 0 0 0 4 4.384V7h16V4.384A.384.384 0 0 0 19.616 4Z"></path></symbol><symbol id="icon-eds-i-tag-medium" viewbox="0 0 24 24"><path d="m12.621 1.998.127.004L20.496 2a1.5 1.5 0 0 1 1.497 1.355L22 3.5l-.005 7.669c.038.456-.133.905-.447 1.206l-9.02 9.018a2.075 2.075 0 0 1-2.932 0l-6.99-6.99a2.075 2.075 0 0 1 .001-2.933L11.61 2.47c.246-.258.573-.418.881-.46l.131-.011Zm.286 2-8.885 8.886a.075.075 0 0 0 0 .106l6.987 6.988c.03.03.077.03.106 0l8.883-8.883L19.999 4l-7.092-.002ZM16 6.5a1.5 1.5 0 0 1 .144 2.993L16 9.5a1.5 1.5 0 0 1 0-3Z"></path></symbol><symbol id="icon-eds-i-trash-medium" viewbox="0 0 24 24"><path d="M12 1c2.717 0 4.913 2.232 4.997 5H21a1 1 0 0 1 0 2h-1v12.5c0 1.389-1.152 2.5-2.556 2.5H6.556C5.152 23 4 21.889 4 20.5V8H3a1 1 0 1 1 0-2h4.003l.001-.051C7.114 3.205 9.3 1 12 1Zm6 7H6v12.5c0 .238.19.448.454.492l.102.008h10.888c.315 0 .556-.232.556-.5V8Zm-4 3a1 1 0 0 1 1 1v6.005a1 1 0 0 1-2 0V12a1 1 0 0 1 1-1Zm-4 0a1 1 0 0 1 1 1v6a1 1 0 0 1-2 0v-6a1 1 0 0 1 1-1Zm2-8c-1.595 0-2.914 1.32-2.996 3h5.991v-.02C14.903 4.31 13.589 3 12 3Z"></path></symbol><symbol id="icon-eds-i-user-account-medium" viewbox="0 0 24 24"><path d="M12 1c6.075 0 11 4.925 11 11s-4.925 11-11 11S1 18.075 1 12 5.925 1 12 1Zm0 16c-1.806 0-3.52.994-4.664 2.698A8.947 8.947 0 0 0 12 21a8.958 8.958 0 0 0 4.664-1.301C15.52 17.994 13.806 17 12 17Zm0-14a9 9 0 0 0-6.25 15.476C7.253 16.304 9.54 15 12 15s4.747 1.304 6.25 3.475A9 9 0 0 0 12 3Zm0 3a4 4 0 1 1 0 8 4 4 0 0 1 0-8Zm0 2a2 2 0 1 0 0 4 2 2 0 0 0 0-4Z"></path></symbol><symbol id="icon-eds-i-user-add-medium" viewbox="0 0 24 24"><path d="M9 1a5 5 0 1 1 0 10A5 5 0 0 1 9 1Zm0 2a3 3 0 1 0 0 6 3 3 0 0 0 0-6Zm9 10a1 1 0 0 1 1 1v3h3a1 1 0 0 1 0 2h-3v3a1 1 0 0 1-2 0v-3h-3a1 1 0 0 1 0-2h3v-3a1 1 0 0 1 1-1Zm-5.545-.15a1 1 0 1 1-.91 1.78 5.713 5.713 0 0 0-5.705.282c-1.67 1.068-2.728 2.927-2.832 4.956L3.004 20 11.5 20a1 1 0 0 1 .993.883L12.5 21a1 1 0 0 1-1 1H2a1 1 0 0 1-1-1v-.876c.028-2.812 1.446-5.416 3.763-6.897a7.713 7.713 0 0 1 7.692-.378Z"></path></symbol><symbol id="icon-eds-i-user-assign-medium" viewbox="0 0 24 24"><path d="M16.226 13.298a1 1 0 0 1 1.414-.01l.084.093a1 1 0 0 1-.073 1.32L15.39 17H22a1 1 0 0 1 0 2h-6.611l2.262 2.298a1 1 0 0 1-1.425 1.404l-3.939-4a1 1 0 0 1 0-1.404l3.94-4Zm-3.771-.449a1 1 0 1 1-.91 1.781 5.713 5.713 0 0 0-5.705.282c-1.67 1.068-2.728 2.927-2.832 4.956L3.004 20 10.5 20a1 1 0 0 1 .993.883L11.5 21a1 1 0 0 1-1 1H2a1 1 0 0 1-1-1v-.876c.028-2.812 1.446-5.416 3.763-6.897a7.713 7.713 0 0 1 7.692-.378ZM9 1a5 5 0 1 1 0 10A5 5 0 0 1 9 1Zm0 2a3 3 0 1 0 0 6 3 3 0 0 0 0-6Z"></path></symbol><symbol id="icon-eds-i-user-block-medium" viewbox="0 0 24 24"><path d="M9 1a5 5 0 1 1 0 10A5 5 0 0 1 9 1Zm0 2a3 3 0 1 0 0 6 3 3 0 0 0 0-6Zm9 10a5 5 0 1 1 0 10 5 5 0 0 1 0-10Zm-5.545-.15a1 1 0 1 1-.91 1.78 5.713 5.713 0 0 0-5.705.282c-1.67 1.068-2.728 2.927-2.832 4.956L3.004 20 11.5 20a1 1 0 0 1 .993.883L12.5 21a1 1 0 0 1-1 1H2a1 1 0 0 1-1-1v-.876c.028-2.812 1.446-5.416 3.763-6.897a7.713 7.713 0 0 1 7.692-.378ZM15 18a3 3 0 0 0 4.294 2.707l-4.001-4c-.188.391-.293.83-.293 1.293Zm3-3c-.463 0-.902.105-1.294.293l4.001 4A3 3 0 0 0 18 15Z"></path></symbol><symbol id="icon-eds-i-user-check-medium" viewbox="0 0 24 24"><path d="M9 1a5 5 0 1 1 0 10A5 5 0 0 1 9 1Zm0 2a3 3 0 1 0 0 6 3 3 0 0 0 0-6Zm13.647 12.237a1 1 0 0 1 .116 1.41l-5.091 6a1 1 0 0 1-1.375.144l-2.909-2.25a1 1 0 1 1 1.224-1.582l2.153 1.665 4.472-5.271a1 1 0 0 1 1.41-.116Zm-8.139-.977c.22.214.428.44.622.678a1 1 0 1 1-1.548 1.266 6.025 6.025 0 0 0-1.795-1.49.86.86 0 0 1-.163-.048l-.079-.036a5.721 5.721 0 0 0-2.62-.63l-.194.006c-2.76.134-5.022 2.177-5.592 4.864l-.035.175-.035.213c-.03.201-.05.405-.06.61L3.003 20 10 20a1 1 0 0 1 .993.883L11 21a1 1 0 0 1-1 1H2a1 1 0 0 1-1-1v-.876l.005-.223.02-.356.02-.222.03-.248.022-.15c.02-.133.044-.265.071-.397.44-2.178 1.725-4.105 3.595-5.301a7.75 7.75 0 0 1 3.755-1.215l.12-.004a7.908 7.908 0 0 1 5.87 2.252Z"></path></symbol><symbol id="icon-eds-i-user-delete-medium" viewbox="0 0 24 24"><path d="M9 1a5 5 0 1 1 0 10A5 5 0 0 1 9 1Zm0 2a3 3 0 1 0 0 6 3 3 0 0 0 0-6ZM4.763 13.227a7.713 7.713 0 0 1 7.692-.378 1 1 0 1 1-.91 1.781 5.713 5.713 0 0 0-5.705.282c-1.67 1.068-2.728 2.927-2.832 4.956L3.004 20H11.5a1 1 0 0 1 .993.883L12.5 21a1 1 0 0 1-1 1H2a1 1 0 0 1-1-1v-.876c.028-2.812 1.446-5.416 3.763-6.897Zm11.421 1.543 2.554 2.553 2.555-2.553a1 1 0 0 1 1.414 1.414l-2.554 2.554 2.554 2.555a1 1 0 0 1-1.414 1.414l-2.555-2.554-2.554 2.554a1 1 0 0 1-1.414-1.414l2.553-2.555-2.553-2.554a1 1 0 0 1 1.414-1.414Z"></path></symbol><symbol id="icon-eds-i-user-edit-medium" viewbox="0 0 24 24"><path d="m19.876 10.77 2.831 2.83a1 1 0 0 1 0 1.415l-7.246 7.246a1 1 0 0 1-.572.284l-3.277.446a1 1 0 0 1-1.125-1.13l.461-3.277a1 1 0 0 1 .283-.567l7.23-7.246a1 1 0 0 1 1.415-.001Zm-7.421 2.08a1 1 0 1 1-.91 1.78 5.713 5.713 0 0 0-5.705.282c-1.67 1.068-2.728 2.927-2.832 4.956L3.004 20 7.5 20a1 1 0 0 1 .993.883L8.5 21a1 1 0 0 1-1 1H2a1 1 0 0 1-1-1v-.876c.028-2.812 1.446-5.416 3.763-6.897a7.713 7.713 0 0 1 7.692-.378Zm6.715.042-6.29 6.3-.23 1.639 1.633-.222 6.302-6.302-1.415-1.415ZM9 1a5 5 0 1 1 0 10A5 5 0 0 1 9 1Zm0 2a3 3 0 1 0 0 6 3 3 0 0 0 0-6Z"></path></symbol><symbol id="icon-eds-i-user-linked-medium" viewbox="0 0 24 24"><path d="M15.65 6c.31 0 .706.066 1.122.274C17.522 6.65 18 7.366 18 8.35v12.3c0 .31-.066.706-.274 1.122-.375.75-1.092 1.228-2.076 1.228H3.35a2.52 2.52 0 0 1-1.122-.274C1.478 22.35 1 21.634 1 20.65V8.35c0-.31.066-.706.274-1.122C1.65 6.478 2.366 6 3.35 6h12.3Zm0 2-12.376.002c-.134.007-.17.04-.21.12A.672.672 0 0 0 3 8.35v12.3c0 .198.028.24.122.287.09.044.2.063.228.063h.887c.788-2.269 2.814-3.5 5.263-3.5 2.45 0 4.475 1.231 5.263 3.5h.887c.198 0 .24-.028.287-.122.044-.09.063-.2.063-.228V8.35c0-.198-.028-.24-.122-.287A.672.672 0 0 0 15.65 8ZM9.5 19.5c-1.36 0-2.447.51-3.06 1.5h6.12c-.613-.99-1.7-1.5-3.06-1.5ZM20.65 1A2.35 2.35 0 0 1 23 3.348V15.65A2.35 2.35 0 0 1 20.65 18H20a1 1 0 0 1 0-2h.65a.35.35 0 0 0 .35-.35V3.348A.35.35 0 0 0 20.65 3H8.35a.35.35 0 0 0-.35.348V4a1 1 0 1 1-2 0v-.652A2.35 2.35 0 0 1 8.35 1h12.3ZM9.5 10a3.5 3.5 0 1 1 0 7 3.5 3.5 0 0 1 0-7Zm0 2a1.5 1.5 0 1 0 0 3 1.5 1.5 0 0 0 0-3Z"></path></symbol><symbol id="icon-eds-i-user-multiple-medium" viewbox="0 0 24 24"><path d="M9 1a5 5 0 1 1 0 10A5 5 0 0 1 9 1Zm6 0a5 5 0 0 1 0 10 1 1 0 0 1-.117-1.993L15 9a3 3 0 0 0 0-6 1 1 0 0 1 0-2ZM9 3a3 3 0 1 0 0 6 3 3 0 0 0 0-6Zm8.857 9.545a7.99 7.99 0 0 1 2.651 1.715A8.31 8.31 0 0 1 23 20.134V21a1 1 0 0 1-1 1h-3a1 1 0 0 1 0-2h1.995l-.005-.153a6.307 6.307 0 0 0-1.673-3.945l-.204-.209a5.99 5.99 0 0 0-1.988-1.287 1 1 0 1 1 .732-1.861Zm-3.349 1.715A8.31 8.31 0 0 1 17 20.134V21a1 1 0 0 1-1 1H2a1 1 0 0 1-1-1v-.877c.044-4.343 3.387-7.908 7.638-8.115a7.908 7.908 0 0 1 5.87 2.252ZM9.016 14l-.285.006c-3.104.15-5.58 2.718-5.725 5.9L3.004 20h11.991l-.005-.153a6.307 6.307 0 0 0-1.673-3.945l-.204-.209A5.924 5.924 0 0 0 9.3 14.008L9.016 14Z"></path></symbol><symbol id="icon-eds-i-user-notify-medium" viewbox="0 0 24 24"><path d="M9 1a5 5 0 1 1 0 10A5 5 0 0 1 9 1Zm0 2a3 3 0 1 0 0 6 3 3 0 0 0 0-6Zm10 18v1a1 1 0 0 1-2 0v-1h-3a1 1 0 0 1 0-2v-2.818C14 13.885 15.777 12 18 12s4 1.885 4 4.182V19a1 1 0 0 1 0 2h-3Zm-6.545-8.15a1 1 0 1 1-.91 1.78 5.713 5.713 0 0 0-5.705.282c-1.67 1.068-2.728 2.927-2.832 4.956L3.004 20 11.5 20a1 1 0 0 1 .993.883L12.5 21a1 1 0 0 1-1 1H2a1 1 0 0 1-1-1v-.876c.028-2.812 1.446-5.416 3.763-6.897a7.713 7.713 0 0 1 7.692-.378ZM18 14c-1.091 0-2 .964-2 2.182V19h4v-2.818c0-1.165-.832-2.098-1.859-2.177L18 14Z"></path></symbol><symbol id="icon-eds-i-user-remove-medium" viewbox="0 0 24 24"><path d="M9 1a5 5 0 1 1 0 10A5 5 0 0 1 9 1Zm0 2a3 3 0 1 0 0 6 3 3 0 0 0 0-6Zm3.455 9.85a1 1 0 1 1-.91 1.78 5.713 5.713 0 0 0-5.705.282c-1.67 1.068-2.728 2.927-2.832 4.956L3.004 20 11.5 20a1 1 0 0 1 .993.883L12.5 21a1 1 0 0 1-1 1H2a1 1 0 0 1-1-1v-.876c.028-2.812 1.446-5.416 3.763-6.897a7.713 7.713 0 0 1 7.692-.378ZM22 17a1 1 0 0 1 0 2h-8a1 1 0 0 1 0-2h8Z"></path></symbol><symbol id="icon-eds-i-user-single-medium" viewbox="0 0 24 24"><path d="M12 1a5 5 0 1 1 0 10 5 5 0 0 1 0-10Zm0 2a3 3 0 1 0 0 6 3 3 0 0 0 0-6Zm-.406 9.008a8.965 8.965 0 0 1 6.596 2.494A9.161 9.161 0 0 1 21 21.025V22a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1v-.985c.05-4.825 3.815-8.777 8.594-9.007Zm.39 1.992-.299.006c-3.63.175-6.518 3.127-6.678 6.775L5 21h13.998l-.009-.268a7.157 7.157 0 0 0-1.97-4.573l-.214-.213A6.967 6.967 0 0 0 11.984 14Z"></path></symbol><symbol id="icon-eds-i-warning-circle-medium" viewbox="0 0 24 24"><path d="M12 1c6.075 0 11 4.925 11 11s-4.925 11-11 11S1 18.075 1 12 5.925 1 12 1Zm0 2a9 9 0 1 0 0 18 9 9 0 0 0 0-18Zm0 11.5a1.5 1.5 0 0 1 .144 2.993L12 17.5a1.5 1.5 0 0 1 0-3ZM12 6a1 1 0 0 1 1 1v5a1 1 0 0 1-2 0V7a1 1 0 0 1 1-1Z"></path></symbol><symbol id="icon-eds-i-warning-filled-medium" viewbox="0 0 24 24"><path d="M12 1c6.075 0 11 4.925 11 11s-4.925 11-11 11S1 18.075 1 12 5.925 1 12 1Zm0 13.5a1.5 1.5 0 0 0 0 3l.144-.007A1.5 1.5 0 0 0 12 14.5ZM12 6a1 1 0 0 0-1 1v5a1 1 0 0 0 2 0V7a1 1 0 0 0-1-1Z"></path></symbol><symbol id="icon-chevron-left-medium" viewbox="0 0 24 24"><path d="M15.7194 3.3054C15.3358 2.90809 14.7027 2.89699 14.3054 3.28061L6.54342 10.7757C6.19804 11.09 6 11.5335 6 12C6 12.4665 6.19804 12.91 6.5218 13.204L14.3054 20.7194C14.7027 21.103 15.3358 21.0919 15.7194 20.6946C16.103 20.2973 16.0919 19.6642 15.6946 19.2806L8.155 12L15.6946 4.71939C16.0614 4.36528 16.099 3.79863 15.8009 3.40105L15.7194 3.3054Z"></path></symbol><symbol id="icon-chevron-right-medium" viewbox="0 0 24 24"><path d="M8.28061 3.3054C8.66423 2.90809 9.29729 2.89699 9.6946 3.28061L17.4566 10.7757C17.802 11.09 18 11.5335 18 12C18 12.4665 17.802 12.91 17.4782 13.204L9.6946 20.7194C9.29729 21.103 8.66423 21.0919 8.28061 20.6946C7.89699 20.2973 7.90809 19.6642 8.3054 19.2806L15.845 12L8.3054 4.71939C7.93865 4.36528 7.90098 3.79863 8.19908 3.40105L8.28061 3.3054Z"></path></symbol><symbol id="icon-eds-alerts" viewbox="0 0 32 32"><path d="M28 12.667c.736 0 1.333.597 1.333 1.333v13.333A3.333 3.333 0 0 1 26 30.667H6a3.333 3.333 0 0 1-3.333-3.334V14a1.333 1.333 0 1 1 2.666 0v1.252L16 21.769l10.667-6.518V14c0-.736.597-1.333 1.333-1.333Zm-1.333 5.71-9.972 6.094c-.427.26-.963.26-1.39 0l-9.972-6.094v8.956c0 .368.299.667.667.667h20a.667.667 0 0 0 .667-.667v-8.956ZM19.333 12a1.333 1.333 0 1 1 0 2.667h-6.666a1.333 1.333 0 1 1 0-2.667h6.666Zm4-10.667a3.333 3.333 0 0 1 3.334 3.334v6.666a1.333 1.333 0 1 1-2.667 0V4.667A.667.667 0 0 0 23.333 4H8.667A.667.667 0 0 0 8 4.667v6.666a1.333 1.333 0 1 1-2.667 0V4.667a3.333 3.333 0 0 1 3.334-3.334h14.666Zm-4 5.334a1.333 1.333 0 0 1 0 2.666h-6.666a1.333 1.333 0 1 1 0-2.666h6.666Z"></path></symbol><symbol id="icon-eds-arrow-up" viewbox="0 0 24 24"><path class="layer" d="m13.002 7.408 4.88 4.88a.99.99 0 0 0 1.32.08l.09-.08c.39-.39.39-1.03 0-1.42l-6.58-6.58a1.01 1.01 0 0 0-1.42 0l-6.58 6.58a1 1 0 0 0-.09 1.32l.08.1a1 1 0 0 0 1.42-.01l4.88-4.87v11.59a.99.99 0 0 0 .88.99l.12.01c.55 0 1-.45 1-1V7.408z" fill-rule="evenodd"></path></symbol><symbol id="icon-eds-checklist" viewbox="0 0 32 32"><path d="M19.2 1.333a3.468 3.468 0 0 1 3.381 2.699L24.667 4C26.515 4 28 5.52 28 7.38v19.906c0 1.86-1.485 3.38-3.333 3.38H7.333c-1.848 0-3.333-1.52-3.333-3.38V7.38C4 5.52 5.485 4 7.333 4h2.093A3.468 3.468 0 0 1 12.8 1.333h6.4ZM9.426 6.667H7.333c-.36 0-.666.312-.666.713v19.906c0 .401.305.714.666.714h17.334c.36 0 .666-.313.666-.714V7.38c0-.4-.305-.713-.646-.714l-2.121.033A3.468 3.468 0 0 1 19.2 9.333h-6.4a3.468 3.468 0 0 1-3.374-2.666Zm12.715 5.606c.586.446.7 1.283.253 1.868l-7.111 9.334a1.333 1.333 0 0 1-1.792.306l-3.556-2.333a1.333 1.333 0 1 1 1.463-2.23l2.517 1.651 6.358-8.344a1.333 1.333 0 0 1 1.868-.252ZM19.2 4h-6.4a.8.8 0 0 0-.8.8v1.067a.8.8 0 0 0 .8.8h6.4a.8.8 0 0 0 .8-.8V4.8a.8.8 0 0 0-.8-.8Z"></path></symbol><symbol id="icon-eds-citation" viewbox="0 0 36 36"><path d="M23.25 1.5a1.5 1.5 0 0 1 1.06.44l8.25 8.25a1.5 1.5 0 0 1 .44 1.06v19.5c0 2.105-1.645 3.75-3.75 3.75H18a1.5 1.5 0 0 1 0-3h11.25c.448 0 .75-.302.75-.75V11.873L22.628 4.5H8.31a.811.811 0 0 0-.8.68l-.011.13V16.5a1.5 1.5 0 0 1-3 0V5.31A3.81 3.81 0 0 1 8.31 1.5h14.94ZM8.223 20.358a.984.984 0 0 1-.192 1.378l-.048.034c-.54.36-.942.676-1.206.951-.59.614-.885 1.395-.885 2.343.115-.028.288-.042.518-.042.662 0 1.26.237 1.791.711.533.474.799 1.074.799 1.799 0 .753-.259 1.352-.777 1.799-.518.446-1.151.669-1.9.669-1.006 0-1.812-.293-2.417-.878C3.302 28.536 3 27.657 3 26.486c0-1.115.165-2.085.496-2.907.331-.823.734-1.513 1.209-2.071.475-.558.971-.997 1.49-1.318a6.01 6.01 0 0 1 .347-.2 1.321 1.321 0 0 1 1.681.368Zm7.5 0a.984.984 0 0 1-.192 1.378l-.048.034c-.54.36-.942.676-1.206.951-.59.614-.885 1.395-.885 2.343.115-.028.288-.042.518-.042.662 0 1.26.237 1.791.711.533.474.799 1.074.799 1.799 0 .753-.259 1.352-.777 1.799-.518.446-1.151.669-1.9.669-1.006 0-1.812-.293-2.417-.878-.604-.586-.906-1.465-.906-2.636 0-1.115.165-2.085.496-2.907.331-.823.734-1.513 1.209-2.071.475-.558.971-.997 1.49-1.318a6.01 6.01 0 0 1 .347-.2 1.321 1.321 0 0 1 1.681.368Z"></path></symbol><symbol id="icon-eds-i-github-medium" viewbox="0 0 24 24"><path d="M 11.964844 0 C 5.347656 0 0 5.269531 0 11.792969 C 0 17.003906 3.425781 21.417969 8.179688 22.976562 C 8.773438 23.09375 8.992188 22.722656 8.992188 22.410156 C 8.992188 22.136719 8.972656 21.203125 8.972656 20.226562 C 5.644531 20.929688 4.953125 18.820312 4.953125 18.820312 C 4.417969 17.453125 3.625 17.101562 3.625 17.101562 C 2.535156 16.378906 3.703125 16.378906 3.703125 16.378906 C 4.914062 16.457031 5.546875 17.589844 5.546875 17.589844 C 6.617188 19.386719 8.339844 18.878906 9.03125 18.566406 C 9.132812 17.804688 9.449219 17.277344 9.785156 16.984375 C 7.132812 16.710938 4.339844 15.695312 4.339844 11.167969 C 4.339844 9.878906 4.8125 8.824219 5.566406 8.003906 C 5.445312 7.710938 5.03125 6.5 5.683594 4.878906 C 5.683594 4.878906 6.695312 4.566406 8.972656 6.089844 C 9.949219 5.832031 10.953125 5.703125 11.964844 5.699219 C 12.972656 5.699219 14.003906 5.835938 14.957031 6.089844 C 17.234375 4.566406 18.242188 4.878906 18.242188 4.878906 C 18.898438 6.5 18.480469 7.710938 18.363281 8.003906 C 19.136719 8.824219 19.589844 9.878906 19.589844 11.167969 C 19.589844 15.695312 16.796875 16.691406 14.125 16.984375 C 14.558594 17.355469 14.933594 18.058594 14.933594 19.171875 C 14.933594 20.753906 14.914062 22.019531 14.914062 22.410156 C 14.914062 22.722656 15.132812 23.09375 15.726562 22.976562 C 20.480469 21.414062 23.910156 17.003906 23.910156 11.792969 C 23.929688 5.269531 18.558594 0 11.964844 0 Z M 11.964844 0 "></path></symbol><symbol id="icon-eds-i-subjects-medium" viewbox="0 0 24 24"><g fill-rule="evenodd" id="icon-subjects-copy" stroke="none" stroke-width="1"><path d="M13.3846154,2 C14.7015971,2 15.7692308,3.06762994 15.7692308,4.38461538 L15.7692308,7.15384615 C15.7692308,8.47082629 14.7015955,9.53846154 13.3846154,9.53846154 L13.1038388,9.53925278 C13.2061091,9.85347965 13.3815528,10.1423885 13.6195822,10.3804178 C13.9722182,10.7330539 14.436524,10.9483278 14.9293854,10.9918129 L15.1153846,11 C16.2068332,11 17.2535347,11.433562 18.0254647,12.2054189 C18.6411944,12.8212361 19.0416785,13.6120766 19.1784166,14.4609738 L19.6153846,14.4615385 C20.932386,14.4615385 22,15.5291672 22,16.8461538 L22,19.6153846 C22,20.9323924 20.9323924,22 19.6153846,22 L16.8461538,22 C15.5291672,22 14.4615385,20.932386 14.4615385,19.6153846 L14.4615385,16.8461538 C14.4615385,15.5291737 15.5291737,14.4615385 16.8461538,14.4615385 L17.126925,14.460779 C17.0246537,14.1465537 16.8492179,13.857633 16.6112344,13.6196157 C16.2144418,13.2228606 15.6764136,13 15.1153846,13 C14.0239122,13 12.9771569,12.5664197 12.2053686,11.7946314 C12.1335167,11.7227795 12.0645962,11.6485444 11.9986839,11.5721119 C11.9354038,11.6485444 11.8664833,11.7227795 11.7946314,11.7946314 C11.0228431,12.5664197 9.97608778,13 8.88461538,13 C8.323576,13 7.78552852,13.2228666 7.38881294,13.6195822 C7.15078359,13.8576115 6.97533988,14.1465203 6.8730696,14.4607472 L7.15384615,14.4615385 C8.47082629,14.4615385 9.53846154,15.5291737 9.53846154,16.8461538 L9.53846154,19.6153846 C9.53846154,20.932386 8.47083276,22 7.15384615,22 L4.38461538,22 C3.06762347,22 2,20.9323876 2,19.6153846 L2,16.8461538 C2,15.5291721 3.06762994,14.4615385 4.38461538,14.4615385 L4.8215823,14.4609378 C4.95831893,13.6120029 5.3588057,12.8211623 5.97459937,12.2053686 C6.69125996,11.488708 7.64500941,11.0636656 8.6514968,11.0066017 L8.88461538,11 C9.44565477,11 9.98370225,10.7771334 10.3804178,10.3804178 C10.6184472,10.1423885 10.7938909,9.85347965 10.8961612,9.53925278 L10.6153846,9.53846154 C9.29840448,9.53846154 8.23076923,8.47082629 8.23076923,7.15384615 L8.23076923,4.38461538 C8.23076923,3.06762994 9.29840286,2 10.6153846,2 L13.3846154,2 Z M7.15384615,16.4615385 L4.38461538,16.4615385 C4.17220099,16.4615385 4,16.63374 4,16.8461538 L4,19.6153846 C4,19.8278134 4.17218833,20 4.38461538,20 L7.15384615,20 C7.36626945,20 7.53846154,19.8278103 7.53846154,19.6153846 L7.53846154,16.8461538 C7.53846154,16.6337432 7.36625679,16.4615385 7.15384615,16.4615385 Z M19.6153846,16.4615385 L16.8461538,16.4615385 C16.6337432,16.4615385 16.4615385,16.6337432 16.4615385,16.8461538 L16.4615385,19.6153846 C16.4615385,19.8278103 16.6337306,20 16.8461538,20 L19.6153846,20 C19.8278229,20 20,19.8278229 20,19.6153846 L20,16.8461538 C20,16.6337306 19.8278103,16.4615385 19.6153846,16.4615385 Z M13.3846154,4 L10.6153846,4 C10.4029708,4 10.2307692,4.17220099 10.2307692,4.38461538 L10.2307692,7.15384615 C10.2307692,7.36625679 10.402974,7.53846154 10.6153846,7.53846154 L13.3846154,7.53846154 C13.597026,7.53846154 13.7692308,7.36625679 13.7692308,7.15384615 L13.7692308,4.38461538 C13.7692308,4.17220099 13.5970292,4 13.3846154,4 Z" fill-rule="nonzero" id="Shape"></path></g></symbol><symbol id="icon-eds-small-arrow-left" viewbox="0 0 16 17"><path d="M14 8.092H2m0 0L8 2M2 8.092l6 6.035" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2"></path></symbol><symbol id="icon-eds-small-arrow-right" viewbox="0 0 16 16"><g fill-rule="evenodd" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2"><path d="M2 8.092h12M8 2l6 6.092M8 14.127l6-6.035"></path></g></symbol></svg>
</div>
<a class="c-skip-link" href="#main">Skip to main content</a>
<header class="eds-c-header" data-eds-c-header="">
<div class="eds-c-header__container" data-eds-c-header-expander-anchor="">
<div class="eds-c-header__brand">
<a data-test="springerlink-logo" data-track="click_imprint_logo" data-track-action="click logo link" data-track-category="unified header" data-track-context="unified header" data-track-label="link" href="https://link.springer.com">
<img alt="SpringerLink" src="/oscar-static/images/darwin/header/img/logo-springerlink-39ee2a28d8.svg"/>
</a>
</div>
<a class="c-header__link eds-c-header__link" href="https://idp.springer.com/auth/personal/springernature?redirect_uri=https://link.springer.com/article/10.1007/s10844-024-00886-5" id="identity-account-widget"><span class="eds-c-header__widget-fragment-title">Log in</span></a>
</div>
<nav aria-label="header navigation" class="eds-c-header__nav">
<div class="eds-c-header__nav-container">
<div class="eds-c-header__item eds-c-header__item--menu">
<a aria-controls="eds-c-header-nav" aria-expanded="false" aria-haspopup="true" class="eds-c-header__link" data-eds-c-header-expander="" href="javascript:;" role="button">
<svg aria-hidden="true" class="eds-c-header__icon" focusable="false" height="24" width="24">
<use xlink:href="#icon-eds-i-menu-medium"></use>
</svg><span>Menu</span>
</a>
</div>
<div class="eds-c-header__item eds-c-header__item--inline-links">
<a class="eds-c-header__link" data-track="nav_find_a_journal" data-track-action="click find a journal" data-track-category="unified header" data-track-context="unified header" data-track-label="link" href="https://link.springer.com/journals/">
                            Find a journal
                        </a>
<a class="eds-c-header__link" data-track="nav_how_to_publish" data-track-action="click publish with us link" data-track-category="unified header" data-track-context="unified header" data-track-label="link" href="https://www.springernature.com/gp/authors">
                            Publish with us
                        </a>
<a class="eds-c-header__link" data-track="nav_track_your_research" data-track-action="click track your research" data-track-category="unified header" data-track-context="unified header" data-track-label="link" href="https://link.springernature.com/home/">
                            Track your research
                        </a>
</div>
<div class="eds-c-header__link-container">
<div class="eds-c-header__item eds-c-header__item--divider">
<a aria-controls="eds-c-header-popup-search" aria-expanded="false" aria-haspopup="true" class="eds-c-header__link" data-eds-c-header-expander="" data-eds-c-header-test-search-btn="" href="javascript:;" role="button">
<svg aria-hidden="true" class="eds-c-header__icon" focusable="false" height="24" width="24">
<use xlink:href="#icon-eds-i-search-medium"></use>
</svg><span>Search</span>
</a>
</div>
<div class="eds-c-header__item ecommerce-cart" id="ecommerce-header-cart-icon-link" style="display:inline-block">
<a class="eds-c-header__link" href="https://order.springer.com/public/cart" style="appearance:none;border:none;background:none;color:inherit;position:relative">
<svg aria-hidden="true" class="eds-c-header__icon" focusable="false" height="24" id="eds-i-cart" viewbox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
<path d="M2 1a1 1 0 0 0 0 2l1.659.001 2.257 12.808a2.599 2.599 0 0 0 2.435 2.185l.167.004 9.976-.001a2.613 2.613 0 0 0 2.61-1.748l.03-.106 1.755-7.82.032-.107a2.546 2.546 0 0 0-.311-1.986l-.108-.157a2.604 2.604 0 0 0-2.197-1.076L6.042 5l-.56-3.17a1 1 0 0 0-.864-.82l-.12-.007L2.001 1ZM20.35 6.996a.63.63 0 0 1 .54.26.55.55 0 0 1 .082.505l-.028.1L19.2 15.63l-.022.05c-.094.177-.282.299-.526.317l-10.145.002a.61.61 0 0 1-.618-.515L6.394 6.999l13.955-.003ZM18 19a2 2 0 1 0 0 4 2 2 0 0 0 0-4ZM8 19a2 2 0 1 0 0 4 2 2 0 0 0 0-4Z" fill="currentColor" fill-rule="nonzero"></path>
</svg><span>Cart</span><span class="cart-info" style="display:none;position:absolute;top:10px;right:45px;background-color:#C65301;color:#fff;width:18px;height:18px;font-size:11px;border-radius:50%;line-height:17.5px;text-align:center"></span></a>
<script>(function () { var exports = {}; if (window.fetch) {
            
            "use strict";
Object.defineProperty(exports, "__esModule", { value: true });
exports.headerWidgetClientInit = void 0;
var headerWidgetClientInit = function (getCartInfo) {
    document.body.addEventListener("updatedCart", function () {
        updateCartIcon();
    }, false);
    return updateCartIcon();
    function updateCartIcon() {
        return getCartInfo()
            .then(function (res) { return res.json(); })
            .then(refreshCartState)
            .catch(function (_) { });
    }
    function refreshCartState(json) {
        var indicator = document.querySelector("#ecommerce-header-cart-icon-link .cart-info");
        /* istanbul ignore else */
        if (indicator && json.itemCount) {
            indicator.style.display = 'block';
            indicator.textContent = json.itemCount > 9 ? '9+' : json.itemCount.toString();
            var moreThanOneItem = json.itemCount > 1;
            indicator.setAttribute('title', "there ".concat(moreThanOneItem ? "are" : "is", " ").concat(json.itemCount, " item").concat(moreThanOneItem ? "s" : "", " in your cart"));
        }
        return json;
    }
};
exports.headerWidgetClientInit = headerWidgetClientInit;

            
            headerWidgetClientInit(
              function () {
                return window.fetch("https://cart.springer.com/cart-info", {
                  credentials: "include",
                  headers: { Accept: "application/json" }
                })
              }
            )
        }})()</script>
</div>
</div>
</div>
</nav>
</header><div class="eds-c-header__expander eds-c-header__expander--search has-tethered u-js-hide" hidden="" id="eds-c-header-popup-search">
<h2 class="eds-c-header__heading">Search</h2>
<div class="u-container">
<search aria-label="Search from the header" class="eds-c-header__search" role="search">
<form action="//link.springer.com/search" data-test="header-search" data-track="search" data-track-action="submit search form" data-track-category="unified header" data-track-context="search from header" data-track-label="form" method="GET">
<label class="eds-c-header__search-label" for="eds-c-header-search">Search by keyword or author</label>
<div class="eds-c-header__search-container">
<input autocomplete="off" class="eds-c-header__search-input" id="eds-c-header-search" name="query" required="" type="search" value=""/>
<button class="eds-c-header__search-button" type="submit">
<svg aria-hidden="true" class="eds-c-header__icon" focusable="false">
<use xlink:href="#icon-eds-i-search-medium"></use>
</svg>
<span class="u-visually-hidden">Search</span>
</button>
</div>
</form>
</search>
</div>
</div><div class="eds-c-header__expander eds-c-header__expander--menu has-tethered u-js-hide" hidden="" id="eds-c-header-nav">
<h2 class="eds-c-header__heading">Navigation</h2>
<ul class="eds-c-header__list">
<li class="eds-c-header__list-item">
<a class="eds-c-header__link" data-track="nav_find_a_journal" data-track-action="click find a journal" data-track-category="unified header" data-track-context="unified header" data-track-label="link" href="https://link.springer.com/journals/">
                        Find a journal
									 </a>
</li>
<li class="eds-c-header__list-item">
<a class="eds-c-header__link" data-track="nav_how_to_publish" data-track-action="click publish with us link" data-track-category="unified header" data-track-context="unified header" data-track-label="link" href="https://www.springernature.com/gp/authors">
                        Publish with us
									 </a>
</li>
<li class="eds-c-header__list-item">
<a class="eds-c-header__link" data-track="nav_track_your_research" data-track-action="click track your research" data-track-category="unified header" data-track-context="unified header" data-track-label="link" href="https://link.springernature.com/home/">
                        Track your research
									 </a>
</li>
</ul>
</div>
<article class="app-masthead__colour-17" id="main" lang="en">
<section aria-label="article masthead" class="app-masthead">
<div class="app-masthead__container">
<div class="app-article-masthead u-sans-serif js-context-bar-sticky-point-masthead" data-test="masthead-component" data-track-component="article">
<div class="app-article-masthead__info">
<nav aria-label="breadcrumbs" data-test="breadcrumbs">
<ol class="c-breadcrumbs c-breadcrumbs--contrast" itemscope="" itemtype="https://schema.org/BreadcrumbList">
<li class="c-breadcrumbs__item" id="breadcrumb0" itemprop="itemListElement" itemscope="" itemtype="https://schema.org/ListItem">
<a class="c-breadcrumbs__link" data-track="click_breadcrumb" data-track-action="breadcrumbs" data-track-category="article" data-track-context="article page" data-track-label="breadcrumb1" href="/" itemprop="item"><span itemprop="name">Home</span></a><meta content="1" itemprop="position"/>
<svg aria-hidden="true" class="c-breadcrumbs__chevron" focusable="false" height="10" role="img" viewbox="0 0 10 10" width="10">
<path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)"></path>
</svg>
</li>
<li class="c-breadcrumbs__item" id="breadcrumb1" itemprop="itemListElement" itemscope="" itemtype="https://schema.org/ListItem">
<a class="c-breadcrumbs__link" data-track="click_breadcrumb" data-track-action="breadcrumbs" data-track-category="article" data-track-context="article page" data-track-label="breadcrumb2" href="/journal/10844" itemprop="item"><span itemprop="name">Journal of Intelligent Information Systems</span></a><meta content="2" itemprop="position"/>
<svg aria-hidden="true" class="c-breadcrumbs__chevron" focusable="false" height="10" role="img" viewbox="0 0 10 10" width="10">
<path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)"></path>
</svg>
</li>
<li class="c-breadcrumbs__item" id="breadcrumb2" itemprop="itemListElement" itemscope="" itemtype="https://schema.org/ListItem">
<span itemprop="name">Article</span><meta content="3" itemprop="position"/>
</li>
</ol>
</nav>
<h1 class="c-article-title" data-article-title="" data-test="article-title">Heterogeneous graphormer for extractive multimodal summarization</h1>
<ul class="c-article-identifiers">
<li class="c-article-identifiers__item" data-test="article-category">Research</li>
<li class="c-article-identifiers__item">
                            Published: <time datetime="2024-09-30">30 September 2024</time>
</li>
</ul>
<ul class="c-article-identifiers c-article-identifiers--cite-list">
<li class="c-article-identifiers__item">
                             (<span data-test="article-publication-year">2024</span>)
                        </li>
<li class="c-article-identifiers__item c-article-identifiers__item--cite">
<a data-track="click" data-track-action="cite this article" data-track-category="article body" data-track-label="link" href="#citeas">Cite this article</a>
</li>
</ul>
<div class="app-article-masthead__buttons" data-test="download-article-link-wrapper" data-track-context="masthead">
<div class="c-pdf-container">
<div class="c-pdf-download u-clear-both u-mb-16">
<a class="u-button u-button--full-width u-button--primary u-justify-content-space-between c-pdf-download__link" data-article-pdf="true" data-draft-ignore="true" data-readcube-pdf-url="true" data-test="pdf-link" data-track="content_download" data-track-action="download pdf" data-track-external="" data-track-label="button" data-track-type="article pdf download" download="" href="/content/pdf/10.1007/s10844-024-00886-5.pdf">
<span class="c-pdf-download__text">Download PDF</span>
<svg aria-hidden="true" class="u-icon" focusable="false" height="16" width="16"><use xlink:href="#icon-eds-i-download-medium"></use></svg>
</a>
</div>
</div>
<p class="app-article-masthead__access">
<svg aria-hidden="true" focusable="false" height="16" role="img" width="16"><use xlink:href="#icon-eds-i-check-filled-medium" xmlns:xlink="http://www.w3.org/1999/xlink"></use></svg>
                                Access provided by Chung Ang University</p>
</div>
</div>
<div class="app-article-masthead__brand">
<a class="app-article-masthead__journal-link" data-track="click_journal_home" data-track-action="journal homepage" data-track-context="article page" data-track-label="link" href="/journal/10844">
<picture>
<source height="159" media="(min-width: 768px)" srcset="https://media.springernature.com/w120/springer-static/cover-hires/journal/10844?as=webp,
                                                    https://media.springernature.com/w316/springer-static/cover-hires/journal/10844?as=webp 2x" type="image/webp" width="120"/>
<img alt="" height="95" src="https://media.springernature.com/w72/springer-static/cover-hires/journal/10844?as=webp" srcset="https://media.springernature.com/w144/springer-static/cover-hires/journal/10844?as=webp 2x" width="72"/>
</picture>
<span class="app-article-masthead__journal-title">Journal of Intelligent Information Systems</span>
</a>
<a class="app-article-masthead__submission-link" data-track="click_aims_and_scope" data-track-action="aims and scope" data-track-context="article page" data-track-label="link" href="https://link.springer.com/journal/10844/aims-and-scope">
                                Aims and scope
                                <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16"><use xlink:href="#icon-eds-i-arrow-right-medium" xmlns:xlink="http://www.w3.org/1999/xlink"></use></svg>
</a>
<a class="app-article-masthead__submission-link" data-track="click_submit_manuscript" data-track-action="submit manuscript" data-track-context="article masthead on springerlink article page" data-track-label="link" href="https://submission.nature.com/new-submission/10844/3">
                                Submit manuscript
                                <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16"><use xlink:href="#icon-eds-i-arrow-right-medium" xmlns:xlink="http://www.w3.org/1999/xlink"></use></svg>
</a>
</div>
</div>
</div>
</section>
<div class="c-article-main u-container u-mt-24 u-mb-32 l-with-sidebar" data-component="article-container" id="main-content">
<main class="u-serif js-main-column" data-track-component="article body">
<div aria-hidden="true" class="c-context-bar u-hide" data-context-bar="" data-test="context-bar">
<div class="c-context-bar__container u-container">
<div class="c-context-bar__title">
                                Heterogeneous graphormer for extractive multimodal summarization
                            </div>
<div data-test="inCoD" data-track-context="sticky banner">
<div class="c-pdf-container">
<div class="c-pdf-download u-clear-both u-mb-16">
<a class="u-button u-button--full-width u-button--primary u-justify-content-space-between c-pdf-download__link" data-article-pdf="true" data-draft-ignore="true" data-readcube-pdf-url="true" data-test="pdf-link" data-track="content_download" data-track-action="download pdf" data-track-external="" data-track-label="button" data-track-type="article pdf download" download="" href="/content/pdf/10.1007/s10844-024-00886-5.pdf">
<span class="c-pdf-download__text">Download PDF</span>
<svg aria-hidden="true" class="u-icon" focusable="false" height="16" width="16"><use xlink:href="#icon-eds-i-download-medium"></use></svg>
</a>
</div>
</div>
</div>
</div>
</div>
<div class="c-article-header">
<header>
<ul class="c-article-author-list c-article-author-list--short js-no-scroll" data-component-authors-activator="authors-list" data-test="authors-list"><li class="c-article-author-list__item"><a data-author-popup="auth-Xiankai-Jiang-Aff1" data-author-search="Jiang, Xiankai" data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Xiankai-Jiang-Aff1">Xiankai Jiang</a><sup class="u-js-hide"><a href="#Aff1" tabindex="-1">1</a></sup><sup class="u-js-hide"> <a href="#na1" tabindex="-1">na1</a></sup> &amp; </li><li class="c-article-author-list__item"><a data-author-popup="auth-Jingqiang-Chen-Aff1" data-author-search="Chen, Jingqiang" data-corresp-id="c1" data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Jingqiang-Chen-Aff1">Jingqiang Chen<svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16"><use xlink:href="#icon-eds-i-mail-medium" xmlns:xlink="http://www.w3.org/1999/xlink"></use></svg></a><sup class="u-js-hide"><a href="#Aff1" tabindex="-1">1</a></sup> </li></ul>
<div data-test="article-metrics">
<ul class="app-article-metrics-bar u-list-reset">
<li class="app-article-metrics-bar__item">
<p class="app-article-metrics-bar__count"><svg aria-hidden="true" class="u-icon app-article-metrics-bar__icon" focusable="false" height="24" width="24">
<use xlink:href="#icon-eds-i-accesses-medium"></use>
</svg>91 <span class="app-article-metrics-bar__label">Accesses</span></p>
</li>
<li class="app-article-metrics-bar__item app-article-metrics-bar__item--metrics">
<p class="app-article-metrics-bar__details"><a data-track="click" data-track-action="view metrics" data-track-label="link" href="/article/10.1007/s10844-024-00886-5/metrics" rel="nofollow">Explore all metrics <svg aria-hidden="true" class="u-icon app-article-metrics-bar__arrow-icon" focusable="false" height="24" width="24">
<use xlink:href="#icon-eds-i-arrow-right-medium"></use>
</svg></a></p>
</li>
</ul>
</div>
<div class="u-mt-32">
</div>
</header>
</div>
<div class="c-article-body" data-article-body="true" data-track-component="article body">
<section aria-labelledby="Abs1" data-title="Abstract" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>Multimodal summarization with multimodal output (MSMO) aims to generate summaries that incorporate both text and images. Existing methods have not effectively leveraged intermodal relationships, such as sentence-image relationships, which are crucial for generating high-quality multimodal summaries. In this paper, we propose a heterogeneous graph-based model for multimodal summarization (HGMS) designed to efficiently leverage intermodal relationships within multimodal data. The model constructs a heterogeneous graph based on the relationships between modalities, containing nodes for words, sentences and images. An enhanced Graphormer is then proposed to update node representations, aiming to more effectively model intricate relationships between multiple modalities. To the best of our knowledge, we are the first to apply Graphormer in the field of graph-based summarization. Experimental results on a large-scale benchmark dataset demonstrate that HGMS achieves state-of-the-art performance in terms of automatic metrics and human evaluations.</p></div></div></section>
<div data-test="cobranding-download">
<div class="note test-pdf-link" id="cobranding-and-download-availability-text">
</div>
</div>
<section aria-labelledby="inline-recommendations" class="c-article-recommendations" data-title="Inline Recommendations" data-track-component="inline-recommendations">
<h3 class="c-article-recommendations-title" id="inline-recommendations">Similar content being viewed by others</h3>
<div class="c-article-recommendations-list">
<div class="c-article-recommendations-list__item">
<article class="c-article-recommendations-card" itemscope="" itemtype="http://schema.org/ScholarlyArticle">
<div class="c-article-recommendations-card__img"><img alt="" loading="lazy" src="https://media.springernature.com/w215h120/springer-static/image/art%3A10.1007%2Fs10844-022-00757-x/MediaObjects/10844_2022_757_Fig1_HTML.png"/></div>
<div class="c-article-recommendations-card__main">
<h3 class="c-article-recommendations-card__heading" itemprop="name headline">
<a class="c-article-recommendations-card__link" data-track="select_recommendations_1" data-track-action="click recommendations inline - 1" data-track-context="inline recommendations" data-track-label="10.1007/s10844-022-00757-x" href="https://link.springer.com/10.1007/s10844-022-00757-x?fromPaywallRec=false" itemprop="url">Extractive text-image summarization with relation-enhanced graph attention network
                                        </a>
</h3>
<div class="c-article-meta-recommendations" data-test="recommendation-info">
<span class="c-article-meta-recommendations__item-type">Article</span>
<span class="c-article-meta-recommendations__date">28 October 2022</span>
</div>
</div>
</article>
</div>
<div class="c-article-recommendations-list__item">
<article class="c-article-recommendations-card" itemscope="" itemtype="http://schema.org/ScholarlyArticle">
<div class="c-article-recommendations-card__img"><img alt="" loading="lazy" src="https://media.springernature.com/w92h120/springer-static/cover-hires/book/978-981-99-6207-5?as=webp"/></div>
<div class="c-article-recommendations-card__main">
<h3 class="c-article-recommendations-card__heading" itemprop="name headline">
<a class="c-article-recommendations-card__link" data-track="select_recommendations_2" data-track-action="click recommendations inline - 2" data-track-context="inline recommendations" data-track-label="10.1007/978-981-99-6207-5_17" href="https://link.springer.com/10.1007/978-981-99-6207-5_17?fromPaywallRec=false" itemprop="url">MCLS: A Large-Scale Multimodal Cross-Lingual Summarization Dataset
                                        </a>
</h3>
<div class="c-article-meta-recommendations" data-test="recommendation-info">
<span class="c-article-meta-recommendations__item-type">Chapter</span>
<span class="c-article-meta-recommendations__date">© 2023</span>
</div>
</div>
</article>
</div>
<div class="c-article-recommendations-list__item">
<article class="c-article-recommendations-card" itemscope="" itemtype="http://schema.org/ScholarlyArticle">
<div class="c-article-recommendations-card__img"><img alt="" loading="lazy" src="https://media.springernature.com/w215h120/springer-static/image/art%3A10.1007%2Fs00521-023-08821-5/MediaObjects/521_2023_8821_Fig1_HTML.png"/></div>
<div class="c-article-recommendations-card__main">
<h3 class="c-article-recommendations-card__heading" itemprop="name headline">
<a class="c-article-recommendations-card__link" data-track="select_recommendations_3" data-track-action="click recommendations inline - 3" data-track-context="inline recommendations" data-track-label="10.1007/s00521-023-08821-5" href="https://link.springer.com/10.1007/s00521-023-08821-5?fromPaywallRec=false" itemprop="url">Topic-guided abstractive multimodal summarization with multimodal output
                                        </a>
</h3>
<div class="c-article-meta-recommendations" data-test="recommendation-info">
<span class="c-article-meta-recommendations__item-type">Article</span>
<span class="c-article-meta-recommendations__date">24 August 2023</span>
</div>
</div>
</article>
</div>
</div>
</section>
<script>
                window.dataLayer = window.dataLayer || [];
                window.dataLayer.push({
                    recommendations: {
                        recommender: 'semantic',
                        model: 'specter',
                        policy_id: 'NA',
                        timestamp: 1728893408,
                        embedded_user: 'null'
                    }
                });
            </script>
<section aria-labelledby="content-related-subjects" data-test="subject-content">
<h3 class="c-article__sub-heading" id="content-related-subjects">Explore related subjects</h3>
<span class="u-sans-serif u-text-s u-display-block u-mb-24">Discover the latest articles, news and stories from top researchers in related subjects.</span>
<ul class="c-article-subject-list" role="list">
<li class="c-article-subject-list__subject">
<a data-track="select_related_subject_1" data-track-context="related subjects from content page" data-track-label="Artificial Intelligence" href="/subject/artificial-intelligence">Artificial Intelligence</a>
</li>
</ul>
</section>
<div class="app-card-service" data-test="article-checklist-banner">
<div>
<a class="app-card-service__link" data-test="article-checklist-banner-link" data-track="click_presubmission_checklist" data-track-action="clicked article page checklist banner test 2 old version" data-track-category="pre-submission-checklist" data-track-context="article page top of reading companion" data-track-label="link" href="https://beta.springernature.com/pre-submission?journalId=10844">
<span class="app-card-service__link-text">Use our pre-submission checklist</span>
<svg aria-hidden="true" class="app-card-service__link-icon" focusable="false"><use xlink:href="#icon-eds-i-arrow-right-small"></use></svg>
</a>
<p class="app-card-service__description">Avoid common mistakes on your manuscript.</p>
</div>
<div class="app-card-service__icon-container">
<svg aria-hidden="true" class="app-card-service__icon" focusable="false">
<use xlink:href="#icon-eds-i-clipboard-check-medium"></use>
</svg>
</div>
</div>
<div class="main-content">
<section data-title="Introduction"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1"><span class="c-article-section__title-number">1 </span>Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>With the rapid development of multimedia data on the Internet, multimodal summarization has attracted widespread attention from researchers. Recently proposed Multimodal Summarization with Multimodal Output (Zhu et al., <a aria-label="Reference 2018" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR45" id="ref-link-section-d326771894e305" title="Zhu, J., Li, H., Liu, T., et al. (2018). MSMO: Multimodal summarization with multimodal output. In: Riloff E, Chiang D, Hockenmaier J, et al (eds) Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Brussels, Belgium, pp 4154–4164. 
                https://doi.org/10.18653/v1/D18-1448
                
              ">2018</a>) (MSMO) that condenses long multimodal news to a short pictorial version, as shown in Fig. <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#Fig1">1</a>. This innovative approach has been substantiated to significantly enhance users’ ability to swiftly grasp key news points, thereby elevating user satisfaction (Zhu et al., <a aria-label="Reference 2018" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR45" id="ref-link-section-d326771894e311" title="Zhu, J., Li, H., Liu, T., et al. (2018). MSMO: Multimodal summarization with multimodal output. In: Riloff E, Chiang D, Hockenmaier J, et al (eds) Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Brussels, Belgium, pp 4154–4164. 
                https://doi.org/10.18653/v1/D18-1448
                
              ">2018</a>).</p><p>While several methods have been proposed to tackle the MSMO task (Zhu et al., <a aria-label="Reference 2018" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR45" id="ref-link-section-d326771894e317" title="Zhu, J., Li, H., Liu, T., et al. (2018). MSMO: Multimodal summarization with multimodal output. In: Riloff E, Chiang D, Hockenmaier J, et al (eds) Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Brussels, Belgium, pp 4154–4164. 
                https://doi.org/10.18653/v1/D18-1448
                
              ">2018</a>, <a aria-label="Reference 2020" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR46" id="ref-link-section-d326771894e320" title="Zhu, J., Zhou, Y., Zhang, J., et al. (2020). Multimodal summarization with guidance of multimodal reference. Proceedings of the AAAI Conference on Artificial Intelligence, 34(05), 9749–975. 
                https://doi.org/10.1609/aaai.v34i05.6525
                
              ">2020</a>; Jiang et al., <a aria-label="Reference 2023" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR8" id="ref-link-section-d326771894e323" title="Jiang, C., Xie, R., Ye, W., et al. (2023). Exploiting pseudo image captions for multimodal summarization. In: Rogers A, Boyd-Graber J, Okazaki N (Eds.), Findings of the association for computational linguistics: ACL 2023. Association for Computational Linguistics, Toronto, Canada, pp 161–175. 
                https://doi.org/10.18653/v1/2023.findings-acl.12
                
              ">2023</a>), effectively handling the relationship between the image and text modalities remains a challenging problem. Zhang et al. (<a aria-label="Reference 2022c" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR43" id="ref-link-section-d326771894e326" title="Zhang, Z., Meng, X., Wang, Y., et al. (2022c). Unims: A unified framework for multimodal summarization with knowledge distillation. Proceedings of the AAAI Conference on Artificial Intelligence 36(10) 11757–11764. 
                https://doi.org/10.1609/aaai.v36i10.21431
                
              ">2022c</a>) extends the text encoder to a multimodal encoder, which takes the concatenation of textual and visual embeddings as input to obtain their contextualized joint representations. However, experimental results indicate that merely concatenating textual and visual embeddings cannot well capture the intricate relationships between modalities, such as sentence-image relationships. Jiang et al. (<a aria-label="Reference 2023" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR8" id="ref-link-section-d326771894e329" title="Jiang, C., Xie, R., Ye, W., et al. (2023). Exploiting pseudo image captions for multimodal summarization. In: Rogers A, Boyd-Graber J, Okazaki N (Eds.), Findings of the association for computational linguistics: ACL 2023. Association for Computational Linguistics, Toronto, Canada, pp 161–175. 
                https://doi.org/10.18653/v1/2023.findings-acl.12
                
              ">2023</a>) takes sentences related to images as additional input rather than relying directly on image information. Although a image-text alignment mechanism was introduced in the sentence selection stage to leverage the relationships between images and sentences, this valuable relationship was not considered in the summarization stage.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-container-section="figure" data-test="figure" data-title="Fig. 1" id="figure-1"><figure><figcaption><b class="c-article-section__figure-caption" data-test="figure-caption-text" id="Fig1">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-action="view figure" data-track-label="image" href="/article/10.1007/s10844-024-00886-5/figures/1" rel="nofollow"><picture><source srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10844-024-00886-5/MediaObjects/10844_2024_886_Fig1_HTML.png?as=webp" type="image/webp"/><img alt="figure 1" aria-describedby="Fig1" height="507" loading="lazy" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10844-024-00886-5/MediaObjects/10844_2024_886_Fig1_HTML.png" width="685"/></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>An example of multimodal summarization with multimodal output task</p></div></div><div class="u-text-right u-hide-print"><a aria-label="Full size image figure 1" class="c-article__pill-button" data-test="article-link" data-track="click" data-track-action="view figure" data-track-dest="link:Figure1 Full size image" data-track-label="button" href="/article/10.1007/s10844-024-00886-5/figures/1" rel="nofollow"><span>Full size image</span><svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16"><use xlink:href="#icon-eds-i-chevron-right-small" xmlns:xlink="http://www.w3.org/1999/xlink"></use></svg></a></div></figure></div><p>We believe that there are extensive many-to-many relationships between images and sentences. By effectively leveraging both the relevance and irrelevance between sentences and images, we can more accurately extract salient sentences and critical images. Taking Fig. <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#Fig1">1</a> as an example, the first sentence succinctly summarizes the main points of the news article: “Eating fruits and vegetables could cut the risk of heart attacks and strokes.” This guides our selection of the vegetable image which is relevant to it. Compared to the third sentence, which is only related to the BMI image, the second sentence mentions both vegetables and BMI. In this case, further selecting the second sentence, which covers different aspects of the topic, can provide a more comprehensive summary.</p><p>To effectively leverage the many-to-many relationships between sentences and images for multimodal summarization, an intuitive approach is to employ graph structures for modeling these relationships. In recent years, numerous studies (Jia et al., <a aria-label="Reference 2020" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR7" id="ref-link-section-d326771894e359" title="Jia, R., Cao, Y., Tang, H., et al. (2020). Neural extractive summarization with hierarchical attentive heterogeneous graph network. In: Webber B, Cohn T, He Y, et al (Eds.), Proceedings of the 2020 conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics, Online, pp 3622–363. 
                https://doi.org/10.18653/v1/2020.emnlp-main.295
                
              ">2020</a>; Song &amp; King, <a aria-label="Reference 2022" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR30" id="ref-link-section-d326771894e362" title="Song, Z., &amp; King, I. (2022). Hierarchical heterogeneous graph attention network for syntax-aware summarization. Proceedings of the AAAI Conference on Artificial Intelligence, 36(10), 11340–1134. 
                https://doi.org/10.1609/aaai.v36i10.21385
                
              ">2022</a>) have been dedicated to exploring the application of Graph Neural Networks (GNNs) in the realm of text summarization with impressive results. Specifically, GNNs are able to model complex relationships between semantic units. By building graphs on semantic units, such as sentences, words (Wang et al., <a aria-label="Reference 2020" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR35" id="ref-link-section-d326771894e365" title="Wang, D., Liu, P., Zheng, Y., et al. (2020). Heterogeneous graph neural networks for extractive document summarization. In: Jurafsky D, Chai J, Schluter N, et al (Eds.), Proceedings of the 58th annual meeting of the association for computational linguistics. Association for Computational Linguistics, Online, pp 6209–6219. 
                https://doi.org/10.18653/v1/2020.acl-main.553
                
              ">2020</a>), latent topics (Cui et al., <a aria-label="Reference 2020" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR5" id="ref-link-section-d326771894e368" title="Cui, P., Hu, L., &amp; Liu, Y. (2020). Enhancing extractive text summarization with topic-aware graph neural networks. In: Scott D, Bel N, Zong C (Eds.), Proceedings of the 28th international conference on computational linguistics. International Committee on Computational Linguistics, Barcelona, Spain (Online), pp 5360–537. 
                https://doi.org/10.18653/v1/2020.coling-main.468
                
              ">2020</a>), or passages (Phan et al., <a aria-label="Reference 2022" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR27" id="ref-link-section-d326771894e371" title="Phan TA, Nguyen NDN, &amp; Bui KHN. (2022). HeterGraphLongSum: Heterogeneous graph neural network with passage aggregation for extractive long document summarization. In: Calzolari N, Huang CR, Kim H, et al (Eds.), Proceedings of the 29th international conference on computational linguistics. International Committee on Computational Linguistics, Gyeongju, Republic of Korea, pp 6248–6258. 
                https://aclanthology.org/2022.coling-1.545
                
              ">2022</a>), GNNs can enhance representations of semantic units for text summarization. Different from these works, we propose a novel heterogeneous graph for multimodal summarization. This graph includes nodes representing words, sentences, and images, with edges connecting sentences to both images and words. In this graph, images and sentences can serve as intermediaries for each other, thus enhancing their representations for multimodal summarization.</p><p>To compute node representations of this graph, we propose the heterogeneous Graphormer (HeterGraphormer for short) by enhancing Graphormer (Ying et al., <a aria-label="Reference 2021" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR39" id="ref-link-section-d326771894e378" title="Ying, C., Cai, T., Luo, S., et al. (2021). Do transformers really perform badly for graph representation? In: Ranzato M, Beygelzimer A, Dauphin YN, et al (Eds.), Advances in Neural Information Processing Systems, pp 28877–28888. 
                https://proceedings.neurips.cc/paper/2021/hash/f1c1592588411002af340cbaedd6fc33-Abstract.html
                
              ">2021</a>) to effectively model intricate relationships between multiple modalities. Graphormer leverages self-attention to enable attention to all nodes when updating nodes, thereby alleviating the over-smoothing issue caused by traditional GNNs. We enhance Graphormer in the follow three aspect. First, we introduce type embedding and apply distinct spatial and edge embeddings for different heterogeneous edges to more effectively handle the heterogeneity of nodes and edges. Second, the centrality embedding was removed to optimize its performance for document graphs. Third, unconnected nodes are considered during node updates, as unrelated relationships are also valuable.</p><p>Overall, we propose a heterogeneous graph-based model for multimodal summarization (HGMS). The model first constructs a heterogeneous graph containing nodes for words, sentences and images. Subsequently, HeterGraphormer is employed to iteratively update the representations of nodes in the heterogeneous graph, aiming to more effectively model intricate relationships between nodes. Experimental results show that our model significantly enhances the performance of multimodal summarization. The contributions of our paper are as follows: </p><ol class="u-list-style-none">
<li>
<span class="u-custom-list-number">1.</span>
<p>We propose a heterogeneous graph-based model for multimodal summarization to efficiently leverage intermodal relationships through the multimodal graph we constructed.</p>
</li>
<li>
<span class="u-custom-list-number">2.</span>
<p>We propose HeterGraphormer to effectively model intricate relationships between multiple modalities. To the best of our knowledge, we are the first to apply Graphormer in the field of graph-based summarization.</p>
</li>
<li>
<span class="u-custom-list-number">3.</span>
<p>Our proposed model has achieved state-of-the-art (SOTA) performance on the MSMO dataset. Extensive experiments demonstrate the effectiveness of our proposed HeterGraphormer.</p>
</li>
</ol></div></div></section><section data-title="Related work"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2"><span class="c-article-section__title-number">2 </span>Related work</h2><div class="c-article-section__content" id="Sec2-content"><h3 class="c-article__sub-heading" id="Sec3"><span class="c-article-section__title-number">2.1 </span>Extractive summarization</h3><p>Text summarization is the task of generating concise summaries for original long documents. Existing text summarization methods can be categorized into extractive and abstractive approaches. Abstractive summarization (Sutskever et al., <a aria-label="Reference 2014" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR31" id="ref-link-section-d326771894e431" title="Sutskever, I., Vinyals, O., &amp; Le, Q.V. (2014). Sequence to sequence learning with neural networks. In: Ghahramani Z, Welling M, Cortes C, et al (Eds.), Advances in Neural Information Processing Systems,[SPACE]
                https://proceedings.neurips.cc/paper/2014/hash/a14ac55a4f27472c5d894ec1c3c743d2-Abstract.html
                
              ">2014</a>; See et al., <a aria-label="Reference 2017" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR29" id="ref-link-section-d326771894e434" title="See, A., Liu, P.J., Manning, C.D. (2017). Get to the point: Summarization with pointer-generator networks. In: Barzilay R, Kan MY (Eds.), Proceedings of the 55th annual meeting of the association for computational linguistics (Volume 1: Long Papers). Association for Computational Linguistics, Vancouver, Canada, pp 1073–1083. 
                https://doi.org/10.18653/v1/P17-1099
                
              ">2017</a>; Paulus et al., <a aria-label="Reference 2018" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR25" id="ref-link-section-d326771894e437" title="Paulus, R., Xiong, C., &amp; Socher, R. (2018). A deep reinforced model for abstractive summarization. In: International conference on learning representations, 
                https://openreview.net/forum?id=HkAClQgA-
                
              ">2018</a>; Zhang et al., <a aria-label="Reference 2020" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR40" id="ref-link-section-d326771894e440" title="Zhang, J., Zhao, Y., Saleh, M., et al. (2020). PEGASUS: pre-training with extracted gap-sentences for abstractive summarization. In: Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, pp 11328–11339. 
                http://proceedings.mlr.press/v119/zhang20ae.html
                
              ">2020</a>; Liu et al., <a aria-label="Reference 2022" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR19" id="ref-link-section-d326771894e443" title="Liu, Y., Liu, P., Radev, D., et al. (2022). BRIO: Bringing order to abstractive summarization. In: Muresan S, Nakov P, Villavicencio A (Eds.), Proceedings of the 60th annual meeting of the association for computational linguistics (Volume 1: Long Papers). Association for Computational Linguistics, Dublin, Ireland, pp 2890–2903. 
                https://doi.org/10.18653/v1/2022.acl-long.207
                
              ">2022</a>) generate a summary from scratch, typically with lower redundancy. Extractive summarization (Cheng &amp; Lapata, <a aria-label="Reference 2016" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR3" id="ref-link-section-d326771894e447" title="Cheng, J., &amp; Lapata, M. (2016). Neural summarization by extracting sentences and words. In: Erk K, Smith NA (Eds.), Proceedings of the 54th annual meeting of the association for computational linguistics (Volume 1: Long Papers). Association for Computational Linguistics, Berlin, Germany, pp 484–494. 
                https://doi.org/10.18653/v1/P16-1046
                
              ">2016</a>; An et al., <a aria-label="Reference 2022" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR1" id="ref-link-section-d326771894e450" title="An, C., Zhong, M., Wu, Z., et al. (2022). CoLo: A contrastive learning based re-ranking framework for one-stage summarization. In: Calzolari N, Huang CR, Kim H, et al (Eds.), Proceedings of the 29th international conference on computational linguistics. International Committee on Computational Linguistics, Gyeongju, Republic of Korea, pp 5783–5793. 
                https://aclanthology.org/2022.coling-1.508
                
              ">2022</a>; Cheng et al., <a aria-label="Reference 2023" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR4" id="ref-link-section-d326771894e453" title="Cheng, X., Shen, Y., &amp; Lu, W. (2023). A set prediction network for extractive summarization. In: Rogers A, Boyd-Graber J, Okazaki N (Eds.), Findings of the association for computational linguistics: ACL 2023. Association for Computational Linguistics, Toronto, Canada, pp 4766–4777. 
                https://doi.org/10.18653/v1/2023.findings-acl.293
                
              ">2023</a>) aims to select salient sentences from original documents.</p><p>Recent research work on extractive summarization covers a variety of methods. SummaRuNNer (Nallapati et al., <a aria-label="Reference 2017" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR22" id="ref-link-section-d326771894e459" title="Nallapati R, Zhai F, &amp; Zhou B. (2017) .Summarunner: A recurrent neural network based sequence model for extractive summarization of documents. Proceedings of the AAAI Conference on Artificial Intelligence 31(1). 
                https://doi.org/10.1609/aaai.v31i1.10958
                
              ">2017</a>) is one of the earliest models to employ an encoder based on Recurrent Neural Networks (RNNs). Narayan et al. (<a aria-label="Reference 2018" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR23" id="ref-link-section-d326771894e462" title="Narayan S, Cohen SB, &amp; Lapata M. (2018). Ranking sentences for extractive summarization with reinforcement learning. In: Walker M, Ji H, Stent A (Eds.) Proceedings of the 2018 conference of the North American chapter of the association for computational linguistics: Human language technologies, Volume 1 (Long Papers). Association for Computational Linguistics, New Orleans, Louisiana, pp 1747–1759. 
                https://doi.org/10.18653/v1/N18-1158
                
              ">2018</a>) proposed a reinforcement learning-based system trained by globally optimizing the ROUGE metric. Liu and Lapata (<a aria-label="Reference 2019" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR17" id="ref-link-section-d326771894e465" title="Liu Y, &amp; Lapata M. (2019). Text summarization with pretrained encoders. In: Inui K, Jiang J, Ng V, et al (Eds.), Proceedings of the 2019 conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). Association for Computational Linguistics, Hong Kong, China, pp 3730–3740. 
                https://doi.org/10.18653/v1/D19-1387
                
              ">2019</a>) leverages the capability of pre-trained language models to effectively capture contextual features, thereby significantly enhancing the performance of summarization. Zhong et al. (<a aria-label="Reference 2020" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR44" id="ref-link-section-d326771894e468" title="Zhong, M., Liu, P., Chen, Y., et al. (2020). Extractive summarization as text matching. In: Jurafsky D, Chai J, Schluter N, et al (Eds.) Proceedings of the 58th annual meeting of the association for computational linguistics. Association for Computational Linguistics, Online, pp 6197–6208. 
                https://doi.org/10.18653/v1/2020.acl-main.552
                
              ">2020</a>) formulates the extractive summarization task as a semantic text matching problem, selecting the set of sentences that most closely match the source document in the semantic space as the summary. Tang et al. (<a aria-label="Reference 2022" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR32" id="ref-link-section-d326771894e471" title="Tang, P., Hu, K., Yan, R., et al. (2022). OTExtSum: Extractive text summarisation with optimal transport. In: Carpuat M, de Marneffe MC, Meza Ruiz IV (eds) Findings of the association for computational linguistics: NAACL 2022. Association for Computational Linguistics, Seattle, United States, pp 1128–1141. 
                https://doi.org/10.18653/v1/2022.findings-naacl.85
                
              ">2022</a>) formulates the extractive summarization task as an Optimal Transport (OT) problem from document to summary.</p><h3 class="c-article__sub-heading" id="Sec4"><span class="c-article-section__title-number">2.2 </span>Multimodal summarization</h3><p>Multimodal summarization takes multiple modalities of data as input, and outputs either text-only (Li et al., <a aria-label="Reference 2018" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR13" id="ref-link-section-d326771894e482" title="Li, H., Zhu, J., Liu, T., et al. (2018). Multi-modal sentence summarization with modality attention and image filtering. In: Lang J (Ed.), Proceedings of the twenty-seventh international joint conference on artificial intelligence, IJCAI 2018, July 13-19, 2018, Stockholm, Sweden. ijcai.org, Stockholm, Sweden, pp 4152–4158. 
                https://doi.org/10.24963/ijcai.2018/577
                
              ">2018</a>; Overbay et al., <a aria-label="Reference 2023" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR24" id="ref-link-section-d326771894e485" title="Overbay, K., Ahn, J., Pesaran zadeh, F., et al. (2023). mRedditSum: A multimodal abstractive summarization dataset of Reddit threads with images. In: Bouamor H, Pino J, Bali K (Eds.), Proceedings of the 2023 conference on empirical methods in natural language processing. Association for Computational Linguistics, Singapore, pp 4117–4132. 
                https://doi.org/10.18653/v1/2023.emnlp-main.251
                
              ">2023</a>; Liang et al., <a aria-label="Reference 2023" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR16" id="ref-link-section-d326771894e488" title="Liang, Y., Meng, F., Xu, J., et al. (2023). Summary-oriented vision modeling for multimodal abstractive summarization. In: Rogers A, Boyd-Graber J, Okazaki N (Eds.), Proceedings of the 61st annual meeting of the association for computational linguistics (Volume 1: Long Papers). Association for Computational Linguistics, Toronto, Canada, pp 2934–2951. 
                https://doi.org/10.18653/v1/2023.acl-long.165
                
              ">2023</a>) or multimodal (Li et al., <a aria-label="Reference 2020" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR15" id="ref-link-section-d326771894e491" title="Li, M., Chen, X., Gao, S., et al (2020) VMSMO: Learning to generate multimodal summary for video-based news articles. In: Webber B, Cohn T, He Y, et al (Eds.), Proceedings of the 2020 conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics, Online, pp 9360–9369. 
                https://doi.org/10.18653/v1/2020.emnlp-main.752
                
              ">2020</a>; Zhang et al., <a aria-label="Reference 2022a" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR41" id="ref-link-section-d326771894e494" title="Zhang, L., Zhang, X., &amp; Pan, J. (2022a). Hierarchical cross-modality semantic correlation learning model for multimodal summarization. Proceedings of the AAAI Conference on Artificial Intelligence 36(10) 11676–11684. 
                https://doi.org/10.1609/aaai.v36i10.21422
                
              ">2022a</a>) summary.</p><p>For multimodal summarization with text-only output, Chen and Zhuge (<a aria-label="Reference 2018" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR2" id="ref-link-section-d326771894e500" title="Chen, J., &amp; Zhuge, H. (2018). Abstractive text-image summarization using multi-modal attentional hierarchical RNN. In: Riloff E, Chiang D, Hockenmaier J, et al (Eds.), Proceedings of the 2018 conference on empirical methods in natural language processing. Association for Computational Linguistics, Brussels, Belgium, pp 4046–4056. 
                https://doi.org/10.18653/v1/D18-1438
                
              ">2018</a>) proposed an abstractive text-image summarization method using a multi-modal attentional hierarchical RNN, which integrates textual and visual information to produce a textual summary. Zhang et al. (<a aria-label="Reference 2022b" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR42" id="ref-link-section-d326771894e503" title="Zhang, M., Zhou, G., Yu, W., et al. (2022b). MAA-PTG: multimodal aspect-aware product title generation. J Intell Inf Syst 59(1) 213–235. 
                https://doi.org/10.1007/S10844-022-00695-8
                
              ">2022b</a>) introduces an aspect precision score function to assess summary relevance and incorporates it using reinforcement learning. Xiao et al. (<a aria-label="Reference 2023" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR37" id="ref-link-section-d326771894e506" title="Xiao M, Zhu J, Lin H, et al. (2023). CFSum coarse-to-fine contribution network for multimodal summarization. In: Rogers A, Boyd-Graber J, Okazaki N (Eds.), Proceedings of the 61st annual meeting of the association for computational linguistics (Volume 1: Long Papers). Association for Computational Linguistics, Toronto, Canada, pp 8538–8553. 
                https://doi.org/10.18653/v1/2023.acl-long.476
                
              ">2023</a>) proposed a coarse-to-fine contribution network to model different contributions of images for summarization. Jin and Chen (<a aria-label="Reference 2024" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR9" id="ref-link-section-d326771894e509" title="Jin, L., &amp; Chen, J. (2024). Self-supervised opinion summarization with multi-modal knowledge graph. J Intell Inf Syst, 62(1), 191–208. 
                https://doi.org/10.1007/S10844-023-00812-1
                
              ">2024</a>) proposes a multimodal knowledge graph constructed from entities in text and images, using structural knowledge in a self-supervised manner.</p><p>For multimodal summarization with multimodal output, the task generates summaries that include multiple modalities, such as text and images, to provide richer and more comprehensive information. Zhu et al. (<a aria-label="Reference 2018" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR45" id="ref-link-section-d326771894e515" title="Zhu, J., Li, H., Liu, T., et al. (2018). MSMO: Multimodal summarization with multimodal output. In: Riloff E, Chiang D, Hockenmaier J, et al (eds) Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Brussels, Belgium, pp 4154–4164. 
                https://doi.org/10.18653/v1/D18-1448
                
              ">2018</a>) first proposed generating a pictorial summary given a document and a collection of images. Zhu et al. (<a aria-label="Reference 2020" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR46" id="ref-link-section-d326771894e518" title="Zhu, J., Zhou, Y., Zhang, J., et al. (2020). Multimodal summarization with guidance of multimodal reference. Proceedings of the AAAI Conference on Artificial Intelligence, 34(05), 9749–975. 
                https://doi.org/10.1609/aaai.v34i05.6525
                
              ">2020</a>) further introduced a extra cross-entropy loss for image selection. Zhang et al. (<a aria-label="Reference 2022c" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR43" id="ref-link-section-d326771894e521" title="Zhang, Z., Meng, X., Wang, Y., et al. (2022c). Unims: A unified framework for multimodal summarization with knowledge distillation. Proceedings of the AAAI Conference on Artificial Intelligence 36(10) 11757–11764. 
                https://doi.org/10.1609/aaai.v36i10.21431
                
              ">2022c</a>) utilizes knowledge distillation with a vision-language pre-trained model to help image selection. Xie et al. (<a aria-label="Reference 2023" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR38" id="ref-link-section-d326771894e524" title="Xie, F., Chen, J., &amp; Chen, K. (2023). Extractive text-image summarization with relation-enhanced graph attention network. J Intell Inf Syst, 61(2), 325–341. 
                https://doi.org/10.1007/S10844-022-00757-X
                
              ">2023</a>) introduces a graph structure, leveraging image captions to establish indirect connections between images and sentences via word nodes, thereby enhancing the association between images and text. Jiang et al. (<a aria-label="Reference 2023" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR8" id="ref-link-section-d326771894e527" title="Jiang, C., Xie, R., Ye, W., et al. (2023). Exploiting pseudo image captions for multimodal summarization. In: Rogers A, Boyd-Graber J, Okazaki N (Eds.), Findings of the association for computational linguistics: ACL 2023. Association for Computational Linguistics, Toronto, Canada, pp 161–175. 
                https://doi.org/10.18653/v1/2023.findings-acl.12
                
              ">2023</a>) introduced an image-text alignment mechanism to convert input images into the most relevant sentences to better capture visual knowledge. However, previous studies have not adequately considered the many-to-many relationships between sentences and images. In this study, we model these relationships using a graph structure to more effectively utilize this crucial information.</p><h3 class="c-article__sub-heading" id="Sec5"><span class="c-article-section__title-number">2.3 </span>Graph-based summarization</h3><p>Early work focused on utilizing content similarity between sentences to construct document graphs and extracting sentences in an unsupervised manner such as TextRank (Mihalcea &amp; Tarau, <a aria-label="Reference 2004" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR21" id="ref-link-section-d326771894e538" title="Mihalcea, R., Tarau, P (2004) TextRank: Bringing order into text. In: Lin D, Wu D (Eds.), Proceedings of the 2004 conference on empirical methods in natural language processing. Association for Computational Linguistics, Barcelona, Spain, pp 404–411. 
                https://aclanthology.org/W04-3252
                
              ">2004</a>) and LexRank (Erkan &amp; Radev, <a aria-label="Reference 2004" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR6" id="ref-link-section-d326771894e541" title="Erkan, G., &amp; Radev, D. R. (2004). Lexrank: Graph-based lexical centrality as salience in text summarization. J Artif Intell Res, 22, 457–47. 
                https://doi.org/10.1613/JAIR.1523
                
              ">2004</a>). Recently, the application of GNNs in text summarization task has attracted widespread attention. Wang et al. (<a aria-label="Reference 2020" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR35" id="ref-link-section-d326771894e544" title="Wang, D., Liu, P., Zheng, Y., et al. (2020). Heterogeneous graph neural networks for extractive document summarization. In: Jurafsky D, Chai J, Schluter N, et al (Eds.), Proceedings of the 58th annual meeting of the association for computational linguistics. Association for Computational Linguistics, Online, pp 6209–6219. 
                https://doi.org/10.18653/v1/2020.acl-main.553
                
              ">2020</a>) constructed a heterogeneous graph, enriching the cross-sentence relations through the word nodes between sentences. Jia et al. (<a aria-label="Reference 2020" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR7" id="ref-link-section-d326771894e547" title="Jia, R., Cao, Y., Tang, H., et al. (2020). Neural extractive summarization with hierarchical attentive heterogeneous graph network. In: Webber B, Cohn T, He Y, et al (Eds.), Proceedings of the 2020 conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics, Online, pp 3622–363. 
                https://doi.org/10.18653/v1/2020.emnlp-main.295
                
              ">2020</a>) proposed a hierarchical heterogeneous graph to extract sentences by simultaneously balancing salience and redundancy. Cui et al. (<a aria-label="Reference 2020" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR5" id="ref-link-section-d326771894e550" title="Cui, P., Hu, L., &amp; Liu, Y. (2020). Enhancing extractive text summarization with topic-aware graph neural networks. In: Scott D, Bel N, Zong C (Eds.), Proceedings of the 28th international conference on computational linguistics. International Committee on Computational Linguistics, Barcelona, Spain (Online), pp 5360–537. 
                https://doi.org/10.18653/v1/2020.coling-main.468
                
              ">2020</a>) incorporates latent topics into graph propagation via a joint neural topic model, facilitating the extraction of crucial information from documents. Jing et al. (<a aria-label="Reference 2021" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR10" id="ref-link-section-d326771894e554" title="Jing, B., You, Z., Yang, T., et al. (2021). Multiplex graph neural network for extractive text summarization. In: Moens MF, Huang X, Specia L, et al. (Eds.), Proceedings of the 2021 conference on empirical methods in natural language processing. Association for Computational Linguistics, Online and Punta Cana, Dominican Republic, pp 133–139. 
                https://doi.org/10.18653/v1/2021.emnlp-main.11
                
              ">2021</a>) proposed to use multiplex graph to model different types of relationships among sentences and words. Song and King (<a aria-label="Reference 2022" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR30" id="ref-link-section-d326771894e557" title="Song, Z., &amp; King, I. (2022). Hierarchical heterogeneous graph attention network for syntax-aware summarization. Proceedings of the AAAI Conference on Artificial Intelligence, 36(10), 11340–1134. 
                https://doi.org/10.1609/aaai.v36i10.21385
                
              ">2022</a>) obtains sentence representations based on constituency trees to leverage syntactic information. Phan et al. (<a aria-label="Reference 2022" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR27" id="ref-link-section-d326771894e560" title="Phan TA, Nguyen NDN, &amp; Bui KHN. (2022). HeterGraphLongSum: Heterogeneous graph neural network with passage aggregation for extractive long document summarization. In: Calzolari N, Huang CR, Kim H, et al (Eds.), Proceedings of the 29th international conference on computational linguistics. International Committee on Computational Linguistics, Gyeongju, Republic of Korea, pp 6248–6258. 
                https://aclanthology.org/2022.coling-1.545
                
              ">2022</a>) introduce passage nodes into the sentence-word graph to enrich the representation of sentences in long documents. In this study, we construct a novel heterogeneous graph for multimodal summarization based on the interrelations between different modalities. Additionally, we propose HeterGraphormer to more effectively model the intricate relationships within the graph.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-container-section="figure" data-test="figure" data-title="Fig. 2" id="figure-2"><figure><figcaption><b class="c-article-section__figure-caption" data-test="figure-caption-text" id="Fig2">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-action="view figure" data-track-label="image" href="/article/10.1007/s10844-024-00886-5/figures/2" rel="nofollow"><picture><source srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10844-024-00886-5/MediaObjects/10844_2024_886_Fig2_HTML.png?as=webp" type="image/webp"/><img alt="figure 2" aria-describedby="Fig2" height="280" loading="lazy" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10844-024-00886-5/MediaObjects/10844_2024_886_Fig2_HTML.png" width="685"/></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>Overview of our proposed HGMS</p></div></div><div class="u-text-right u-hide-print"><a aria-label="Full size image figure 2" class="c-article__pill-button" data-test="article-link" data-track="click" data-track-action="view figure" data-track-dest="link:Figure2 Full size image" data-track-label="button" href="/article/10.1007/s10844-024-00886-5/figures/2" rel="nofollow"><span>Full size image</span><svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16"><use xlink:href="#icon-eds-i-chevron-right-small" xmlns:xlink="http://www.w3.org/1999/xlink"></use></svg></a></div></figure></div></div></div></section><section data-title="Method"><div class="c-article-section" id="Sec6-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec6"><span class="c-article-section__title-number">3 </span>Method</h2><div class="c-article-section__content" id="Sec6-content"><h3 class="c-article__sub-heading" id="Sec7"><span class="c-article-section__title-number">3.1 </span>Problem formulation</h3><p>Given a multimodal document <span class="mathjax-tex">\(\{T, P\}\)</span>, where <span class="mathjax-tex">\(T=\{s_1,s_2,\cdots ,s_m\}\)</span> is a text document consisting of <i>m</i> sentences and <span class="mathjax-tex">\(P=\{p_1,p_2,\cdots ,p_k\}\)</span> is a collection of <i>k</i> images, the model summarizes multimodal document into a multimodal summary <span class="mathjax-tex">\(\mathcal {S}=\{\mathcal {S}_t,\mathcal {S}_p\}\)</span>, where <span class="mathjax-tex">\(\mathcal {S}_t\)</span> denotes the textual summary extracted from <i>T</i> and <span class="mathjax-tex">\(\mathcal {S}_p\)</span> is a subset of images selected from the image collection <i>P</i>.</p><h3 class="c-article__sub-heading" id="Sec8"><span class="c-article-section__title-number">3.2 </span>Graph construction</h3><p>In order to model the relationship between sentences and images, we construct a heterogeneous graph, as shown in the Fig. <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#Fig2">2</a>. In this graph, there are three types of nodes: word, sentence, and image. Each word node corresponds to the identical word appearing multiple times in the document. Each sentence and image node corresponds to the sentences and images in the document.</p><p>For the connection between nodes, we first connect words and sentences through inclusion relationships. To enhance model efficiency, we retained only those word nodes connected to two or more sentences. Furthermore, we connect all sentence and image pairs, rather than limiting connections to relevant pairs. In order to accurately discern the significance of connections, we introduce TF-IDF and Image-Text Match (ITM) scores as the weights of these two edges respectively.</p><p>Given a undirected graph <span class="mathjax-tex">\(G = \{V, E\}\)</span>,where <i>V</i> denotes the set of nodes and <i>E</i> represents edge weights between nodes. Our heterogeneous graph can be formally defined as <span class="mathjax-tex">\( V=V_w \cup V_s \cup V_p \)</span> and <span class="mathjax-tex">\(E=E_{sw} \cup E_{sp} \)</span>, where <span class="mathjax-tex">\(V_w\)</span>, <span class="mathjax-tex">\(V_s\)</span>, and <span class="mathjax-tex">\(V_p\)</span> denote the sets of word, sentence, and image nodes, respectively. <span class="mathjax-tex">\(E_{sw}=\{e^{sw}_{11}, \cdots , e^{sw}_{mu}\}\)</span> and <span class="mathjax-tex">\(E_{sp}=\{e^{sp}_{11}, \cdots , e^{sp}_{mk}\}\)</span> represent sentence-word and sentence-image edge weights respectively. Here <span class="mathjax-tex">\(e^{sw}_{ij} \in [0, 1]\)</span> denotes the TF-IDF score between sentence <span class="mathjax-tex">\(s_i\)</span> and word <span class="mathjax-tex">\(w_j\)</span>, while <span class="mathjax-tex">\(e^{sp}_{ij} \in [0, 1]\)</span> represents the ITM score between sentence <span class="mathjax-tex">\(s_i\)</span> and image <span class="mathjax-tex">\(p_j\)</span>, computed via the BLIP (Li et al., <a aria-label="Reference 2022" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR14" id="ref-link-section-d326771894e1412" title="Li, J., Li, D., Xiong, C., et al. (2022). BLIP: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In: Chaudhuri K, Jegelka S, Song L, et al (Eds.), International conference on machine learning, ICML 2022, 17-23 July 2022, PMLR, Baltimore, Maryland, USA, Proceedings of Machine Learning Research, 162 pp 12888–12900">2022</a>).</p><p>For node features, we use GloVe (Pennington et al., <a aria-label="Reference 2014" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR26" id="ref-link-section-d326771894e1418" title="Pennington, J., Socher, R., &amp; Manning, C. (2014). GloVe: Global vectors for word representation. In: Moschitti A, Pang B, Daelemans W (Eds.) Proceedings of the 2014 conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics, Doha, Qatar, pp 1532–1543. 
                https://doi.org/10.3115/v1/D14-1162
                
              ">2014</a>), RoBERTa (Liu et al., <a aria-label="Reference 2019" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR18" id="ref-link-section-d326771894e1421" title="Liu, Y., Ott, M., Goyal, N., et al. (2019). Roberta: A robustly optimized BERT pretraining approach. 
                arXiv:1907.11692
                
              
                https://arxiv.org/abs/1907.11692
                
              ">2019</a>), and BLIP to obtain node representations of words, sentences, and images, respectively.</p><div class="c-article-equation" id="Equ1"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} x^w_i&amp;= \text {GloVe}(w_i),\end{aligned}$$</span></div><div class="c-article-equation__number">
                    (1)
                </div></div><div class="c-article-equation" id="Equ2"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} x^s_i&amp;= \text {RoBERTa}(s_i)W_s + \text {PE}(i),\end{aligned}$$</span></div><div class="c-article-equation__number">
                    (2)
                </div></div><div class="c-article-equation" id="Equ3"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} x^p_i&amp;= \text {BLIP}(p_i)W_p, \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (3)
                </div></div><p>where <span class="mathjax-tex">\( x^w_i \in \mathbb {R}^d \)</span>, <span class="mathjax-tex">\( W_s \in \mathbb {R}^{d_s \times d} \)</span>, <span class="mathjax-tex">\( W_p \in \mathbb {R}^{d_p \times d} \)</span> are projection matrices and <span class="mathjax-tex">\(\text {PE}(i)\)</span> denotes the Position Encoding proposed by Vaswani et al. (<a aria-label="Reference 2017" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR33" id="ref-link-section-d326771894e1817" title="Vaswani A, Shazeer N, Parmar N, et al. (2017). Attention is all you need. In: Guyon I, Luxburg UV, Bengio S, et al (Eds.), Advances in Neural Information Processing Systems,[SPACE]
                https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html
                
              ">2017</a>).</p><h3 class="c-article__sub-heading" id="Sec9"><span class="c-article-section__title-number">3.3 </span>Graph encoder</h3><p>Graphormer integrates graph structural information into the Transformer (Vaswani et al., <a aria-label="Reference 2017" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR33" id="ref-link-section-d326771894e1828" title="Vaswani A, Shazeer N, Parmar N, et al. (2017). Attention is all you need. In: Guyon I, Luxburg UV, Bengio S, et al (Eds.), Advances in Neural Information Processing Systems,[SPACE]
                https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html
                
              ">2017</a>) architecture using centrality embedding, spatial embedding, and edge embedding to effectively learn node representations. To better apply it to our constructed heterogeneous graph, we propose HeterGraphormer, which introduces several enhancements designed to better model the relationships between heterogeneous nodes.</p><p>Before input, we add type embedding to each node to distinguish different node types. Let <i>h</i> denote the node’s hidden state, and the initial hidden state can be defined as</p><div class="c-article-equation" id="Equ4"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} h^{\tau }_i = x^{\tau }_i + z_{\tau }, \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (4)
                </div></div><p>where <span class="mathjax-tex">\(z \in \mathbb {R}^d\)</span> are learnable embedding vectors specified by the node type <span class="mathjax-tex">\(\tau \in \{w,s,i\}\)</span>.</p><p>It is noteworthy that we did not use centrality embedding, which adds embeddings to node representations based on the degrees of nodes to differentiate their importance, as the degree of nodes in document graphs does not accurately reflect their significance. We prove this assumption via the ablation study in the experiment section.</p><h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec10"><span class="c-article-section__title-number">3.3.1 </span>Attention with spatial embedding</h4><p>Due to the global receptive field characteristic of the Transformer, it can effectively attend to information from any position when updating node representations. To differentiate nodes at different distances, Graphormer introduces spatial embedding, which assigns a unique embedding for each distance as a weight in the self-attention module. This enables the model to accurately distinguish distance variances between nodes in the graph while maintaining its capacity for global information processing. Following  Ying et al. (<a aria-label="Reference 2021" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR39" id="ref-link-section-d326771894e1983" title="Ying, C., Cai, T., Luo, S., et al. (2021). Do transformers really perform badly for graph representation? In: Ranzato M, Beygelzimer A, Dauphin YN, et al (Eds.), Advances in Neural Information Processing Systems, pp 28877–28888. 
                https://proceedings.neurips.cc/paper/2021/hash/f1c1592588411002af340cbaedd6fc33-Abstract.html
                
              ">2021</a>), we adopt the shortest path distance as the distance metric between nodes, as it is the most direct and widely used approach.</p><p>Concretely, let <span class="mathjax-tex">\(\phi (i,j)\)</span> denote the distance between nodes <span class="mathjax-tex">\(v_i\)</span> and <span class="mathjax-tex">\(v_j\)</span>. If a path exists between the nodes, <span class="mathjax-tex">\(\phi \)</span> outputs the shortest path distance between them; if not, it outputs a special value, i.e., -1. In this paper, we consider both the relevance and irrelevance between different modalities. For each feasible value and the -1 value corresponding to unconnected heterogeneous nodes, we assign a learnable scalar that serves as a weight in the self-attention module. Denote <span class="mathjax-tex">\(A_{ij}\)</span> as the (<i>i</i>, <i>j</i>)-element of the attention matrix <i>A</i>, we have:</p><div class="c-article-equation" id="Equ5"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} A_{ij}=\frac{(h_iW_{Q})(h_jW_{K})^T}{\sqrt{d_k}} \cdot b_{\phi (i,j)}, \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (5)
                </div></div><p>where <span class="mathjax-tex">\(b_{\phi (i,j)}\)</span> is a learnable scalar indexed by <span class="mathjax-tex">\(\phi (i,j)\)</span>, and shared across all layers. <span class="mathjax-tex">\(W_{Q}\)</span> and <span class="mathjax-tex">\(W_{K}\)</span> are learnable weight matrices used to transform <span class="mathjax-tex">\(h_i\)</span> and <span class="mathjax-tex">\(h_j\)</span> into query and key vectors, respectively. <span class="mathjax-tex">\(d_k\)</span> denotes the dimensionality of the key vectors.</p><h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec11"><span class="c-article-section__title-number">3.3.2 </span>Edge embedding</h4><p>Edge embedding is proposed to encode edge features, allowing the model to incorporate edge information when updating node representations.</p><p>We use the product of weights along the shortest path as the weight between two nodes, rather than averaging, because the product more accurately reflects the connectivity of the path. For example, when the shortest path <span class="mathjax-tex">\(\text {SP}_{ij} = \{1, 0\}\)</span>, the product is 0, which more accurately represents the connection strength compared to the average value of 0.5. Concretely, let <span class="mathjax-tex">\(\omega (i,j)\)</span> denotes the product of weights along one of the shortest paths <span class="mathjax-tex">\(\text {SP}_{ij}=(e_1,e_2,\cdots ,e_N)\)</span> from <span class="mathjax-tex">\(v_i\)</span> to <span class="mathjax-tex">\(v_j\)</span>, expressed as follows:</p><div class="c-article-equation" id="Equ6"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} \omega (i,j)&amp;= \prod _{n=1}^{N} e_n. \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (6)
                </div></div><p>For each discretized value of <span class="mathjax-tex">\(\omega (i,j)\)</span>, we assign a learnable scalar that serves as a weight in the self-attention module. Equation (<a data-track="click" data-track-action="equation anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#Equ5">5</a>) is further modified as follows:</p><div class="c-article-equation" id="Equ7"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} A_{ij}=\frac{(h_iW_{Q})(h_jW_{K})^T}{\sqrt{d_k}} \cdot (b_{\phi (i,j)} + c_{\lfloor \omega (i,j) \cdot a\rfloor }), \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (7)
                </div></div><p>where <span class="mathjax-tex">\(c_{\lfloor \omega (i,j) \cdot a\rfloor }\)</span> is a learnable scalar indexed by the floor value of the product of <span class="mathjax-tex">\(\omega (i,j)\)</span> and the hyperparameter <i>a</i>.</p><p>Moreover, given the varying significance of heterogeneous connections between nodes, such as sentences to images and sentences to sentences, we employ distinct spatial and edge embeddings for diverse heterogeneous connections.</p><h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec12"><span class="c-article-section__title-number">3.3.3 </span>Heter graphormer layer</h4><p>The HeterGraphormer layer can be formally characterized as follows:</p><div class="c-article-equation" id="Equ8"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} {\begin{matrix} H^{'(l)} &amp; = \text {MHA}(\text {LN}(H^{(l-1)}), E) + H^{(l-1)},\\ H^{(l)} &amp; = \text {FFN}(\text {LN}(H^{'(l)})) + H^{'(l)}, \end{matrix}} \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (8)
                </div></div><p>where <span class="mathjax-tex">\(H^{'(l)}\)</span> denotes the node representations output from layer <i>l</i>, <span class="mathjax-tex">\(\text {LN}\)</span> signifies layer normalization and <span class="mathjax-tex">\(\text {FFN}\)</span> refers to feed-forward network. <span class="mathjax-tex">\(\text {MHA}\)</span> represents multi-head self-attention, represented as</p><div class="c-article-equation" id="Equ9"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} \begin{aligned} \text {MHA}(H, E) = \text {Concat}(\text {head}_1, \ldots , \text {head}_N) W^O,\\ \text {head}_i = \text {Attention}_i(H, E)=\text {softmax}(A)HW^V_i, \end{aligned} \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (9)
                </div></div><p>where <span class="mathjax-tex">\(W^O\)</span> is a learnable weight matrix used to linearly combine the outputs of all attention heads, <span class="mathjax-tex">\(W^V\)</span> is a learnable weight matrix used to transform <i>H</i> into value vectors.</p><p>Given the existence of two types of edges on the graph, we sequentially update the sentence-word subgraph and the sentence-image subgraph, formulated as follows:</p><div class="c-article-equation" id="Equ10"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} {\begin{matrix} [H^{'(l)}_s, H^{(l)}_w] &amp; = \text {layer}([H^{(l-1)}_s, H^{(l-1)}_w], E_{sw}),\\ [H^{(l)}_s, H^{(l)}_p] &amp; = \text {layer}([H^{'(l)}_s, H^{(l-1)}_p], E_{sp}).\\ \end{matrix}} \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (10)
                </div></div><h3 class="c-article__sub-heading" id="Sec13"><span class="c-article-section__title-number">3.4 </span>Multimodal summarizer</h3><p>After passing <i>L</i> HeterGraphormer layers, we obtain the final sentence node representations <span class="mathjax-tex">\(H^{L}_s = \{h^s_1, h^s_2, \cdots , h^s_m \}\)</span> and image node representations <span class="mathjax-tex">\(H^{L}_p = \{h^p_1, h^p_2, \cdots , h^p_k \}\)</span>. Subsequently, these representations are fed into a single feed-forward layer for label prediction:</p><div class="c-article-equation" id="Equ11"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} \begin{aligned} \hat{y}^s_i&amp;= \text {sigmoid}(W_{s1}h^s_i),\\ \hat{y}^p_i&amp;= \text {sigmoid}(W_{p1}h^p_i),\\ \end{aligned} \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (11)
                </div></div><p>where <span class="mathjax-tex">\(W_{s1}\)</span>, <span class="mathjax-tex">\(W_{p1} \)</span> are trainable parameters.</p><p>For sentence extraction, our objective is to learn a binary label <span class="mathjax-tex">\(y^s_i \in \{0, 1\}\)</span> for each sentence <span class="mathjax-tex">\(s_i\)</span>, where <span class="mathjax-tex">\(y^s_i = 1\)</span> means that the sentence <span class="mathjax-tex">\(s_i\)</span> should be included in the textual summary <span class="mathjax-tex">\(\mathcal {S}_t\)</span>, and <span class="mathjax-tex">\(y^s_i = 0\)</span> means that the sentence <span class="mathjax-tex">\(s_i\)</span> should be excluded from the summary. The ground truth labels, referred to as ORACLE, is extracted using the greedy algorithm introduced by Nallapati et al. (<a aria-label="Reference 2017" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR22" id="ref-link-section-d326771894e4570" title="Nallapati R, Zhai F, &amp; Zhou B. (2017) .Summarunner: A recurrent neural network based sequence model for extractive summarization of documents. Proceedings of the AAAI Conference on Artificial Intelligence 31(1). 
                https://doi.org/10.1609/aaai.v31i1.10958
                
              ">2017</a>). In detail, the algorithm generates a summary consisting of multiple sentences, which are selected greedily to maximize the ROUGE score against the gold summary<sup><a href="#Fn1"><span class="u-visually-hidden">Footnote </span>1</a></sup>.</p><p>The binary cross-entropy loss of the sentence classifier is expressed as</p><div class="c-article-equation" id="Equ12"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} \mathcal {L}_s = -\frac{1}{m}\sum _{i=1}^{m} \left( y^s_i \log (\hat{y}^s_i) + (1 - y^s_i) \log (1 - \hat{y}^s_i)\right) \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (12)
                </div></div><p>Following Zhang et al. (<a aria-label="Reference 2022c" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR43" id="ref-link-section-d326771894e4743" title="Zhang, Z., Meng, X., Wang, Y., et al. (2022c). Unims: A unified framework for multimodal summarization with knowledge distillation. Proceedings of the AAAI Conference on Artificial Intelligence 36(10) 11757–11764. 
                https://doi.org/10.1609/aaai.v36i10.21431
                
              ">2022c</a>), we use BLIP as the teacher model to compute the ITM score between the gold summary <span class="mathjax-tex">\(\mathcal {T}\)</span> and each image <span class="mathjax-tex">\(p_i\)</span>. The mean squared error (MSE) loss is formulated as follows:</p><div class="c-article-equation" id="Equ13"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} \mathcal {L}_p = \frac{1}{k} \sum _{i=1}^{k} (\hat{y}^p_i - \text {match}(\mathcal {T},p_i))^2. \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (13)
                </div></div><p>The final loss of our model is the linear combination of two parts of loss with hyperparameter <span class="mathjax-tex">\(\lambda \)</span> to balance their weights:</p><div class="c-article-equation" id="Equ14"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} \mathcal {L} = \lambda \mathcal {L}_s + (1-\lambda )\mathcal {L}_p. \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (14)
                </div></div></div></div></section><section data-title="Experiments setting"><div class="c-article-section" id="Sec14-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec14"><span class="c-article-section__title-number">4 </span>Experiments setting</h2><div class="c-article-section__content" id="Sec14-content"><h3 class="c-article__sub-heading" id="Sec15"><span class="c-article-section__title-number">4.1 </span>Dataset</h3><div class="c-article-table" data-container-section="table" data-test="inline-table" id="table-1"><figure><figcaption class="c-article-table__figcaption"><b data-test="table-caption" id="Tab1">Table 1 MSMO dataset statistics</b></figcaption><div class="u-text-right u-hide-print"><a aria-label="Full size table 1" class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" href="/article/10.1007/s10844-024-00886-5/tables/1" rel="nofollow"><span>Full size table</span><svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16"><use xlink:href="#icon-eds-i-chevron-right-small" xmlns:xlink="http://www.w3.org/1999/xlink"></use></svg></a></div></figure></div><p>We use the MSMO dataset build by Zhu et al. (<a aria-label="Reference 2018" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR45" id="ref-link-section-d326771894e5158" title="Zhu, J., Li, H., Liu, T., et al. (2018). MSMO: Multimodal summarization with multimodal output. In: Riloff E, Chiang D, Hockenmaier J, et al (eds) Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Brussels, Belgium, pp 4154–4164. 
                https://doi.org/10.18653/v1/D18-1448
                
              ">2018</a>) for multimodal summarization. It contains news articles from the <i>Daily Mail</i> website<sup><a href="#Fn2"><span class="u-visually-hidden">Footnote </span>2</a></sup>, each paired with corresponding images and a text summary. The dataset includes 293,965 articles for training, 10,355 articles for validation, and 10,261 articles for testing. In the test set, at most three images are annotated as reference images. Image captions are excluded from the dataset for generalization. More details are illustrated in Table <a data-track="click" data-track-action="table anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#Tab1">1</a>.</p><h3 class="c-article__sub-heading" id="Sec16"><span class="c-article-section__title-number">4.2 </span>Implementation details</h3><p>We use the roberta-base<sup><a href="#Fn3"><span class="u-visually-hidden">Footnote </span>3</a></sup> version of RoBERTa and the blip-itm-base-coco<sup><a href="#Fn4"><span class="u-visually-hidden">Footnote </span>4</a></sup> version of BLIP to implement our models in all experiments. We conducted fine-tuning on the last 4 layers of RoBERTa while keeping BLIP frozen during the training process. We trained the model on an NVIDIA GeForce RTX 3090, setting the learning rate for BERT at 5e-6 and for other parameters at 5e-5. The batch size was set to 24, and we utilized the Adam optimizer (Kingma &amp; Ba, <a aria-label="Reference 2015" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR11" id="ref-link-section-d326771894e5208" title="Kingma, D.P., &amp; Ba, J (2015). Adam: A method for stochastic optimization. In: Bengio Y, LeCun Y (Eds.), 3rd International conference on learning representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings,[SPACE]
                http://arxiv.org/abs/1412.6980
                
              ">2015</a>). The hidden state dimension <i>d</i> of the nodes was set to 256. We constrain the number of sentences and images to 50 and 16, respectively. There are 6 layers of HeterGraphormer, each layer has 32 attention heads, and the dimension of FFN is set to 512. The hyperparameter <i>a</i> is set to 10 for discretizing edge weights and <span class="mathjax-tex">\(\lambda \)</span> is set to 0.8 to balance the loss of sentence and image selection. We select the best checkpoint according to the validation loss and report the results on the test set. For sentence extraction, we rank sentences in descending order based on their scores, selecting the top-3 while filtering out those with scores below 0.185.</p><h3 class="c-article__sub-heading" id="Sec17"><span class="c-article-section__title-number">4.3 </span>Baselines</h3><p>We compare our model with existing text and multimodal summarization methods:</p><p><b>ORACLE</b> generates a summary consisting of multiple sentences, which are selected by a greedy algorithm (Nallapati et al., <a aria-label="Reference 2017" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR22" id="ref-link-section-d326771894e5248" title="Nallapati R, Zhai F, &amp; Zhou B. (2017) .Summarunner: A recurrent neural network based sequence model for extractive summarization of documents. Proceedings of the AAAI Conference on Artificial Intelligence 31(1). 
                https://doi.org/10.1609/aaai.v31i1.10958
                
              ">2017</a>) to maximize the ROUGE score against the gold summary.</p><p><b>LEAD-3</b> selects the first three sentences of the document as the text summary.</p><p><b>ATG</b>/<b>ATL</b>/<b>HAN</b> are proposed by Zhu et al. (<a aria-label="Reference 2018" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR45" id="ref-link-section-d326771894e5267" title="Zhu, J., Li, H., Liu, T., et al. (2018). MSMO: Multimodal summarization with multimodal output. In: Riloff E, Chiang D, Hockenmaier J, et al (eds) Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Brussels, Belgium, pp 4154–4164. 
                https://doi.org/10.18653/v1/D18-1448
                
              ">2018</a>), incorporating visual information into the pointer generator network (See et al., <a aria-label="Reference 2017" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR29" id="ref-link-section-d326771894e5270" title="See, A., Liu, P.J., Manning, C.D. (2017). Get to the point: Summarization with pointer-generator networks. In: Barzilay R, Kan MY (Eds.), Proceedings of the 55th annual meeting of the association for computational linguistics (Volume 1: Long Papers). Association for Computational Linguistics, Vancouver, Canada, pp 1073–1083. 
                https://doi.org/10.18653/v1/P17-1099
                
              ">2017</a>) through visual attention on global features, local features, and hierarchical local features of the images, respectively. The visual attention distributions are then utilized to select images.</p><p><b>MOF</b> (Zhu et al., <a aria-label="Reference 2020" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR46" id="ref-link-section-d326771894e5279" title="Zhu, J., Zhou, Y., Zhang, J., et al. (2020). Multimodal summarization with guidance of multimodal reference. Proceedings of the AAAI Conference on Artificial Intelligence, 34(05), 9749–975. 
                https://doi.org/10.1609/aaai.v34i05.6525
                
              ">2020</a>) introduces a multimodal objective function into ATG. Among the four MOF variants, we chose the one with the best overall performance.</p><p><b>UniMS</b> (Zhang et al., <a aria-label="Reference 2022c" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR43" id="ref-link-section-d326771894e5287" title="Zhang, Z., Meng, X., Wang, Y., et al. (2022c). Unims: A unified framework for multimodal summarization with knowledge distillation. Proceedings of the AAAI Conference on Artificial Intelligence 36(10) 11757–11764. 
                https://doi.org/10.1609/aaai.v36i10.21431
                
              ">2022c</a>) extends the text encoder of BART Lewis et al. (<a aria-label="Reference 2020" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR12" id="ref-link-section-d326771894e5290" title="Lewis, M., Liu, Y., Goyal, N., et al. (2020). BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In: Jurafsky D, Chai J, Schluter N, et al (Eds.), Proceedings of the 58th annual meeting of the association for computational linguistics. Association for Computational Linguistics, Online, pp 7871–7880. 
                https://doi.org/10.18653/v1/2020.acl-main.703
                
              ">2020</a>) to a multimodal encoder and utilizes knowledge distillation to help image selection.</p><p><b>SITA</b> (Jiang et al., <a aria-label="Reference 2023" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR8" id="ref-link-section-d326771894e5298" title="Jiang, C., Xie, R., Ye, W., et al. (2023). Exploiting pseudo image captions for multimodal summarization. In: Rogers A, Boyd-Graber J, Okazaki N (Eds.), Findings of the association for computational linguistics: ACL 2023. Association for Computational Linguistics, Toronto, Canada, pp 161–175. 
                https://doi.org/10.18653/v1/2023.findings-acl.12
                
              ">2023</a>) selects sentences most relevant to the images, using them as additional input instead of utilizing image information directly. The selection of images is based on the ROUGE-L of these sentences.</p><p><b>HGMS(GAT)</b>: To compare the performance gap between HeterGraphormer and GAT (Velickovic et al., <a aria-label="Reference 2017" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR34" id="ref-link-section-d326771894e5306" title="Velickovic, P., Cucurull, G., Casanova, A., et al. (2017). Graph attention networks. stat, 1050(20), 10–48550.">2017</a>), we implemented a GAT-based HGMS. Following the method proposed by Wang et al. (<a aria-label="Reference 2020" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR35" id="ref-link-section-d326771894e5309" title="Wang, D., Liu, P., Zheng, Y., et al. (2020). Heterogeneous graph neural networks for extractive document summarization. In: Jurafsky D, Chai J, Schluter N, et al (Eds.), Proceedings of the 58th annual meeting of the association for computational linguistics. Association for Computational Linguistics, Online, pp 6209–6219. 
                https://doi.org/10.18653/v1/2020.acl-main.553
                
              ">2020</a>), the GAT incorporates considerations for both neighboring nodes and edge weights in updating nodes, and the number of layers is 2.</p><h3 class="c-article__sub-heading" id="Sec18"><span class="c-article-section__title-number">4.4 </span>Evaluation metrics</h3><p>The quality of generated textual summary is evaluated by <b>ROUGE</b>. For image selection, image precision(<b>IP</b>) indicates whether the chosen image is present in the image references. <span class="mathjax-tex">\({\textbf {M}}_{\text {sim}}\)</span> is an image-text relevance metric which calculates the maximum similarity between the image and each selected sentence by cross-modal retrieval model. For details regarding IP and <span class="mathjax-tex">\(\text {M}_{\text {sim}}\)</span>, please refer to the work of Zhu et al. (<a aria-label="Reference 2018" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR45" id="ref-link-section-d326771894e5371" title="Zhu, J., Li, H., Liu, T., et al. (2018). MSMO: Multimodal summarization with multimodal output. In: Riloff E, Chiang D, Hockenmaier J, et al (eds) Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Brussels, Belgium, pp 4154–4164. 
                https://doi.org/10.18653/v1/D18-1448
                
              ">2018</a>).</p></div></div></section><section data-title="Results and analysis"><div class="c-article-section" id="Sec19-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec19"><span class="c-article-section__title-number">5 </span>Results and analysis</h2><div class="c-article-section__content" id="Sec19-content"><h3 class="c-article__sub-heading" id="Sec20"><span class="c-article-section__title-number">5.1 </span>Automatic evaluation</h3><div class="c-article-table" data-container-section="table" data-test="inline-table" id="table-2"><figure><figcaption class="c-article-table__figcaption"><b data-test="table-caption" id="Tab2">Table 2 Automatic evaluation on MSMO</b></figcaption><div class="u-text-right u-hide-print"><a aria-label="Full size table 2" class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" href="/article/10.1007/s10844-024-00886-5/tables/2" rel="nofollow"><span>Full size table</span><svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16"><use xlink:href="#icon-eds-i-chevron-right-small" xmlns:xlink="http://www.w3.org/1999/xlink"></use></svg></a></div></figure></div><p>The performance of baseline methods and our models is shown in Table <a data-track="click" data-track-action="table anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#Tab2">2</a>. The first block contains the ORACLE upper bound and LEAD-3 baseline, and the second block includes the multimodal summarization models. We present our models in the third block.</p><p>Compared to the baseline methods, our HGMS model has demonstrated significant improvements across all metrics, including text summarization quality, image selection precision, image-text relevance, and the overall quality of multimodal summary.</p><p>Regarding the image selection precision metric (IP), UniMS achieves a noteworthy advantage over its predecessor baselines by distilling knowledge within a vision-language pre-trained model. SITA achieves further improvements by leveraging sentences most relevant to images in the text as cues for image selection. By leveraging the inherent many-to-many relationships between sentences and images, our model achieves a significant performance boost compared to SITA (e.g., 79.72 vs. 76.41 in the IP metric).</p><p>In terms of the <span class="mathjax-tex">\(\text {M}_{\text {sim}}\)</span> metric for evaluating the relevance between sentences and images, our model achieved a performance improvement of over 40% compared to SITA (e.g., 46.56 vs. 33.47 of <span class="mathjax-tex">\(\text {M}_{\text {sim}}\)</span>). This is mainly attributed to the introduction of ITM scores as weights for the edges between sentences and images, making the model more inclined to select relevant sentence-image pairs.</p><p>Concerning text summarization quality metrics, our model exhibits notable improvements in both Rouge1 and Rouge2 (e.g., an increase of 1.32 and 1.05, respectively, compared to SITA). However, Rouge-L shows a marginal improvement of only 0.05. This indicates that the introduction of word nodes makes the model focus more on word-level semantic information.</p><p>For different graph encoders, HGMS based on HeterGraphormer outperforms HGMS im-plemented with GAT across all metrics. This suggests that HeterGraphormer has a significant advantage in more effectively modeling intricate relationships between nodes.</p><div class="c-article-table" data-container-section="table" data-test="inline-table" id="table-3"><figure><figcaption class="c-article-table__figcaption"><b data-test="table-caption" id="Tab3">Table 3 Ablation studies on the test set</b></figcaption><div class="u-text-right u-hide-print"><a aria-label="Full size table 3" class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" href="/article/10.1007/s10844-024-00886-5/tables/3" rel="nofollow"><span>Full size table</span><svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16"><use xlink:href="#icon-eds-i-chevron-right-small" xmlns:xlink="http://www.w3.org/1999/xlink"></use></svg></a></div></figure></div><h3 class="c-article__sub-heading" id="Sec21"><span class="c-article-section__title-number">5.2 </span>Ablation study</h3><p>We perform the ablation study to investigate the potential influence of different components. As shown in Table <a data-track="click" data-track-action="table anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#Tab3">3</a>, we design the following experimental settings: </p><ol class="u-list-style-none">
<li>
<span class="u-custom-list-number">(1)</span>
<p>We retain the centrality embedding used to represent node degree information in Graphormer.</p>
</li>
<li>
<span class="u-custom-list-number">(2)</span>
<p>The type embedding used to differentiate between different node types was removed.</p>
</li>
<li>
<span class="u-custom-list-number">(3)</span>
<p>We employ identical spatial and edge embeddings for each heterogeneous connection.</p>
</li>
<li>
<span class="u-custom-list-number">(4)</span>
<p>During the update of node features, we no longer consider heterogeneous nodes without connections. Sentence-image pairs are only connected when the ITM score is greater than 0.5.</p>
</li>
<li>
<span class="u-custom-list-number">(5)</span>
<p>We changed the method of calculating the weights between nodes from a product approach to averaging.</p>
</li>
<li>
<span class="u-custom-list-number">(6)</span>
<p>We removed the TF-IDF and ITM weights on the edges used to distinguish the importance of connections.</p>
</li>
<li>
<span class="u-custom-list-number">(7)</span>
<p>We replaced the BLIP weights used in the model with CLIP(Radford et al., <a aria-label="Reference 2021" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR28" id="ref-link-section-d326771894e6275" title="Radford A, Kim JW, Hallacy C, et al (2021) Learning transferable visual models from natural language supervision. In: Meila M, Zhang T (Eds.), Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, pp 8748–8763. 
                http://proceedings.mlr.press/v139/radford21a.html
                
              ">2021</a>).</p>
</li>
<li>
<span class="u-custom-list-number">(8)</span>
<p>The pre-trained RoBERTa was replaced with a bidirectional LSTM.</p>
</li>
</ol><p>After the removal of edge weights, there is a notable decrease in the IP metric, indicating that the ITM scores on the edges between sentences and images play a crucial role in image selection. Regarding the enhancements made to Graphormer (from 1 to 3), the experimental results substantiate the effectiveness of these enhancements.</p><div class="c-article-table" data-container-section="table" data-test="inline-table" id="table-4"><figure><figcaption class="c-article-table__figcaption"><b data-test="table-caption" id="Tab4">Table 4 Experimental results with different methods of building image references</b></figcaption><div class="u-text-right u-hide-print"><a aria-label="Full size table 4" class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" href="/article/10.1007/s10844-024-00886-5/tables/4" rel="nofollow"><span>Full size table</span><svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16"><use xlink:href="#icon-eds-i-chevron-right-small" xmlns:xlink="http://www.w3.org/1999/xlink"></use></svg></a></div></figure></div><h3 class="c-article__sub-heading" id="Sec22"><span class="c-article-section__title-number">5.3 </span>Image reference</h3><p>To investigate the impact of employing diverse image reference strategies during model training, we additionally incorporated a strategy named ROUGE-Top. This method selects an image with a caption that has the highest ROUGE score for each article as the reference image. As shown in Table <a data-track="click" data-track-action="table anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#Tab4">4</a>, compared to the knowledge distillation method, the IP score of the ROUGE-Top strategy has significantly increased by 3.69, indicating the effectiveness of this strategy. It is noteworthy that the IP score achieved through the ROUGE-Top strategy significantly surpassed its label upper bound (79.89), indicating the model’s success in leveraging the intricate relationships between sentences and images to enhance its capability in selecting critical images. However, as the ROUGE-Top strategy heavily relies on the quality of image captions, for the sake of model generalization, we choose to employ knowledge distillation.</p><h3 class="c-article__sub-heading" id="Sec23"><span class="c-article-section__title-number">5.4 </span>Layer number analysis</h3><p>In order to further analyze the advantages of HeterGraphormer over GAT, we conducted a study on the performance of the models at different numbers of layers. As illustrated in Fig. <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#Fig3">3</a>, we compare the Rouge scores of HGMS using HeterGraphormer and GAT as encoders across different layer depths. We observed that with an increase in the number of layers, the performance gap between HeterGraphormer and GAT gradually widened. hough we introduced residual connections between layers in GAT to alleviate the issue of over-smoothing, the optimal number of layers for GAT was only 2, beyond which the performance started to decline. In contrast, HeterGraphormer, with its ability to attend to all nodes during node updates, remained unaffected by over-smoothing issue. With increasing layer depth, its performance also improves, stabilizing after 6 layers. This suggests that HeterGraphormer can enhance its ability to model intricate relationships by increasing the number of layers.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-container-section="figure" data-test="figure" data-title="Fig. 3" id="figure-3"><figure><figcaption><b class="c-article-section__figure-caption" data-test="figure-caption-text" id="Fig3">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-action="view figure" data-track-label="image" href="/article/10.1007/s10844-024-00886-5/figures/3" rel="nofollow"><picture><source srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10844-024-00886-5/MediaObjects/10844_2024_886_Fig3_HTML.png?as=webp" type="image/webp"/><img alt="figure 3" aria-describedby="Fig3" height="493" loading="lazy" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10844-024-00886-5/MediaObjects/10844_2024_886_Fig3_HTML.png" width="685"/></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>The ROUGE scores on validation set when the number of layers increases. <span class="mathjax-tex">\(\hat{R}\)</span> is the mean of R-1, R-2 and R-L</p></div></div><div class="u-text-right u-hide-print"><a aria-label="Full size image figure 3" class="c-article__pill-button" data-test="article-link" data-track="click" data-track-action="view figure" data-track-dest="link:Figure3 Full size image" data-track-label="button" href="/article/10.1007/s10844-024-00886-5/figures/3" rel="nofollow"><span>Full size image</span><svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16"><use xlink:href="#icon-eds-i-chevron-right-small" xmlns:xlink="http://www.w3.org/1999/xlink"></use></svg></a></div></figure></div><div class="c-article-section__figure js-c-reading-companion-figures-item" data-container-section="figure" data-test="figure" data-title="Fig. 4" id="figure-4"><figure><figcaption><b class="c-article-section__figure-caption" data-test="figure-caption-text" id="Fig4">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-action="view figure" data-track-label="image" href="/article/10.1007/s10844-024-00886-5/figures/4" rel="nofollow"><picture><source srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10844-024-00886-5/MediaObjects/10844_2024_886_Fig4_HTML.png?as=webp" type="image/webp"/><img alt="figure 4" aria-describedby="Fig4" height="497" loading="lazy" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10844-024-00886-5/MediaObjects/10844_2024_886_Fig4_HTML.png" width="685"/></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>Visualization of sentence and image embeddings for 100 documents in the test set</p></div></div><div class="u-text-right u-hide-print"><a aria-label="Full size image figure 4" class="c-article__pill-button" data-test="article-link" data-track="click" data-track-action="view figure" data-track-dest="link:Figure4 Full size image" data-track-label="button" href="/article/10.1007/s10844-024-00886-5/figures/4" rel="nofollow"><span>Full size image</span><svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16"><use xlink:href="#icon-eds-i-chevron-right-small" xmlns:xlink="http://www.w3.org/1999/xlink"></use></svg></a></div></figure></div><h3 class="c-article__sub-heading" id="Sec24"><span class="c-article-section__title-number">5.5 </span>Embedding analysis</h3><p>To explore the sentence and image embeddings generated by HGMS, we employ the t-SNE (van der Maaten &amp; Hinton, <a aria-label="Reference 2008" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR20" id="ref-link-section-d326771894e6502" title="van der Maaten L, &amp; Hinton G (2008) Visualizing data using t-sne. Journal of Machine Learning Research 9(86), 2579–2605. 
                http://jmlr.org/papers/v9/vandermaaten08a.html
                
              ">2008</a>) algorithm to map the embeddings of the final layer outputs from HeterGraphormer to a two-dimensional space for visualization. We randomly selected 100 documents from the test set, comprising approximately 2500 sentences and 600 images. As illustrated in Fig. <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#Fig4">4</a>, we use blue and gray points to represent ground truth and non-ground truth sentences, respectively, while yellow and black points denote reference and non-reference images, respectively.</p><p>It can be observed that ground truth sentences exhibit a clustering tendency in the bottom-right area, indicating the model’s effectiveness in distinguishing salient sentences. Notably, compared to reference images, non-reference images show a higher degree of clustering, indicating that the model has better recognition ability for irrelevant images.</p><h3 class="c-article__sub-heading" id="Sec25"><span class="c-article-section__title-number">5.6 </span>Human evaluation</h3><p>In addition to the automatic evaluations, we conducted human evaluations to assess the performance of models. Following Wu et al. (<a aria-label="Reference 2021" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR36" id="ref-link-section-d326771894e6519" title="Wu W, Li W, Xiao X, et al. (2021). BASS: Boosting abstractive summarization with unified semantic graph. In: Proceedings of the 59th annual meeting of the association for computational linguistics and the 11th international joint conference on natural language processing (Volume 1: Long Papers). Association for Computational Linguistics, Online, pp 6052–6067.
                https://doi.org/10.18653/v1/2021.acl-long.472
                
              ">2021</a>), we randomly select 50 samples from the test set of MSMO dataset. Each sample is independently annotated by three different participants, and the rating is the average of their evaluations. Annotators evaluate the overall quality of summaries by ranking them based on the following three criteria simultaneously: (1) <i>Informativeness</i>: Does the summary effectively convey the crucial information from the source document? (2) <i>Succinctness</i>: Is the summary succinct, avoiding unnecessary details? (3) <i>Relevance</i>: Does the selected image match the textual summary? To further distinguish the differences between models, especially when their average rankings are similar, we quantified the rankings. Rankings range from 1 to 4, allowing ties, with scores corresponding to 2, 1, -1, -2, respectively. The final rating for each model is determined by averaging the scores across all test samples.</p><div class="c-article-table" data-container-section="table" data-test="inline-table" id="table-5"><figure><figcaption class="c-article-table__figcaption"><b data-test="table-caption" id="Tab5">Table 5 Ranking results of system summaries by human evaluation</b></figcaption><div class="u-text-right u-hide-print"><a aria-label="Full size table 5" class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" href="/article/10.1007/s10844-024-00886-5/tables/5" rel="nofollow"><span>Full size table</span><svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16"><use xlink:href="#icon-eds-i-chevron-right-small" xmlns:xlink="http://www.w3.org/1999/xlink"></use></svg></a></div></figure></div><div class="c-article-table" data-container-section="table" data-test="inline-table" id="table-6"><figure><figcaption class="c-article-table__figcaption"><b data-test="table-caption" id="Tab6">Table 6 An example of multimodal summarization generation</b></figcaption><div class="u-text-right u-hide-print"><a aria-label="Full size table 6" class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" href="/article/10.1007/s10844-024-00886-5/tables/6" rel="nofollow"><span>Full size table</span><svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16"><use xlink:href="#icon-eds-i-chevron-right-small" xmlns:xlink="http://www.w3.org/1999/xlink"></use></svg></a></div></figure></div><p>Table <a data-track="click" data-track-action="table anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#Tab5">5</a> presents the human evaluation results on three strong baseline models and our proposed HGMS. The agreement between the annotators was calculated using Fleiss’ Kappa, with a value of 0.538. The results show that HGMS is able to generate higher quality multi-modal summaries compared to other models. Additionally, the human evaluation results further validate the effectiveness of our proposed HeterGraphormer.</p><h3 class="c-article__sub-heading" id="Sec26"><span class="c-article-section__title-number">5.7 </span>Case study</h3><p>Table <a data-track="click" data-track-action="table anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#Tab6">6</a> presents a typical example selected from test set consisting of the input article, the gold summary, the summary generated by the strong baseline SITA, and the summary produced by our proposed HGMS model. The highlighted text corresponds to entities present in the image. As BLIP does not have the ability to detect entities, these entities were manually annotated. Notably, the summary output by our model shows high similarity to the gold summary, effectively summarizing key information related to ”islands” and ”cruise ships”, while accurately selecting the image most relevant to these themes. Compared to the SITA model, which uses Rouge scores between pseudo image captions and textual summary for image selection, our model can select critical images more accurately.</p><p>Additionally, Table <a data-track="click" data-track-action="table anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#Tab7">7</a> present the ITM scores calculated using BLIP between sentences (S1, S2, S3, S4) and images (IMG1, IMG2). It is evident that the many-to-many relationships between sentences and images play an important role in accurately extracting salient sentences and critical images. This further validates the effectiveness of the proposed graph structure.</p><div class="c-article-table" data-container-section="table" data-test="inline-table" id="table-7"><figure><figcaption class="c-article-table__figcaption"><b data-test="table-caption" id="Tab7">Table 7 The matching scores between the images and the sentences</b></figcaption><div class="u-text-right u-hide-print"><a aria-label="Full size table 7" class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" href="/article/10.1007/s10844-024-00886-5/tables/7" rel="nofollow"><span>Full size table</span><svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16"><use xlink:href="#icon-eds-i-chevron-right-small" xmlns:xlink="http://www.w3.org/1999/xlink"></use></svg></a></div></figure></div></div></div></section><section data-title="Conclusion"><div class="c-article-section" id="Sec27-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec27"><span class="c-article-section__title-number">6 </span>Conclusion</h2><div class="c-article-section__content" id="Sec27-content"><p>This paper presents HGMS for multimodal summarization. HGMS constructs a multimodal heterogeneous graph by considering both related and unrelated relationships between modalities, and applies HeterGraphormer on the graph to effectively model intermodal relationships. Experimental results show HGMS outperforms the current state-of-the-art model on MSMO datasets, which demonstrates the effectiveness of our method. Furthermore, our proposed HeterGraphormer can be extended to other NLP tasks, offering potential performance improvements for such tasks.</p></div></div></section><section data-title="Limitations"><div class="c-article-section" id="Sec28-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec28"><span class="c-article-section__title-number">7 </span>Limitations</h2><div class="c-article-section__content" id="Sec28-content"><p>Due to the utilization of Byte Pair Encoding (BPE) tokenization in the pre-trained language model, we opt to employ it only as a sentence encoder, without leveraging its word-level features. This decision stems from the inherent characteristics of BPE tokenization, which may result in the establishment of connections between originally unrelated sentence nodes after tokenization.</p><p>Another limitation of this study is the lack of diverse datasets. Currently, only one publicly available dataset is accessible for use. Thus, the analysis and conclusions of our study may be influenced by the limitations of the dataset. Future research could address this limitation by gathering additional datasets to enhance the reliability and generalizability of the study findings.</p></div></div></section>
</div>
<section data-title="Data Availability"><div class="c-article-section" id="data-availability-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="data-availability">Data Availability</h2><div class="c-article-section__content" id="data-availability-content">
<p>No datasets were generated or analysed during the current study.</p>
</div></div></section><section data-title="Code Availability"><div class="c-article-section" id="code-availability-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="code-availability">Code Availability</h2><div class="c-article-section__content" id="code-availability-content">
<p>Code is available at <a href="https://github.com/anonymous272218/HGMS">https://github.com/anonymous272218/HGMS</a></p>
</div></div></section><section data-title="Notes"><div class="c-article-section" id="notes-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="notes">Notes</h2><div class="c-article-section__content" id="notes-content"><ol class="c-article-footnote c-article-footnote--listed"><li class="c-article-footnote--listed__item" data-counter="1." id="Fn1"><div class="c-article-footnote--listed__content"><p>Gold summary refers to the ideal, human-generated summary that serves as a standard for evaluating the quality of automated text summarization systems.</p></div></li><li class="c-article-footnote--listed__item" data-counter="2." id="Fn2"><div class="c-article-footnote--listed__content"><p><a href="http://www.dailymail.co.uk">http://www.dailymail.co.uk</a></p></div></li><li class="c-article-footnote--listed__item" data-counter="3." id="Fn3"><div class="c-article-footnote--listed__content"><p><a href="https://huggingface.co/FacebookAI">https://huggingface.co/FacebookAI</a></p></div></li><li class="c-article-footnote--listed__item" data-counter="4." id="Fn4"><div class="c-article-footnote--listed__content"><p><a href="https://huggingface.co/Salesforce">https://huggingface.co/Salesforce</a></p></div></li></ol></div></div></section><div id="MagazineFulltextArticleBodySuffix"><section aria-labelledby="Bib1" data-title="References"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ul class="c-article-references" data-track-component="outbound reference" data-track-context="references section"><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR1">An, C., Zhong, M., Wu, Z., et al. (2022). CoLo: A contrastive learning based re-ranking framework for one-stage summarization. In: Calzolari N, Huang CR, Kim H, et al (Eds.), Proceedings of the 29th international conference on computational linguistics. <i>International Committee on Computational Linguistics</i>, Gyeongju, Republic of Korea, pp 5783–5793. <a data-track="click_references" data-track-action="external reference" data-track-label="https://aclanthology.org/2022.coling-1.508" data-track-value="external reference" href="https://aclanthology.org/2022.coling-1.508">https://aclanthology.org/2022.coling-1.508</a></p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR2">Chen, J., &amp; Zhuge, H. (2018). Abstractive text-image summarization using multi-modal attentional hierarchical RNN. In: Riloff E, Chiang D, Hockenmaier J, et al (Eds.), Proceedings of the 2018 conference on empirical methods in natural language processing. <i>Association for Computational Linguistics,</i> Brussels, Belgium, pp 4046–4056. <a data-track="click_references" data-track-action="external reference" data-track-label="10.18653/v1/D18-1438" data-track-value="external reference" href="https://doi.org/10.18653/v1/D18-1438">https://doi.org/10.18653/v1/D18-1438</a></p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR3">Cheng, J., &amp; Lapata, M. (2016). Neural summarization by extracting sentences and words. In: Erk K, Smith NA (Eds.), Proceedings of the 54th annual meeting of the association for computational linguistics (Volume 1: Long Papers). <i>Association for Computational Linguistics,</i> Berlin, Germany, pp 484–494. <a data-track="click_references" data-track-action="external reference" data-track-label="10.18653/v1/P16-1046" data-track-value="external reference" href="https://doi.org/10.18653/v1/P16-1046">https://doi.org/10.18653/v1/P16-1046</a></p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR4">Cheng, X., Shen, Y., &amp; Lu, W. (2023). A set prediction network for extractive summarization. In: Rogers A, Boyd-Graber J, Okazaki N (Eds.), Findings of the association for computational linguistics: ACL 2023. <i>Association for Computational Linguistics,</i> Toronto, Canada, pp 4766–4777. <a data-track="click_references" data-track-action="external reference" data-track-label="10.18653/v1/2023.findings-acl.293" data-track-value="external reference" href="https://doi.org/10.18653/v1/2023.findings-acl.293">https://doi.org/10.18653/v1/2023.findings-acl.293</a></p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR5">Cui, P., Hu, L., &amp; Liu, Y. (2020). Enhancing extractive text summarization with topic-aware graph neural networks. In: Scott D, Bel N, Zong C (Eds.), Proceedings of the 28th international conference on computational linguistics. <i>International Committee on Computational Linguistics,</i> Barcelona, Spain (Online), pp 5360–537. <a data-track="click_references" data-track-action="external reference" data-track-label="10.18653/v1/2020.coling-main.468" data-track-value="external reference" href="https://doi.org/10.18653/v1/2020.coling-main.468">https://doi.org/10.18653/v1/2020.coling-main.468</a></p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR6">Erkan, G., &amp; Radev, D. R. (2004). Lexrank: Graph-based lexical centrality as salience in text summarization. <i>J Artif Intell Res,</i> <i>22</i>, 457–47. <a data-track="click_references" data-track-action="external reference" data-track-label="10.1613/JAIR.1523" data-track-value="external reference" href="https://doi.org/10.1613/JAIR.1523">https://doi.org/10.1613/JAIR.1523</a></p><p class="c-article-references__links u-hide-print"><a aria-label="Article reference 6" data-doi="10.1613/JAIR.1523" data-track="click_references" data-track-action="article reference" data-track-item_id="10.1613/JAIR.1523" data-track-label="10.1613/JAIR.1523" data-track-value="article reference" href="https://doi.org/10.1613%2FJAIR.1523" rel="nofollow noopener">Article</a> 
    <a aria-label="Google Scholar reference 6" data-track="click_references" data-track-action="google scholar reference" data-track-item_id="link" data-track-label="link" data-track-value="google scholar reference" href="http://scholar.google.com/scholar_lookup?&amp;title=Lexrank%3A%20Graph-based%20lexical%20centrality%20as%20salience%20in%20text%20summarization&amp;journal=J%20Artif%20Intell%20Res&amp;doi=10.1613%2FJAIR.1523&amp;volume=22&amp;pages=457-47&amp;publication_year=2004&amp;author=Erkan%2CG&amp;author=Radev%2CDR" rel="nofollow noopener">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR7">Jia, R., Cao, Y., Tang, H., et al. (2020). Neural extractive summarization with hierarchical attentive heterogeneous graph network. In: Webber B, Cohn T, He Y, et al (Eds.), Proceedings of the 2020 conference on Empirical Methods in Natural Language Processing (EMNLP). <i>Association for Computational Linguistics,</i> Online, pp 3622–363. <a data-track="click_references" data-track-action="external reference" data-track-label="10.18653/v1/2020.emnlp-main.295" data-track-value="external reference" href="https://doi.org/10.18653/v1/2020.emnlp-main.295">https://doi.org/10.18653/v1/2020.emnlp-main.295</a></p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR8">Jiang, C., Xie, R., Ye, W., et al. (2023). Exploiting pseudo image captions for multimodal summarization. In: Rogers A, Boyd-Graber J, Okazaki N (Eds.), Findings of the association for computational linguistics: ACL 2023. <i>Association for Computational Linguistics,</i> Toronto, Canada, pp 161–175. <a data-track="click_references" data-track-action="external reference" data-track-label="10.18653/v1/2023.findings-acl.12" data-track-value="external reference" href="https://doi.org/10.18653/v1/2023.findings-acl.12">https://doi.org/10.18653/v1/2023.findings-acl.12</a></p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR9">Jin, L., &amp; Chen, J. (2024). Self-supervised opinion summarization with multi-modal knowledge graph. <i>J Intell Inf Syst,</i> <i>62</i>(1), 191–208. <a data-track="click_references" data-track-action="external reference" data-track-label="10.1007/S10844-023-00812-1" data-track-value="external reference" href="https://doi.org/10.1007/S10844-023-00812-1">https://doi.org/10.1007/S10844-023-00812-1</a></p><p class="c-article-references__links u-hide-print"><a aria-label="Article reference 9" data-doi="10.1007/S10844-023-00812-1" data-track="click_references" data-track-action="article reference" data-track-item_id="10.1007/S10844-023-00812-1" data-track-label="10.1007/S10844-023-00812-1" data-track-value="article reference" href="https://link.springer.com/doi/10.1007/S10844-023-00812-1" rel="noopener">Article</a> 
    <a aria-label="MathSciNet reference 9" data-track="click_references" data-track-action="mathscinet reference" data-track-item_id="link" data-track-label="link" data-track-value="mathscinet reference" href="http://www.ams.org/mathscinet-getitem?mr=4702630" rel="nofollow noopener">MathSciNet</a> 
    <a aria-label="Google Scholar reference 9" data-track="click_references" data-track-action="google scholar reference" data-track-item_id="link" data-track-label="link" data-track-value="google scholar reference" href="http://scholar.google.com/scholar_lookup?&amp;title=Self-supervised%20opinion%20summarization%20with%20multi-modal%20knowledge%20graph&amp;journal=J%20Intell%20Inf%20Syst&amp;doi=10.1007%2FS10844-023-00812-1&amp;volume=62&amp;issue=1&amp;pages=191-208&amp;publication_year=2024&amp;author=Jin%2CL&amp;author=Chen%2CJ" rel="nofollow noopener">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR10">Jing, B., You, Z., Yang, T., et al. (2021). Multiplex graph neural network for extractive text summarization. In: Moens MF, Huang X, Specia L, et al. (Eds.), Proceedings of the 2021 conference on empirical methods in natural language processing. <i>Association for Computational Linguistics,</i> Online and Punta Cana, Dominican Republic, pp 133–139. <a data-track="click_references" data-track-action="external reference" data-track-label="10.18653/v1/2021.emnlp-main.11" data-track-value="external reference" href="https://doi.org/10.18653/v1/2021.emnlp-main.11">https://doi.org/10.18653/v1/2021.emnlp-main.11</a></p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR11">Kingma, D.P., &amp; Ba, J (2015). Adam: A method for stochastic optimization. In: Bengio Y, LeCun Y (Eds.), 3rd International conference on learning representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, <i>Conference Track Proceedings,</i>[SPACE]<a data-track="click_references" data-track-action="external reference" data-track-label="http://arxiv.org/abs/1412.6980" data-track-value="external reference" href="http://arxiv.org/abs/1412.6980">http://arxiv.org/abs/1412.6980</a></p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR12">Lewis, M., Liu, Y., Goyal, N., et al. (2020). BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In: Jurafsky D, Chai J, Schluter N, et al (Eds.), Proceedings of the 58th annual meeting of the association for computational linguistics. <i>Association for Computational Linguistics,</i> Online, pp 7871–7880. <a data-track="click_references" data-track-action="external reference" data-track-label="10.18653/v1/2020.acl-main.703" data-track-value="external reference" href="https://doi.org/10.18653/v1/2020.acl-main.703">https://doi.org/10.18653/v1/2020.acl-main.703</a></p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR13">Li, H., Zhu, J., Liu, T., et al. (2018). Multi-modal sentence summarization with modality attention and image filtering. In: Lang J (Ed.), Proceedings of the twenty-seventh international joint conference on artificial intelligence, IJCAI 2018, July 13-19, 2018, Stockholm, Sweden. ijcai.org, Stockholm, Sweden, pp 4152–4158. <a data-track="click_references" data-track-action="external reference" data-track-label="10.24963/ijcai.2018/577" data-track-value="external reference" href="https://doi.org/10.24963/ijcai.2018/577">https://doi.org/10.24963/ijcai.2018/577</a></p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR14">Li, J., Li, D., Xiong, C., et al. (2022). BLIP: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In: Chaudhuri K, Jegelka S, Song L, et al (Eds.), International conference on machine learning, ICML 2022, 17-23 July 2022, PMLR, Baltimore, Maryland, USA, <i>Proceedings of Machine Learning Research,</i> <i>162</i> pp 12888–12900</p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR15">Li, M., Chen, X., Gao, S., et al (2020) VMSMO: Learning to generate multimodal summary for video-based news articles. In: Webber B, Cohn T, He Y, et al (Eds.), Proceedings of the 2020 conference on Empirical Methods in Natural Language Processing (EMNLP). <i>Association for Computational Linguistics,</i> Online, pp 9360–9369. <a data-track="click_references" data-track-action="external reference" data-track-label="10.18653/v1/2020.emnlp-main.752" data-track-value="external reference" href="https://doi.org/10.18653/v1/2020.emnlp-main.752">https://doi.org/10.18653/v1/2020.emnlp-main.752</a></p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR16">Liang, Y., Meng, F., Xu, J., et al. (2023). Summary-oriented vision modeling for multimodal abstractive summarization. In: Rogers A, Boyd-Graber J, Okazaki N (Eds.), Proceedings of the 61st annual meeting of the association for computational linguistics (Volume 1: Long Papers). <i>Association for Computational Linguistics,</i> Toronto, Canada, pp 2934–2951. <a data-track="click_references" data-track-action="external reference" data-track-label="10.18653/v1/2023.acl-long.165" data-track-value="external reference" href="https://doi.org/10.18653/v1/2023.acl-long.165">https://doi.org/10.18653/v1/2023.acl-long.165</a></p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR17">Liu Y, &amp; Lapata M. (2019). Text summarization with pretrained encoders. In: Inui K, Jiang J, Ng V, et al (Eds.), Proceedings of the 2019 conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). <i>Association for Computational Linguistics,</i> Hong Kong, China, pp 3730–3740. <a data-track="click_references" data-track-action="external reference" data-track-label="10.18653/v1/D19-1387" data-track-value="external reference" href="https://doi.org/10.18653/v1/D19-1387">https://doi.org/10.18653/v1/D19-1387</a></p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR18">Liu, Y., Ott, M., Goyal, N., et al. (2019). Roberta: A robustly optimized BERT pretraining approach. <a data-track="click_references" data-track-action="external reference" data-track-label="http://arxiv.org/abs/1907.11692" data-track-value="external reference" href="http://arxiv.org/abs/1907.11692">arXiv:1907.11692</a><a data-track="click_references" data-track-action="external reference" data-track-label="https://arxiv.org/abs/1907.11692" data-track-value="external reference" href="https://arxiv.org/abs/1907.11692">https://arxiv.org/abs/1907.11692</a></p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR19">Liu, Y., Liu, P., Radev, D., et al. (2022). BRIO: Bringing order to abstractive summarization. In: Muresan S, Nakov P, Villavicencio A (Eds.), Proceedings of the 60th annual meeting of the association for computational linguistics (Volume 1: Long Papers). <i>Association for Computational Linguistics,</i> Dublin, Ireland, pp 2890–2903. <a data-track="click_references" data-track-action="external reference" data-track-label="10.18653/v1/2022.acl-long.207" data-track-value="external reference" href="https://doi.org/10.18653/v1/2022.acl-long.207">https://doi.org/10.18653/v1/2022.acl-long.207</a></p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR20">van der Maaten L, &amp; Hinton G (2008) Visualizing data using t-sne. <i>Journal of Machine Learning Research</i> <i>9</i>(86), 2579–2605. <a data-track="click_references" data-track-action="external reference" data-track-label="http://jmlr.org/papers/v9/vandermaaten08a.html" data-track-value="external reference" href="http://jmlr.org/papers/v9/vandermaaten08a.html">http://jmlr.org/papers/v9/vandermaaten08a.html</a></p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR21">Mihalcea, R., Tarau, P (2004) TextRank: Bringing order into text. In: Lin D, Wu D (Eds.), Proceedings of the 2004 conference on empirical methods in natural language processing. <i>Association for Computational Linguistics,</i> Barcelona, Spain, pp 404–411. <a data-track="click_references" data-track-action="external reference" data-track-label="https://aclanthology.org/W04-3252" data-track-value="external reference" href="https://aclanthology.org/W04-3252">https://aclanthology.org/W04-3252</a></p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR22">Nallapati R, Zhai F, &amp; Zhou B. (2017) .Summarunner: A recurrent neural network based sequence model for extractive summarization of documents. <i>Proceedings of the AAAI Conference on Artificial Intelligence</i> <i>31</i>(1). <a data-track="click_references" data-track-action="external reference" data-track-label="10.1609/aaai.v31i1.10958" data-track-value="external reference" href="https://doi.org/10.1609/aaai.v31i1.10958">https://doi.org/10.1609/aaai.v31i1.10958</a></p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR23">Narayan S, Cohen SB, &amp; Lapata M. (2018). Ranking sentences for extractive summarization with reinforcement learning. In: Walker M, Ji H, Stent A (Eds.) Proceedings of the 2018 conference of the North American chapter of the association for computational linguistics: Human language technologies, Volume 1 (Long Papers). <i>Association for Computational Linguistics,</i> New Orleans, Louisiana, pp 1747–1759. <a data-track="click_references" data-track-action="external reference" data-track-label="10.18653/v1/N18-1158" data-track-value="external reference" href="https://doi.org/10.18653/v1/N18-1158">https://doi.org/10.18653/v1/N18-1158</a></p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR24">Overbay, K., Ahn, J., Pesaran zadeh, F., et al. (2023). mRedditSum: A multimodal abstractive summarization dataset of Reddit threads with images. In: Bouamor H, Pino J, Bali K (Eds.), Proceedings of the 2023 conference on empirical methods in natural language processing. <i>Association for Computational Linguistics,</i> Singapore, pp 4117–4132. <a data-track="click_references" data-track-action="external reference" data-track-label="10.18653/v1/2023.emnlp-main.251" data-track-value="external reference" href="https://doi.org/10.18653/v1/2023.emnlp-main.251">https://doi.org/10.18653/v1/2023.emnlp-main.251</a></p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR25">Paulus, R., Xiong, C., &amp; Socher, R. (2018). A deep reinforced model for abstractive summarization. In: International conference on learning representations, <a data-track="click_references" data-track-action="external reference" data-track-label="https://openreview.net/forum?id=HkAClQgA-" data-track-value="external reference" href="https://openreview.net/forum?id=HkAClQgA-">https://openreview.net/forum?id=HkAClQgA-</a></p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR26">Pennington, J., Socher, R., &amp; Manning, C. (2014). GloVe: Global vectors for word representation. In: Moschitti A, Pang B, Daelemans W (Eds.) Proceedings of the 2014 conference on Empirical Methods in Natural Language Processing (EMNLP). <i>Association for Computational Linguistics,</i> Doha, Qatar, pp 1532–1543. <a data-track="click_references" data-track-action="external reference" data-track-label="10.3115/v1/D14-1162" data-track-value="external reference" href="https://doi.org/10.3115/v1/D14-1162">https://doi.org/10.3115/v1/D14-1162</a></p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR27">Phan TA, Nguyen NDN, &amp; Bui KHN. (2022). HeterGraphLongSum: Heterogeneous graph neural network with passage aggregation for extractive long document summarization. In: Calzolari N, Huang CR, Kim H, et al (Eds.), Proceedings of the 29th international conference on computational linguistics. <i>International Committee on Computational Linguistics,</i> Gyeongju, Republic of Korea, pp 6248–6258. <a data-track="click_references" data-track-action="external reference" data-track-label="https://aclanthology.org/2022.coling-1.545" data-track-value="external reference" href="https://aclanthology.org/2022.coling-1.545">https://aclanthology.org/2022.coling-1.545</a></p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR28">Radford A, Kim JW, Hallacy C, et al (2021) Learning transferable visual models from natural language supervision. In: Meila M, Zhang T (Eds.), <i>Proceedings of the 38th International Conference on Machine Learning,</i> ICML 2021, 18-24 July 2021, Virtual Event, pp 8748–8763. <a data-track="click_references" data-track-action="external reference" data-track-label="http://proceedings.mlr.press/v139/radford21a.html" data-track-value="external reference" href="http://proceedings.mlr.press/v139/radford21a.html">http://proceedings.mlr.press/v139/radford21a.html</a></p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR29">See, A., Liu, P.J., Manning, C.D. (2017). Get to the point: Summarization with pointer-generator networks. In: Barzilay R, Kan MY (Eds.), Proceedings of the 55th annual meeting of the association for computational linguistics (Volume 1: Long Papers). <i>Association for Computational Linguistics,</i> Vancouver, Canada, pp 1073–1083. <a data-track="click_references" data-track-action="external reference" data-track-label="10.18653/v1/P17-1099" data-track-value="external reference" href="https://doi.org/10.18653/v1/P17-1099">https://doi.org/10.18653/v1/P17-1099</a></p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR30">Song, Z., &amp; King, I. (2022). Hierarchical heterogeneous graph attention network for syntax-aware summarization. <i>Proceedings of the AAAI Conference on Artificial Intelligence,</i> <i>36</i>(10), 11340–1134. <a data-track="click_references" data-track-action="external reference" data-track-label="10.1609/aaai.v36i10.21385" data-track-value="external reference" href="https://doi.org/10.1609/aaai.v36i10.21385">https://doi.org/10.1609/aaai.v36i10.21385</a></p><p class="c-article-references__links u-hide-print"><a aria-label="Article reference 30" data-doi="10.1609/aaai.v36i10.21385" data-track="click_references" data-track-action="article reference" data-track-item_id="10.1609/aaai.v36i10.21385" data-track-label="10.1609/aaai.v36i10.21385" data-track-value="article reference" href="https://doi.org/10.1609%2Faaai.v36i10.21385" rel="nofollow noopener">Article</a> 
    <a aria-label="Google Scholar reference 30" data-track="click_references" data-track-action="google scholar reference" data-track-item_id="link" data-track-label="link" data-track-value="google scholar reference" href="http://scholar.google.com/scholar_lookup?&amp;title=Hierarchical%20heterogeneous%20graph%20attention%20network%20for%20syntax-aware%20summarization&amp;journal=Proceedings%20of%20the%20AAAI%20Conference%20on%20Artificial%20Intelligence&amp;doi=10.1609%2Faaai.v36i10.21385&amp;volume=36&amp;issue=10&amp;pages=11340-1134&amp;publication_year=2022&amp;author=Song%2CZ&amp;author=King%2CI" rel="nofollow noopener">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR31">Sutskever, I., Vinyals, O., &amp; Le, Q.V. (2014). Sequence to sequence learning with neural networks. In: Ghahramani Z, Welling M, Cortes C, et al (Eds.), <i>Advances in Neural Information Processing Systems,</i>[SPACE]<a data-track="click_references" data-track-action="external reference" data-track-label="https://proceedings.neurips.cc/paper/2014/hash/a14ac55a4f27472c5d894ec1c3c743d2-Abstract.html" data-track-value="external reference" href="https://proceedings.neurips.cc/paper/2014/hash/a14ac55a4f27472c5d894ec1c3c743d2-Abstract.html">https://proceedings.neurips.cc/paper/2014/hash/a14ac55a4f27472c5d894ec1c3c743d2-Abstract.html</a></p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR32">Tang, P., Hu, K., Yan, R., et al. (2022). OTExtSum: Extractive text summarisation with optimal transport. In: Carpuat M, de Marneffe MC, Meza Ruiz IV (eds) Findings of the association for computational linguistics: NAACL 2022. <i>Association for Computational Linguistics,</i> Seattle, United States, pp 1128–1141. <a data-track="click_references" data-track-action="external reference" data-track-label="10.18653/v1/2022.findings-naacl.85" data-track-value="external reference" href="https://doi.org/10.18653/v1/2022.findings-naacl.85">https://doi.org/10.18653/v1/2022.findings-naacl.85</a></p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR33">Vaswani A, Shazeer N, Parmar N, et al. (2017). Attention is all you need. In: Guyon I, Luxburg UV, Bengio S, et al (Eds.), <i>Advances in Neural Information Processing Systems,</i>[SPACE]<a data-track="click_references" data-track-action="external reference" data-track-label="https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html" data-track-value="external reference" href="https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html">https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html</a></p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR34">Velickovic, P., Cucurull, G., Casanova, A., et al. (2017). <i>Graph attention networks. stat,</i> <i>1050</i>(20), 10–48550.</p><p class="c-article-references__links u-hide-print"><a aria-label="Google Scholar reference 34" data-track="click_references" data-track-action="google scholar reference" data-track-item_id="link" data-track-label="link" data-track-value="google scholar reference" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=Graph%20attention%20networks.%20stat&amp;volume=1050&amp;issue=20&amp;pages=10-48550&amp;publication_year=2017&amp;author=Velickovic%2CP&amp;author=Cucurull%2CG&amp;author=Casanova%2CA" rel="nofollow noopener">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR35">Wang, D., Liu, P., Zheng, Y., et al. (2020). Heterogeneous graph neural networks for extractive document summarization. In: Jurafsky D, Chai J, Schluter N, et al (Eds.), Proceedings of the 58th annual meeting of the association for computational linguistics. <i>Association for Computational Linguistics,</i> Online, pp 6209–6219. <a data-track="click_references" data-track-action="external reference" data-track-label="10.18653/v1/2020.acl-main.553" data-track-value="external reference" href="https://doi.org/10.18653/v1/2020.acl-main.553">https://doi.org/10.18653/v1/2020.acl-main.553</a></p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR36">Wu W, Li W, Xiao X, et al. (2021). BASS: Boosting abstractive summarization with unified semantic graph. In: Proceedings of the 59th annual meeting of the association for computational linguistics and the 11th international joint conference on natural language processing (Volume 1: Long Papers). <i>Association for Computational Linguistics,</i> Online, pp 6052–6067.<a data-track="click_references" data-track-action="external reference" data-track-label="10.18653/v1/2021.acl-long.472" data-track-value="external reference" href="https://doi.org/10.18653/v1/2021.acl-long.472">https://doi.org/10.18653/v1/2021.acl-long.472</a></p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR37">Xiao M, Zhu J, Lin H, et al. (2023). CFSum coarse-to-fine contribution network for multimodal summarization. In: Rogers A, Boyd-Graber J, Okazaki N (Eds.), Proceedings of the 61st annual meeting of the association for computational linguistics (Volume 1: Long Papers). <i>Association for Computational Linguistics,</i> Toronto, Canada, pp 8538–8553. <a data-track="click_references" data-track-action="external reference" data-track-label="10.18653/v1/2023.acl-long.476" data-track-value="external reference" href="https://doi.org/10.18653/v1/2023.acl-long.476">https://doi.org/10.18653/v1/2023.acl-long.476</a></p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR38">Xie, F., Chen, J., &amp; Chen, K. (2023). Extractive text-image summarization with relation-enhanced graph attention network. <i>J Intell Inf Syst,</i> <i>61</i>(2), 325–341. <a data-track="click_references" data-track-action="external reference" data-track-label="10.1007/S10844-022-00757-X" data-track-value="external reference" href="https://doi.org/10.1007/S10844-022-00757-X">https://doi.org/10.1007/S10844-022-00757-X</a></p><p class="c-article-references__links u-hide-print"><a aria-label="Article reference 38" data-doi="10.1007/S10844-022-00757-X" data-track="click_references" data-track-action="article reference" data-track-item_id="10.1007/S10844-022-00757-X" data-track-label="10.1007/S10844-022-00757-X" data-track-value="article reference" href="https://link.springer.com/doi/10.1007/S10844-022-00757-X" rel="noopener">Article</a> 
    <a aria-label="Google Scholar reference 38" data-track="click_references" data-track-action="google scholar reference" data-track-item_id="link" data-track-label="link" data-track-value="google scholar reference" href="http://scholar.google.com/scholar_lookup?&amp;title=Extractive%20text-image%20summarization%20with%20relation-enhanced%20graph%20attention%20network&amp;journal=J%20Intell%20Inf%20Syst&amp;doi=10.1007%2FS10844-022-00757-X&amp;volume=61&amp;issue=2&amp;pages=325-341&amp;publication_year=2023&amp;author=Xie%2CF&amp;author=Chen%2CJ&amp;author=Chen%2CK" rel="nofollow noopener">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR39">Ying, C., Cai, T., Luo, S., et al. (2021). Do transformers really perform badly for graph representation? In: Ranzato M, Beygelzimer A, Dauphin YN, et al (Eds.), <i>Advances in Neural Information Processing Systems,</i> pp 28877–28888. <a data-track="click_references" data-track-action="external reference" data-track-label="https://proceedings.neurips.cc/paper/2021/hash/f1c1592588411002af340cbaedd6fc33-Abstract.html" data-track-value="external reference" href="https://proceedings.neurips.cc/paper/2021/hash/f1c1592588411002af340cbaedd6fc33-Abstract.html">https://proceedings.neurips.cc/paper/2021/hash/f1c1592588411002af340cbaedd6fc33-Abstract.html</a></p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR40">Zhang, J., Zhao, Y., Saleh, M., et al. (2020). PEGASUS: pre-training with extracted gap-sentences for abstractive summarization. In: Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, pp 11328–11339. <a data-track="click_references" data-track-action="external reference" data-track-label="http://proceedings.mlr.press/v119/zhang20ae.html" data-track-value="external reference" href="http://proceedings.mlr.press/v119/zhang20ae.html">http://proceedings.mlr.press/v119/zhang20ae.html</a></p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR41">Zhang, L., Zhang, X., &amp; Pan, J. (2022a). Hierarchical cross-modality semantic correlation learning model for multimodal summarization. <i>Proceedings of the AAAI Conference on Artificial Intelligence</i> <i>36</i>(10) 11676–11684. <a data-track="click_references" data-track-action="external reference" data-track-label="10.1609/aaai.v36i10.21422" data-track-value="external reference" href="https://doi.org/10.1609/aaai.v36i10.21422">https://doi.org/10.1609/aaai.v36i10.21422</a></p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR42">Zhang, M., Zhou, G., Yu, W., et al. (2022b). MAA-PTG: multimodal aspect-aware product title generation. <i>J Intell Inf Syst</i> <i>59</i>(1) 213–235. <a data-track="click_references" data-track-action="external reference" data-track-label="10.1007/S10844-022-00695-8" data-track-value="external reference" href="https://doi.org/10.1007/S10844-022-00695-8">https://doi.org/10.1007/S10844-022-00695-8</a></p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR43">Zhang, Z., Meng, X., Wang, Y., et al. (2022c). Unims: A unified framework for multimodal summarization with knowledge distillation. <i>Proceedings of the AAAI Conference on Artificial Intelligence</i> <i>36</i>(10) 11757–11764. <a data-track="click_references" data-track-action="external reference" data-track-label="10.1609/aaai.v36i10.21431" data-track-value="external reference" href="https://doi.org/10.1609/aaai.v36i10.21431">https://doi.org/10.1609/aaai.v36i10.21431</a></p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR44">Zhong, M., Liu, P., Chen, Y., et al. (2020). Extractive summarization as text matching. In: Jurafsky D, Chai J, Schluter N, et al (Eds.) Proceedings of the 58th annual meeting of the association for computational linguistics. <i>Association for Computational Linguistics,</i> Online, pp 6197–6208. <a data-track="click_references" data-track-action="external reference" data-track-label="10.18653/v1/2020.acl-main.552" data-track-value="external reference" href="https://doi.org/10.18653/v1/2020.acl-main.552">https://doi.org/10.18653/v1/2020.acl-main.552</a></p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR45">Zhu, J., Li, H., Liu, T., et al. (2018). MSMO: Multimodal summarization with multimodal output. In: Riloff E, Chiang D, Hockenmaier J, et al (eds) Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. <i>Association for Computational Linguistics,</i> Brussels, Belgium, pp 4154–4164. <a data-track="click_references" data-track-action="external reference" data-track-label="10.18653/v1/D18-1448" data-track-value="external reference" href="https://doi.org/10.18653/v1/D18-1448">https://doi.org/10.18653/v1/D18-1448</a></p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR46">Zhu, J., Zhou, Y., Zhang, J., et al. (2020). Multimodal summarization with guidance of multimodal reference. <i>Proceedings of the AAAI Conference on Artificial Intelligence,</i> <i>34</i>(05), 9749–975. <a data-track="click_references" data-track-action="external reference" data-track-label="10.1609/aaai.v34i05.6525" data-track-value="external reference" href="https://doi.org/10.1609/aaai.v34i05.6525">https://doi.org/10.1609/aaai.v34i05.6525</a></p><p class="c-article-references__links u-hide-print"><a aria-label="Article reference 46" data-doi="10.1609/aaai.v34i05.6525" data-track="click_references" data-track-action="article reference" data-track-item_id="10.1609/aaai.v34i05.6525" data-track-label="10.1609/aaai.v34i05.6525" data-track-value="article reference" href="https://doi.org/10.1609%2Faaai.v34i05.6525" rel="nofollow noopener">Article</a> 
    <a aria-label="Google Scholar reference 46" data-track="click_references" data-track-action="google scholar reference" data-track-item_id="link" data-track-label="link" data-track-value="google scholar reference" href="http://scholar.google.com/scholar_lookup?&amp;title=Multimodal%20summarization%20with%20guidance%20of%20multimodal%20reference&amp;journal=Proceedings%20of%20the%20AAAI%20Conference%20on%20Artificial%20Intelligence&amp;doi=10.1609%2Faaai.v34i05.6525&amp;volume=34&amp;issue=05&amp;pages=9749-975&amp;publication_year=2020&amp;author=Zhu%2CJ&amp;author=Zhou%2CY&amp;author=Zhang%2CJ" rel="nofollow noopener">
                    Google Scholar</a> 
                </p></li></ul><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="https://citation-needed.springer.com/v2/references/10.1007/s10844-024-00886-5?format=refman&amp;flavour=references" rel="nofollow">Download references<svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16"><use xlink:href="#icon-eds-i-download-medium" xmlns:xlink="http://www.w3.org/1999/xlink"></use></svg></a></p></div></div></div></section></div><section data-title="Funding"><div class="c-article-section" id="Fun-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Fun">Funding</h2><div class="c-article-section__content" id="Fun-content"><p>This research was sponsored by the National Natural Science Foundation of China (No.61806101).</p></div></div></section><section aria-labelledby="author-information" data-title="Author information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><span class="c-article-author-information__subtitle u-visually-hidden" id="author-notes">Author notes</span><ol class="c-article-author-information__list"><li class="c-article-author-information__item" id="na1"><p>Xiankai Jiang and Jingqiang Chen are both equally contributed.</p></li></ol><h3 class="c-article__sub-heading" id="affiliations">Authors and Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">School of Computer Science, Nanjing University of Posts and Telecommunications, Nanjing, 210049, Jiangsu, China</p><p class="c-article-author-affiliation__authors-list">Xiankai Jiang &amp; Jingqiang Chen</p></li></ol><div class="u-js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Xiankai-Jiang-Aff1"><span class="c-article-authors-search__title u-h3 js-search-name">Xiankai Jiang</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" href="/search?dc.creator=Xiankai%20Jiang" rel="nofollow">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" data-track="click" data-track-action="author link - pubmed" data-track-label="link" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Xiankai%20Jiang" rel="nofollow">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" data-track="click" data-track-action="author link - scholar" data-track-label="link" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Xiankai%20Jiang%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" rel="nofollow">Google Scholar</a></span></p></div></div></li><li id="auth-Jingqiang-Chen-Aff1"><span class="c-article-authors-search__title u-h3 js-search-name">Jingqiang Chen</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" href="/search?dc.creator=Jingqiang%20Chen" rel="nofollow">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" data-track="click" data-track-action="author link - pubmed" data-track-label="link" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Jingqiang%20Chen" rel="nofollow">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" data-track="click" data-track-action="author link - scholar" data-track-label="link" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Jingqiang%20Chen%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" rel="nofollow">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="contributions">Contributions</h3><p>X.J. and J.C. wrote the manuscript . All authors reviewed the manuscript.</p><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a href="mailto:cjq@njupt.edu.cn" id="corresp-c1">Jingqiang Chen</a>.</p></div></div></section><section data-title="Ethics declarations"><div class="c-article-section" id="ethics-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="ethics">Ethics declarations</h2><div class="c-article-section__content" id="ethics-content">
<h3 class="c-article__sub-heading" id="FPar1">Competing Interests</h3>
<p>The authors declare no competing interests.</p>
<h3 class="c-article__sub-heading" id="FPar2">Ethics Approval and Consent to Participate</h3>
<p>Not Applicable.</p>
<h3 class="c-article__sub-heading" id="FPar3">Consent for Publication</h3>
<p>The authors declare that they consent for publication.</p>
</div></div></section><section data-title="Additional information"><div class="c-article-section" id="additional-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="additional-information">Additional information</h2><div class="c-article-section__content" id="additional-information-content"><h3 class="c-article__sub-heading">Publisher's Note</h3><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></div></div></section><section data-title="Rights and permissions"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p>Springer Nature or its licensor (e.g. a society or other partner) holds exclusive rights to this article under a publishing agreement with the author(s) or other rightsholder(s); author self-archiving of the accepted manuscript version of this article is solely governed by the terms of such publishing agreement and applicable law.</p><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Heterogeneous%20graphormer%20for%20extractive%20multimodal%20summarization&amp;author=Xiankai%20Jiang%20et%20al&amp;contentID=10.1007%2Fs10844-024-00886-5&amp;copyright=The%20Author%28s%29%2C%20under%20exclusive%20licence%20to%20Springer%20Science%2BBusiness%20Media%2C%20LLC%2C%20part%20of%20Springer%20Nature&amp;publication=0925-9902&amp;publicationDate=2024-09-30&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and permissions</a></p></div></div></section><section aria-labelledby="article-info" data-title="About this article"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="u-hide-print c-bibliographic-information__column c-bibliographic-information__column--border"><a data-crossmark="10.1007/s10844-024-00886-5" data-test="crossmark" data-track="click" data-track-action="Click Crossmark" data-track-label="link" href="https://crossmark.crossref.org/dialog/?doi=10.1007/s10844-024-00886-5" rel="noopener" target="_blank"><img alt="Check for updates. Verify currency and authenticity via CrossMark" height="81" loading="lazy" src="data:image/svg+xml;base64,<svg height="81" width="57" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m17.35 35.45 21.3-14.2v-17.03h-21.3" fill="#989898"/><path d="m38.65 35.45-21.3-14.2v-17.03h21.3" fill="#747474"/><path d="m28 .5c-12.98 0-23.5 10.52-23.5 23.5s10.52 23.5 23.5 23.5 23.5-10.52 23.5-23.5c0-6.23-2.48-12.21-6.88-16.62-4.41-4.4-10.39-6.88-16.62-6.88zm0 41.25c-9.8 0-17.75-7.95-17.75-17.75s7.95-17.75 17.75-17.75 17.75 7.95 17.75 17.75c0 4.71-1.87 9.22-5.2 12.55s-7.84 5.2-12.55 5.2z" fill="#535353"/><path d="m41 36c-5.81 6.23-15.23 7.45-22.43 2.9-7.21-4.55-10.16-13.57-7.03-21.5l-4.92-3.11c-4.95 10.7-1.19 23.42 8.78 29.71 9.97 6.3 23.07 4.22 30.6-4.86z" fill="#9c9c9c"/><path d="m.2 58.45c0-.75.11-1.42.33-2.01s.52-1.09.91-1.5c.38-.41.83-.73 1.34-.94.51-.22 1.06-.32 1.65-.32.56 0 1.06.11 1.51.35.44.23.81.5 1.1.81l-.91 1.01c-.24-.24-.49-.42-.75-.56-.27-.13-.58-.2-.93-.2-.39 0-.73.08-1.05.23-.31.16-.58.37-.81.66-.23.28-.41.63-.53 1.04-.13.41-.19.88-.19 1.39 0 1.04.23 1.86.68 2.46.45.59 1.06.88 1.84.88.41 0 .77-.07 1.07-.23s.59-.39.85-.68l.91 1c-.38.43-.8.76-1.28.99-.47.22-1 .34-1.58.34-.59 0-1.13-.1-1.64-.31-.5-.2-.94-.51-1.31-.91-.38-.4-.67-.9-.88-1.48-.22-.59-.33-1.26-.33-2.02zm8.4-5.33h1.61v2.54l-.05 1.33c.29-.27.61-.51.96-.72s.76-.31 1.24-.31c.73 0 1.27.23 1.61.71.33.47.5 1.14.5 2.02v4.31h-1.61v-4.1c0-.57-.08-.97-.25-1.21-.17-.23-.45-.35-.83-.35-.3 0-.56.08-.79.22-.23.15-.49.36-.78.64v4.8h-1.61zm7.37 6.45c0-.56.09-1.06.26-1.51.18-.45.42-.83.71-1.14.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.36c.07.62.29 1.1.65 1.44.36.33.82.5 1.38.5.29 0 .57-.04.83-.13s.51-.21.76-.37l.55 1.01c-.33.21-.69.39-1.09.53-.41.14-.83.21-1.26.21-.48 0-.92-.08-1.34-.25-.41-.16-.76-.4-1.07-.7-.31-.31-.55-.69-.72-1.13-.18-.44-.26-.95-.26-1.52zm4.6-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.07.45-.31.29-.5.73-.58 1.3zm2.5.62c0-.57.09-1.08.28-1.53.18-.44.43-.82.75-1.13s.69-.54 1.1-.71c.42-.16.85-.24 1.31-.24.45 0 .84.08 1.17.23s.61.34.85.57l-.77 1.02c-.19-.16-.38-.28-.56-.37-.19-.09-.39-.14-.61-.14-.56 0-1.01.21-1.35.63-.35.41-.52.97-.52 1.67 0 .69.17 1.24.51 1.66.34.41.78.62 1.32.62.28 0 .54-.06.78-.17.24-.12.45-.26.64-.42l.67 1.03c-.33.29-.69.51-1.08.65-.39.15-.78.23-1.18.23-.46 0-.9-.08-1.31-.24-.4-.16-.75-.39-1.05-.7s-.53-.69-.7-1.13c-.17-.45-.25-.96-.25-1.53zm6.91-6.45h1.58v6.17h.05l2.54-3.16h1.77l-2.35 2.8 2.59 4.07h-1.75l-1.77-2.98-1.08 1.23v1.75h-1.58zm13.69 1.27c-.25-.11-.5-.17-.75-.17-.58 0-.87.39-.87 1.16v.75h1.34v1.27h-1.34v5.6h-1.61v-5.6h-.92v-1.2l.92-.07v-.72c0-.35.04-.68.13-.98.08-.31.21-.57.4-.79s.42-.39.71-.51c.28-.12.63-.18 1.04-.18.24 0 .48.02.69.07.22.05.41.1.57.17zm.48 5.18c0-.57.09-1.08.27-1.53.17-.44.41-.82.72-1.13.3-.31.65-.54 1.04-.71.39-.16.8-.24 1.23-.24s.84.08 1.24.24c.4.17.74.4 1.04.71s.54.69.72 1.13c.19.45.28.96.28 1.53s-.09 1.08-.28 1.53c-.18.44-.42.82-.72 1.13s-.64.54-1.04.7-.81.24-1.24.24-.84-.08-1.23-.24-.74-.39-1.04-.7c-.31-.31-.55-.69-.72-1.13-.18-.45-.27-.96-.27-1.53zm1.65 0c0 .69.14 1.24.43 1.66.28.41.68.62 1.18.62.51 0 .9-.21 1.19-.62.29-.42.44-.97.44-1.66 0-.7-.15-1.26-.44-1.67-.29-.42-.68-.63-1.19-.63-.5 0-.9.21-1.18.63-.29.41-.43.97-.43 1.67zm6.48-3.44h1.33l.12 1.21h.05c.24-.44.54-.79.88-1.02.35-.24.7-.36 1.07-.36.32 0 .59.05.78.14l-.28 1.4-.33-.09c-.11-.01-.23-.02-.38-.02-.27 0-.56.1-.86.31s-.55.58-.77 1.1v4.2h-1.61zm-47.87 15h1.61v4.1c0 .57.08.97.25 1.2.17.24.44.35.81.35.3 0 .57-.07.8-.22.22-.15.47-.39.73-.73v-4.7h1.61v6.87h-1.32l-.12-1.01h-.04c-.3.36-.63.64-.98.86-.35.21-.76.32-1.24.32-.73 0-1.27-.24-1.61-.71-.33-.47-.5-1.14-.5-2.02zm9.46 7.43v2.16h-1.61v-9.59h1.33l.12.72h.05c.29-.24.61-.45.97-.63.35-.17.72-.26 1.1-.26.43 0 .81.08 1.15.24.33.17.61.4.84.71.24.31.41.68.53 1.11.13.42.19.91.19 1.44 0 .59-.09 1.11-.25 1.57-.16.47-.38.85-.65 1.16-.27.32-.58.56-.94.73-.35.16-.72.25-1.1.25-.3 0-.6-.07-.9-.2s-.59-.31-.87-.56zm0-2.3c.26.22.5.37.73.45.24.09.46.13.66.13.46 0 .84-.2 1.15-.6.31-.39.46-.98.46-1.77 0-.69-.12-1.22-.35-1.61-.23-.38-.61-.57-1.13-.57-.49 0-.99.26-1.52.77zm5.87-1.69c0-.56.08-1.06.25-1.51.16-.45.37-.83.65-1.14.27-.3.58-.54.93-.71s.71-.25 1.08-.25c.39 0 .73.07 1 .2.27.14.54.32.81.55l-.06-1.1v-2.49h1.61v9.88h-1.33l-.11-.74h-.06c-.25.25-.54.46-.88.64-.33.18-.69.27-1.06.27-.87 0-1.56-.32-2.07-.95s-.76-1.51-.76-2.65zm1.67-.01c0 .74.13 1.31.4 1.7.26.38.65.58 1.15.58.51 0 .99-.26 1.44-.77v-3.21c-.24-.21-.48-.36-.7-.45-.23-.08-.46-.12-.7-.12-.45 0-.82.19-1.13.59-.31.39-.46.95-.46 1.68zm6.35 1.59c0-.73.32-1.3.97-1.71.64-.4 1.67-.68 3.08-.84 0-.17-.02-.34-.07-.51-.05-.16-.12-.3-.22-.43s-.22-.22-.38-.3c-.15-.06-.34-.1-.58-.1-.34 0-.68.07-1 .2s-.63.29-.93.47l-.59-1.08c.39-.24.81-.45 1.28-.63.47-.17.99-.26 1.54-.26.86 0 1.51.25 1.93.76s.63 1.25.63 2.21v4.07h-1.32l-.12-.76h-.05c-.3.27-.63.48-.98.66s-.73.27-1.14.27c-.61 0-1.1-.19-1.48-.56-.38-.36-.57-.85-.57-1.46zm1.57-.12c0 .3.09.53.27.67.19.14.42.21.71.21.28 0 .54-.07.77-.2s.48-.31.73-.56v-1.54c-.47.06-.86.13-1.18.23-.31.09-.57.19-.76.31s-.33.25-.41.4c-.09.15-.13.31-.13.48zm6.29-3.63h-.98v-1.2l1.06-.07.2-1.88h1.34v1.88h1.75v1.27h-1.75v3.28c0 .8.32 1.2.97 1.2.12 0 .24-.01.37-.04.12-.03.24-.07.34-.11l.28 1.19c-.19.06-.4.12-.64.17-.23.05-.49.08-.76.08-.4 0-.74-.06-1.02-.18-.27-.13-.49-.3-.67-.52-.17-.21-.3-.48-.37-.78-.08-.3-.12-.64-.12-1.01zm4.36 2.17c0-.56.09-1.06.27-1.51s.41-.83.71-1.14c.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.37c.08.62.29 1.1.65 1.44.36.33.82.5 1.38.5.3 0 .58-.04.84-.13.25-.09.51-.21.76-.37l.54 1.01c-.32.21-.69.39-1.09.53s-.82.21-1.26.21c-.47 0-.92-.08-1.33-.25-.41-.16-.77-.4-1.08-.7-.3-.31-.54-.69-.72-1.13-.17-.44-.26-.95-.26-1.52zm4.61-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.08.45-.31.29-.5.73-.57 1.3zm3.01 2.23c.31.24.61.43.92.57.3.13.63.2.98.2.38 0 .65-.08.83-.23s.27-.35.27-.6c0-.14-.05-.26-.13-.37-.08-.1-.2-.2-.34-.28-.14-.09-.29-.16-.47-.23l-.53-.22c-.23-.09-.46-.18-.69-.3-.23-.11-.44-.24-.62-.4s-.33-.35-.45-.55c-.12-.21-.18-.46-.18-.75 0-.61.23-1.1.68-1.49.44-.38 1.06-.57 1.83-.57.48 0 .91.08 1.29.25s.71.36.99.57l-.74.98c-.24-.17-.49-.32-.73-.42-.25-.11-.51-.16-.78-.16-.35 0-.6.07-.76.21-.17.15-.25.33-.25.54 0 .14.04.26.12.36s.18.18.31.26c.14.07.29.14.46.21l.54.19c.23.09.47.18.7.29s.44.24.64.4c.19.16.34.35.46.58.11.23.17.5.17.82 0 .3-.06.58-.17.83-.12.26-.29.48-.51.68-.23.19-.51.34-.84.45-.34.11-.72.17-1.15.17-.48 0-.95-.09-1.41-.27-.46-.19-.86-.41-1.2-.68z" fill="#535353"/></g></svg>" width="57"/></a></div><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Jiang, X., Chen, J. Heterogeneous graphormer for extractive multimodal summarization.
                    <i>J Intell Inf Syst</i>  (2024). https://doi.org/10.1007/s10844-024-00886-5</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-external="" data-track-label="link" href="https://citation-needed.springer.com/v2/references/10.1007/s10844-024-00886-5?format=refman&amp;flavour=citation" rel="nofollow">Download citation<svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16"><use xlink:href="#icon-eds-i-download-medium" xmlns:xlink="http://www.w3.org/1999/xlink"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2024-04-14">14 April 2024</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Revised<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2024-09-04">04 September 2024</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2024-09-05">05 September 2024</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2024-09-30">30 September 2024</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--full-width"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value">https://doi.org/10.1007/s10844-024-00886-5</span></p></li></ul><div data-component="share-box"><div class="c-article-share-box u-display-block"><h3 class="c-article__sub-heading">Share this article</h3><p class="c-article-share-box__description">Anyone you share the following link with will be able to read this content:</p><button class="js-get-share-url c-article-share-box__button" data-track="click" data-track-action="get shareable link" data-track-external="" data-track-label="button" id="get-share-url" type="button">Get shareable link</button><div class="js-no-share-url-container u-display-none" hidden=""><p class="js-c-article-share-box__no-sharelink-info c-article-share-box__no-sharelink-info">Sorry, a shareable link is not currently available for this article.</p></div><div class="js-share-url-container u-display-none" hidden=""><p class="js-share-url c-article-share-box__only-read-input" data-track="click" data-track-action="select share url" data-track-label="button" id="share-url"></p><button class="js-copy-share-url c-article-share-box__button--link-like" data-track="click" data-track-action="copy share url" data-track-external="" data-track-label="button" id="copy-share-url" type="button">Copy to clipboard</button></div><p class="js-c-article-share-box__additional-info c-article-share-box__additional-info">
                            Provided by the Springer Nature SharedIt content-sharing initiative
                        </p></div></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span><a data-track="click" data-track-action="view keyword" data-track-label="link" href='/search?query=Summarization&amp;facet-discipline="Computer%20Science"'>Summarization</a></span></li><li class="c-article-subject-list__subject"><span><a data-track="click" data-track-action="view keyword" data-track-label="link" href='/search?query=Extractive%20summarization&amp;facet-discipline="Computer%20Science"'>Extractive summarization</a></span></li><li class="c-article-subject-list__subject"><span><a data-track="click" data-track-action="view keyword" data-track-label="link" href='/search?query=Multi-modal%20summarization&amp;facet-discipline="Computer%20Science"'>Multi-modal summarization</a></span></li><li class="c-article-subject-list__subject"><span><a data-track="click" data-track-action="view keyword" data-track-label="link" href='/search?query=Graph%20neural%20networks&amp;facet-discipline="Computer%20Science"'>Graph neural networks</a></span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
</div>
</main>
<div class="c-article-sidebar u-text-sm u-hide-print l-with-sidebar__sidebar" data-container-type="reading-companion" data-track-component="reading companion" id="sidebar">
<aside>
<div class="app-card-service" data-test="article-checklist-banner">
<div>
<a class="app-card-service__link" data-test="article-checklist-banner-link" data-track="click_presubmission_checklist" data-track-action="clicked article page checklist banner test 2 old version" data-track-category="pre-submission-checklist" data-track-context="article page top of reading companion" data-track-label="link" href="https://beta.springernature.com/pre-submission?journalId=10844">
<span class="app-card-service__link-text">Use our pre-submission checklist</span>
<svg aria-hidden="true" class="app-card-service__link-icon" focusable="false"><use xlink:href="#icon-eds-i-arrow-right-small"></use></svg>
</a>
<p class="app-card-service__description">Avoid common mistakes on your manuscript.</p>
</div>
<div class="app-card-service__icon-container">
<svg aria-hidden="true" class="app-card-service__icon" focusable="false">
<use xlink:href="#icon-eds-i-clipboard-check-medium"></use>
</svg>
</div>
</div>
<div data-test="collections">
</div>
<div data-test="editorial-summary">
</div>
<div class="c-reading-companion">
<div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
<div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
<div class="u-lazy-ad-wrapper u-mt-16 u-show" data-component-mpu=""><div class="c-ad c-ad--300x250">
<div class="c-ad__inner">
<p class="c-ad__label">Advertisement</p>
<div class="div-gpt-ad grade-c-hide" data-gpt="" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=s10844-024-00886-5;" data-gpt-unitpath="/270604982/springerlink/10844/article" data-pa11y-ignore="" data-test="MPU1-ad" id="div-gpt-ad-MPU1">
</div>
</div>
</div>
</div>
</div>
<div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
<div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
</div>
</div>
</aside>
</div>
</div>
</article>
<div class="app-elements">
<footer>
<div class="eds-c-footer">
<div class="eds-c-footer__container">
<div class="eds-c-footer__grid eds-c-footer__group--separator">
<div class="eds-c-footer__group">
<h3 class="eds-c-footer__heading">Discover content</h3>
<ul class="eds-c-footer__list">
<li class="eds-c-footer__item"><a class="eds-c-footer__link" data-track="nav_journals_a_z" data-track-action="journals a-z" data-track-context="unified footer" data-track-label="link" href="https://link.springer.com/journals/a/1">Journals A-Z</a></li>
<li class="eds-c-footer__item"><a class="eds-c-footer__link" data-track="nav_books_a_z" data-track-action="books a-z" data-track-context="unified footer" data-track-label="link" href="https://link.springer.com/books/a/1">Books A-Z</a></li>
</ul>
</div>
<div class="eds-c-footer__group">
<h3 class="eds-c-footer__heading">Publish with us</h3>
<ul class="eds-c-footer__list">
<li class="eds-c-footer__item"><a class="eds-c-footer__link" data-track="nav_journal_finder" data-track-action="journal finder" data-track-context="unified footer" data-track-label="link" href="https://link.springer.com/journals">Journal finder</a></li>
<li class="eds-c-footer__item"><a class="eds-c-footer__link" data-track="nav_publish_your_research" data-track-action="publish your research" data-track-context="unified footer" data-track-label="link" href="https://www.springernature.com/gp/authors">Publish your research</a></li>
<li class="eds-c-footer__item"><a class="eds-c-footer__link" data-track="nav_open_access_publishing" data-track-action="open access publishing" data-track-context="unified footer" data-track-label="link" href="https://www.springernature.com/gp/open-research/about/the-fundamentals-of-open-access-and-open-research">Open access publishing</a></li>
</ul>
</div>
<div class="eds-c-footer__group">
<h3 class="eds-c-footer__heading">Products and services</h3>
<ul class="eds-c-footer__list">
<li class="eds-c-footer__item"><a class="eds-c-footer__link" data-track="nav_our_products" data-track-action="our products" data-track-context="unified footer" data-track-label="link" href="https://www.springernature.com/gp/products">Our products</a></li>
<li class="eds-c-footer__item"><a class="eds-c-footer__link" data-track="nav_librarians" data-track-action="librarians" data-track-context="unified footer" data-track-label="link" href="https://www.springernature.com/gp/librarians">Librarians</a></li>
<li class="eds-c-footer__item"><a class="eds-c-footer__link" data-track="nav_societies" data-track-action="societies" data-track-context="unified footer" data-track-label="link" href="https://www.springernature.com/gp/societies">Societies</a></li>
<li class="eds-c-footer__item"><a class="eds-c-footer__link" data-track="nav_partners_and_advertisers" data-track-action="partners and advertisers" data-track-context="unified footer" data-track-label="link" href="https://www.springernature.com/gp/partners">Partners and advertisers</a></li>
</ul>
</div>
<div class="eds-c-footer__group">
<h3 class="eds-c-footer__heading">Our imprints</h3>
<ul class="eds-c-footer__list">
<li class="eds-c-footer__item"><a class="eds-c-footer__link" data-track="nav_imprint_Springer" data-track-action="Springer" data-track-context="unified footer" data-track-label="link" href="https://www.springer.com/">Springer</a></li>
<li class="eds-c-footer__item"><a class="eds-c-footer__link" data-track="nav_imprint_Nature_Portfolio" data-track-action="Nature Portfolio" data-track-context="unified footer" data-track-label="link" href="https://www.nature.com/">Nature Portfolio</a></li>
<li class="eds-c-footer__item"><a class="eds-c-footer__link" data-track="nav_imprint_BMC" data-track-action="BMC" data-track-context="unified footer" data-track-label="link" href="https://www.biomedcentral.com/">BMC</a></li>
<li class="eds-c-footer__item"><a class="eds-c-footer__link" data-track="nav_imprint_Palgrave_Macmillan" data-track-action="Palgrave Macmillan" data-track-context="unified footer" data-track-label="link" href="https://www.palgrave.com/">Palgrave Macmillan</a></li>
<li class="eds-c-footer__item"><a class="eds-c-footer__link" data-track="nav_imprint_Apress" data-track-action="Apress" data-track-context="unified footer" data-track-label="link" href="https://www.apress.com/">Apress</a></li>
</ul>
</div>
</div>
</div>
<div class="eds-c-footer__container">
<nav aria-label="footer navigation">
<ul class="eds-c-footer__links">
<li class="eds-c-footer__item">
<button class="eds-c-footer__link" data-cc-action="preferences" data-track="dialog_manage_cookies" data-track-action="Manage cookies" data-track-context="unified footer" data-track-label="link"><span class="eds-c-footer__button-text">Your privacy choices/Manage cookies</span></button>
</li>
<li class="eds-c-footer__item">
<a class="eds-c-footer__link" data-track="nav_california_privacy_statement" data-track-action="california privacy statement" data-track-context="unified footer" data-track-label="link" href="https://www.springernature.com/gp/legal/ccpa">Your US state privacy rights</a>
</li>
<li class="eds-c-footer__item">
<a class="eds-c-footer__link" data-track="nav_accessibility_statement" data-track-action="accessibility statement" data-track-context="unified footer" data-track-label="link" href="https://www.springernature.com/gp/info/accessibility">Accessibility statement</a>
</li>
<li class="eds-c-footer__item">
<a class="eds-c-footer__link" data-track="nav_terms_and_conditions" data-track-action="terms and conditions" data-track-context="unified footer" data-track-label="link" href="https://link.springer.com/termsandconditions">Terms and conditions</a>
</li>
<li class="eds-c-footer__item">
<a class="eds-c-footer__link" data-track="nav_privacy_policy" data-track-action="privacy policy" data-track-context="unified footer" data-track-label="link" href="https://link.springer.com/privacystatement">Privacy policy</a>
</li>
<li class="eds-c-footer__item">
<a class="eds-c-footer__link" data-track="nav_help_and_support" data-track-action="help and support" data-track-context="unified footer" data-track-label="link" href="https://support.springernature.com/en/support/home">Help and support</a>
</li>
<li class="eds-c-footer__item">
<a class="eds-c-footer__link" data-track-action="cancel contracts here" href="https://support.springernature.com/en/support/solutions/articles/6000255911-subscription-cancellations">Cancel contracts here</a>
</li>
</ul>
</nav>
<div class="eds-c-footer__user">
<p class="eds-c-footer__user-info">
<span data-test="footer-user-ip">165.194.104.233</span>
</p>
<p class="eds-c-footer__user-info" data-test="footer-business-partners">South Korea Trial Consortium (3001948335)  - Chung Ang University (3000120396)  - 7029 SpringerLink South Korea KESLI Korean Journal Package (3000256740)  - South Korea Trial Consortium (3000522537)  - 5539 SpringerLink South Korea KESLI Full OJA Consortium - (3000171421)  - 9823  SpringerLink South Korea Shinwon (3000699960)  - South Korea Full eJournal Con - Academic 2015-2017 (3991460179)  - 8064 South Korea KESLI Korean Consortium (3000251005)  - 5539 SpringerLink South Korea KESLI Full eJournal Consortium - Academic (3000175460) </p>
</div>
<a class="eds-c-footer__link" href="https://www.springernature.com/">
<img alt="Springer Nature" height="20" loading="lazy" src="/oscar-static/images/logo-springernature-white-19dd4ba190.svg" width="200"/>
</a>
<p class="eds-c-footer__legal" data-test="copyright">© 2024 Springer Nature</p>
</div>
</div>
</footer>
</div>
<script nomodule="true" src="/oscar-static/js/global-article-es5-bundle-dad1690b0d.js"></script><script src="/oscar-static/js/global-article-es6-bundle-e7d03c4cb3.js" type="module"></script><div data-cc-ghost="" style="height: 317px;"></div><script id="fetch-contextual-ads-campaign-data" type="text/javascript">(function(){var g=function(a){a=new CustomEvent("campaignDataLoaded",{detail:a});document.dispatchEvent(a)},b=google_tag_manager["rm"]["50443292"](73);if(b){var k=1500;b=google_tag_manager["rm"]["50443292"](74);b="link"===b?google_tag_manager["rm"]["50443292"](75):google_tag_manager["rm"]["50443292"](76);var m=function(a,c){var h=!1,l=setTimeout(function(){h=!0;c(null)},k),d=new XMLHttpRequest;d.onload=function(){var e=null,f=[];if(200===d.status){try{e=JSON.parse(d.responseText)}catch(p){e={}}f=e.campaigns||[]}h||(clearTimeout(l),c(f.length?f.join(","):null))};d.open("GET",
"/platform/contextual?doi\x3d"+a);d.send()},n=function(a,c){a&&-1===a.indexOf("not set")?m(a,c):setTimeout(c,1)};n(b,function(a){window.campaignsForContextualAds=a;g(a);window.dataLayer.push({content:{article:{campaignID:a}}})})}else window.campaignsForContextualAds="",g(null)})();</script>
<script id="gtm-setup-accessdetailsloaded-handler" type="text/javascript">document.addEventListener("accessdetailsloaded",function(a){a=a.detail||{};var b={event:"update-access-details"};a.institutional_business_partner_ids&&a.resolved_by||console.log("BPID data could not be retrieved from /exposed-details");a.logged_in_to_sn_profile||console.log("logged_in_to_sn_profile could not be retrieved from /exposed-details");var c=a.institutional_business_partner_ids&&a.institutional_business_partner_ids.join?a.institutional_business_partner_ids.join(";"):"",d=a.resolved_by&&a.resolved_by.join?
a.resolved_by.join(";"):"",e=a.personal_business_partner_id_found,f=a.logged_in_to_sn_profile,g=a.snid;b.user={};b.user.profile={};b.user.profile.profileInfo={resolvedBy:d||null,bpid:c||null,personal:e||null,logged_in_to_sn_profile:f,snid:g||null};b.session={};b.session.authentication={};b.session.authentication.token=a.token||null;b.session.authentication.legacy={};window.dataLayer.push(b);window.idpUserDataLoaded=!0},!1);window.dataLayer.push({event:"accessdetailsloaded-handler-added"});
window.accessDetailsLoadedHandlerAdded=!0;</script><script id="polyfill-matches" type="text/javascript">Element.prototype.matches||(Element.prototype.matches=Element.prototype.matchesSelector||Element.prototype.mozMatchesSelector||Element.prototype.msMatchesSelector||Element.prototype.oMatchesSelector||Element.prototype.webkitMatchesSelector||function(a){a=(this.document||this.ownerDocument).querySelectorAll(a);for(var b=a.length;0<=--b&&a.item(b)!==this;);return-1<b});</script><script id="fetch-idp-user-data" type="text/javascript">(function(a){if("function"===typeof window.CustomEvent)return!1;var c=function(d,b){b=b||{};var e=document.createEvent("CustomEvent");e.initCustomEvent(d,b.bubbles||!1,b.cancelable||!1,b.detail||a);return e};c.prototype=window.Event.prototype;window.CustomEvent=c})();var parse=function(a,c){try{return 200===a?JSON.parse(c):null}catch(d){return null}},dispatch=function(a){a=new CustomEvent("accessdetailsloaded",{detail:a});document.dispatchEvent(a)},site=google_tag_manager["rm"]["50443292"](107),idpUrl;
switch(site){case "nature":idpUrl=-1<window.location.hostname.indexOf("local-www")||-1<window.location.hostname.indexOf("test-www")?"https://staging-idp.nature.com/exposed-details":"https://idp.nature.com/exposed-details";break;case "link":idpUrl=-1<window.location.hostname.indexOf("link-qa")?"https://staging-idp.springer.com/exposed-details":"https://idp.springer.com/exposed-details";break;case "springer":idpUrl=-1<window.location.hostname.indexOf("local-www")||-1<window.location.hostname.indexOf("test-www")?
"https://staging-idp.springer.com/exposed-details":"https://idp.springer.com/exposed-details"}if(void 0!==idpUrl){var transport=new XMLHttpRequest;transport.open("GET",idpUrl,!0);transport.withCredentials=!0;transport.onreadystatechange=function(){4===transport.readyState&&dispatch(parse(transport.status,transport.responseText))};transport.send()}else dispatch(null);</script><script id="create-ad-slots" type="text/javascript">function createAndLoadAds(){function h(b){"nature"===a&&google_tag_manager["rm"]["50443292"](136)(b);"bmc"!==a&&"springeropen"!==a||google_tag_manager["rm"]["50443292"](148)(b);("link"===a&&"oscar"===google_tag_manager["rm"]["50443292"](149)||"springer"===a||"link"===a&&!0===google_tag_manager["rm"]["50443292"](150))&&google_tag_manager["rm"]["50443292"](175)(b);"link"===a&&"bunsen"===google_tag_manager["rm"]["50443292"](176)&&google_tag_manager["rm"]["50443292"](201)(b);"link"===a&&"Core"===google_tag_manager["rm"]["50443292"](202)&&google_tag_manager["rm"]["50443292"](214)(b)}window.googletag=window.googletag||{cmd:[]};var a=google_tag_manager["rm"]["50443292"](215),e=google_tag_manager["rm"]["50443292"](217);
if(-1===window.location.search.indexOf("hide_ads\x3dtrue"))if(window.adSlots&&0!==Object.keys(window.adSlots).length)console.log("Ads previously loaded. Will not update ad slots until next page load.");else{window.adSlots||(window.adSlots={});window.getAd=function(b,f){for(var c in window.adSlots)if(-1<c.indexOf(b)){if("object"===f)return adSlots[c];if("slot"===f)return adSlots[c].slot}};googletag.cmd.push(function(){googletag.pubads().setPrivacySettings({limitedAds:e});console.log("limitedAds is ",
e);googletag.pubads().setRequestNonPersonalizedAds(google_tag_manager["rm"]["50443292"](219));googletag.pubads().enableSingleRequest();googletag.pubads().disableInitialLoad()});for(var g=document.querySelectorAll("[data-gpt]"),d=0;g[d];++d)h(g[d]);googletag.cmd.push(function(){googletag.enableServices()})}}createAndLoadAds();</script>
</body></html>