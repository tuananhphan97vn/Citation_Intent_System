<a data-cc-action="privacy" href="https://link.springer.com/privacystatement">privacy policy</a><<<<<<<<<<<>>>>>>>>>>>><a class="c-skip-link" href="#main">Skip to main content</a><<<<<<<<<<<>>>>>>>>>>>><a data-test="springerlink-logo" data-track="click_imprint_logo" data-track-action="click logo link" data-track-category="unified header" data-track-context="unified header" data-track-label="link" href="https://link.springer.com">
<img alt="SpringerLink" src="/oscar-static/images/darwin/header/img/logo-springerlink-39ee2a28d8.svg"/>
</a><<<<<<<<<<<>>>>>>>>>>>><a class="c-header__link eds-c-header__link" href="https://idp.springer.com/auth/personal/springernature?redirect_uri=https://link.springer.com/article/10.1007/s10844-024-00886-5" id="identity-account-widget"><span class="eds-c-header__widget-fragment-title">Log in</span></a><<<<<<<<<<<>>>>>>>>>>>><a aria-controls="eds-c-header-nav" aria-expanded="false" aria-haspopup="true" class="eds-c-header__link" data-eds-c-header-expander="" href="javascript:;" role="button">
<svg aria-hidden="true" class="eds-c-header__icon" focusable="false" height="24" width="24">
<use xlink:href="#icon-eds-i-menu-medium"></use>
</svg><span>Menu</span>
</a><<<<<<<<<<<>>>>>>>>>>>><a class="eds-c-header__link" data-track="nav_find_a_journal" data-track-action="click find a journal" data-track-category="unified header" data-track-context="unified header" data-track-label="link" href="https://link.springer.com/journals/">
                            Find a journal
                        </a><<<<<<<<<<<>>>>>>>>>>>><a class="eds-c-header__link" data-track="nav_how_to_publish" data-track-action="click publish with us link" data-track-category="unified header" data-track-context="unified header" data-track-label="link" href="https://www.springernature.com/gp/authors">
                            Publish with us
                        </a><<<<<<<<<<<>>>>>>>>>>>><a class="eds-c-header__link" data-track="nav_track_your_research" data-track-action="click track your research" data-track-category="unified header" data-track-context="unified header" data-track-label="link" href="https://link.springernature.com/home/">
                            Track your research
                        </a><<<<<<<<<<<>>>>>>>>>>>><a aria-controls="eds-c-header-popup-search" aria-expanded="false" aria-haspopup="true" class="eds-c-header__link" data-eds-c-header-expander="" data-eds-c-header-test-search-btn="" href="javascript:;" role="button">
<svg aria-hidden="true" class="eds-c-header__icon" focusable="false" height="24" width="24">
<use xlink:href="#icon-eds-i-search-medium"></use>
</svg><span>Search</span>
</a><<<<<<<<<<<>>>>>>>>>>>><a class="eds-c-header__link" href="https://order.springer.com/public/cart" style="appearance:none;border:none;background:none;color:inherit;position:relative">
<svg aria-hidden="true" class="eds-c-header__icon" focusable="false" height="24" id="eds-i-cart" viewbox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
<path d="M2 1a1 1 0 0 0 0 2l1.659.001 2.257 12.808a2.599 2.599 0 0 0 2.435 2.185l.167.004 9.976-.001a2.613 2.613 0 0 0 2.61-1.748l.03-.106 1.755-7.82.032-.107a2.546 2.546 0 0 0-.311-1.986l-.108-.157a2.604 2.604 0 0 0-2.197-1.076L6.042 5l-.56-3.17a1 1 0 0 0-.864-.82l-.12-.007L2.001 1ZM20.35 6.996a.63.63 0 0 1 .54.26.55.55 0 0 1 .082.505l-.028.1L19.2 15.63l-.022.05c-.094.177-.282.299-.526.317l-10.145.002a.61.61 0 0 1-.618-.515L6.394 6.999l13.955-.003ZM18 19a2 2 0 1 0 0 4 2 2 0 0 0 0-4ZM8 19a2 2 0 1 0 0 4 2 2 0 0 0 0-4Z" fill="currentColor" fill-rule="nonzero"></path>
</svg><span>Cart</span><span class="cart-info" style="display:none;position:absolute;top:10px;right:45px;background-color:#C65301;color:#fff;width:18px;height:18px;font-size:11px;border-radius:50%;line-height:17.5px;text-align:center"></span></a><<<<<<<<<<<>>>>>>>>>>>><a class="eds-c-header__link" data-track="nav_find_a_journal" data-track-action="click find a journal" data-track-category="unified header" data-track-context="unified header" data-track-label="link" href="https://link.springer.com/journals/">
                        Find a journal
									 </a><<<<<<<<<<<>>>>>>>>>>>><a class="eds-c-header__link" data-track="nav_how_to_publish" data-track-action="click publish with us link" data-track-category="unified header" data-track-context="unified header" data-track-label="link" href="https://www.springernature.com/gp/authors">
                        Publish with us
									 </a><<<<<<<<<<<>>>>>>>>>>>><a class="eds-c-header__link" data-track="nav_track_your_research" data-track-action="click track your research" data-track-category="unified header" data-track-context="unified header" data-track-label="link" href="https://link.springernature.com/home/">
                        Track your research
									 </a><<<<<<<<<<<>>>>>>>>>>>><a class="c-breadcrumbs__link" data-track="click_breadcrumb" data-track-action="breadcrumbs" data-track-category="article" data-track-context="article page" data-track-label="breadcrumb1" href="/" itemprop="item"><span itemprop="name">Home</span></a><<<<<<<<<<<>>>>>>>>>>>><a class="c-breadcrumbs__link" data-track="click_breadcrumb" data-track-action="breadcrumbs" data-track-category="article" data-track-context="article page" data-track-label="breadcrumb2" href="/journal/10844" itemprop="item"><span itemprop="name">Journal of Intelligent Information Systems</span></a><<<<<<<<<<<>>>>>>>>>>>><a data-track="click" data-track-action="cite this article" data-track-category="article body" data-track-label="link" href="#citeas">Cite this article</a><<<<<<<<<<<>>>>>>>>>>>><a class="u-button u-button--full-width u-button--primary u-justify-content-space-between c-pdf-download__link" data-article-pdf="true" data-draft-ignore="true" data-readcube-pdf-url="true" data-test="pdf-link" data-track="content_download" data-track-action="download pdf" data-track-external="" data-track-label="button" data-track-type="article pdf download" download="" href="/content/pdf/10.1007/s10844-024-00886-5.pdf">
<span class="c-pdf-download__text">Download PDF</span>
<svg aria-hidden="true" class="u-icon" focusable="false" height="16" width="16"><use xlink:href="#icon-eds-i-download-medium"></use></svg>
</a><<<<<<<<<<<>>>>>>>>>>>><a class="app-article-masthead__journal-link" data-track="click_journal_home" data-track-action="journal homepage" data-track-context="article page" data-track-label="link" href="/journal/10844">
<picture>
<source height="159" media="(min-width: 768px)" srcset="https://media.springernature.com/w120/springer-static/cover-hires/journal/10844?as=webp,
                                                    https://media.springernature.com/w316/springer-static/cover-hires/journal/10844?as=webp 2x" type="image/webp" width="120"/>
<img alt="" height="95" src="https://media.springernature.com/w72/springer-static/cover-hires/journal/10844?as=webp" srcset="https://media.springernature.com/w144/springer-static/cover-hires/journal/10844?as=webp 2x" width="72"/>
</picture>
<span class="app-article-masthead__journal-title">Journal of Intelligent Information Systems</span>
</a><<<<<<<<<<<>>>>>>>>>>>><a class="app-article-masthead__submission-link" data-track="click_aims_and_scope" data-track-action="aims and scope" data-track-context="article page" data-track-label="link" href="https://link.springer.com/journal/10844/aims-and-scope">
                                Aims and scope
                                <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16"><use xlink:href="#icon-eds-i-arrow-right-medium" xmlns:xlink="http://www.w3.org/1999/xlink"></use></svg>
</a><<<<<<<<<<<>>>>>>>>>>>><a class="app-article-masthead__submission-link" data-track="click_submit_manuscript" data-track-action="submit manuscript" data-track-context="article masthead on springerlink article page" data-track-label="link" href="https://submission.nature.com/new-submission/10844/3">
                                Submit manuscript
                                <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16"><use xlink:href="#icon-eds-i-arrow-right-medium" xmlns:xlink="http://www.w3.org/1999/xlink"></use></svg>
</a><<<<<<<<<<<>>>>>>>>>>>><a class="u-button u-button--full-width u-button--primary u-justify-content-space-between c-pdf-download__link" data-article-pdf="true" data-draft-ignore="true" data-readcube-pdf-url="true" data-test="pdf-link" data-track="content_download" data-track-action="download pdf" data-track-external="" data-track-label="button" data-track-type="article pdf download" download="" href="/content/pdf/10.1007/s10844-024-00886-5.pdf">
<span class="c-pdf-download__text">Download PDF</span>
<svg aria-hidden="true" class="u-icon" focusable="false" height="16" width="16"><use xlink:href="#icon-eds-i-download-medium"></use></svg>
</a><<<<<<<<<<<>>>>>>>>>>>><a data-author-popup="auth-Xiankai-Jiang-Aff1" data-author-search="Jiang, Xiankai" data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Xiankai-Jiang-Aff1">Xiankai Jiang</a><<<<<<<<<<<>>>>>>>>>>>><a href="#Aff1" tabindex="-1">1</a><<<<<<<<<<<>>>>>>>>>>>><a href="#na1" tabindex="-1">na1</a><<<<<<<<<<<>>>>>>>>>>>><a data-author-popup="auth-Jingqiang-Chen-Aff1" data-author-search="Chen, Jingqiang" data-corresp-id="c1" data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Jingqiang-Chen-Aff1">Jingqiang Chen<svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16"><use xlink:href="#icon-eds-i-mail-medium" xmlns:xlink="http://www.w3.org/1999/xlink"></use></svg></a><<<<<<<<<<<>>>>>>>>>>>><a href="#Aff1" tabindex="-1">1</a><<<<<<<<<<<>>>>>>>>>>>><a data-track="click" data-track-action="view metrics" data-track-label="link" href="/article/10.1007/s10844-024-00886-5/metrics" rel="nofollow">Explore all metrics <svg aria-hidden="true" class="u-icon app-article-metrics-bar__arrow-icon" focusable="false" height="24" width="24">
<use xlink:href="#icon-eds-i-arrow-right-medium"></use>
</svg></a><<<<<<<<<<<>>>>>>>>>>>><a class="c-article-recommendations-card__link" data-track="select_recommendations_1" data-track-action="click recommendations inline - 1" data-track-context="inline recommendations" data-track-label="10.1007/s10844-022-00757-x" href="https://link.springer.com/10.1007/s10844-022-00757-x?fromPaywallRec=false" itemprop="url">Extractive text-image summarization with relation-enhanced graph attention network
                                        </a><<<<<<<<<<<>>>>>>>>>>>><a class="c-article-recommendations-card__link" data-track="select_recommendations_2" data-track-action="click recommendations inline - 2" data-track-context="inline recommendations" data-track-label="10.1007/978-981-99-6207-5_17" href="https://link.springer.com/10.1007/978-981-99-6207-5_17?fromPaywallRec=false" itemprop="url">MCLS: A Large-Scale Multimodal Cross-Lingual Summarization Dataset
                                        </a><<<<<<<<<<<>>>>>>>>>>>><a class="c-article-recommendations-card__link" data-track="select_recommendations_3" data-track-action="click recommendations inline - 3" data-track-context="inline recommendations" data-track-label="10.1007/s00521-023-08821-5" href="https://link.springer.com/10.1007/s00521-023-08821-5?fromPaywallRec=false" itemprop="url">Topic-guided abstractive multimodal summarization with multimodal output
                                        </a><<<<<<<<<<<>>>>>>>>>>>><a data-track="select_related_subject_1" data-track-context="related subjects from content page" data-track-label="Artificial Intelligence" href="/subject/artificial-intelligence">Artificial Intelligence</a><<<<<<<<<<<>>>>>>>>>>>><a class="app-card-service__link" data-test="article-checklist-banner-link" data-track="click_presubmission_checklist" data-track-action="clicked article page checklist banner test 2 old version" data-track-category="pre-submission-checklist" data-track-context="article page top of reading companion" data-track-label="link" href="https://beta.springernature.com/pre-submission?journalId=10844">
<span class="app-card-service__link-text">Use our pre-submission checklist</span>
<svg aria-hidden="true" class="app-card-service__link-icon" focusable="false"><use xlink:href="#icon-eds-i-arrow-right-small"></use></svg>
</a><<<<<<<<<<<>>>>>>>>>>>><a aria-label="Reference 2018" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR45" id="ref-link-section-d17414101e305" title="Zhu, J., Li, H., Liu, T., et al. (2018). MSMO: Multimodal summarization with multimodal output. In: Riloff E, Chiang D, Hockenmaier J, et al (eds) Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Brussels, Belgium, pp 4154–4164.                 https://doi.org/10.18653/v1/D18-1448                              ">2018</a><<<<<<<<<<<>>>>>>>>>>>><a data-track="click" data-track-action="figure anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#Fig1">1</a><<<<<<<<<<<>>>>>>>>>>>><a aria-label="Reference 2018" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR45" id="ref-link-section-d17414101e311" title="Zhu, J., Li, H., Liu, T., et al. (2018). MSMO: Multimodal summarization with multimodal output. In: Riloff E, Chiang D, Hockenmaier J, et al (eds) Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Brussels, Belgium, pp 4154–4164.                 https://doi.org/10.18653/v1/D18-1448                              ">2018</a><<<<<<<<<<<>>>>>>>>>>>><a aria-label="Reference 2018" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR45" id="ref-link-section-d17414101e317" title="Zhu, J., Li, H., Liu, T., et al. (2018). MSMO: Multimodal summarization with multimodal output. In: Riloff E, Chiang D, Hockenmaier J, et al (eds) Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Brussels, Belgium, pp 4154–4164.                 https://doi.org/10.18653/v1/D18-1448                              ">2018</a><<<<<<<<<<<>>>>>>>>>>>><a aria-label="Reference 2020" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR46" id="ref-link-section-d17414101e320" title="Zhu, J., Zhou, Y., Zhang, J., et al. (2020). Multimodal summarization with guidance of multimodal reference. Proceedings of the AAAI Conference on Artificial Intelligence, 34(05), 9749–975.                 https://doi.org/10.1609/aaai.v34i05.6525                              ">2020</a><<<<<<<<<<<>>>>>>>>>>>><a aria-label="Reference 2023" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR8" id="ref-link-section-d17414101e323" title="Jiang, C., Xie, R., Ye, W., et al. (2023). Exploiting pseudo image captions for multimodal summarization. In: Rogers A, Boyd-Graber J, Okazaki N (Eds.), Findings of the association for computational linguistics: ACL 2023. Association for Computational Linguistics, Toronto, Canada, pp 161–175.                 https://doi.org/10.18653/v1/2023.findings-acl.12                              ">2023</a><<<<<<<<<<<>>>>>>>>>>>><a aria-label="Reference 2022c" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR43" id="ref-link-section-d17414101e326" title="Zhang, Z., Meng, X., Wang, Y., et al. (2022c). Unims: A unified framework for multimodal summarization with knowledge distillation. Proceedings of the AAAI Conference on Artificial Intelligence 36(10) 11757–11764.                 https://doi.org/10.1609/aaai.v36i10.21431                              ">2022c</a><<<<<<<<<<<>>>>>>>>>>>><a aria-label="Reference 2023" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR8" id="ref-link-section-d17414101e329" title="Jiang, C., Xie, R., Ye, W., et al. (2023). Exploiting pseudo image captions for multimodal summarization. In: Rogers A, Boyd-Graber J, Okazaki N (Eds.), Findings of the association for computational linguistics: ACL 2023. Association for Computational Linguistics, Toronto, Canada, pp 161–175.                 https://doi.org/10.18653/v1/2023.findings-acl.12                              ">2023</a><<<<<<<<<<<>>>>>>>>>>>><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-action="view figure" data-track-label="image" href="/article/10.1007/s10844-024-00886-5/figures/1" rel="nofollow"><picture><source srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10844-024-00886-5/MediaObjects/10844_2024_886_Fig1_HTML.png?as=webp" type="image/webp"/><img alt="figure 1" aria-describedby="Fig1" height="507" loading="lazy" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10844-024-00886-5/MediaObjects/10844_2024_886_Fig1_HTML.png" width="685"/></picture></a><<<<<<<<<<<>>>>>>>>>>>><a aria-label="Full size image figure 1" class="c-article__pill-button" data-test="article-link" data-track="click" data-track-action="view figure" data-track-dest="link:Figure1 Full size image" data-track-label="button" href="/article/10.1007/s10844-024-00886-5/figures/1" rel="nofollow"><span>Full size image</span><svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16"><use xlink:href="#icon-eds-i-chevron-right-small" xmlns:xlink="http://www.w3.org/1999/xlink"></use></svg></a><<<<<<<<<<<>>>>>>>>>>>><a data-track="click" data-track-action="figure anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#Fig1">1</a><<<<<<<<<<<>>>>>>>>>>>><a aria-label="Reference 2020" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR7" id="ref-link-section-d17414101e359" title="Jia, R., Cao, Y., Tang, H., et al. (2020). Neural extractive summarization with hierarchical attentive heterogeneous graph network. In: Webber B, Cohn T, He Y, et al (Eds.), Proceedings of the 2020 conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics, Online, pp 3622–363.                 https://doi.org/10.18653/v1/2020.emnlp-main.295                              ">2020</a><<<<<<<<<<<>>>>>>>>>>>><a aria-label="Reference 2022" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR30" id="ref-link-section-d17414101e362" title="Song, Z., &amp; King, I. (2022). Hierarchical heterogeneous graph attention network for syntax-aware summarization. Proceedings of the AAAI Conference on Artificial Intelligence, 36(10), 11340–1134.                 https://doi.org/10.1609/aaai.v36i10.21385                              ">2022</a><<<<<<<<<<<>>>>>>>>>>>><a aria-label="Reference 2020" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR35" id="ref-link-section-d17414101e365" title="Wang, D., Liu, P., Zheng, Y., et al. (2020). Heterogeneous graph neural networks for extractive document summarization. In: Jurafsky D, Chai J, Schluter N, et al (Eds.), Proceedings of the 58th annual meeting of the association for computational linguistics. Association for Computational Linguistics, Online, pp 6209–6219.                 https://doi.org/10.18653/v1/2020.acl-main.553                              ">2020</a><<<<<<<<<<<>>>>>>>>>>>><a aria-label="Reference 2020" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR5" id="ref-link-section-d17414101e368" title="Cui, P., Hu, L., &amp; Liu, Y. (2020). Enhancing extractive text summarization with topic-aware graph neural networks. In: Scott D, Bel N, Zong C (Eds.), Proceedings of the 28th international conference on computational linguistics. International Committee on Computational Linguistics, Barcelona, Spain (Online), pp 5360–537.                 https://doi.org/10.18653/v1/2020.coling-main.468                              ">2020</a><<<<<<<<<<<>>>>>>>>>>>><a aria-label="Reference 2022" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR27" id="ref-link-section-d17414101e371" title="Phan TA, Nguyen NDN, &amp; Bui KHN. (2022). HeterGraphLongSum: Heterogeneous graph neural network with passage aggregation for extractive long document summarization. In: Calzolari N, Huang CR, Kim H, et al (Eds.), Proceedings of the 29th international conference on computational linguistics. International Committee on Computational Linguistics, Gyeongju, Republic of Korea, pp 6248–6258.                 https://aclanthology.org/2022.coling-1.545                              ">2022</a><<<<<<<<<<<>>>>>>>>>>>><a aria-label="Reference 2021" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR39" id="ref-link-section-d17414101e378" title="Ying, C., Cai, T., Luo, S., et al. (2021). Do transformers really perform badly for graph representation? In: Ranzato M, Beygelzimer A, Dauphin YN, et al (Eds.), Advances in Neural Information Processing Systems, pp 28877–28888.                 https://proceedings.neurips.cc/paper/2021/hash/f1c1592588411002af340cbaedd6fc33-Abstract.html                              ">2021</a><<<<<<<<<<<>>>>>>>>>>>><a aria-label="Reference 2014" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR31" id="ref-link-section-d17414101e431" title="Sutskever, I., Vinyals, O., &amp; Le, Q.V. (2014). Sequence to sequence learning with neural networks. In: Ghahramani Z, Welling M, Cortes C, et al (Eds.), Advances in Neural Information Processing Systems,[SPACE]                https://proceedings.neurips.cc/paper/2014/hash/a14ac55a4f27472c5d894ec1c3c743d2-Abstract.html                              ">2014</a><<<<<<<<<<<>>>>>>>>>>>><a aria-label="Reference 2017" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR29" id="ref-link-section-d17414101e434" title="See, A., Liu, P.J., Manning, C.D. (2017). Get to the point: Summarization with pointer-generator networks. In: Barzilay R, Kan MY (Eds.), Proceedings of the 55th annual meeting of the association for computational linguistics (Volume 1: Long Papers). Association for Computational Linguistics, Vancouver, Canada, pp 1073–1083.                 https://doi.org/10.18653/v1/P17-1099                              ">2017</a><<<<<<<<<<<>>>>>>>>>>>><a aria-label="Reference 2018" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR25" id="ref-link-section-d17414101e437" title="Paulus, R., Xiong, C., &amp; Socher, R. (2018). A deep reinforced model for abstractive summarization. In: International conference on learning representations,                 https://openreview.net/forum?id=HkAClQgA-                              ">2018</a><<<<<<<<<<<>>>>>>>>>>>><a aria-label="Reference 2020" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR40" id="ref-link-section-d17414101e440" title="Zhang, J., Zhao, Y., Saleh, M., et al. (2020). PEGASUS: pre-training with extracted gap-sentences for abstractive summarization. In: Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, pp 11328–11339.                 http://proceedings.mlr.press/v119/zhang20ae.html                              ">2020</a><<<<<<<<<<<>>>>>>>>>>>><a aria-label="Reference 2022" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR19" id="ref-link-section-d17414101e443" title="Liu, Y., Liu, P., Radev, D., et al. (2022). BRIO: Bringing order to abstractive summarization. In: Muresan S, Nakov P, Villavicencio A (Eds.), Proceedings of the 60th annual meeting of the association for computational linguistics (Volume 1: Long Papers). Association for Computational Linguistics, Dublin, Ireland, pp 2890–2903.                 https://doi.org/10.18653/v1/2022.acl-long.207                              ">2022</a><<<<<<<<<<<>>>>>>>>>>>><a aria-label="Reference 2016" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR3" id="ref-link-section-d17414101e447" title="Cheng, J., &amp; Lapata, M. (2016). Neural summarization by extracting sentences and words. In: Erk K, Smith NA (Eds.), Proceedings of the 54th annual meeting of the association for computational linguistics (Volume 1: Long Papers). Association for Computational Linguistics, Berlin, Germany, pp 484–494.                 https://doi.org/10.18653/v1/P16-1046                              ">2016</a><<<<<<<<<<<>>>>>>>>>>>><a aria-label="Reference 2022" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR1" id="ref-link-section-d17414101e450" title="An, C., Zhong, M., Wu, Z., et al. (2022). CoLo: A contrastive learning based re-ranking framework for one-stage summarization. In: Calzolari N, Huang CR, Kim H, et al (Eds.), Proceedings of the 29th international conference on computational linguistics. International Committee on Computational Linguistics, Gyeongju, Republic of Korea, pp 5783–5793.                 https://aclanthology.org/2022.coling-1.508                              ">2022</a><<<<<<<<<<<>>>>>>>>>>>><a aria-label="Reference 2023" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR4" id="ref-link-section-d17414101e453" title="Cheng, X., Shen, Y., &amp; Lu, W. (2023). A set prediction network for extractive summarization. In: Rogers A, Boyd-Graber J, Okazaki N (Eds.), Findings of the association for computational linguistics: ACL 2023. Association for Computational Linguistics, Toronto, Canada, pp 4766–4777.                 https://doi.org/10.18653/v1/2023.findings-acl.293                              ">2023</a><<<<<<<<<<<>>>>>>>>>>>><a aria-label="Reference 2017" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR22" id="ref-link-section-d17414101e459" title="Nallapati R, Zhai F, &amp; Zhou B. (2017) .Summarunner: A recurrent neural network based sequence model for extractive summarization of documents. Proceedings of the AAAI Conference on Artificial Intelligence 31(1).                 https://doi.org/10.1609/aaai.v31i1.10958                              ">2017</a><<<<<<<<<<<>>>>>>>>>>>><a aria-label="Reference 2018" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR23" id="ref-link-section-d17414101e462" title="Narayan S, Cohen SB, &amp; Lapata M. (2018). Ranking sentences for extractive summarization with reinforcement learning. In: Walker M, Ji H, Stent A (Eds.) Proceedings of the 2018 conference of the North American chapter of the association for computational linguistics: Human language technologies, Volume 1 (Long Papers). Association for Computational Linguistics, New Orleans, Louisiana, pp 1747–1759.                 https://doi.org/10.18653/v1/N18-1158                              ">2018</a><<<<<<<<<<<>>>>>>>>>>>><a aria-label="Reference 2019" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR17" id="ref-link-section-d17414101e465" title="Liu Y, &amp; Lapata M. (2019). Text summarization with pretrained encoders. In: Inui K, Jiang J, Ng V, et al (Eds.), Proceedings of the 2019 conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). Association for Computational Linguistics, Hong Kong, China, pp 3730–3740.                 https://doi.org/10.18653/v1/D19-1387                              ">2019</a><<<<<<<<<<<>>>>>>>>>>>><a aria-label="Reference 2020" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR44" id="ref-link-section-d17414101e468" title="Zhong, M., Liu, P., Chen, Y., et al. (2020). Extractive summarization as text matching. In: Jurafsky D, Chai J, Schluter N, et al (Eds.) Proceedings of the 58th annual meeting of the association for computational linguistics. Association for Computational Linguistics, Online, pp 6197–6208.                 https://doi.org/10.18653/v1/2020.acl-main.552                              ">2020</a><<<<<<<<<<<>>>>>>>>>>>><a aria-label="Reference 2022" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR32" id="ref-link-section-d17414101e471" title="Tang, P., Hu, K., Yan, R., et al. (2022). OTExtSum: Extractive text summarisation with optimal transport. In: Carpuat M, de Marneffe MC, Meza Ruiz IV (eds) Findings of the association for computational linguistics: NAACL 2022. Association for Computational Linguistics, Seattle, United States, pp 1128–1141.                 https://doi.org/10.18653/v1/2022.findings-naacl.85                              ">2022</a><<<<<<<<<<<>>>>>>>>>>>><a aria-label="Reference 2018" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR13" id="ref-link-section-d17414101e482" title="Li, H., Zhu, J., Liu, T., et al. (2018). Multi-modal sentence summarization with modality attention and image filtering. In: Lang J (Ed.), Proceedings of the twenty-seventh international joint conference on artificial intelligence, IJCAI 2018, July 13-19, 2018, Stockholm, Sweden. ijcai.org, Stockholm, Sweden, pp 4152–4158.                 https://doi.org/10.24963/ijcai.2018/577                              ">2018</a><<<<<<<<<<<>>>>>>>>>>>><a aria-label="Reference 2023" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR24" id="ref-link-section-d17414101e485" title="Overbay, K., Ahn, J., Pesaran zadeh, F., et al. (2023). mRedditSum: A multimodal abstractive summarization dataset of Reddit threads with images. In: Bouamor H, Pino J, Bali K (Eds.), Proceedings of the 2023 conference on empirical methods in natural language processing. Association for Computational Linguistics, Singapore, pp 4117–4132.                 https://doi.org/10.18653/v1/2023.emnlp-main.251                              ">2023</a><<<<<<<<<<<>>>>>>>>>>>><a aria-label="Reference 2023" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR16" id="ref-link-section-d17414101e488" title="Liang, Y., Meng, F., Xu, J., et al. (2023). Summary-oriented vision modeling for multimodal abstractive summarization. In: Rogers A, Boyd-Graber J, Okazaki N (Eds.), Proceedings of the 61st annual meeting of the association for computational linguistics (Volume 1: Long Papers). Association for Computational Linguistics, Toronto, Canada, pp 2934–2951.                 https://doi.org/10.18653/v1/2023.acl-long.165                              ">2023</a><<<<<<<<<<<>>>>>>>>>>>><a aria-label="Reference 2020" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR15" id="ref-link-section-d17414101e491" title="Li, M., Chen, X., Gao, S., et al (2020) VMSMO: Learning to generate multimodal summary for video-based news articles. In: Webber B, Cohn T, He Y, et al (Eds.), Proceedings of the 2020 conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics, Online, pp 9360–9369.                 https://doi.org/10.18653/v1/2020.emnlp-main.752                              ">2020</a><<<<<<<<<<<>>>>>>>>>>>><a aria-label="Reference 2022a" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR41" id="ref-link-section-d17414101e494" title="Zhang, L., Zhang, X., &amp; Pan, J. (2022a). Hierarchical cross-modality semantic correlation learning model for multimodal summarization. Proceedings of the AAAI Conference on Artificial Intelligence 36(10) 11676–11684.                 https://doi.org/10.1609/aaai.v36i10.21422                              ">2022a</a><<<<<<<<<<<>>>>>>>>>>>><a aria-label="Reference 2018" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR2" id="ref-link-section-d17414101e500" title="Chen, J., &amp; Zhuge, H. (2018). Abstractive text-image summarization using multi-modal attentional hierarchical RNN. In: Riloff E, Chiang D, Hockenmaier J, et al (Eds.), Proceedings of the 2018 conference on empirical methods in natural language processing. Association for Computational Linguistics, Brussels, Belgium, pp 4046–4056.                 https://doi.org/10.18653/v1/D18-1438                              ">2018</a><<<<<<<<<<<>>>>>>>>>>>><a aria-label="Reference 2022b" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR42" id="ref-link-section-d17414101e503" title="Zhang, M., Zhou, G., Yu, W., et al. (2022b). MAA-PTG: multimodal aspect-aware product title generation. J Intell Inf Syst 59(1) 213–235.                 https://doi.org/10.1007/S10844-022-00695-8                              ">2022b</a><<<<<<<<<<<>>>>>>>>>>>><a aria-label="Reference 2023" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR37" id="ref-link-section-d17414101e506" title="Xiao M, Zhu J, Lin H, et al. (2023). CFSum coarse-to-fine contribution network for multimodal summarization. In: Rogers A, Boyd-Graber J, Okazaki N (Eds.), Proceedings of the 61st annual meeting of the association for computational linguistics (Volume 1: Long Papers). Association for Computational Linguistics, Toronto, Canada, pp 8538–8553.                 https://doi.org/10.18653/v1/2023.acl-long.476                              ">2023</a><<<<<<<<<<<>>>>>>>>>>>><a aria-label="Reference 2024" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR9" id="ref-link-section-d17414101e509" title="Jin, L., &amp; Chen, J. (2024). Self-supervised opinion summarization with multi-modal knowledge graph. J Intell Inf Syst, 62(1), 191–208.                 https://doi.org/10.1007/S10844-023-00812-1                              ">2024</a><<<<<<<<<<<>>>>>>>>>>>><a aria-label="Reference 2018" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR45" id="ref-link-section-d17414101e515" title="Zhu, J., Li, H., Liu, T., et al. (2018). MSMO: Multimodal summarization with multimodal output. In: Riloff E, Chiang D, Hockenmaier J, et al (eds) Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Brussels, Belgium, pp 4154–4164.                 https://doi.org/10.18653/v1/D18-1448                              ">2018</a><<<<<<<<<<<>>>>>>>>>>>><a aria-label="Reference 2020" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR46" id="ref-link-section-d17414101e518" title="Zhu, J., Zhou, Y., Zhang, J., et al. (2020). Multimodal summarization with guidance of multimodal reference. Proceedings of the AAAI Conference on Artificial Intelligence, 34(05), 9749–975.                 https://doi.org/10.1609/aaai.v34i05.6525                              ">2020</a><<<<<<<<<<<>>>>>>>>>>>><a aria-label="Reference 2022c" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR43" id="ref-link-section-d17414101e521" title="Zhang, Z., Meng, X., Wang, Y., et al. (2022c). Unims: A unified framework for multimodal summarization with knowledge distillation. Proceedings of the AAAI Conference on Artificial Intelligence 36(10) 11757–11764.                 https://doi.org/10.1609/aaai.v36i10.21431                              ">2022c</a><<<<<<<<<<<>>>>>>>>>>>><a aria-label="Reference 2023" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR38" id="ref-link-section-d17414101e524" title="Xie, F., Chen, J., &amp; Chen, K. (2023). Extractive text-image summarization with relation-enhanced graph attention network. J Intell Inf Syst, 61(2), 325–341.                 https://doi.org/10.1007/S10844-022-00757-X                              ">2023</a><<<<<<<<<<<>>>>>>>>>>>><a aria-label="Reference 2023" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR8" id="ref-link-section-d17414101e527" title="Jiang, C., Xie, R., Ye, W., et al. (2023). Exploiting pseudo image captions for multimodal summarization. In: Rogers A, Boyd-Graber J, Okazaki N (Eds.), Findings of the association for computational linguistics: ACL 2023. Association for Computational Linguistics, Toronto, Canada, pp 161–175.                 https://doi.org/10.18653/v1/2023.findings-acl.12                              ">2023</a><<<<<<<<<<<>>>>>>>>>>>><a aria-label="Reference 2004" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR21" id="ref-link-section-d17414101e538" title="Mihalcea, R., Tarau, P (2004) TextRank: Bringing order into text. In: Lin D, Wu D (Eds.), Proceedings of the 2004 conference on empirical methods in natural language processing. Association for Computational Linguistics, Barcelona, Spain, pp 404–411.                 https://aclanthology.org/W04-3252                              ">2004</a><<<<<<<<<<<>>>>>>>>>>>><a aria-label="Reference 2004" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR6" id="ref-link-section-d17414101e541" title="Erkan, G., &amp; Radev, D. R. (2004). Lexrank: Graph-based lexical centrality as salience in text summarization. J Artif Intell Res, 22, 457–47.                 https://doi.org/10.1613/JAIR.1523                              ">2004</a><<<<<<<<<<<>>>>>>>>>>>><a aria-label="Reference 2020" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR35" id="ref-link-section-d17414101e544" title="Wang, D., Liu, P., Zheng, Y., et al. (2020). Heterogeneous graph neural networks for extractive document summarization. In: Jurafsky D, Chai J, Schluter N, et al (Eds.), Proceedings of the 58th annual meeting of the association for computational linguistics. Association for Computational Linguistics, Online, pp 6209–6219.                 https://doi.org/10.18653/v1/2020.acl-main.553                              ">2020</a><<<<<<<<<<<>>>>>>>>>>>><a aria-label="Reference 2020" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR7" id="ref-link-section-d17414101e547" title="Jia, R., Cao, Y., Tang, H., et al. (2020). Neural extractive summarization with hierarchical attentive heterogeneous graph network. In: Webber B, Cohn T, He Y, et al (Eds.), Proceedings of the 2020 conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics, Online, pp 3622–363.                 https://doi.org/10.18653/v1/2020.emnlp-main.295                              ">2020</a><<<<<<<<<<<>>>>>>>>>>>><a aria-label="Reference 2020" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR5" id="ref-link-section-d17414101e550" title="Cui, P., Hu, L., &amp; Liu, Y. (2020). Enhancing extractive text summarization with topic-aware graph neural networks. In: Scott D, Bel N, Zong C (Eds.), Proceedings of the 28th international conference on computational linguistics. International Committee on Computational Linguistics, Barcelona, Spain (Online), pp 5360–537.                 https://doi.org/10.18653/v1/2020.coling-main.468                              ">2020</a><<<<<<<<<<<>>>>>>>>>>>><a aria-label="Reference 2021" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR10" id="ref-link-section-d17414101e554" title="Jing, B., You, Z., Yang, T., et al. (2021). Multiplex graph neural network for extractive text summarization. In: Moens MF, Huang X, Specia L, et al. (Eds.), Proceedings of the 2021 conference on empirical methods in natural language processing. Association for Computational Linguistics, Online and Punta Cana, Dominican Republic, pp 133–139.                 https://doi.org/10.18653/v1/2021.emnlp-main.11                              ">2021</a><<<<<<<<<<<>>>>>>>>>>>><a aria-label="Reference 2022" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR30" id="ref-link-section-d17414101e557" title="Song, Z., &amp; King, I. (2022). Hierarchical heterogeneous graph attention network for syntax-aware summarization. Proceedings of the AAAI Conference on Artificial Intelligence, 36(10), 11340–1134.                 https://doi.org/10.1609/aaai.v36i10.21385                              ">2022</a><<<<<<<<<<<>>>>>>>>>>>><a aria-label="Reference 2022" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR27" id="ref-link-section-d17414101e560" title="Phan TA, Nguyen NDN, &amp; Bui KHN. (2022). HeterGraphLongSum: Heterogeneous graph neural network with passage aggregation for extractive long document summarization. In: Calzolari N, Huang CR, Kim H, et al (Eds.), Proceedings of the 29th international conference on computational linguistics. International Committee on Computational Linguistics, Gyeongju, Republic of Korea, pp 6248–6258.                 https://aclanthology.org/2022.coling-1.545                              ">2022</a><<<<<<<<<<<>>>>>>>>>>>><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-action="view figure" data-track-label="image" href="/article/10.1007/s10844-024-00886-5/figures/2" rel="nofollow"><picture><source srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10844-024-00886-5/MediaObjects/10844_2024_886_Fig2_HTML.png?as=webp" type="image/webp"/><img alt="figure 2" aria-describedby="Fig2" height="280" loading="lazy" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10844-024-00886-5/MediaObjects/10844_2024_886_Fig2_HTML.png" width="685"/></picture></a><<<<<<<<<<<>>>>>>>>>>>><a aria-label="Full size image figure 2" class="c-article__pill-button" data-test="article-link" data-track="click" data-track-action="view figure" data-track-dest="link:Figure2 Full size image" data-track-label="button" href="/article/10.1007/s10844-024-00886-5/figures/2" rel="nofollow"><span>Full size image</span><svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16"><use xlink:href="#icon-eds-i-chevron-right-small" xmlns:xlink="http://www.w3.org/1999/xlink"></use></svg></a><<<<<<<<<<<>>>>>>>>>>>><a data-track="click" data-track-action="figure anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#Fig2">2</a><<<<<<<<<<<>>>>>>>>>>>><a aria-label="Reference 2022" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR14" id="ref-link-section-d17414101e1412" title="Li, J., Li, D., Xiong, C., et al. (2022). BLIP: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In: Chaudhuri K, Jegelka S, Song L, et al (Eds.), International conference on machine learning, ICML 2022, 17-23 July 2022, PMLR, Baltimore, Maryland, USA, Proceedings of Machine Learning Research, 162 pp 12888–12900">2022</a><<<<<<<<<<<>>>>>>>>>>>><a aria-label="Reference 2014" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR26" id="ref-link-section-d17414101e1418" title="Pennington, J., Socher, R., &amp; Manning, C. (2014). GloVe: Global vectors for word representation. In: Moschitti A, Pang B, Daelemans W (Eds.) Proceedings of the 2014 conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics, Doha, Qatar, pp 1532–1543.                 https://doi.org/10.3115/v1/D14-1162                              ">2014</a><<<<<<<<<<<>>>>>>>>>>>><a aria-label="Reference 2019" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR18" id="ref-link-section-d17414101e1421" title="Liu, Y., Ott, M., Goyal, N., et al. (2019). Roberta: A robustly optimized BERT pretraining approach.                 arXiv:1907.11692                                              https://arxiv.org/abs/1907.11692                              ">2019</a><<<<<<<<<<<>>>>>>>>>>>><a aria-label="Reference 2017" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR33" id="ref-link-section-d17414101e1817" title="Vaswani A, Shazeer N, Parmar N, et al. (2017). Attention is all you need. In: Guyon I, Luxburg UV, Bengio S, et al (Eds.), Advances in Neural Information Processing Systems,[SPACE]                https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html                              ">2017</a><<<<<<<<<<<>>>>>>>>>>>><a aria-label="Reference 2017" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR33" id="ref-link-section-d17414101e1828" title="Vaswani A, Shazeer N, Parmar N, et al. (2017). Attention is all you need. In: Guyon I, Luxburg UV, Bengio S, et al (Eds.), Advances in Neural Information Processing Systems,[SPACE]                https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html                              ">2017</a><<<<<<<<<<<>>>>>>>>>>>><a aria-label="Reference 2021" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR39" id="ref-link-section-d17414101e1983" title="Ying, C., Cai, T., Luo, S., et al. (2021). Do transformers really perform badly for graph representation? In: Ranzato M, Beygelzimer A, Dauphin YN, et al (Eds.), Advances in Neural Information Processing Systems, pp 28877–28888.                 https://proceedings.neurips.cc/paper/2021/hash/f1c1592588411002af340cbaedd6fc33-Abstract.html                              ">2021</a><<<<<<<<<<<>>>>>>>>>>>><a data-track="click" data-track-action="equation anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#Equ5">5</a><<<<<<<<<<<>>>>>>>>>>>><a aria-label="Reference 2017" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR22" id="ref-link-section-d17414101e4570" title="Nallapati R, Zhai F, &amp; Zhou B. (2017) .Summarunner: A recurrent neural network based sequence model for extractive summarization of documents. Proceedings of the AAAI Conference on Artificial Intelligence 31(1).                 https://doi.org/10.1609/aaai.v31i1.10958                              ">2017</a><<<<<<<<<<<>>>>>>>>>>>><a href="#Fn1"><span class="u-visually-hidden">Footnote </span>1</a><<<<<<<<<<<>>>>>>>>>>>><a aria-label="Reference 2022c" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR43" id="ref-link-section-d17414101e4743" title="Zhang, Z., Meng, X., Wang, Y., et al. (2022c). Unims: A unified framework for multimodal summarization with knowledge distillation. Proceedings of the AAAI Conference on Artificial Intelligence 36(10) 11757–11764.                 https://doi.org/10.1609/aaai.v36i10.21431                              ">2022c</a><<<<<<<<<<<>>>>>>>>>>>><a aria-label="Full size table 1" class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" href="/article/10.1007/s10844-024-00886-5/tables/1" rel="nofollow"><span>Full size table</span><svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16"><use xlink:href="#icon-eds-i-chevron-right-small" xmlns:xlink="http://www.w3.org/1999/xlink"></use></svg></a><<<<<<<<<<<>>>>>>>>>>>><a aria-label="Reference 2018" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR45" id="ref-link-section-d17414101e5158" title="Zhu, J., Li, H., Liu, T., et al. (2018). MSMO: Multimodal summarization with multimodal output. In: Riloff E, Chiang D, Hockenmaier J, et al (eds) Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Brussels, Belgium, pp 4154–4164.                 https://doi.org/10.18653/v1/D18-1448                              ">2018</a><<<<<<<<<<<>>>>>>>>>>>><a href="#Fn2"><span class="u-visually-hidden">Footnote </span>2</a><<<<<<<<<<<>>>>>>>>>>>><a data-track="click" data-track-action="table anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#Tab1">1</a><<<<<<<<<<<>>>>>>>>>>>><a href="#Fn3"><span class="u-visually-hidden">Footnote </span>3</a><<<<<<<<<<<>>>>>>>>>>>><a href="#Fn4"><span class="u-visually-hidden">Footnote </span>4</a><<<<<<<<<<<>>>>>>>>>>>><a aria-label="Reference 2015" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR11" id="ref-link-section-d17414101e5208" title="Kingma, D.P., &amp; Ba, J (2015). Adam: A method for stochastic optimization. In: Bengio Y, LeCun Y (Eds.), 3rd International conference on learning representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings,[SPACE]                http://arxiv.org/abs/1412.6980                              ">2015</a><<<<<<<<<<<>>>>>>>>>>>><a aria-label="Reference 2017" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR22" id="ref-link-section-d17414101e5248" title="Nallapati R, Zhai F, &amp; Zhou B. (2017) .Summarunner: A recurrent neural network based sequence model for extractive summarization of documents. Proceedings of the AAAI Conference on Artificial Intelligence 31(1).                 https://doi.org/10.1609/aaai.v31i1.10958                              ">2017</a><<<<<<<<<<<>>>>>>>>>>>><a aria-label="Reference 2018" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR45" id="ref-link-section-d17414101e5267" title="Zhu, J., Li, H., Liu, T., et al. (2018). MSMO: Multimodal summarization with multimodal output. In: Riloff E, Chiang D, Hockenmaier J, et al (eds) Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Brussels, Belgium, pp 4154–4164.                 https://doi.org/10.18653/v1/D18-1448                              ">2018</a><<<<<<<<<<<>>>>>>>>>>>><a aria-label="Reference 2017" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR29" id="ref-link-section-d17414101e5270" title="See, A., Liu, P.J., Manning, C.D. (2017). Get to the point: Summarization with pointer-generator networks. In: Barzilay R, Kan MY (Eds.), Proceedings of the 55th annual meeting of the association for computational linguistics (Volume 1: Long Papers). Association for Computational Linguistics, Vancouver, Canada, pp 1073–1083.                 https://doi.org/10.18653/v1/P17-1099                              ">2017</a><<<<<<<<<<<>>>>>>>>>>>><a aria-label="Reference 2020" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR46" id="ref-link-section-d17414101e5279" title="Zhu, J., Zhou, Y., Zhang, J., et al. (2020). Multimodal summarization with guidance of multimodal reference. Proceedings of the AAAI Conference on Artificial Intelligence, 34(05), 9749–975.                 https://doi.org/10.1609/aaai.v34i05.6525                              ">2020</a><<<<<<<<<<<>>>>>>>>>>>><a aria-label="Reference 2022c" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR43" id="ref-link-section-d17414101e5287" title="Zhang, Z., Meng, X., Wang, Y., et al. (2022c). Unims: A unified framework for multimodal summarization with knowledge distillation. Proceedings of the AAAI Conference on Artificial Intelligence 36(10) 11757–11764.                 https://doi.org/10.1609/aaai.v36i10.21431                              ">2022c</a><<<<<<<<<<<>>>>>>>>>>>><a aria-label="Reference 2020" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR12" id="ref-link-section-d17414101e5290" title="Lewis, M., Liu, Y., Goyal, N., et al. (2020). BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In: Jurafsky D, Chai J, Schluter N, et al (Eds.), Proceedings of the 58th annual meeting of the association for computational linguistics. Association for Computational Linguistics, Online, pp 7871–7880.                 https://doi.org/10.18653/v1/2020.acl-main.703                              ">2020</a><<<<<<<<<<<>>>>>>>>>>>><a aria-label="Reference 2023" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR8" id="ref-link-section-d17414101e5298" title="Jiang, C., Xie, R., Ye, W., et al. (2023). Exploiting pseudo image captions for multimodal summarization. In: Rogers A, Boyd-Graber J, Okazaki N (Eds.), Findings of the association for computational linguistics: ACL 2023. Association for Computational Linguistics, Toronto, Canada, pp 161–175.                 https://doi.org/10.18653/v1/2023.findings-acl.12                              ">2023</a><<<<<<<<<<<>>>>>>>>>>>><a aria-label="Reference 2017" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR34" id="ref-link-section-d17414101e5306" title="Velickovic, P., Cucurull, G., Casanova, A., et al. (2017). Graph attention networks. stat, 1050(20), 10–48550.">2017</a><<<<<<<<<<<>>>>>>>>>>>><a aria-label="Reference 2020" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR35" id="ref-link-section-d17414101e5309" title="Wang, D., Liu, P., Zheng, Y., et al. (2020). Heterogeneous graph neural networks for extractive document summarization. In: Jurafsky D, Chai J, Schluter N, et al (Eds.), Proceedings of the 58th annual meeting of the association for computational linguistics. Association for Computational Linguistics, Online, pp 6209–6219.                 https://doi.org/10.18653/v1/2020.acl-main.553                              ">2020</a><<<<<<<<<<<>>>>>>>>>>>><a aria-label="Reference 2018" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR45" id="ref-link-section-d17414101e5371" title="Zhu, J., Li, H., Liu, T., et al. (2018). MSMO: Multimodal summarization with multimodal output. In: Riloff E, Chiang D, Hockenmaier J, et al (eds) Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Brussels, Belgium, pp 4154–4164.                 https://doi.org/10.18653/v1/D18-1448                              ">2018</a><<<<<<<<<<<>>>>>>>>>>>><a aria-label="Full size table 2" class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" href="/article/10.1007/s10844-024-00886-5/tables/2" rel="nofollow"><span>Full size table</span><svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16"><use xlink:href="#icon-eds-i-chevron-right-small" xmlns:xlink="http://www.w3.org/1999/xlink"></use></svg></a><<<<<<<<<<<>>>>>>>>>>>><a data-track="click" data-track-action="table anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#Tab2">2</a><<<<<<<<<<<>>>>>>>>>>>><a aria-label="Full size table 3" class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" href="/article/10.1007/s10844-024-00886-5/tables/3" rel="nofollow"><span>Full size table</span><svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16"><use xlink:href="#icon-eds-i-chevron-right-small" xmlns:xlink="http://www.w3.org/1999/xlink"></use></svg></a><<<<<<<<<<<>>>>>>>>>>>><a data-track="click" data-track-action="table anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#Tab3">3</a><<<<<<<<<<<>>>>>>>>>>>><a aria-label="Reference 2021" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR28" id="ref-link-section-d17414101e6275" title="Radford A, Kim JW, Hallacy C, et al (2021) Learning transferable visual models from natural language supervision. In: Meila M, Zhang T (Eds.), Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, pp 8748–8763.                 http://proceedings.mlr.press/v139/radford21a.html                              ">2021</a><<<<<<<<<<<>>>>>>>>>>>><a aria-label="Full size table 4" class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" href="/article/10.1007/s10844-024-00886-5/tables/4" rel="nofollow"><span>Full size table</span><svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16"><use xlink:href="#icon-eds-i-chevron-right-small" xmlns:xlink="http://www.w3.org/1999/xlink"></use></svg></a><<<<<<<<<<<>>>>>>>>>>>><a data-track="click" data-track-action="table anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#Tab4">4</a><<<<<<<<<<<>>>>>>>>>>>><a data-track="click" data-track-action="figure anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#Fig3">3</a><<<<<<<<<<<>>>>>>>>>>>><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-action="view figure" data-track-label="image" href="/article/10.1007/s10844-024-00886-5/figures/3" rel="nofollow"><picture><source srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10844-024-00886-5/MediaObjects/10844_2024_886_Fig3_HTML.png?as=webp" type="image/webp"/><img alt="figure 3" aria-describedby="Fig3" height="493" loading="lazy" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10844-024-00886-5/MediaObjects/10844_2024_886_Fig3_HTML.png" width="685"/></picture></a><<<<<<<<<<<>>>>>>>>>>>><a aria-label="Full size image figure 3" class="c-article__pill-button" data-test="article-link" data-track="click" data-track-action="view figure" data-track-dest="link:Figure3 Full size image" data-track-label="button" href="/article/10.1007/s10844-024-00886-5/figures/3" rel="nofollow"><span>Full size image</span><svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16"><use xlink:href="#icon-eds-i-chevron-right-small" xmlns:xlink="http://www.w3.org/1999/xlink"></use></svg></a><<<<<<<<<<<>>>>>>>>>>>><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-action="view figure" data-track-label="image" href="/article/10.1007/s10844-024-00886-5/figures/4" rel="nofollow"><picture><source srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10844-024-00886-5/MediaObjects/10844_2024_886_Fig4_HTML.png?as=webp" type="image/webp"/><img alt="figure 4" aria-describedby="Fig4" height="497" loading="lazy" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10844-024-00886-5/MediaObjects/10844_2024_886_Fig4_HTML.png" width="685"/></picture></a><<<<<<<<<<<>>>>>>>>>>>><a aria-label="Full size image figure 4" class="c-article__pill-button" data-test="article-link" data-track="click" data-track-action="view figure" data-track-dest="link:Figure4 Full size image" data-track-label="button" href="/article/10.1007/s10844-024-00886-5/figures/4" rel="nofollow"><span>Full size image</span><svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16"><use xlink:href="#icon-eds-i-chevron-right-small" xmlns:xlink="http://www.w3.org/1999/xlink"></use></svg></a><<<<<<<<<<<>>>>>>>>>>>><a aria-label="Reference 2008" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR20" id="ref-link-section-d17414101e6502" title="van der Maaten L, &amp; Hinton G (2008) Visualizing data using t-sne. Journal of Machine Learning Research 9(86), 2579–2605.                 http://jmlr.org/papers/v9/vandermaaten08a.html                              ">2008</a><<<<<<<<<<<>>>>>>>>>>>><a data-track="click" data-track-action="figure anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#Fig4">4</a><<<<<<<<<<<>>>>>>>>>>>><a aria-label="Reference 2021" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#ref-CR36" id="ref-link-section-d17414101e6519" title="Wu W, Li W, Xiao X, et al. (2021). BASS: Boosting abstractive summarization with unified semantic graph. In: Proceedings of the 59th annual meeting of the association for computational linguistics and the 11th international joint conference on natural language processing (Volume 1: Long Papers). Association for Computational Linguistics, Online, pp 6052–6067.                https://doi.org/10.18653/v1/2021.acl-long.472                              ">2021</a><<<<<<<<<<<>>>>>>>>>>>><a aria-label="Full size table 5" class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" href="/article/10.1007/s10844-024-00886-5/tables/5" rel="nofollow"><span>Full size table</span><svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16"><use xlink:href="#icon-eds-i-chevron-right-small" xmlns:xlink="http://www.w3.org/1999/xlink"></use></svg></a><<<<<<<<<<<>>>>>>>>>>>><a aria-label="Full size table 6" class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" href="/article/10.1007/s10844-024-00886-5/tables/6" rel="nofollow"><span>Full size table</span><svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16"><use xlink:href="#icon-eds-i-chevron-right-small" xmlns:xlink="http://www.w3.org/1999/xlink"></use></svg></a><<<<<<<<<<<>>>>>>>>>>>><a data-track="click" data-track-action="table anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#Tab5">5</a><<<<<<<<<<<>>>>>>>>>>>><a data-track="click" data-track-action="table anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#Tab6">6</a><<<<<<<<<<<>>>>>>>>>>>><a data-track="click" data-track-action="table anchor" data-track-label="link" href="/article/10.1007/s10844-024-00886-5#Tab7">7</a><<<<<<<<<<<>>>>>>>>>>>><a aria-label="Full size table 7" class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" href="/article/10.1007/s10844-024-00886-5/tables/7" rel="nofollow"><span>Full size table</span><svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16"><use xlink:href="#icon-eds-i-chevron-right-small" xmlns:xlink="http://www.w3.org/1999/xlink"></use></svg></a><<<<<<<<<<<>>>>>>>>>>>><a href="https://github.com/anonymous272218/HGMS">https://github.com/anonymous272218/HGMS</a><<<<<<<<<<<>>>>>>>>>>>><a href="http://www.dailymail.co.uk">http://www.dailymail.co.uk</a><<<<<<<<<<<>>>>>>>>>>>><a href="https://huggingface.co/FacebookAI">https://huggingface.co/FacebookAI</a><<<<<<<<<<<>>>>>>>>>>>><a href="https://huggingface.co/Salesforce">https://huggingface.co/Salesforce</a><<<<<<<<<<<>>>>>>>>>>>><a data-track="click_references" data-track-action="external reference" data-track-label="https://aclanthology.org/2022.coling-1.508" data-track-value="external reference" href="https://aclanthology.org/2022.coling-1.508">https://aclanthology.org/2022.coling-1.508</a><<<<<<<<<<<>>>>>>>>>>>><a data-track="click_references" data-track-action="external reference" data-track-label="10.18653/v1/D18-1438" data-track-value="external reference" href="https://doi.org/10.18653/v1/D18-1438">https://doi.org/10.18653/v1/D18-1438</a><<<<<<<<<<<>>>>>>>>>>>><a data-track="click_references" data-track-action="external reference" data-track-label="10.18653/v1/P16-1046" data-track-value="external reference" href="https://doi.org/10.18653/v1/P16-1046">https://doi.org/10.18653/v1/P16-1046</a><<<<<<<<<<<>>>>>>>>>>>><a data-track="click_references" data-track-action="external reference" data-track-label="10.18653/v1/2023.findings-acl.293" data-track-value="external reference" href="https://doi.org/10.18653/v1/2023.findings-acl.293">https://doi.org/10.18653/v1/2023.findings-acl.293</a><<<<<<<<<<<>>>>>>>>>>>><a data-track="click_references" data-track-action="external reference" data-track-label="10.18653/v1/2020.coling-main.468" data-track-value="external reference" href="https://doi.org/10.18653/v1/2020.coling-main.468">https://doi.org/10.18653/v1/2020.coling-main.468</a><<<<<<<<<<<>>>>>>>>>>>><a data-track="click_references" data-track-action="external reference" data-track-label="10.1613/JAIR.1523" data-track-value="external reference" href="https://doi.org/10.1613/JAIR.1523">https://doi.org/10.1613/JAIR.1523</a><<<<<<<<<<<>>>>>>>>>>>><a aria-label="Article reference 6" data-doi="10.1613/JAIR.1523" data-track="click_references" data-track-action="article reference" data-track-item_id="10.1613/JAIR.1523" data-track-label="10.1613/JAIR.1523" data-track-value="article reference" href="https://doi.org/10.1613%2FJAIR.1523" rel="nofollow noopener">Article</a><<<<<<<<<<<>>>>>>>>>>>><a aria-label="Google Scholar reference 6" data-track="click_references" data-track-action="google scholar reference" data-track-item_id="link" data-track-label="link" data-track-value="google scholar reference" href="http://scholar.google.com/scholar_lookup?&amp;title=Lexrank%3A%20Graph-based%20lexical%20centrality%20as%20salience%20in%20text%20summarization&amp;journal=J%20Artif%20Intell%20Res&amp;doi=10.1613%2FJAIR.1523&amp;volume=22&amp;pages=457-47&amp;publication_year=2004&amp;author=Erkan%2CG&amp;author=Radev%2CDR" rel="nofollow noopener">
                    Google Scholar</a><<<<<<<<<<<>>>>>>>>>>>><a data-track="click_references" data-track-action="external reference" data-track-label="10.18653/v1/2020.emnlp-main.295" data-track-value="external reference" href="https://doi.org/10.18653/v1/2020.emnlp-main.295">https://doi.org/10.18653/v1/2020.emnlp-main.295</a><<<<<<<<<<<>>>>>>>>>>>><a data-track="click_references" data-track-action="external reference" data-track-label="10.18653/v1/2023.findings-acl.12" data-track-value="external reference" href="https://doi.org/10.18653/v1/2023.findings-acl.12">https://doi.org/10.18653/v1/2023.findings-acl.12</a><<<<<<<<<<<>>>>>>>>>>>><a data-track="click_references" data-track-action="external reference" data-track-label="10.1007/S10844-023-00812-1" data-track-value="external reference" href="https://doi.org/10.1007/S10844-023-00812-1">https://doi.org/10.1007/S10844-023-00812-1</a><<<<<<<<<<<>>>>>>>>>>>><a aria-label="Article reference 9" data-doi="10.1007/S10844-023-00812-1" data-track="click_references" data-track-action="article reference" data-track-item_id="10.1007/S10844-023-00812-1" data-track-label="10.1007/S10844-023-00812-1" data-track-value="article reference" href="https://link.springer.com/doi/10.1007/S10844-023-00812-1" rel="noopener">Article</a><<<<<<<<<<<>>>>>>>>>>>><a aria-label="MathSciNet reference 9" data-track="click_references" data-track-action="mathscinet reference" data-track-item_id="link" data-track-label="link" data-track-value="mathscinet reference" href="http://www.ams.org/mathscinet-getitem?mr=4702630" rel="nofollow noopener">MathSciNet</a><<<<<<<<<<<>>>>>>>>>>>><a aria-label="Google Scholar reference 9" data-track="click_references" data-track-action="google scholar reference" data-track-item_id="link" data-track-label="link" data-track-value="google scholar reference" href="http://scholar.google.com/scholar_lookup?&amp;title=Self-supervised%20opinion%20summarization%20with%20multi-modal%20knowledge%20graph&amp;journal=J%20Intell%20Inf%20Syst&amp;doi=10.1007%2FS10844-023-00812-1&amp;volume=62&amp;issue=1&amp;pages=191-208&amp;publication_year=2024&amp;author=Jin%2CL&amp;author=Chen%2CJ" rel="nofollow noopener">
                    Google Scholar</a><<<<<<<<<<<>>>>>>>>>>>><a data-track="click_references" data-track-action="external reference" data-track-label="10.18653/v1/2021.emnlp-main.11" data-track-value="external reference" href="https://doi.org/10.18653/v1/2021.emnlp-main.11">https://doi.org/10.18653/v1/2021.emnlp-main.11</a><<<<<<<<<<<>>>>>>>>>>>><a data-track="click_references" data-track-action="external reference" data-track-label="http://arxiv.org/abs/1412.6980" data-track-value="external reference" href="http://arxiv.org/abs/1412.6980">http://arxiv.org/abs/1412.6980</a><<<<<<<<<<<>>>>>>>>>>>><a data-track="click_references" data-track-action="external reference" data-track-label="10.18653/v1/2020.acl-main.703" data-track-value="external reference" href="https://doi.org/10.18653/v1/2020.acl-main.703">https://doi.org/10.18653/v1/2020.acl-main.703</a><<<<<<<<<<<>>>>>>>>>>>><a data-track="click_references" data-track-action="external reference" data-track-label="10.24963/ijcai.2018/577" data-track-value="external reference" href="https://doi.org/10.24963/ijcai.2018/577">https://doi.org/10.24963/ijcai.2018/577</a><<<<<<<<<<<>>>>>>>>>>>><a data-track="click_references" data-track-action="external reference" data-track-label="10.18653/v1/2020.emnlp-main.752" data-track-value="external reference" href="https://doi.org/10.18653/v1/2020.emnlp-main.752">https://doi.org/10.18653/v1/2020.emnlp-main.752</a><<<<<<<<<<<>>>>>>>>>>>><a data-track="click_references" data-track-action="external reference" data-track-label="10.18653/v1/2023.acl-long.165" data-track-value="external reference" href="https://doi.org/10.18653/v1/2023.acl-long.165">https://doi.org/10.18653/v1/2023.acl-long.165</a><<<<<<<<<<<>>>>>>>>>>>><a data-track="click_references" data-track-action="external reference" data-track-label="10.18653/v1/D19-1387" data-track-value="external reference" href="https://doi.org/10.18653/v1/D19-1387">https://doi.org/10.18653/v1/D19-1387</a><<<<<<<<<<<>>>>>>>>>>>><a data-track="click_references" data-track-action="external reference" data-track-label="http://arxiv.org/abs/1907.11692" data-track-value="external reference" href="http://arxiv.org/abs/1907.11692">arXiv:1907.11692</a><<<<<<<<<<<>>>>>>>>>>>><a data-track="click_references" data-track-action="external reference" data-track-label="https://arxiv.org/abs/1907.11692" data-track-value="external reference" href="https://arxiv.org/abs/1907.11692">https://arxiv.org/abs/1907.11692</a><<<<<<<<<<<>>>>>>>>>>>><a data-track="click_references" data-track-action="external reference" data-track-label="10.18653/v1/2022.acl-long.207" data-track-value="external reference" href="https://doi.org/10.18653/v1/2022.acl-long.207">https://doi.org/10.18653/v1/2022.acl-long.207</a><<<<<<<<<<<>>>>>>>>>>>><a data-track="click_references" data-track-action="external reference" data-track-label="http://jmlr.org/papers/v9/vandermaaten08a.html" data-track-value="external reference" href="http://jmlr.org/papers/v9/vandermaaten08a.html">http://jmlr.org/papers/v9/vandermaaten08a.html</a><<<<<<<<<<<>>>>>>>>>>>><a data-track="click_references" data-track-action="external reference" data-track-label="https://aclanthology.org/W04-3252" data-track-value="external reference" href="https://aclanthology.org/W04-3252">https://aclanthology.org/W04-3252</a><<<<<<<<<<<>>>>>>>>>>>><a data-track="click_references" data-track-action="external reference" data-track-label="10.1609/aaai.v31i1.10958" data-track-value="external reference" href="https://doi.org/10.1609/aaai.v31i1.10958">https://doi.org/10.1609/aaai.v31i1.10958</a><<<<<<<<<<<>>>>>>>>>>>><a data-track="click_references" data-track-action="external reference" data-track-label="10.18653/v1/N18-1158" data-track-value="external reference" href="https://doi.org/10.18653/v1/N18-1158">https://doi.org/10.18653/v1/N18-1158</a><<<<<<<<<<<>>>>>>>>>>>><a data-track="click_references" data-track-action="external reference" data-track-label="10.18653/v1/2023.emnlp-main.251" data-track-value="external reference" href="https://doi.org/10.18653/v1/2023.emnlp-main.251">https://doi.org/10.18653/v1/2023.emnlp-main.251</a><<<<<<<<<<<>>>>>>>>>>>><a data-track="click_references" data-track-action="external reference" data-track-label="https://openreview.net/forum?id=HkAClQgA-" data-track-value="external reference" href="https://openreview.net/forum?id=HkAClQgA-">https://openreview.net/forum?id=HkAClQgA-</a><<<<<<<<<<<>>>>>>>>>>>><a data-track="click_references" data-track-action="external reference" data-track-label="10.3115/v1/D14-1162" data-track-value="external reference" href="https://doi.org/10.3115/v1/D14-1162">https://doi.org/10.3115/v1/D14-1162</a><<<<<<<<<<<>>>>>>>>>>>><a data-track="click_references" data-track-action="external reference" data-track-label="https://aclanthology.org/2022.coling-1.545" data-track-value="external reference" href="https://aclanthology.org/2022.coling-1.545">https://aclanthology.org/2022.coling-1.545</a><<<<<<<<<<<>>>>>>>>>>>><a data-track="click_references" data-track-action="external reference" data-track-label="http://proceedings.mlr.press/v139/radford21a.html" data-track-value="external reference" href="http://proceedings.mlr.press/v139/radford21a.html">http://proceedings.mlr.press/v139/radford21a.html</a><<<<<<<<<<<>>>>>>>>>>>><a data-track="click_references" data-track-action="external reference" data-track-label="10.18653/v1/P17-1099" data-track-value="external reference" href="https://doi.org/10.18653/v1/P17-1099">https://doi.org/10.18653/v1/P17-1099</a><<<<<<<<<<<>>>>>>>>>>>><a data-track="click_references" data-track-action="external reference" data-track-label="10.1609/aaai.v36i10.21385" data-track-value="external reference" href="https://doi.org/10.1609/aaai.v36i10.21385">https://doi.org/10.1609/aaai.v36i10.21385</a><<<<<<<<<<<>>>>>>>>>>>><a aria-label="Article reference 30" data-doi="10.1609/aaai.v36i10.21385" data-track="click_references" data-track-action="article reference" data-track-item_id="10.1609/aaai.v36i10.21385" data-track-label="10.1609/aaai.v36i10.21385" data-track-value="article reference" href="https://doi.org/10.1609%2Faaai.v36i10.21385" rel="nofollow noopener">Article</a><<<<<<<<<<<>>>>>>>>>>>><a aria-label="Google Scholar reference 30" data-track="click_references" data-track-action="google scholar reference" data-track-item_id="link" data-track-label="link" data-track-value="google scholar reference" href="http://scholar.google.com/scholar_lookup?&amp;title=Hierarchical%20heterogeneous%20graph%20attention%20network%20for%20syntax-aware%20summarization&amp;journal=Proceedings%20of%20the%20AAAI%20Conference%20on%20Artificial%20Intelligence&amp;doi=10.1609%2Faaai.v36i10.21385&amp;volume=36&amp;issue=10&amp;pages=11340-1134&amp;publication_year=2022&amp;author=Song%2CZ&amp;author=King%2CI" rel="nofollow noopener">
                    Google Scholar</a><<<<<<<<<<<>>>>>>>>>>>><a data-track="click_references" data-track-action="external reference" data-track-label="https://proceedings.neurips.cc/paper/2014/hash/a14ac55a4f27472c5d894ec1c3c743d2-Abstract.html" data-track-value="external reference" href="https://proceedings.neurips.cc/paper/2014/hash/a14ac55a4f27472c5d894ec1c3c743d2-Abstract.html">https://proceedings.neurips.cc/paper/2014/hash/a14ac55a4f27472c5d894ec1c3c743d2-Abstract.html</a><<<<<<<<<<<>>>>>>>>>>>><a data-track="click_references" data-track-action="external reference" data-track-label="10.18653/v1/2022.findings-naacl.85" data-track-value="external reference" href="https://doi.org/10.18653/v1/2022.findings-naacl.85">https://doi.org/10.18653/v1/2022.findings-naacl.85</a><<<<<<<<<<<>>>>>>>>>>>><a data-track="click_references" data-track-action="external reference" data-track-label="https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html" data-track-value="external reference" href="https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html">https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html</a><<<<<<<<<<<>>>>>>>>>>>><a aria-label="Google Scholar reference 34" data-track="click_references" data-track-action="google scholar reference" data-track-item_id="link" data-track-label="link" data-track-value="google scholar reference" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=Graph%20attention%20networks.%20stat&amp;volume=1050&amp;issue=20&amp;pages=10-48550&amp;publication_year=2017&amp;author=Velickovic%2CP&amp;author=Cucurull%2CG&amp;author=Casanova%2CA" rel="nofollow noopener">
                    Google Scholar</a><<<<<<<<<<<>>>>>>>>>>>><a data-track="click_references" data-track-action="external reference" data-track-label="10.18653/v1/2020.acl-main.553" data-track-value="external reference" href="https://doi.org/10.18653/v1/2020.acl-main.553">https://doi.org/10.18653/v1/2020.acl-main.553</a><<<<<<<<<<<>>>>>>>>>>>><a data-track="click_references" data-track-action="external reference" data-track-label="10.18653/v1/2021.acl-long.472" data-track-value="external reference" href="https://doi.org/10.18653/v1/2021.acl-long.472">https://doi.org/10.18653/v1/2021.acl-long.472</a><<<<<<<<<<<>>>>>>>>>>>><a data-track="click_references" data-track-action="external reference" data-track-label="10.18653/v1/2023.acl-long.476" data-track-value="external reference" href="https://doi.org/10.18653/v1/2023.acl-long.476">https://doi.org/10.18653/v1/2023.acl-long.476</a><<<<<<<<<<<>>>>>>>>>>>><a data-track="click_references" data-track-action="external reference" data-track-label="10.1007/S10844-022-00757-X" data-track-value="external reference" href="https://doi.org/10.1007/S10844-022-00757-X">https://doi.org/10.1007/S10844-022-00757-X</a><<<<<<<<<<<>>>>>>>>>>>><a aria-label="Article reference 38" data-doi="10.1007/S10844-022-00757-X" data-track="click_references" data-track-action="article reference" data-track-item_id="10.1007/S10844-022-00757-X" data-track-label="10.1007/S10844-022-00757-X" data-track-value="article reference" href="https://link.springer.com/doi/10.1007/S10844-022-00757-X" rel="noopener">Article</a><<<<<<<<<<<>>>>>>>>>>>><a aria-label="Google Scholar reference 38" data-track="click_references" data-track-action="google scholar reference" data-track-item_id="link" data-track-label="link" data-track-value="google scholar reference" href="http://scholar.google.com/scholar_lookup?&amp;title=Extractive%20text-image%20summarization%20with%20relation-enhanced%20graph%20attention%20network&amp;journal=J%20Intell%20Inf%20Syst&amp;doi=10.1007%2FS10844-022-00757-X&amp;volume=61&amp;issue=2&amp;pages=325-341&amp;publication_year=2023&amp;author=Xie%2CF&amp;author=Chen%2CJ&amp;author=Chen%2CK" rel="nofollow noopener">
                    Google Scholar</a><<<<<<<<<<<>>>>>>>>>>>><a data-track="click_references" data-track-action="external reference" data-track-label="https://proceedings.neurips.cc/paper/2021/hash/f1c1592588411002af340cbaedd6fc33-Abstract.html" data-track-value="external reference" href="https://proceedings.neurips.cc/paper/2021/hash/f1c1592588411002af340cbaedd6fc33-Abstract.html">https://proceedings.neurips.cc/paper/2021/hash/f1c1592588411002af340cbaedd6fc33-Abstract.html</a><<<<<<<<<<<>>>>>>>>>>>><a data-track="click_references" data-track-action="external reference" data-track-label="http://proceedings.mlr.press/v119/zhang20ae.html" data-track-value="external reference" href="http://proceedings.mlr.press/v119/zhang20ae.html">http://proceedings.mlr.press/v119/zhang20ae.html</a><<<<<<<<<<<>>>>>>>>>>>><a data-track="click_references" data-track-action="external reference" data-track-label="10.1609/aaai.v36i10.21422" data-track-value="external reference" href="https://doi.org/10.1609/aaai.v36i10.21422">https://doi.org/10.1609/aaai.v36i10.21422</a><<<<<<<<<<<>>>>>>>>>>>><a data-track="click_references" data-track-action="external reference" data-track-label="10.1007/S10844-022-00695-8" data-track-value="external reference" href="https://doi.org/10.1007/S10844-022-00695-8">https://doi.org/10.1007/S10844-022-00695-8</a><<<<<<<<<<<>>>>>>>>>>>><a data-track="click_references" data-track-action="external reference" data-track-label="10.1609/aaai.v36i10.21431" data-track-value="external reference" href="https://doi.org/10.1609/aaai.v36i10.21431">https://doi.org/10.1609/aaai.v36i10.21431</a><<<<<<<<<<<>>>>>>>>>>>><a data-track="click_references" data-track-action="external reference" data-track-label="10.18653/v1/2020.acl-main.552" data-track-value="external reference" href="https://doi.org/10.18653/v1/2020.acl-main.552">https://doi.org/10.18653/v1/2020.acl-main.552</a><<<<<<<<<<<>>>>>>>>>>>><a data-track="click_references" data-track-action="external reference" data-track-label="10.18653/v1/D18-1448" data-track-value="external reference" href="https://doi.org/10.18653/v1/D18-1448">https://doi.org/10.18653/v1/D18-1448</a><<<<<<<<<<<>>>>>>>>>>>><a data-track="click_references" data-track-action="external reference" data-track-label="10.1609/aaai.v34i05.6525" data-track-value="external reference" href="https://doi.org/10.1609/aaai.v34i05.6525">https://doi.org/10.1609/aaai.v34i05.6525</a><<<<<<<<<<<>>>>>>>>>>>><a aria-label="Article reference 46" data-doi="10.1609/aaai.v34i05.6525" data-track="click_references" data-track-action="article reference" data-track-item_id="10.1609/aaai.v34i05.6525" data-track-label="10.1609/aaai.v34i05.6525" data-track-value="article reference" href="https://doi.org/10.1609%2Faaai.v34i05.6525" rel="nofollow noopener">Article</a><<<<<<<<<<<>>>>>>>>>>>><a aria-label="Google Scholar reference 46" data-track="click_references" data-track-action="google scholar reference" data-track-item_id="link" data-track-label="link" data-track-value="google scholar reference" href="http://scholar.google.com/scholar_lookup?&amp;title=Multimodal%20summarization%20with%20guidance%20of%20multimodal%20reference&amp;journal=Proceedings%20of%20the%20AAAI%20Conference%20on%20Artificial%20Intelligence&amp;doi=10.1609%2Faaai.v34i05.6525&amp;volume=34&amp;issue=05&amp;pages=9749-975&amp;publication_year=2020&amp;author=Zhu%2CJ&amp;author=Zhou%2CY&amp;author=Zhang%2CJ" rel="nofollow noopener">
                    Google Scholar</a><<<<<<<<<<<>>>>>>>>>>>><a data-track="click" data-track-action="download citation references" data-track-label="link" href="https://citation-needed.springer.com/v2/references/10.1007/s10844-024-00886-5?format=refman&amp;flavour=references" rel="nofollow">Download references<svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16"><use xlink:href="#icon-eds-i-download-medium" xmlns:xlink="http://www.w3.org/1999/xlink"></use></svg></a><<<<<<<<<<<>>>>>>>>>>>><a class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" href="/search?dc.creator=Xiankai%20Jiang" rel="nofollow">View author publications</a><<<<<<<<<<<>>>>>>>>>>>><a class="c-article-identifiers__item" data-track="click" data-track-action="author link - pubmed" data-track-label="link" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Xiankai%20Jiang" rel="nofollow">PubMed</a><<<<<<<<<<<>>>>>>>>>>>><a class="c-article-identifiers__item" data-track="click" data-track-action="author link - scholar" data-track-label="link" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Xiankai%20Jiang%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" rel="nofollow">Google Scholar</a><<<<<<<<<<<>>>>>>>>>>>><a class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" href="/search?dc.creator=Jingqiang%20Chen" rel="nofollow">View author publications</a><<<<<<<<<<<>>>>>>>>>>>><a class="c-article-identifiers__item" data-track="click" data-track-action="author link - pubmed" data-track-label="link" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Jingqiang%20Chen" rel="nofollow">PubMed</a><<<<<<<<<<<>>>>>>>>>>>><a class="c-article-identifiers__item" data-track="click" data-track-action="author link - scholar" data-track-label="link" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Jingqiang%20Chen%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" rel="nofollow">Google Scholar</a><<<<<<<<<<<>>>>>>>>>>>><a href="mailto:cjq@njupt.edu.cn" id="corresp-c1">Jingqiang Chen</a><<<<<<<<<<<>>>>>>>>>>>><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Heterogeneous%20graphormer%20for%20extractive%20multimodal%20summarization&amp;author=Xiankai%20Jiang%20et%20al&amp;contentID=10.1007%2Fs10844-024-00886-5&amp;copyright=The%20Author%28s%29%2C%20under%20exclusive%20licence%20to%20Springer%20Science%2BBusiness%20Media%2C%20LLC%2C%20part%20of%20Springer%20Nature&amp;publication=0925-9902&amp;publicationDate=2024-09-30&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and permissions</a><<<<<<<<<<<>>>>>>>>>>>><a data-crossmark="10.1007/s10844-024-00886-5" data-test="crossmark" data-track="click" data-track-action="Click Crossmark" data-track-label="link" href="https://crossmark.crossref.org/dialog/?doi=10.1007/s10844-024-00886-5" rel="noopener" target="_blank"><img alt="Check for updates. Verify currency and authenticity via CrossMark" height="81" loading="lazy" src="data:image/svg+xml;base64,<svg height="81" width="57" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m17.35 35.45 21.3-14.2v-17.03h-21.3" fill="#989898"/><path d="m38.65 35.45-21.3-14.2v-17.03h21.3" fill="#747474"/><path d="m28 .5c-12.98 0-23.5 10.52-23.5 23.5s10.52 23.5 23.5 23.5 23.5-10.52 23.5-23.5c0-6.23-2.48-12.21-6.88-16.62-4.41-4.4-10.39-6.88-16.62-6.88zm0 41.25c-9.8 0-17.75-7.95-17.75-17.75s7.95-17.75 17.75-17.75 17.75 7.95 17.75 17.75c0 4.71-1.87 9.22-5.2 12.55s-7.84 5.2-12.55 5.2z" fill="#535353"/><path d="m41 36c-5.81 6.23-15.23 7.45-22.43 2.9-7.21-4.55-10.16-13.57-7.03-21.5l-4.92-3.11c-4.95 10.7-1.19 23.42 8.78 29.71 9.97 6.3 23.07 4.22 30.6-4.86z" fill="#9c9c9c"/><path d="m.2 58.45c0-.75.11-1.42.33-2.01s.52-1.09.91-1.5c.38-.41.83-.73 1.34-.94.51-.22 1.06-.32 1.65-.32.56 0 1.06.11 1.51.35.44.23.81.5 1.1.81l-.91 1.01c-.24-.24-.49-.42-.75-.56-.27-.13-.58-.2-.93-.2-.39 0-.73.08-1.05.23-.31.16-.58.37-.81.66-.23.28-.41.63-.53 1.04-.13.41-.19.88-.19 1.39 0 1.04.23 1.86.68 2.46.45.59 1.06.88 1.84.88.41 0 .77-.07 1.07-.23s.59-.39.85-.68l.91 1c-.38.43-.8.76-1.28.99-.47.22-1 .34-1.58.34-.59 0-1.13-.1-1.64-.31-.5-.2-.94-.51-1.31-.91-.38-.4-.67-.9-.88-1.48-.22-.59-.33-1.26-.33-2.02zm8.4-5.33h1.61v2.54l-.05 1.33c.29-.27.61-.51.96-.72s.76-.31 1.24-.31c.73 0 1.27.23 1.61.71.33.47.5 1.14.5 2.02v4.31h-1.61v-4.1c0-.57-.08-.97-.25-1.21-.17-.23-.45-.35-.83-.35-.3 0-.56.08-.79.22-.23.15-.49.36-.78.64v4.8h-1.61zm7.37 6.45c0-.56.09-1.06.26-1.51.18-.45.42-.83.71-1.14.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.36c.07.62.29 1.1.65 1.44.36.33.82.5 1.38.5.29 0 .57-.04.83-.13s.51-.21.76-.37l.55 1.01c-.33.21-.69.39-1.09.53-.41.14-.83.21-1.26.21-.48 0-.92-.08-1.34-.25-.41-.16-.76-.4-1.07-.7-.31-.31-.55-.69-.72-1.13-.18-.44-.26-.95-.26-1.52zm4.6-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.07.45-.31.29-.5.73-.58 1.3zm2.5.62c0-.57.09-1.08.28-1.53.18-.44.43-.82.75-1.13s.69-.54 1.1-.71c.42-.16.85-.24 1.31-.24.45 0 .84.08 1.17.23s.61.34.85.57l-.77 1.02c-.19-.16-.38-.28-.56-.37-.19-.09-.39-.14-.61-.14-.56 0-1.01.21-1.35.63-.35.41-.52.97-.52 1.67 0 .69.17 1.24.51 1.66.34.41.78.62 1.32.62.28 0 .54-.06.78-.17.24-.12.45-.26.64-.42l.67 1.03c-.33.29-.69.51-1.08.65-.39.15-.78.23-1.18.23-.46 0-.9-.08-1.31-.24-.4-.16-.75-.39-1.05-.7s-.53-.69-.7-1.13c-.17-.45-.25-.96-.25-1.53zm6.91-6.45h1.58v6.17h.05l2.54-3.16h1.77l-2.35 2.8 2.59 4.07h-1.75l-1.77-2.98-1.08 1.23v1.75h-1.58zm13.69 1.27c-.25-.11-.5-.17-.75-.17-.58 0-.87.39-.87 1.16v.75h1.34v1.27h-1.34v5.6h-1.61v-5.6h-.92v-1.2l.92-.07v-.72c0-.35.04-.68.13-.98.08-.31.21-.57.4-.79s.42-.39.71-.51c.28-.12.63-.18 1.04-.18.24 0 .48.02.69.07.22.05.41.1.57.17zm.48 5.18c0-.57.09-1.08.27-1.53.17-.44.41-.82.72-1.13.3-.31.65-.54 1.04-.71.39-.16.8-.24 1.23-.24s.84.08 1.24.24c.4.17.74.4 1.04.71s.54.69.72 1.13c.19.45.28.96.28 1.53s-.09 1.08-.28 1.53c-.18.44-.42.82-.72 1.13s-.64.54-1.04.7-.81.24-1.24.24-.84-.08-1.23-.24-.74-.39-1.04-.7c-.31-.31-.55-.69-.72-1.13-.18-.45-.27-.96-.27-1.53zm1.65 0c0 .69.14 1.24.43 1.66.28.41.68.62 1.18.62.51 0 .9-.21 1.19-.62.29-.42.44-.97.44-1.66 0-.7-.15-1.26-.44-1.67-.29-.42-.68-.63-1.19-.63-.5 0-.9.21-1.18.63-.29.41-.43.97-.43 1.67zm6.48-3.44h1.33l.12 1.21h.05c.24-.44.54-.79.88-1.02.35-.24.7-.36 1.07-.36.32 0 .59.05.78.14l-.28 1.4-.33-.09c-.11-.01-.23-.02-.38-.02-.27 0-.56.1-.86.31s-.55.58-.77 1.1v4.2h-1.61zm-47.87 15h1.61v4.1c0 .57.08.97.25 1.2.17.24.44.35.81.35.3 0 .57-.07.8-.22.22-.15.47-.39.73-.73v-4.7h1.61v6.87h-1.32l-.12-1.01h-.04c-.3.36-.63.64-.98.86-.35.21-.76.32-1.24.32-.73 0-1.27-.24-1.61-.71-.33-.47-.5-1.14-.5-2.02zm9.46 7.43v2.16h-1.61v-9.59h1.33l.12.72h.05c.29-.24.61-.45.97-.63.35-.17.72-.26 1.1-.26.43 0 .81.08 1.15.24.33.17.61.4.84.71.24.31.41.68.53 1.11.13.42.19.91.19 1.44 0 .59-.09 1.11-.25 1.57-.16.47-.38.85-.65 1.16-.27.32-.58.56-.94.73-.35.16-.72.25-1.1.25-.3 0-.6-.07-.9-.2s-.59-.31-.87-.56zm0-2.3c.26.22.5.37.73.45.24.09.46.13.66.13.46 0 .84-.2 1.15-.6.31-.39.46-.98.46-1.77 0-.69-.12-1.22-.35-1.61-.23-.38-.61-.57-1.13-.57-.49 0-.99.26-1.52.77zm5.87-1.69c0-.56.08-1.06.25-1.51.16-.45.37-.83.65-1.14.27-.3.58-.54.93-.71s.71-.25 1.08-.25c.39 0 .73.07 1 .2.27.14.54.32.81.55l-.06-1.1v-2.49h1.61v9.88h-1.33l-.11-.74h-.06c-.25.25-.54.46-.88.64-.33.18-.69.27-1.06.27-.87 0-1.56-.32-2.07-.95s-.76-1.51-.76-2.65zm1.67-.01c0 .74.13 1.31.4 1.7.26.38.65.58 1.15.58.51 0 .99-.26 1.44-.77v-3.21c-.24-.21-.48-.36-.7-.45-.23-.08-.46-.12-.7-.12-.45 0-.82.19-1.13.59-.31.39-.46.95-.46 1.68zm6.35 1.59c0-.73.32-1.3.97-1.71.64-.4 1.67-.68 3.08-.84 0-.17-.02-.34-.07-.51-.05-.16-.12-.3-.22-.43s-.22-.22-.38-.3c-.15-.06-.34-.1-.58-.1-.34 0-.68.07-1 .2s-.63.29-.93.47l-.59-1.08c.39-.24.81-.45 1.28-.63.47-.17.99-.26 1.54-.26.86 0 1.51.25 1.93.76s.63 1.25.63 2.21v4.07h-1.32l-.12-.76h-.05c-.3.27-.63.48-.98.66s-.73.27-1.14.27c-.61 0-1.1-.19-1.48-.56-.38-.36-.57-.85-.57-1.46zm1.57-.12c0 .3.09.53.27.67.19.14.42.21.71.21.28 0 .54-.07.77-.2s.48-.31.73-.56v-1.54c-.47.06-.86.13-1.18.23-.31.09-.57.19-.76.31s-.33.25-.41.4c-.09.15-.13.31-.13.48zm6.29-3.63h-.98v-1.2l1.06-.07.2-1.88h1.34v1.88h1.75v1.27h-1.75v3.28c0 .8.32 1.2.97 1.2.12 0 .24-.01.37-.04.12-.03.24-.07.34-.11l.28 1.19c-.19.06-.4.12-.64.17-.23.05-.49.08-.76.08-.4 0-.74-.06-1.02-.18-.27-.13-.49-.3-.67-.52-.17-.21-.3-.48-.37-.78-.08-.3-.12-.64-.12-1.01zm4.36 2.17c0-.56.09-1.06.27-1.51s.41-.83.71-1.14c.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.37c.08.62.29 1.1.65 1.44.36.33.82.5 1.38.5.3 0 .58-.04.84-.13.25-.09.51-.21.76-.37l.54 1.01c-.32.21-.69.39-1.09.53s-.82.21-1.26.21c-.47 0-.92-.08-1.33-.25-.41-.16-.77-.4-1.08-.7-.3-.31-.54-.69-.72-1.13-.17-.44-.26-.95-.26-1.52zm4.61-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.08.45-.31.29-.5.73-.57 1.3zm3.01 2.23c.31.24.61.43.92.57.3.13.63.2.98.2.38 0 .65-.08.83-.23s.27-.35.27-.6c0-.14-.05-.26-.13-.37-.08-.1-.2-.2-.34-.28-.14-.09-.29-.16-.47-.23l-.53-.22c-.23-.09-.46-.18-.69-.3-.23-.11-.44-.24-.62-.4s-.33-.35-.45-.55c-.12-.21-.18-.46-.18-.75 0-.61.23-1.1.68-1.49.44-.38 1.06-.57 1.83-.57.48 0 .91.08 1.29.25s.71.36.99.57l-.74.98c-.24-.17-.49-.32-.73-.42-.25-.11-.51-.16-.78-.16-.35 0-.6.07-.76.21-.17.15-.25.33-.25.54 0 .14.04.26.12.36s.18.18.31.26c.14.07.29.14.46.21l.54.19c.23.09.47.18.7.29s.44.24.64.4c.19.16.34.35.46.58.11.23.17.5.17.82 0 .3-.06.58-.17.83-.12.26-.29.48-.51.68-.23.19-.51.34-.84.45-.34.11-.72.17-1.15.17-.48 0-.95-.09-1.41-.27-.46-.19-.86-.41-1.2-.68z" fill="#535353"/></g></svg>" width="57"/></a><<<<<<<<<<<>>>>>>>>>>>><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-external="" data-track-label="link" href="https://citation-needed.springer.com/v2/references/10.1007/s10844-024-00886-5?format=refman&amp;flavour=citation" rel="nofollow">Download citation<svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16"><use xlink:href="#icon-eds-i-download-medium" xmlns:xlink="http://www.w3.org/1999/xlink"></use></svg></a><<<<<<<<<<<>>>>>>>>>>>><a data-track="click" data-track-action="view keyword" data-track-label="link" href='/search?query=Summarization&amp;facet-discipline="Computer%20Science"'>Summarization</a><<<<<<<<<<<>>>>>>>>>>>><a data-track="click" data-track-action="view keyword" data-track-label="link" href='/search?query=Extractive%20summarization&amp;facet-discipline="Computer%20Science"'>Extractive summarization</a><<<<<<<<<<<>>>>>>>>>>>><a data-track="click" data-track-action="view keyword" data-track-label="link" href='/search?query=Multi-modal%20summarization&amp;facet-discipline="Computer%20Science"'>Multi-modal summarization</a><<<<<<<<<<<>>>>>>>>>>>><a data-track="click" data-track-action="view keyword" data-track-label="link" href='/search?query=Graph%20neural%20networks&amp;facet-discipline="Computer%20Science"'>Graph neural networks</a><<<<<<<<<<<>>>>>>>>>>>><a class="app-card-service__link" data-test="article-checklist-banner-link" data-track="click_presubmission_checklist" data-track-action="clicked article page checklist banner test 2 old version" data-track-category="pre-submission-checklist" data-track-context="article page top of reading companion" data-track-label="link" href="https://beta.springernature.com/pre-submission?journalId=10844">
<span class="app-card-service__link-text">Use our pre-submission checklist</span>
<svg aria-hidden="true" class="app-card-service__link-icon" focusable="false"><use xlink:href="#icon-eds-i-arrow-right-small"></use></svg>
</a><<<<<<<<<<<>>>>>>>>>>>><a class="eds-c-footer__link" data-track="nav_journals_a_z" data-track-action="journals a-z" data-track-context="unified footer" data-track-label="link" href="https://link.springer.com/journals/a/1">Journals A-Z</a><<<<<<<<<<<>>>>>>>>>>>><a class="eds-c-footer__link" data-track="nav_books_a_z" data-track-action="books a-z" data-track-context="unified footer" data-track-label="link" href="https://link.springer.com/books/a/1">Books A-Z</a><<<<<<<<<<<>>>>>>>>>>>><a class="eds-c-footer__link" data-track="nav_journal_finder" data-track-action="journal finder" data-track-context="unified footer" data-track-label="link" href="https://link.springer.com/journals">Journal finder</a><<<<<<<<<<<>>>>>>>>>>>><a class="eds-c-footer__link" data-track="nav_publish_your_research" data-track-action="publish your research" data-track-context="unified footer" data-track-label="link" href="https://www.springernature.com/gp/authors">Publish your research</a><<<<<<<<<<<>>>>>>>>>>>><a class="eds-c-footer__link" data-track="nav_open_access_publishing" data-track-action="open access publishing" data-track-context="unified footer" data-track-label="link" href="https://www.springernature.com/gp/open-research/about/the-fundamentals-of-open-access-and-open-research">Open access publishing</a><<<<<<<<<<<>>>>>>>>>>>><a class="eds-c-footer__link" data-track="nav_our_products" data-track-action="our products" data-track-context="unified footer" data-track-label="link" href="https://www.springernature.com/gp/products">Our products</a><<<<<<<<<<<>>>>>>>>>>>><a class="eds-c-footer__link" data-track="nav_librarians" data-track-action="librarians" data-track-context="unified footer" data-track-label="link" href="https://www.springernature.com/gp/librarians">Librarians</a><<<<<<<<<<<>>>>>>>>>>>><a class="eds-c-footer__link" data-track="nav_societies" data-track-action="societies" data-track-context="unified footer" data-track-label="link" href="https://www.springernature.com/gp/societies">Societies</a><<<<<<<<<<<>>>>>>>>>>>><a class="eds-c-footer__link" data-track="nav_partners_and_advertisers" data-track-action="partners and advertisers" data-track-context="unified footer" data-track-label="link" href="https://www.springernature.com/gp/partners">Partners and advertisers</a><<<<<<<<<<<>>>>>>>>>>>><a class="eds-c-footer__link" data-track="nav_imprint_Springer" data-track-action="Springer" data-track-context="unified footer" data-track-label="link" href="https://www.springer.com/">Springer</a><<<<<<<<<<<>>>>>>>>>>>><a class="eds-c-footer__link" data-track="nav_imprint_Nature_Portfolio" data-track-action="Nature Portfolio" data-track-context="unified footer" data-track-label="link" href="https://www.nature.com/">Nature Portfolio</a><<<<<<<<<<<>>>>>>>>>>>><a class="eds-c-footer__link" data-track="nav_imprint_BMC" data-track-action="BMC" data-track-context="unified footer" data-track-label="link" href="https://www.biomedcentral.com/">BMC</a><<<<<<<<<<<>>>>>>>>>>>><a class="eds-c-footer__link" data-track="nav_imprint_Palgrave_Macmillan" data-track-action="Palgrave Macmillan" data-track-context="unified footer" data-track-label="link" href="https://www.palgrave.com/">Palgrave Macmillan</a><<<<<<<<<<<>>>>>>>>>>>><a class="eds-c-footer__link" data-track="nav_imprint_Apress" data-track-action="Apress" data-track-context="unified footer" data-track-label="link" href="https://www.apress.com/">Apress</a><<<<<<<<<<<>>>>>>>>>>>><a class="eds-c-footer__link" data-track="nav_california_privacy_statement" data-track-action="california privacy statement" data-track-context="unified footer" data-track-label="link" href="https://www.springernature.com/gp/legal/ccpa">Your US state privacy rights</a><<<<<<<<<<<>>>>>>>>>>>><a class="eds-c-footer__link" data-track="nav_accessibility_statement" data-track-action="accessibility statement" data-track-context="unified footer" data-track-label="link" href="https://www.springernature.com/gp/info/accessibility">Accessibility statement</a><<<<<<<<<<<>>>>>>>>>>>><a class="eds-c-footer__link" data-track="nav_terms_and_conditions" data-track-action="terms and conditions" data-track-context="unified footer" data-track-label="link" href="https://link.springer.com/termsandconditions">Terms and conditions</a><<<<<<<<<<<>>>>>>>>>>>><a class="eds-c-footer__link" data-track="nav_privacy_policy" data-track-action="privacy policy" data-track-context="unified footer" data-track-label="link" href="https://link.springer.com/privacystatement">Privacy policy</a><<<<<<<<<<<>>>>>>>>>>>><a class="eds-c-footer__link" data-track="nav_help_and_support" data-track-action="help and support" data-track-context="unified footer" data-track-label="link" href="https://support.springernature.com/en/support/home">Help and support</a><<<<<<<<<<<>>>>>>>>>>>><a class="eds-c-footer__link" data-track-action="cancel contracts here" href="https://support.springernature.com/en/support/solutions/articles/6000255911-subscription-cancellations">Cancel contracts here</a><<<<<<<<<<<>>>>>>>>>>>><a class="eds-c-footer__link" href="https://www.springernature.com/">
<img alt="Springer Nature" height="20" loading="lazy" src="/oscar-static/images/logo-springernature-white-19dd4ba190.svg" width="200"/>
</a><<<<<<<<<<<>>>>>>>>>>>>